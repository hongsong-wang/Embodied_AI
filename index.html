<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Embodied AI</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Embodied AI</div><br>
<div id='section'>PaperID: <span id='pid'>1, <a href='https://arxiv.org/pdf/2602.14135.pdf' target='_blank'>https://arxiv.org/pdf/2602.14135.pdf</a></span>   <span><a href='https://github.com/Beijing-AISI/ForesightSafety-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haibo Tong, Feifei Zhao, Linghao Feng, Ruoyu Wu, Ruolin Chen, Lu Jia, Zhou Zhao, Jindong Li, Tenglong Li, Erliang Lin, Shuai Yang, Enmeng Lu, Yinqian Sun, Qian Zhang, Zizhe Ruan, Jinyu Fan, Zeyang Yue, Ping Wu, Huangrui Li, Chengyi Sun, Yi Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.14135">ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.
<div id='section'>PaperID: <span id='pid'>2, <a href='https://arxiv.org/pdf/2602.11598.pdf' target='_blank'>https://arxiv.org/pdf/2602.11598.pdf</a></span>   <span><a href='https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zedong Chu, Shichao Xie, Xiaolong Wu, Yanfen Shen, Minghua Luo, Zhengbo Wang, Fei Liu, Xiaoxu Leng, Junjun Hu, Mingyang Yin, Jia Lu, Yingnan Guo, Kai Yang, Jiawei Han, Xu Chen, Yanqing Zhu, Yuxiang Zhao, Xin Liu, Yirong Yang, Ye He, Jiahang Wang, Yang Cai, Tianlin Zhang, Li Gao, Liu Liu, Mingchao Sun, Fan Jiang, Chiyu Wang, Zhicheng Liu, Hongyu Pan, Honglin Han, Zhining Gu, Kuan Yang, Jianfang Zhang, Di Jing, Zihao Guan, Wei Guo, Guoqing Liu, Di Yang, Xiangpo Yang, Menglin Yang, Hongguang Xing, Weiguo Li, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.11598">ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation. To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.
<div id='section'>PaperID: <span id='pid'>3, <a href='https://arxiv.org/pdf/2602.11575.pdf' target='_blank'>https://arxiv.org/pdf/2602.11575.pdf</a></span>   <span><a href='https://syeon-yoo.github.io/ready-go-site/' target='_blank'>  GitHub</a></span> <span><a href='https://syeon-yoo.github.io/ready-go-site/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Seungyeon Yoo, Youngseok Jang, Dabin Kim, Youngsoo Han, Seungwoo Jung, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.11575">ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation models often struggle in real-world dynamic environments due to limited robustness to the sim-to-real gap and the difficulty of training policies tailored to target deployment environments (e.g., households, restaurants, and factories). Although real-to-sim navigation simulation using 3D Gaussian Splatting (GS) can mitigate these challenges, prior GS-based works have considered only static scenes or non-photorealistic human obstacles built from simulator assets, despite the importance of safe navigation in dynamic environments. To address these issues, we propose ReaDy-Go, a novel real-to-sim simulation pipeline that synthesizes photorealistic dynamic scenarios in target environments by augmenting a reconstructed static GS scene with dynamic human GS obstacles, and trains navigation policies using the generated datasets. The pipeline provides three key contributions: (1) a dynamic GS simulator that integrates static scene GS with a human animation module, enabling the insertion of animatable human GS avatars and the synthesis of plausible human motions from 2D trajectories, (2) a navigation dataset generation framework that leverages the simulator along with a robot expert planner designed for dynamic GS representations and a human planner, and (3) robust navigation policies to both the sim-to-real gap and moving obstacles. The proposed simulator generates thousands of photorealistic navigation scenarios with animatable human GS avatars from arbitrary viewpoints. ReaDy-Go outperforms baselines across target environments in both simulation and real-world experiments, demonstrating improved navigation performance even after sim-to-real transfer and in the presence of moving obstacles. Moreover, zero-shot sim-to-real deployment in an unseen environment indicates its generalization potential. Project page: https://syeon-yoo.github.io/ready-go-site/.
<div id='section'>PaperID: <span id='pid'>4, <a href='https://arxiv.org/pdf/2602.11214.pdf' target='_blank'>https://arxiv.org/pdf/2602.11214.pdf</a></span>   <span><a href='https://github.com/kav-institute/ddmdn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Manuel Hetzel, Kerim Turacan, Hannes Reichert, Konrad Doll, Bernhard Sick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.11214">DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.
<div id='section'>PaperID: <span id='pid'>5, <a href='https://arxiv.org/pdf/2602.10116.pdf' target='_blank'>https://arxiv.org/pdf/2602.10116.pdf</a></span>   <span><a href='https://nvlabs.github.io/sage' target='_blank'>  GitHub</a></span> <span><a href='https://nvlabs.github.io/sage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongchi Xia, Xuan Li, Zhaoshuo Li, Qianli Ma, Jiashu Xu, Ming-Yu Liu, Yin Cui, Tsung-Yi Lin, Wei-Chiu Ma, Shenlong Wang, Shuran Song, Fangyin Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.10116">SAGE: Scalable Agentic 3D Scene Generation for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.
<div id='section'>PaperID: <span id='pid'>6, <a href='https://arxiv.org/pdf/2602.10109.pdf' target='_blank'>https://arxiv.org/pdf/2602.10109.pdf</a></span>   <span><a href='https://internrobotics.github.io/internvla-m1.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinhui Ye, Fangjing Wang, Ning Gao, Junqiu Yu, Yangkun Zhu, Bin Wang, Jinyu Zhang, Weiyang Jin, Yanwei Fu, Feng Zheng, Yilun Chen, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.10109">ST4VLA: Spatially Guided Training for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/
<div id='section'>PaperID: <span id='pid'>7, <a href='https://arxiv.org/pdf/2602.08440.pdf' target='_blank'>https://arxiv.org/pdf/2602.08440.pdf</a></span>   <span><a href='https://steervla.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tian Gao, Celine Tan, Catherine Glossop, Timothy Gao, Jiankai Sun, Kyle Stachowicz, Shirley Wu, Oier Mees, Dorsa Sadigh, Sergey Levine, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.08440">SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.
<div id='section'>PaperID: <span id='pid'>8, <a href='https://arxiv.org/pdf/2602.07506.pdf' target='_blank'>https://arxiv.org/pdf/2602.07506.pdf</a></span>   <span><a href='https://lipzh5.github.io/VividFace/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Peizhen Li, Longbing Cao, Xiao-Ming Wu, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07506">VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.
<div id='section'>PaperID: <span id='pid'>9, <a href='https://arxiv.org/pdf/2602.07341.pdf' target='_blank'>https://arxiv.org/pdf/2602.07341.pdf</a></span>   <span><a href='https://cyberyyc.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yicheng Yang, Ruijiao Li, Lifeng Wang, Shuai Zheng, Shunzheng Ma, Keyu Zhang, Tuoyu Sun, Chenyun Dai, Jie Ding, Zhuo Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07341">Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.
<div id='section'>PaperID: <span id='pid'>10, <a href='https://arxiv.org/pdf/2602.04315.pdf' target='_blank'>https://arxiv.org/pdf/2602.04315.pdf</a></span>   <span><a href='https://aigeeksgroup.github.io/GeneralVLA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AIGeeksGroup/GeneralVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04315">GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.
<div id='section'>PaperID: <span id='pid'>11, <a href='https://arxiv.org/pdf/2602.04228.pdf' target='_blank'>https://arxiv.org/pdf/2602.04228.pdf</a></span>   <span><a href='https://cognition2actionlab.github.io/VLA-TMEE.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu, Xiaoguang Zhao, Pengwei Wang, Zhongyuan Wang, Lei Xing, Shanghang Zhang, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04228">Reshaping Action Error Distributions for Reliable Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: https://cognition2actionlab.github.io/VLA-TMEE.github.io/
<div id='section'>PaperID: <span id='pid'>12, <a href='https://arxiv.org/pdf/2602.04184.pdf' target='_blank'>https://arxiv.org/pdf/2602.04184.pdf</a></span>   <span><a href='https://github.com/Mi3-Lab/doScenes-VLM-Planning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Angel Martinez-Sanchez, Parthib Roy, Ross Greer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04184">Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning
<div id='section'>PaperID: <span id='pid'>13, <a href='https://arxiv.org/pdf/2602.03054.pdf' target='_blank'>https://arxiv.org/pdf/2602.03054.pdf</a></span>   <span><a href='https://byc-sophie.github.io/Towards-Considerate-Embodied-AI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuanchen Bai, Ruixiang Han, Niti Parikh, Wendy Ju, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03054">Towards Considerate Embodied AI: Co-Designing Situated Multi-Site Healthcare Robots from Abstract Concepts to High-Fidelity Prototypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-design is essential for grounding embodied artificial intelligence (AI) systems in real-world contexts, especially high-stakes domains such as healthcare. While prior work has explored multidisciplinary collaboration, iterative prototyping, and support for non-technical participants, few have interwoven these into a sustained co-design process. Such efforts often target one context and low-fidelity stages, limiting the generalizability of findings and obscuring how participants' ideas evolve. To address these limitations, we conducted a 14-week workshop with a multidisciplinary team of 22 participants, centered around how embodied AI can reduce non-value-added task burdens in three healthcare settings: emergency departments, long-term rehabilitation facilities, and sleep disorder clinics. We found that the iterative progression from abstract brainstorming to high-fidelity prototypes, supported by educational scaffolds, enabled participants to understand real-world trade-offs and generate more deployable solutions. We propose eight guidelines for co-designing more considerate embodied AI: attuned to context, responsive to social dynamics, mindful of expectations, and grounded in deployment. Project Page: https://byc-sophie.github.io/Towards-Considerate-Embodied-AI/
<div id='section'>PaperID: <span id='pid'>14, <a href='https://arxiv.org/pdf/2602.02590.pdf' target='_blank'>https://arxiv.org/pdf/2602.02590.pdf</a></span>   <span><a href='https://github.com/LuoXubo/StepNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xubo Luo, Aodi Wu, Haodong Han, Xue Wan, Wei Zhang, Leizheng Shu, Ruisuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.02590">StepNav: Structured Trajectory Priors for Efficient and Multimodal Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation is fundamental to autonomous systems, yet generating reliable trajectories in cluttered and uncertain environments remains a core challenge. Recent generative models promise end-to-end synthesis, but their reliance on unstructured noise priors often yields unsafe, inefficient, or unimodal plans that cannot meet real-time requirements. We propose StepNav, a novel framework that bridges this gap by introducing structured, multimodal trajectory priors derived from variational principles. StepNav first learns a geometry-aware success probability field to identify all feasible navigation corridors. These corridors are then used to construct an explicit, multi-modal mixture prior that initializes a conditional flow-matching process. This refinement is formulated as an optimal control problem with explicit smoothness and safety regularization. By replacing unstructured noise with physically-grounded candidates, StepNav generates safer and more efficient plans in significantly fewer steps. Experiments in both simulation and real-world benchmarks demonstrate consistent improvements in robustness, efficiency, and safety over state-of-the-art generative planners, advancing reliable trajectory generation for practical autonomous navigation. The code has been released at https://github.com/LuoXubo/StepNav.
<div id='section'>PaperID: <span id='pid'>15, <a href='https://arxiv.org/pdf/2602.01166.pdf' target='_blank'>https://arxiv.org/pdf/2602.01166.pdf</a></span>   <span><a href='https://loveju1y.github.io/Latent-Reasoning-VLA/' target='_blank'>  GitHub</a></span> <span><a href='https://loveju1y.github.io/Latent-Reasoning-VLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuanghao Bai, Jing Lyu, Wanqi Zhou, Zhe Li, Dakai Wang, Lei Xing, Xiaoguang Zhao, Pengwei Wang, Zhongyuan Wang, Cheng Chi, Badong Chen, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.01166">Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.
<div id='section'>PaperID: <span id='pid'>16, <a href='https://arxiv.org/pdf/2602.00839.pdf' target='_blank'>https://arxiv.org/pdf/2602.00839.pdf</a></span>   <span><a href='https://longxiang-ai.github.io/TransNormal' target='_blank'>  GitHub</a></span> <span><a href='https://longxiang-ai.github.io/TransNormal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingwei Li, Hehe Fan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00839">TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.
<div id='section'>PaperID: <span id='pid'>17, <a href='https://arxiv.org/pdf/2602.00551.pdf' target='_blank'>https://arxiv.org/pdf/2602.00551.pdf</a></span>   <span><a href='https://github.com/4amGodvzx/apex' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/4amGodvzx/apex' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen, Jianqiang Xiao, Shuo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00551">APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}
<div id='section'>PaperID: <span id='pid'>18, <a href='https://arxiv.org/pdf/2602.00288.pdf' target='_blank'>https://arxiv.org/pdf/2602.00288.pdf</a></span>   <span><a href='https://baiqi-li.github.io/timeblind_project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Baiqi Li, Kangyi Zhao, Ce Zhang, Chancharik Mitra, Jean de Dieu Nyandwi, Gedas Bertasius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00288">TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .
<div id='section'>PaperID: <span id='pid'>19, <a href='https://arxiv.org/pdf/2601.22046.pdf' target='_blank'>https://arxiv.org/pdf/2601.22046.pdf</a></span>   <span><a href='https://city-super.github.io/PLANING/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu, Tao Lu, Junting Dong, Yu Zhang, Bo Dai, Mulin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.22046">PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .
<div id='section'>PaperID: <span id='pid'>20, <a href='https://arxiv.org/pdf/2601.20334.pdf' target='_blank'>https://arxiv.org/pdf/2601.20334.pdf</a></span>   <span><a href='https://github.com/robiemusketeer/faea-sim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Brian Y. Tsui, Alan Y. Fang, Tiffany J. Hwu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20334">Demonstration-Free Robotic Control via LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim
<div id='section'>PaperID: <span id='pid'>21, <a href='https://arxiv.org/pdf/2601.17885.pdf' target='_blank'>https://arxiv.org/pdf/2601.17885.pdf</a></span>   <span><a href='https://peafowlvla.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17885">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding. In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors. On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation. Project website: https://peafowlvla.github.io/.
<div id='section'>PaperID: <span id='pid'>22, <a href='https://arxiv.org/pdf/2601.17657.pdf' target='_blank'>https://arxiv.org/pdf/2601.17657.pdf</a></span>   <span><a href='https://github.com/taewan2002/space-clip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Taewan Cho, Taeryang Kim, Andrew Jaeyong Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17657">SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip
<div id='section'>PaperID: <span id='pid'>23, <a href='https://arxiv.org/pdf/2601.15282.pdf' target='_blank'>https://arxiv.org/pdf/2601.15282.pdf</a></span>   <span><a href='https://dagroup-pku.github.io/ReVidgen.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/DAGroup-PKU/ReVidgen/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15282">Rethinking Video Generation Model for the Embodied World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.
<div id='section'>PaperID: <span id='pid'>24, <a href='https://arxiv.org/pdf/2601.12291.pdf' target='_blank'>https://arxiv.org/pdf/2601.12291.pdf</a></span>   <span><a href='https://rpl-cs-ucl.github.io/OpenNavMap_page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianhao Jiao, Changkun Liu, Jingwen Yu, Boyi Liu, Qianyi Zhang, Yue Wang, Dimitrios Kanoulas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12291">OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments. While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data. To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction. Our method unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization to robust, coarse-to-fine submap alignment without requiring pre-built 3D models. Evaluations on the Map-Free benchmark demonstrate superior accuracy over structure-from-motion and regression baselines, achieving an average translation error of 0.62m. Furthermore, the system maintains global consistency across 15km of multi-session data with an absolute trajectory error below 3m for map merging. Finally, we validate practical utility through 12 successful autonomous image-goal navigation tasks on simulated and physical robots. Code and datasets will be publicly available in https://rpl-cs-ucl.github.io/OpenNavMap_page.
<div id='section'>PaperID: <span id='pid'>25, <a href='https://arxiv.org/pdf/2601.11250.pdf' target='_blank'>https://arxiv.org/pdf/2601.11250.pdf</a></span>   <span><a href='https://github.com/RobotControlStack/vlagents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tobias Jülg, Khaled Gamal, Nisarga Nilavadi, Pierre Krack, Seongjin Bien, Michael Krawez, Florian Walter, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11250">VLAgents: A Policy Server for Efficient VLA Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents
<div id='section'>PaperID: <span id='pid'>26, <a href='https://arxiv.org/pdf/2601.09708.pdf' target='_blank'>https://arxiv.org/pdf/2601.09708.pdf</a></span>   <span><a href='https://jasper0314-huang.github.io/fast-thinkact/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09708">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.
<div id='section'>PaperID: <span id='pid'>27, <a href='https://arxiv.org/pdf/2601.09512.pdf' target='_blank'>https://arxiv.org/pdf/2601.09512.pdf</a></span>   <span><a href='https://tum-lsy.github.io/clare' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ralf Römer, Yi Zhang, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09512">CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.
<div id='section'>PaperID: <span id='pid'>28, <a href='https://arxiv.org/pdf/2601.08665.pdf' target='_blank'>https://arxiv.org/pdf/2601.08665.pdf</a></span>   <span><a href='https://wsakobe.github.io/VLingNav-web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoan Wang, Yuanfei Luo, Xingyu Chen, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen, Yangang Zhang, Junzhi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08665">VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.
<div id='section'>PaperID: <span id='pid'>29, <a href='https://arxiv.org/pdf/2601.05991.pdf' target='_blank'>https://arxiv.org/pdf/2601.05991.pdf</a></span>   <span><a href='https://jiayuding031020.github.io/ambi3d/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiayu Ding, Haoran Tang, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05991">Open-Vocabulary 3D Instruction Ambiguity Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.
<div id='section'>PaperID: <span id='pid'>30, <a href='https://arxiv.org/pdf/2601.05810.pdf' target='_blank'>https://arxiv.org/pdf/2601.05810.pdf</a></span>   <span><a href='https://anc891203.github.io/SceneFoundry-Demo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05810">SceneFoundry: Generating Interactive Infinite 3D Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research. project page: https://anc891203.github.io/SceneFoundry-Demo/
<div id='section'>PaperID: <span id='pid'>31, <a href='https://arxiv.org/pdf/2601.05172.pdf' target='_blank'>https://arxiv.org/pdf/2601.05172.pdf</a></span>   <span><a href='https://github.com/ziplab/CoV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05172">CoV: Chain-of-View Prompting for Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached. We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .
<div id='section'>PaperID: <span id='pid'>32, <a href='https://arxiv.org/pdf/2601.02295.pdf' target='_blank'>https://arxiv.org/pdf/2601.02295.pdf</a></span>   <span><a href='https://dannymcy.github.io/cyclevla/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenyang Ma, Guangyu Yang, Kai Lu, Shitong Xu, Bill Byrne, Niki Trigoni, Andrew Markham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02295">CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/
<div id='section'>PaperID: <span id='pid'>33, <a href='https://arxiv.org/pdf/2512.22519.pdf' target='_blank'>https://arxiv.org/pdf/2512.22519.pdf</a></span>   <span><a href='https://uark-aicv.github.io/OBEYED_VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Khoa Vo, Taisei Hanyu, Yuki Ikebe, Trong Thang Pham, Nhat Chung, Minh Nhat Vu, Duy Nguyen Ho Minh, Anh Nguyen, Anthony Gunderman, Chase Rainwater, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22519">Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance. To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects. On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.
<div id='section'>PaperID: <span id='pid'>34, <a href='https://arxiv.org/pdf/2512.22342.pdf' target='_blank'>https://arxiv.org/pdf/2512.22342.pdf</a></span>   <span><a href='https://0309hws.github.io/VL-LN.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wensi Huang, Shaohao Zhu, Meng Wei, Jinming Xu, Xihui Liu, Hanqing Wang, Tai Wang, Feng Zhao, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22342">VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/
<div id='section'>PaperID: <span id='pid'>35, <a href='https://arxiv.org/pdf/2512.16909.pdf' target='_blank'>https://arxiv.org/pdf/2512.16909.pdf</a></span>   <span><a href='https://hybridrobotics.github.io/MomaGraph/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16909">MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.
<div id='section'>PaperID: <span id='pid'>36, <a href='https://arxiv.org/pdf/2512.16760.pdf' target='_blank'>https://arxiv.org/pdf/2512.16760.pdf</a></span>   <span><a href='https://github.com/worldbench/awesome-vla-for-ad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianshuai Hu, Xiaolu Liu, Song Wang, Yiyao Zhu, Ao Liang, Lingdong Kong, Guoyang Zhao, Zeying Gong, Jun Cen, Zhiyu Huang, Xiaoshuai Hao, Linfeng Li, Hang Song, Xiangtai Li, Jun Ma, Shaojie Shen, Jianke Zhu, Dacheng Tao, Ziwei Liu, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16760">Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.
<div id='section'>PaperID: <span id='pid'>37, <a href='https://arxiv.org/pdf/2512.15933.pdf' target='_blank'>https://arxiv.org/pdf/2512.15933.pdf</a></span>   <span><a href='https://dwipddalal.github.io/AgentNav/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15933">City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/
<div id='section'>PaperID: <span id='pid'>38, <a href='https://arxiv.org/pdf/2512.11908.pdf' target='_blank'>https://arxiv.org/pdf/2512.11908.pdf</a></span>   <span><a href='https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks' target='_blank'>  GitHub</a></span> <span><a href=' https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Heng Zhang, Rui Dai, Gokhan Solak, Pokuang Zhou, Yu She, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11908">Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.
<div id='section'>PaperID: <span id='pid'>39, <a href='https://arxiv.org/pdf/2512.11891.pdf' target='_blank'>https://arxiv.org/pdf/2512.11891.pdf</a></span>   <span><a href='https://vlsa-aegis.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Songqiao Hu, Zeyi Liu, Shuang Liu, Jun Cen, Zihan Meng, Xiao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11891">VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.
<div id='section'>PaperID: <span id='pid'>40, <a href='https://arxiv.org/pdf/2512.11769.pdf' target='_blank'>https://arxiv.org/pdf/2512.11769.pdf</a></span>   <span><a href='https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoyu Ma, Zhengqing Yuan, Zheyuan Zhang, Kaiwen Shi, Lichao Sun, Yanfang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11769">BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.
<div id='section'>PaperID: <span id='pid'>41, <a href='https://arxiv.org/pdf/2512.11362.pdf' target='_blank'>https://arxiv.org/pdf/2512.11362.pdf</a></span>   <span><a href='https://suyuz1.github.io/VLA-Survey-Anatomy/' target='_blank'>  GitHub</a></span> <span><a href='https://suyuz1.github.io/VLA-Survey-Anatomy/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chao Xu, Suyu Zhang, Yang Liu, Baigui Sun, Weihong Chen, Bo Xu, Qi Liu, Juncheng Wang, Shujun Wang, Shan Luo, Jan Peters, Athanasios V. Vasilakos, Stefanos Zafeiriou, Jiankang Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11362">An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/VLA-Survey-Anatomy/}{project page}.
<div id='section'>PaperID: <span id='pid'>42, <a href='https://arxiv.org/pdf/2512.10958.pdf' target='_blank'>https://arxiv.org/pdf/2512.10958.pdf</a></span>   <span><a href='https://worldbench.github.io/worldlens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ao Liang, Lingdong Kong, Tianyi Yan, Hongsi Liu, Wesley Yang, Ziqi Huang, Wei Yin, Jialong Zuo, Yixuan Hu, Dekai Zhu, Dongyue Lu, Youquan Liu, Guangfeng Jiang, Linfeng Li, Xiangtai Li, Long Zhuo, Lai Xing Ng, Benoit R. Cottereau, Changxin Gao, Liang Pan, Wei Tsang Ooi, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10958">WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.
<div id='section'>PaperID: <span id='pid'>43, <a href='https://arxiv.org/pdf/2512.10605.pdf' target='_blank'>https://arxiv.org/pdf/2512.10605.pdf</a></span>   <span><a href='https://github.com/LegendLeoChen/LEO-RobotAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lihuang Chen, Xiangyu Luo, Jun Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10605">LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.
<div id='section'>PaperID: <span id='pid'>44, <a href='https://arxiv.org/pdf/2512.09928.pdf' target='_blank'>https://arxiv.org/pdf/2512.09928.pdf</a></span>   <span><a href='https://github.com/OpenHelix-Team/HiF-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09928">HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.
<div id='section'>PaperID: <span id='pid'>45, <a href='https://arxiv.org/pdf/2512.09927.pdf' target='_blank'>https://arxiv.org/pdf/2512.09927.pdf</a></span>   <span><a href='https://github.com/Jasper-aaa/TEAM-VLA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Jasper-aaa/TEAM-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Ye, Jiaqi Ma, Jun Cen, Zhihe Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09927">Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}
<div id='section'>PaperID: <span id='pid'>46, <a href='https://arxiv.org/pdf/2512.07237.pdf' target='_blank'>https://arxiv.org/pdf/2512.07237.pdf</a></span>   <span><a href='https://github.com/chengzhag/UCPE' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/chengzhag/UCPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Cheng Zhang, Boying Li, Meng Wei, Yan-Pei Cao, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07237">Unified Camera Positional Encoding for Controlled Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.
<div id='section'>PaperID: <span id='pid'>47, <a href='https://arxiv.org/pdf/2512.05060.pdf' target='_blank'>https://arxiv.org/pdf/2512.05060.pdf</a></span>   <span><a href='https://hustvl.github.io/4DLangVGGT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hustvl/4DLangVGGT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hustvl/4DLangVGGT,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xianfeng Wu, Yajing Bai, Minghan Li, Xianzu Wu, Xueqi Zhao, Zhongyuan Lai, Wenyu Liu, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05060">4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT
<div id='section'>PaperID: <span id='pid'>48, <a href='https://arxiv.org/pdf/2512.04686.pdf' target='_blank'>https://arxiv.org/pdf/2512.04686.pdf</a></span>   <span><a href='https://github.com/WangYipu2002/CrossPoint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, Zhiyuan Ma, Daniel Dajun Zeng, Xiaolong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04686">Towards Cross-View Point Correspondence in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.
<div id='section'>PaperID: <span id='pid'>49, <a href='https://arxiv.org/pdf/2512.04515.pdf' target='_blank'>https://arxiv.org/pdf/2512.04515.pdf</a></span>   <span><a href='https://aigeeksgroup.github.io/EgoLCD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AIGeeksGroup/EgoLCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Liuzhou Zhang, Jiarui Ye, Yuanlei Wang, Ming Zhong, Mingju Cao, Wanke Xia, Bowen Zeng, Zeyu Zhang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04515">EgoLCD: Egocentric Video Generation with Long Context Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.
<div id='section'>PaperID: <span id='pid'>50, <a href='https://arxiv.org/pdf/2512.01204.pdf' target='_blank'>https://arxiv.org/pdf/2512.01204.pdf</a></span>   <span><a href='https://d-robotics-ai-lab.github.io/TabletopGen.project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziqian Wang, Yonghao He, Licheng Yang, Wei Zou, Hongxuan Ma, Liu Liu, Wei Sui, Yuxin Guo, Hu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01204">TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI -- especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.
<div id='section'>PaperID: <span id='pid'>51, <a href='https://arxiv.org/pdf/2512.01031.pdf' target='_blank'>https://arxiv.org/pdf/2512.01031.pdf</a></span>   <span><a href='https://github.com/mit-han-lab/vlash' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaming Tang, Yufei Sun, Yilong Zhao, Shang Yang, Yujun Lin, Zhuoyang Zhang, James Hou, Yao Lu, Zhijian Liu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01031">VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash
<div id='section'>PaperID: <span id='pid'>52, <a href='https://arxiv.org/pdf/2511.23034.pdf' target='_blank'>https://arxiv.org/pdf/2511.23034.pdf</a></span>   <span><a href='https://mm-robot.github.io/distill_latent_action/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zuolei Li, Xingyu Gao, Xiaofan Wang, Jianlong Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23034">LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.
<div id='section'>PaperID: <span id='pid'>53, <a href='https://arxiv.org/pdf/2511.21161.pdf' target='_blank'>https://arxiv.org/pdf/2511.21161.pdf</a></span>   <span><a href='https://xuhu0529.github.io/MarketGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xu Hu, Yiyang Feng, Junran Peng, Jiawei He, Liyi Chen, Chuanchen Luo, Xucheng Yin, Qing Li, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21161">MarketGen: A Scalable Simulation Platform with Auto-Generated Embodied Supermarket Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of embodied agents for complex commercial environments is hindered by a critical gap in existing robotics datasets and benchmarks, which primarily focus on household or tabletop settings with short-horizon tasks. To address this limitation, we introduce MarketGen, a scalable simulation platform with automatic scene generation for complex supermarket environments. MarketGen features a novel agent-based Procedural Content Generation (PCG) framework. It uniquely supports multi-modal inputs (text and reference images) and integrates real-world design principles to automatically generate complete, structured, and realistic supermarkets. We also provide an extensive and diverse 3D asset library with a total of 1100+ supermarket goods and parameterized facilities assets. Building on this generative foundation, we propose a novel benchmark for assessing supermarket agents, featuring two daily tasks in a supermarket: (1) Checkout Unloading: long-horizon tabletop tasks for cashier agents, and (2) In-Aisle Item Collection: complex mobile manipulation tasks for salesperson agents. We validate our platform and benchmark through extensive experiments, including the deployment of a modular agent system and successful sim-to-real transfer. MarketGen provides a comprehensive framework to accelerate research in embodied AI for complex commercial applications.
<div id='section'>PaperID: <span id='pid'>54, <a href='https://arxiv.org/pdf/2511.21135.pdf' target='_blank'>https://arxiv.org/pdf/2511.21135.pdf</a></span>   <span><a href='https://amap-eai.github.io/SocialNav/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyi Chen, Yingnan Guo, Zedong Chu, Minghua Luo, Yanfen Shen, Mingchao Sun, Junjun Hu, Shichao Xie, Kuan Yang, Pei Shi, Zhining Gu, Lu Liu, Honglin Han, Xiaolong Wu, Mu Xu, Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21135">SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation that adheres to social norms remains an open research challenge. Our \textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical "brain-action" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/
<div id='section'>PaperID: <span id='pid'>55, <a href='https://arxiv.org/pdf/2511.21025.pdf' target='_blank'>https://arxiv.org/pdf/2511.21025.pdf</a></span>   <span><a href='https://github.com/bronyayang/CaptionQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shijia Yang, Yunong Liu, Bohan Zhai, Ximeng Sun, Zicheng Liu, Emad Barsoum, Manling Li, Chenfeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21025">CaptionQA: Is Your Caption as Useful as the Image Itself?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.
<div id='section'>PaperID: <span id='pid'>56, <a href='https://arxiv.org/pdf/2511.20841.pdf' target='_blank'>https://arxiv.org/pdf/2511.20841.pdf</a></span>   <span><a href='https://ekjt.github.io/OVAL-Grasp/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Edmond Tong, Advaith Balaji, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20841">OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL-Grasp, a zero-shot open-vocabulary approach to task-oriented, affordance based grasping that uses large-language models and vision-language models to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL-Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task oriented grasping baselines on experiments with 20 household objects with 3 unique tasks for each. OVAL-Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real-world experiments with the Fetch mobile manipulator. Additionally, OVAL-Grasp finds correct object parts under partial occlusions, demonstrating a part selection success rate of 80% in cluttered scenes. We also demonstrate OVAL-Grasp's efficacy in scenarios that rely on visual features for part selection, and show the benefit of a modular design through our ablation experiments. Our project webpage is available at https://ekjt.github.io/OVAL-Grasp/
<div id='section'>PaperID: <span id='pid'>57, <a href='https://arxiv.org/pdf/2511.20693.pdf' target='_blank'>https://arxiv.org/pdf/2511.20693.pdf</a></span>   <span><a href='https://github.com/pandawei-ele/A2FLOW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingming Zhao, Xiaokang Wei, Yuanqi Shao, Kaiwen Zhou, Lin Yang, Siwei Rao, Junhui Zhan, Zhitang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20693">$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW
<div id='section'>PaperID: <span id='pid'>58, <a href='https://arxiv.org/pdf/2511.20620.pdf' target='_blank'>https://arxiv.org/pdf/2511.20620.pdf</a></span>   <span><a href='https://ai4ce.github.io/wanderland/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinhao Liu, Jiaqi Li, Youming Deng, Ruxin Chen, Yingjia Zhang, Yifei Ma, Li Guo, Yiming Li, Jing Zhang, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20620">Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.
<div id='section'>PaperID: <span id='pid'>59, <a href='https://arxiv.org/pdf/2511.19768.pdf' target='_blank'>https://arxiv.org/pdf/2511.19768.pdf</a></span>   <span><a href='https://noahfrahm.github.io/Prune-Then-Plan-project-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Noah Frahm, Prakrut Patel, Yue Zhang, Shoubin Yu, Mohit Bansal, Roni Sengupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19768">Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.
<div id='section'>PaperID: <span id='pid'>60, <a href='https://arxiv.org/pdf/2511.19430.pdf' target='_blank'>https://arxiv.org/pdf/2511.19430.pdf</a></span>   <span><a href='https://github.com/H-EmbodVis/GRANT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/H-EmbodVis/GRANT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Dingkang Liang, Cheng Zhang, Xiaopeng Xu, Jianzhong Ju, Zhenbo Luo, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19430">Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT
<div id='section'>PaperID: <span id='pid'>61, <a href='https://arxiv.org/pdf/2511.18685.pdf' target='_blank'>https://arxiv.org/pdf/2511.18685.pdf</a></span>   <span><a href='https://cfg-bench.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://cfg-bench.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Dayong Liu, Chao Xu, Weihong Chen, Suyu Zhang, Juncheng Wang, Jiankang Deng, Baigui Sun, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18685">Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents. Project page: \href{https://cfg-bench.github.io/}{https://cfg-bench.github.io/}.
<div id='section'>PaperID: <span id='pid'>62, <a href='https://arxiv.org/pdf/2511.18127.pdf' target='_blank'>https://arxiv.org/pdf/2511.18127.pdf</a></span>   <span><a href='https://github.com/ut-vision/SFHand' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruicong Liu, Yifei Huang, Liangyang Ouyang, Caixin Kang, Yoichi Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18127">SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.
<div id='section'>PaperID: <span id='pid'>63, <a href='https://arxiv.org/pdf/2511.16567.pdf' target='_blank'>https://arxiv.org/pdf/2511.16567.pdf</a></span>   <span><a href='https://matchlab-imperial.github.io/poma3d/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ye Mao, Weixun Luo, Ranran Huang, Junpeng Jing, Krystian Mikolajczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16567">POMA-3D: The Point Map Way to 3D Scene Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/
<div id='section'>PaperID: <span id='pid'>64, <a href='https://arxiv.org/pdf/2511.16518.pdf' target='_blank'>https://arxiv.org/pdf/2511.16518.pdf</a></span>   <span><a href='https://github.com/XiaomiMiMo/MiMo-Embodied' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/XiaomiMiMo/MiMo-Embodied' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoshuai Hao, Lei Zhou, Zhijian Huang, Zhiwen Hou, Yingbo Tang, Lingfeng Zhang, Guang Li, Zheng Lu, Shuhuai Ren, Xianhui Meng, Yuchen Zhang, Jing Wu, Jinghui Lu, Chenxu Dang, Jiayi Guan, Jianhua Wu, Zhiyi Hou, Hanbing Li, Shumeng Xia, Mingliang Zhou, Yinan Zheng, Zihao Yue, Shuhao Gu, Hao Tian, Yuannan Shen, Jianwei Cui, Wen Zhang, Shaoqing Xu, Bing Wang, Haiyang Sun, Zeyu Zhu, Yuncheng Jiang, Zibin Guo, Chuhong Gong, Chaofan Zhang, Wenbo Ding, Kun Ma, Guang Chen, Rui Cai, Diyun Xiang, Heng Qu, Fuli Luo, Hangjun Ye, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16518">MiMo-Embodied: X-Embodied Foundation Model Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
<div id='section'>PaperID: <span id='pid'>65, <a href='https://arxiv.org/pdf/2511.16407.pdf' target='_blank'>https://arxiv.org/pdf/2511.16407.pdf</a></span>   <span><a href='https://github.com/XizoB/LAOF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xizhou Bu, Jiexi Lyu, Fulei Sun, Ruichen Yang, Zhiqiang Ma, Wei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16407">LAOF: Robust Latent Action Learning with Optical Flow Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.
<div id='section'>PaperID: <span id='pid'>66, <a href='https://arxiv.org/pdf/2511.14396.pdf' target='_blank'>https://arxiv.org/pdf/2511.14396.pdf</a></span>   <span><a href='https://qhemu.github.io/CCoL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14396">Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
<div id='section'>PaperID: <span id='pid'>67, <a href='https://arxiv.org/pdf/2511.14149.pdf' target='_blank'>https://arxiv.org/pdf/2511.14149.pdf</a></span>   <span><a href='https://github.com/pythongod-exe/iGaussian' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Wang, Linqing Zhao, Xiuwei Xu, Jiwen Lu, Haibin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14149">iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian
<div id='section'>PaperID: <span id='pid'>68, <a href='https://arxiv.org/pdf/2511.14148.pdf' target='_blank'>https://arxiv.org/pdf/2511.14148.pdf</a></span>   <span><a href='https://github.com/YuhuaJiang2002/AsyncVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14148">AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.
<div id='section'>PaperID: <span id='pid'>69, <a href='https://arxiv.org/pdf/2511.12040.pdf' target='_blank'>https://arxiv.org/pdf/2511.12040.pdf</a></span>   <span><a href='https://xinyuanhu66.github.io/SRSplat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinyuan Hu, Changyue Shi, Chuxiao Yang, Minghao Chen, Jiajun Ding, Tao Wei, Chen Wei, Zhou Yu, Min Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12040">SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.
<div id='section'>PaperID: <span id='pid'>70, <a href='https://arxiv.org/pdf/2511.06385.pdf' target='_blank'>https://arxiv.org/pdf/2511.06385.pdf</a></span>   <span><a href='https://tum-lsy.github.io/pacs/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ralf Römer, Julian Balletshofer, Jakob Thumm, Marco Pavone, Angela P. Schoellig, Matthias Althoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06385">From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.
<div id='section'>PaperID: <span id='pid'>71, <a href='https://arxiv.org/pdf/2511.05304.pdf' target='_blank'>https://arxiv.org/pdf/2511.05304.pdf</a></span>   <span><a href='https://github.com/sailgt/psiUnity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Akhil Ajikumar, Sahil Mayenkar, Steven Yoo, Sakib Reza, Mohsen Moghaddam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05304">psiUnity: A Platform for Multimodal Data-Driven XR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended reality (XR) research increasingly relies on the ability to stream and synchronize multimodal data between headsets and immersive applications for data-driven interaction and experimentation. However, developers face a critical gap: the Platform for Situated Intelligence (psi), which excels at deterministic temporal alignment and multimodal data management, has been largely inaccessible to the dominant Unity/MRTK ecosystem used for HoloLens development. We introduce psiUnity, an open-source C# integration that bridges psi's .NET libraries with Unity 2022.3 and MRTK3 for HoloLens 2. psiUnity enables bidirectional, real-time streaming of head pose, hand tracking, gaze, IMU, audio, and depth sensor data (AHAT and long-throw) with microsecond-level temporal precision, allowing Unity applications to both consume and produce synchronized multimodal data streams. By embedding psi's native serialization, logging, and temporal coordination directly within Unity's architecture, psiUnity extends psi beyond its previous StereoKit limitations and empowers the HRI, HCI, and embodied-AI communities to develop reproducible, data-driven XR interactions and experiments within the familiar Unity environment. The integration is available at https://github.com/sailgt/psiUnity.
<div id='section'>PaperID: <span id='pid'>72, <a href='https://arxiv.org/pdf/2511.05275.pdf' target='_blank'>https://arxiv.org/pdf/2511.05275.pdf</a></span>   <span><a href='https://jellyho.github.io/TwinVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hokyun Im, Euijin Jeong, Jianlong Fu, Andrey Kolobov, Youngwoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05275">TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) trained on large-scale robotic datasets have demonstrated strong performance on manipulation tasks, including bimanual tasks. However, because most public datasets focus on single-arm demonstrations, adapting VLAs for bimanual tasks typically requires substantial additional bimanual data and fine-tuning. To address this challenge, we introduce TwinVLA, a modular framework that composes two copies of a pretrained single-arm VLA into a coordinated bimanual VLA. Unlike monolithic cross-embodiment models trained on mixtures of single-arm and bimanual data, TwinVLA improves both data efficiency and performance by composing pretrained single-arm policies. Across diverse bimanual tasks in real-world and simulation settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model without requiring any bimanual pretraining. Furthermore, it narrows the gap to state-of-the-art model, $π_0$ which rely on extensive proprietary bimanual data and compute cost. These results establish our modular composition approach as a data-efficient and scalable path toward high-performance bimanual manipulation, leveraging public single-arm data.
<div id='section'>PaperID: <span id='pid'>73, <a href='https://arxiv.org/pdf/2511.05203.pdf' target='_blank'>https://arxiv.org/pdf/2511.05203.pdf</a></span>   <span><a href='https://linusnep.github.io/SIL/' target='_blank'>  GitHub</a></span> <span><a href='https://linusnep.github.io/SIL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Linus Nwankwo, Björn Ellensohn, Christian Rauch, Elmar Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05203">Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's autonomous agents can understand free-form natural language instructions and execute long-horizon tasks in a manner akin to human-level reasoning. These capabilities are mostly driven by large-scale pre-trained foundation models (FMs). However, the approaches with which these models are grounded for human-robot interaction (HRI) perpetuate a master-apprentice model, where the apprentice (embodied agent) passively receives and executes the master's (human's) commands without reciprocal learning. This reactive interaction approach does not capture the co-adaptive dynamics inherent in everyday multi-turn human-human interactions. To address this, we propose a Symbiotic Interactive Learning (SIL) approach that enables both the master and the apprentice to co-adapt through mutual, bidirectional interactions. We formalised SIL as a co-adaptation process within a shared latent task space, where the agent and human maintain joint belief states that evolve based on interaction history. This enables the agent to move beyond reactive execution to proactive clarification, adaptive suggestions, and shared plan refinement. To realise these novel behaviours, we leveraged pre-trained FMs for spatial perception and reasoning, alongside a lightweight latent encoder that grounds the models' outputs into task-specific representations. Furthermore, to ensure stability as the tasks evolve, we augment SIL with a memory architecture that prevents the forgetting of learned task-space representations. We validate SIL on both simulated and real-world embodied tasks, including instruction following, information retrieval, query-oriented reasoning, and interactive dialogues. Demos and resources are public at:~\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.
<div id='section'>PaperID: <span id='pid'>74, <a href='https://arxiv.org/pdf/2511.03497.pdf' target='_blank'>https://arxiv.org/pdf/2511.03497.pdf</a></span>   <span><a href='https://github.com/binabik-ai/mcp-rosbags' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lei Fu, Sahar Salimpour, Leonardo Militano, Harry Edelman, Jorge Peña Queralta, Giovanni Toffetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03497">ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.
<div id='section'>PaperID: <span id='pid'>75, <a href='https://arxiv.org/pdf/2511.02776.pdf' target='_blank'>https://arxiv.org/pdf/2511.02776.pdf</a></span>   <span><a href='https://xr-1-vla.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02776">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in large-scale robotic datasets and vision-language models (VLMs) has advanced research on vision-language-action (VLA) models. However, existing VLA models still face two fundamental challenges: (i) producing precise low-level actions from high-dimensional observations, (ii) bridging domain gaps across heterogeneous data sources, including diverse robot embodiments and human demonstrations. Existing methods often encode latent variables from either visual dynamics or robotic actions to guide policy learning, but they fail to fully exploit the complementary multi-modal knowledge present in large-scale, heterogeneous datasets. In this work, we present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable VLA learning across diverse robots, tasks, and environments. XR-1 introduces the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and robotic motion. UVMC addresses these challenges by (i) serving as an intermediate representation between the observations and actions, and (ii) aligning multimodal dynamic information from heterogeneous data sources to capture complementary knowledge. To effectively exploit UVMC, we propose a three-stage training paradigm: (i) self-supervised UVMC learning, (ii) UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and (iii) task-specific post-training. We validate XR-1 through extensive real-world experiments with more than 14,000 rollouts on six different robot embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently outperforms state-of-the-art baselines such as $π_{0.5}$, $π_0$, RDT, UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel objects, background variations, distractors, and illumination changes. Our project is at https://xr-1-vla.github.io/.
<div id='section'>PaperID: <span id='pid'>76, <a href='https://arxiv.org/pdf/2511.00998.pdf' target='_blank'>https://arxiv.org/pdf/2511.00998.pdf</a></span>   <span><a href='https://ziyeeee.github.io/gaudp.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziye Wang, Li Kang, Yiran Qin, Jiahua Ma, Zhanglin Peng, Lei Bai, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00998">GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, effective coordination in embodied multi-agent systems has remained a fundamental challenge, particularly in scenarios where agents must balance individual perspectives with global environmental awareness. Existing approaches often struggle to balance fine-grained local control with comprehensive scene understanding, resulting in limited scalability and compromised collaboration quality. In this paper, we present GauDP, a novel Gaussian-image synergistic representation that facilitates scalable, perception-aware imitation learning in multi-agent collaborative systems. Specifically, GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations, then dynamically redistributes 3D Gaussian attributes to each agent's local perspective. This enables all agents to adaptively query task-critical features from the shared scene representation while maintaining their individual viewpoints. This design facilitates both fine-grained control and globally coherent behavior without requiring additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our method achieves superior performance over existing image-based methods and approaches the effectiveness of point-cloud-driven methods, while maintaining strong scalability as the number of agents increases.
<div id='section'>PaperID: <span id='pid'>77, <a href='https://arxiv.org/pdf/2511.00598.pdf' target='_blank'>https://arxiv.org/pdf/2511.00598.pdf</a></span>   <span><a href='https://github.com/Zi-Xuan-Sun/GDROS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixuan Sun, Shuaifeng Zhi, Ruize Li, Jingyuan Xia, Yongxiang Liu, Weidong Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00598">GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Registration of optical and synthetic aperture radar (SAR) remote sensing images serves as a critical foundation for image fusion and visual navigation tasks. This task is particularly challenging because of their modal discrepancy, primarily manifested as severe nonlinear radiometric differences (NRD), geometric distortions, and noise variations. Under large geometric transformations, existing classical template-based and sparse keypoint-based strategies struggle to achieve reliable registration results for optical-SAR image pairs. To address these limitations, we propose GDROS, a geometry-guided dense registration framework leveraging global cross-modal image interactions. First, we extract cross-modal deep features from optical and SAR images through a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D correlation volume is constructed and iteratively refined to establish pixel-wise dense correspondences. Subsequently, we implement a least squares regression (LSR) module to geometrically constrain the predicted dense optical flow field. Such geometry guidance mitigates prediction divergence by directly imposing an estimated affine transformation on the final flow predictions. Extensive experiments have been conducted on three representative datasets WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial resolutions, demonstrating robust performance of our proposed method across different imaging resolutions. Qualitative and quantitative results show that GDROS significantly outperforms current state-of-the-art methods in all metrics. Our source code will be released at: https://github.com/Zi-Xuan-Sun/GDROS.
<div id='section'>PaperID: <span id='pid'>78, <a href='https://arxiv.org/pdf/2510.26909.pdf' target='_blank'>https://arxiv.org/pdf/2510.26909.pdf</a></span>   <span><a href='https://leggedrobotics.github.io/navitrace_webpage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tim Windecker, Manthan Patel, Moritz Reuss, Richard Schwarzkopf, Cesar Cadena, Rudolf Lioutikov, Marco Hutter, Jonas Frey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26909">NaviTrace: Evaluating Embodied Navigation of Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/.
<div id='section'>PaperID: <span id='pid'>79, <a href='https://arxiv.org/pdf/2510.26583.pdf' target='_blank'>https://arxiv.org/pdf/2510.26583.pdf</a></span>   <span><a href='https://github.com/baaivision/Emu3.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26583">Emu3.5: Native Multimodal Models are World Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.
<div id='section'>PaperID: <span id='pid'>80, <a href='https://arxiv.org/pdf/2510.25760.pdf' target='_blank'>https://arxiv.org/pdf/2510.25760.pdf</a></span>   <span><a href='https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25760">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.
<div id='section'>PaperID: <span id='pid'>81, <a href='https://arxiv.org/pdf/2510.24118.pdf' target='_blank'>https://arxiv.org/pdf/2510.24118.pdf</a></span>   <span><a href='https://weekgoodday.github.io/lagmemo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haotian Zhou, Xiaole Wang, He Li, Fusheng Sun, Shengyu Guo, Guolei Qi, Jianghuan Xu, Huijing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24118">LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating to a designated goal using visual information is a fundamental capability for intelligent robots. Most classical visual navigation methods are restricted to single-goal, single-modality, and closed set goal settings. To address the practical demands of multi-modal, open-vocabulary goal queries and multi-goal visual navigation, we propose LagMemo, a navigation system that leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo constructs a unified 3D language memory. With incoming task goals, the system queries the memory, predicts candidate goal locations, and integrates a local perception-based verification mechanism to dynamically match and validate goals during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a high-quality core split distilled from GOAT-Bench tailored to multi-modal open-vocabulary multi-goal visual navigation. Experimental results show that LagMemo's memory module enables effective multi-modal open-vocabulary goal localization, and that LagMemo outperforms state-of-the-art methods in multi-goal visual navigation. Project page: https://weekgoodday.github.io/lagmemo
<div id='section'>PaperID: <span id='pid'>82, <a href='https://arxiv.org/pdf/2510.22140.pdf' target='_blank'>https://arxiv.org/pdf/2510.22140.pdf</a></span>   <span><a href='https://github.com/jiangguangan/STG-Avatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Guangan Jiang, Tianzi Zhang, Dong Li, Zhenjun Zhao, Haoang Li, Mingrui Li, Hongyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22140">STG-Avatar: Animatable Human Avatars via Spacetime Gaussian</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic animatable human avatars from monocular videos are crucial for advancing human-robot interaction and enhancing immersive virtual experiences. While recent research on 3DGS-based human avatars has made progress, it still struggles with accurately representing detailed features of non-rigid objects (e.g., clothing deformations) and dynamic regions (e.g., rapidly moving limbs). To address these challenges, we present STG-Avatar, a 3DGS-based framework for high-fidelity animatable human avatar reconstruction. Specifically, our framework introduces a rigid-nonrigid coupled deformation framework that synergistically integrates Spacetime Gaussians (STG) with linear blend skinning (LBS). In this hybrid design, LBS enables real-time skeletal control by driving global pose transformations, while STG complements it through spacetime adaptive optimization of 3D Gaussians. Furthermore, we employ optical flow to identify high-dynamic regions and guide the adaptive densification of 3D Gaussians in these regions. Experimental results demonstrate that our method consistently outperforms state-of-the-art baselines in both reconstruction quality and operational efficiency, achieving superior quantitative metrics while retaining real-time rendering capabilities. Our code is available at https://github.com/jiangguangan/STG-Avatar
<div id='section'>PaperID: <span id='pid'>83, <a href='https://arxiv.org/pdf/2510.21817.pdf' target='_blank'>https://arxiv.org/pdf/2510.21817.pdf</a></span>   <span><a href='https://lxysl.github.io/VITA-E/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoyu Liu, Chaoyou Fu, Chi Yan, Chu Wu, Haihan Gao, Yi-Fan Zhang, Shaoqi Dong, Cheng Qian, Bin Luo, Xiuyong Yang, Guanwu Li, Yusheng Cai, Yunhang Shen, Deqiang Jiang, Haoyu Cao, Xing Sun, Caifeng Shan, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21817">VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.
<div id='section'>PaperID: <span id='pid'>84, <a href='https://arxiv.org/pdf/2510.20685.pdf' target='_blank'>https://arxiv.org/pdf/2510.20685.pdf</a></span>   <span><a href='https://bigtree765.github.io/C-Nav-project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ming-Ming Yu, Fei Zhu, Wenzhuo Liu, Yirong Yang, Qunbo Wang, Wenjun Wu, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20685">C-NAV: Towards Self-Evolving Continual Object Navigation in Open World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents are expected to perform object navigation in dynamic, open-world environments. However, existing approaches typically rely on static trajectories and a fixed set of object categories during training, overlooking the real-world requirement for continual adaptation to evolving scenarios. To facilitate related studies, we introduce the continual object navigation benchmark, which requires agents to acquire navigation skills for new object categories while avoiding catastrophic forgetting of previously learned knowledge. To tackle this challenge, we propose C-Nav, a continual visual navigation framework that integrates two key innovations: (1) A dual-path anti-forgetting mechanism, which comprises feature distillation that aligns multi-modal inputs into a consistent representation space to ensure representation consistency, and feature replay that retains temporal features within the action decoder to ensure policy consistency. (2) An adaptive sampling strategy that selects diverse and informative experiences, thereby reducing redundancy and minimizing memory overhead. Extensive experiments across multiple model architectures demonstrate that C-Nav consistently outperforms existing approaches, achieving superior performance even compared to baselines with full trajectory retention, while significantly lowering memory requirements. The code will be publicly available at https://bigtree765.github.io/C-Nav-project.
<div id='section'>PaperID: <span id='pid'>85, <a href='https://arxiv.org/pdf/2510.20578.pdf' target='_blank'>https://arxiv.org/pdf/2510.20578.pdf</a></span>   <span><a href='https://zterobot.github.io/EmbodiedBrain.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20578">EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at https://zterobot.github.io/EmbodiedBrain.github.io.
<div id='section'>PaperID: <span id='pid'>86, <a href='https://arxiv.org/pdf/2510.19400.pdf' target='_blank'>https://arxiv.org/pdf/2510.19400.pdf</a></span>   <span><a href='https://github.com/microsoft/MV-RoboBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19400">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) are essential to Embodied AI, enabling robots to perceive, reason, and act in complex environments. They also serve as the foundation for the recent Vision-Language-Action (VLA) models. Yet most evaluations of VLMs focus on single-view settings, leaving their ability to integrate multi-view information underexplored. At the same time, multi-camera setups are increasingly standard in robotic platforms, as they provide complementary perspectives to mitigate occlusion and depth ambiguity. Whether VLMs can effectively leverage such multi-view inputs for robotic reasoning therefore remains an open question. To bridge this gap, we introduce MV-RoboBench, a benchmark specifically designed to evaluate the multi-view spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench consists of 1.7k manually curated QA items across eight subtasks, divided into two primary categories: spatial understanding and robotic execution. We evaluate a diverse set of existing VLMs, including both open-source and closed-source models, along with enhanced versions incorporating CoT-inspired techniques. The results show that state-of-the-art models remain far below human performance, underscoring the substantial challenges VLMs face in multi-view robotic perception. Additionally, our analysis uncovers two key findings: (i) spatial intelligence and robotic task execution are positively correlated in multi-view robotic scenarios; and (ii) strong performance on existing general-purpose single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks assessed by our benchmark. We release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing not only data but also a standardized evaluation protocol for multi-view embodied reasoning.
<div id='section'>PaperID: <span id='pid'>87, <a href='https://arxiv.org/pdf/2510.16732.pdf' target='_blank'>https://arxiv.org/pdf/2510.16732.pdf</a></span>   <span><a href='https://github.com/Li-Zn-H/AwesomeWorldModels' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Li-Zn-H/AwesomeWorldModels' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinqing Li, Xin He, Le Zhang, Yun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16732">A Comprehensive Survey on World Models for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.
<div id='section'>PaperID: <span id='pid'>88, <a href='https://arxiv.org/pdf/2510.16281.pdf' target='_blank'>https://arxiv.org/pdf/2510.16281.pdf</a></span>   <span><a href='https://yilin-wu98.github.io/steering-reasoning-vla/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yilin Wu, Anqi Li, Tucker Hermans, Fabio Ramos, Andrea Bajcsy, Claudia P'erez-D'Arpino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16281">Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/
<div id='section'>PaperID: <span id='pid'>89, <a href='https://arxiv.org/pdf/2510.13237.pdf' target='_blank'>https://arxiv.org/pdf/2510.13237.pdf</a></span>   <span><a href='https://edpa-attack.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13237">Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.
<div id='section'>PaperID: <span id='pid'>90, <a href='https://arxiv.org/pdf/2510.13054.pdf' target='_blank'>https://arxiv.org/pdf/2510.13054.pdf</a></span>   <span><a href='https://vla0.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ankit Goyal, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13054">VLA-0: Building State-of-the-Art VLAs with Zero Modification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) hold immense promise for enabling generalist robot manipulation. However, the best way to build them remains an open question. Current approaches often add complexity, such as modifying the existing vocabulary of a Vision-Language Model (VLM) with action tokens or introducing special action heads. Curiously, the simplest strategy of representing actions directly as text has remained largely unexplored. This work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only effective; it is surprisingly powerful. With the right design, VLA-0 outperforms more involved models. On LIBERO, a popular benchmark for evaluating VLAs, VLA-0 outperforms all existing methods trained on the same robotic data, including $π_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without large-scale robotics-specific training, it outperforms methods trained on large-scale robotic data, like $π_0.5$-KI, $π_0$, GR00T-N1 and MolmoAct. These findings also translate to the real world, where VLA-0 outperforms SmolVLA, a VLA model pre-trained on large-scale real data. This paper summarizes our unexpected findings and spells out the specific techniques required to unlock the high performance of this simple yet potent VLA design. Visual results, code, and trained models are provided here: https://vla0.github.io/.
<div id='section'>PaperID: <span id='pid'>91, <a href='https://arxiv.org/pdf/2510.09269.pdf' target='_blank'>https://arxiv.org/pdf/2510.09269.pdf</a></span>   <span><a href='https://goba-attack.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zirun Zhou, Zhengyang Xiao, Haochuan Xu, Jing Sun, Di Wang, Jingfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09269">Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language-action (VLA) models have greatly improved embodied AI, enabling robots to follow natural language instructions and perform diverse tasks. However, their reliance on uncurated training datasets raises serious security concerns. Existing backdoor attacks on VLAs mostly assume white-box access and result in task failures instead of enforcing specific actions. In this work, we reveal a more practical threat: attackers can manipulate VLAs by simply injecting physical objects as triggers into the training dataset. We propose goal-oriented backdoor attacks (GoBA), where the VLA behaves normally in the absence of physical triggers but executes predefined and goal-oriented actions in the presence of physical triggers. Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO that incorporates diverse physical triggers and goal-oriented backdoor actions. In addition, we propose a three-level evaluation that categorizes the victim VLA's actions under GoBA into three states: nothing to do, try to do, and success to do. Experiments show that GoBA enables the victim VLA to successfully achieve the backdoor goal in 97 percentage of inputs when the physical trigger is present, while causing zero performance degradation on clean inputs. Finally, by investigating factors related to GoBA, we find that the action trajectory and trigger color significantly influence attack performance, while trigger size has surprisingly little effect. The code and BadLIBERO dataset are accessible via the project page at https://goba-attack.github.io/.
<div id='section'>PaperID: <span id='pid'>92, <a href='https://arxiv.org/pdf/2510.08713.pdf' target='_blank'>https://arxiv.org/pdf/2510.08713.pdf</a></span>   <span><a href='https://github.com/F1y1113/UniWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifei Dong, Fengyi Wu, Guangyu Chen, Zhi-Qi Cheng, Qiyu Hu, Yuxuan Zhou, Jingdong Sun, Jun-Yan He, Qi Dai, Alexander G Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08713">Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.
<div id='section'>PaperID: <span id='pid'>93, <a href='https://arxiv.org/pdf/2510.07791.pdf' target='_blank'>https://arxiv.org/pdf/2510.07791.pdf</a></span>   <span><a href='https://github.com/X-Luffy/GTR-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07791">GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has attracted much attention due to its importance for Autonomous Driving, Embodied AI and General Artificial Intelligence. Existing spatial-temporal benchmarks mainly focus on egocentric perspective reasoning with images/video context, or geographic perspective reasoning with graphics context (eg. a map), thus fail to assess VLMs' geographic spatial-temporal intelligence with both images/video and graphics context, which is important for areas like traffic management and emergency response. To address the gaps, we introduce Geo-Temporal Reasoning benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of moving targets in a large-scale camera network. GTR-Bench is more challenging as it requires multiple perspective switches between maps and videos, joint reasoning across multiple videos with non-overlapping fields of view, and inference over spatial-temporal regions that are unobserved by any video context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags behind human performance (78.61%) on geo-temporal reasoning. Moreover, our comprehensive analysis on GTR-Bench reveals three primary deficiencies of current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in temporal forecasting, which leads to worse performance on temporal-emphasized tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to comprehend or align the map data with multi-view video inputs. We believe GTR-Bench offers valuable insights and opens up new opportunities for research and applications in spatial-temporal intelligence. Benchmark and code will be released at https://github.com/X-Luffy/GTR-Bench.
<div id='section'>PaperID: <span id='pid'>94, <a href='https://arxiv.org/pdf/2510.06710.pdf' target='_blank'>https://arxiv.org/pdf/2510.06710.pdf</a></span>   <span><a href='https://github.com/RLinf/RLinf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongzhi Zang, Mingjie Wei, Si Xu, Yongji Wu, Zhen Guo, Yuanqing Wang, Hao Lin, Peihong Wang, Liangzhi Shi, Yuqing Xie, Zhexuan Xu, Zhihao Liu, Kang Chen, Wenhao Tang, Quanlu Zhang, Weinan Zhang, Chao Yu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06710">RLinf-VLA: A Unified and Efficient Framework for Reinforcement Learning of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language-action (VLA) models have motivated the extension of their capabilities to embodied settings, where reinforcement learning (RL) offers a principled way to optimize task success through interaction. However, existing methods remain fragmented, lacking both a unified platform for fair comparison across architectures and algorithms and an efficient system design for scalable training. To address these challenges, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. RLinf-VLA achieves unification by providing a unified interface that standardizes the integration of diverse VLA architectures, multiple RL algorithms, and heterogeneous simulators, enabling extensibility. To ensure efficiency, the system adopts a flexible resource allocation architecture for rendering, inference, and training workloads in RL pipelines. In particular, for GPU-parallelized simulators, RLinf-VLA introduces a hybrid fine-grained pipeline allocation strategy, yielding a 1.61x-1.88x training speedup. Using this unified system, models trained with RLinf-VLA demonstrate consistent performance improvements of approximately 20-85% across multiple simulation benchmarks, including LIBERO, ManiSkill, and RoboTwin. Furthermore, we distill a set of training practices for effective RL-based VLA training. We position RLinf-VLA as a foundational system to enable efficient, unified, and reproducible research in embodied intelligence.
<div id='section'>PaperID: <span id='pid'>95, <a href='https://arxiv.org/pdf/2510.05684.pdf' target='_blank'>https://arxiv.org/pdf/2510.05684.pdf</a></span>   <span><a href='https://worv-ai.github.io/d2e/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Suwhan Choi, Jaeyoon Jung, Haebin Seong, Minchan Kim, Minyeong Kim, Yongjun Cho, Yoonshik Kim, Yubeen Park, Youngjae Yu, Yunsung Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05684">D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
<div id='section'>PaperID: <span id='pid'>96, <a href='https://arxiv.org/pdf/2510.04898.pdf' target='_blank'>https://arxiv.org/pdf/2510.04898.pdf</a></span>   <span><a href='https://github.com/MasterXiong/HyperVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04898">HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA
<div id='section'>PaperID: <span id='pid'>97, <a href='https://arxiv.org/pdf/2510.04246.pdf' target='_blank'>https://arxiv.org/pdf/2510.04246.pdf</a></span>   <span><a href='https://huiwon-jang.github.io/contextvla' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04246">ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.
<div id='section'>PaperID: <span id='pid'>98, <a href='https://arxiv.org/pdf/2510.03909.pdf' target='_blank'>https://arxiv.org/pdf/2510.03909.pdf</a></span>   <span><a href='https://hyelinnam.github.io/Cameo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hyelin Nam, Hyojun Go, Byeongjun Park, Byung-Hoon Kim, Hyungjin Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03909">Generating Human Motion Videos using a Cascaded Text-to-Video Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. Despite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. In this work, we propose CAMEO, a cascaded framework for general human motion video generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. Specifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. Furthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. We demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M-VDM combination, while highlighting its versatility across diverse use cases.
<div id='section'>PaperID: <span id='pid'>99, <a href='https://arxiv.org/pdf/2510.03827.pdf' target='_blank'>https://arxiv.org/pdf/2510.03827.pdf</a></span>   <span><a href='https://github.com/Zxy-MLlab/LIBERO-PRO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xueyang Zhou, Yangming Xu, Guiyao Tie, Yongchao Chen, Guowen Zhang, Duanfeng Chu, Pan Zhou, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03827">LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LIBERO has emerged as a widely adopted benchmark for evaluating Vision-Language-Action (VLA) models; however, its current training and evaluation settings are problematic, often leading to inflated performance estimates and preventing fair model comparison. To address these issues, we introduce LIBERO-PRO, an extended LIBERO benchmark that systematically evaluates model performance under reasonable perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments. Experimental results reveal that, although existing models achieve over 90% accuracy under the standard LIBERO evaluation, their performance collapses to 0.0% under our generalized setting. Crucially, this discrepancy exposes the models' reliance on rote memorization of action sequences and environment layouts from the training set, rather than genuine task understanding or environmental perception. For instance, models persist in executing grasping actions when the target object is replaced with irrelevant items, and their outputs remain unchanged even when given corrupted instructions or even messy tokens. These findings expose the severe flaws in current evaluation practices, and we call on the community to abandon misleading methodologies in favor of robust assessments of model generalization and comprehension. Our code is available at: https://github.com/Zxy-MLlab/LIBERO-PRO.
<div id='section'>PaperID: <span id='pid'>100, <a href='https://arxiv.org/pdf/2510.03142.pdf' target='_blank'>https://arxiv.org/pdf/2510.03142.pdf</a></span>   <span><a href='https://pku-epic.github.io/MM-Nav-Web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03142">MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.
<div id='section'>PaperID: <span id='pid'>101, <a href='https://arxiv.org/pdf/2510.03031.pdf' target='_blank'>https://arxiv.org/pdf/2510.03031.pdf</a></span>   <span><a href='https://github.com/test-bai-cpu/LHMP-with-MoDs.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03031">Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term human motion prediction (LHMP) is important for the safe and efficient operation of autonomous robots and vehicles in environments shared with humans. Accurate predictions are important for applications including motion planning, tracking, human-robot interaction, and safety monitoring. In this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or spatio-temporal motion patterns as environment features, to achieve LHMP for horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that supports various types of MoDs and includes a ranking method to output the most likely predicted trajectory, improving practical utility in robotics. Further, a time-conditioned MoD is introduced to capture motion patterns that vary across different times of day. We evaluate MoD-LHMP instantiated with three types of MoDs. Experiments on two real-world datasets show that MoD-informed method outperforms learning-based ones, with up to 50\% improvement in average displacement error, and the time-conditioned variant achieves the highest accuracy overall. Project code is available at https://github.com/test-bai-cpu/LHMP-with-MoDs.git
<div id='section'>PaperID: <span id='pid'>102, <a href='https://arxiv.org/pdf/2510.01642.pdf' target='_blank'>https://arxiv.org/pdf/2510.01642.pdf</a></span>   <span><a href='https://jimntu.github.io/FailSafe/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zijun Lin, Jiafei Duan, Haoquan Fang, Dieter Fox, Ranjay Krishna, Cheston Tan, Bihan Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01642">FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason about and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure-action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arm detect and recover from potential failures, improving the performance of three state-of-the-art VLA models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, and robotic embodiments. We plan to release the FailSafe code to the community.
<div id='section'>PaperID: <span id='pid'>103, <a href='https://arxiv.org/pdf/2510.01623.pdf' target='_blank'>https://arxiv.org/pdf/2510.01623.pdf</a></span>   <span><a href='https://github.com/GigaAI-research/VLA-R1' target='_blank'>  GitHub</a></span> <span><a href='https://gigaai-research.github.io/VLA-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, Zheng Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01623">VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.
<div id='section'>PaperID: <span id='pid'>104, <a href='https://arxiv.org/pdf/2510.01545.pdf' target='_blank'>https://arxiv.org/pdf/2510.01545.pdf</a></span>   <span><a href='https://metadriverse.github.io/ppl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyuan Cai, Zhenghao Peng, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01545">Predictive Preference Learning from Human Interventions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl
<div id='section'>PaperID: <span id='pid'>105, <a href='https://arxiv.org/pdf/2510.00695.pdf' target='_blank'>https://arxiv.org/pdf/2510.00695.pdf</a></span>   <span><a href='https://myungkyukoo.github.io/hamlet/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Myungkyu Koo, Daewon Choi, Taeyoung Kim, Kyungmin Lee, Changyeon Kim, Younggyo Seo, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00695">HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inherently, robotic manipulation tasks are history-dependent: leveraging past context could be beneficial. However, most existing Vision-Language-Action models (VLAs) have been designed without considering this aspect, i.e., they rely solely on the current observation, ignoring preceding context. In this paper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the historical context during action prediction. Specifically, we introduce moment tokens that compactly encode perceptual information at each timestep. Their representations are initialized with time-contrastive learning, allowing them to better capture temporally distinctive aspects. Next, we employ a lightweight memory module that integrates the moment tokens across past timesteps into memory features, which are then leveraged for action prediction. Through empirical evaluation, we show that HAMLET successfully transforms a state-of-the-art VLA into a history-aware policy, especially demonstrating significant improvements on long-horizon tasks that require historical context. In particular, on top of GR00T N1.5, HAMLET achieves an average success rate of 76.4% on history-dependent real-world tasks, surpassing the baseline performance by 47.2%. Furthermore, HAMLET pushes prior art performance from 64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on LIBERO, highlighting its effectiveness even under generic robot-manipulation benchmarks.
<div id='section'>PaperID: <span id='pid'>106, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.
<div id='section'>PaperID: <span id='pid'>107, <a href='https://arxiv.org/pdf/2509.24907.pdf' target='_blank'>https://arxiv.org/pdf/2509.24907.pdf</a></span>   <span><a href='https://github.com/thanhlong103/social-interaction-detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Thanh Long Nguyen, Duc Phu Nguyen, Thanh Thao Ton Nu, Quan Le, Thuan Hoang Tran, Manh Duong Phung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24907">Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>{Recognizing human interactions is essential for social robots as it enables them to navigate safely and naturally in shared environments. Conventional robotic systems however often focus on obstacle avoidance, neglecting social cues necessary for seamless human-robot interaction. To address this gap, we propose a framework to recognize human group interactions for socially aware navigation. Our method utilizes color and depth frames from a monocular RGB-D camera to estimate 3D human keypoints and positions. Principal component analysis (PCA) is then used to determine dominant interaction directions. The shoelace formula is finally applied to compute interest points and engagement areas. Extensive experiments have been conducted to evaluate the validity of the proposed method. The results show that our method is capable of recognizing group interactions across different scenarios with varying numbers of individuals. It also achieves high-speed performance, processing each frame in approximately 4 ms on a single-board computer used in robotic systems. The method is implemented as a ROS 2 package making it simple to integrate into existing navigation systems. Source code is available at https://github.com/thanhlong103/social-interaction-detector
<div id='section'>PaperID: <span id='pid'>108, <a href='https://arxiv.org/pdf/2509.24591.pdf' target='_blank'>https://arxiv.org/pdf/2509.24591.pdf</a></span>   <span><a href='https://haozhuo-zhang.github.io/PoseDiff-project-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24591">PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.
<div id='section'>PaperID: <span id='pid'>109, <a href='https://arxiv.org/pdf/2509.24001.pdf' target='_blank'>https://arxiv.org/pdf/2509.24001.pdf</a></span>   <span><a href='http://github.com/kocurvik/nico_gaze' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Matej Palider, Omar Eldardeer, Viktor Kocur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24001">Gaze Estimation for Human-Robot Interaction: Analysis Using the NICO Platform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper evaluates the current gaze estimation methods within an HRI context of a shared workspace scenario. We introduce a new, annotated dataset collected with the NICO robotic platform. We evaluate four state-of-the-art gaze estimation models. The evaluation shows that the angular errors are close to those reported on general-purpose benchmarks. However, when expressed in terms of distance in the shared workspace the best median error is 16.48 cm quantifying the practical limitations of current methods. We conclude by discussing these limitations and offering recommendations on how to best integrate gaze estimation as a modality in HRI systems.
<div id='section'>PaperID: <span id='pid'>110, <a href='https://arxiv.org/pdf/2509.23931.pdf' target='_blank'>https://arxiv.org/pdf/2509.23931.pdf</a></span>   <span><a href='https://github.com/AutoLab-SAI-SJTU/AutoPrune' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanshi Wang, Yuhao Xu, Zekun Xu, Jin Gao, Yufan Liu, Weiming Hu, Ke Wang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23931">AutoPrune: Each Complexity Deserves a Pruning Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The established redundancy in visual tokens within large vision-language models allows pruning to effectively reduce their substantial computational demands. Previous methods typically employ heuristic layer-specific pruning strategies where, although the number of tokens removed may differ across decoder layers, the overall pruning schedule is fixed and applied uniformly to all input samples and tasks, failing to align token elimination with the model's holistic reasoning trajectory. Cognitive science indicates that human visual processing often begins with broad exploration to accumulate evidence before narrowing focus as the target becomes distinct. Our experiments reveal an analogous pattern in these models. This observation suggests that neither a fixed pruning schedule nor a heuristic layer-wise strategy can optimally accommodate the diverse complexities inherent in different inputs. To overcome this limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a training-free, plug-and-play framework that tailors pruning policies to varying sample and task complexities. Specifically, AutoPrune quantifies the mutual information between visual and textual tokens, then projects this signal to a budget-constrained logistic retention curve. Each such logistic curve, defined by its unique shape, corresponds to the specific complexity of different tasks and can guarantee adherence to predefined computational constraints. We evaluate AutoPrune on standard vision-language tasks and on Vision-Language-Action models for autonomous driving. Notably, when applied to LLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference FLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all tasks. This corresponds to a 9.1% improvement over the recent work PDrop, demonstrating the effectiveness. Code is available at https://github.com/AutoLab-SAI-SJTU/AutoPrune.
<div id='section'>PaperID: <span id='pid'>111, <a href='https://arxiv.org/pdf/2509.23823.pdf' target='_blank'>https://arxiv.org/pdf/2509.23823.pdf</a></span>   <span><a href='https://github.com/Tian-Nian/control_your_robot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tian Nian, Weijie Ke, Yao Mu, Tianxing Chen, Shaolong Zhu, Bingshan Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23823">Control Your Robot: A Unified System for Robot Control and Policy Deployment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms.
<div id='section'>PaperID: <span id='pid'>112, <a href='https://arxiv.org/pdf/2509.23244.pdf' target='_blank'>https://arxiv.org/pdf/2509.23244.pdf</a></span>   <span><a href='https://github.com/MatanShamir1/gr_envs' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MatanShamir1/gr_libs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shamir Matan, Elhadad Osher, Nageris Ben, Mirsky Reuth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23244">Online Dynamic Goal Recognition in Gym Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal Recognition (GR) is the task of inferring an agent's intended goal from partial observations of its behavior, typically in an online and one-shot setting. Despite recent advances in model-free GR, particularly in applications such as human-robot interaction, surveillance, and assistive systems, the field remains fragmented due to inconsistencies in benchmarks, domains, and evaluation protocols. To address this, we introduce gr-libs (https://github.com/MatanShamir1/gr_libs) and gr-envs (https://github.com/MatanShamir1/gr_envs), two complementary open-source frameworks that support the development, evaluation, and comparison of GR algorithms in Gym-compatible environments. gr-libs includes modular implementations of MDP-based GR baselines, diagnostic tools, and evaluation utilities. gr-envs provides a curated suite of environments adapted for dynamic and goal-directed behavior, along with wrappers that ensure compatibility with standard reinforcement learning toolkits. Together, these libraries offer a standardized, extensible, and reproducible platform for advancing GR research. Both packages are open-source and available on GitHub and PyPI.
<div id='section'>PaperID: <span id='pid'>113, <a href='https://arxiv.org/pdf/2509.22093.pdf' target='_blank'>https://arxiv.org/pdf/2509.22093.pdf</a></span>   <span><a href='https://vla-adp.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://vla-adp.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaohuan Pei, Yuxing Chen, Siyu Xu, Yunke Wang, Yuheng Shi, Chang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22093">Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. Motivated by this observation, we propose \textbf{A}ction-aware \textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. Extensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (\textit{e.g.} $1.35 \times$ speed up on OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: \href{https://vla-adp.github.io/}{ADP.com}.
<div id='section'>PaperID: <span id='pid'>114, <a href='https://arxiv.org/pdf/2509.21790.pdf' target='_blank'>https://arxiv.org/pdf/2509.21790.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/Longscape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Shang, Lei Jin, Yiding Ma, Xin Zhang, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21790">LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.
<div id='section'>PaperID: <span id='pid'>115, <a href='https://arxiv.org/pdf/2509.21776.pdf' target='_blank'>https://arxiv.org/pdf/2509.21776.pdf</a></span>   <span><a href='https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hyeonseong Kim, Roy El-Helou, Seungbeen Lee, Sungjoon Choi, Matthew Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21776">The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Playful deception, a common feature in human social interactions, remains underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice Cream (TIC) vendor routine, we investigate how bounded, culturally familiar forms of deception influence user trust, enjoyment, and engagement during robotic handovers. We design a robotic manipulator equipped with a custom end-effector and implement five TIC-inspired trick policies that deceptively delay the handover of an ice cream-shaped object. Through a mixed-design user study with 91 participants, we evaluate the effects of playful deception and interaction duration on user experience. Results reveal that TIC-inspired deception significantly enhances enjoyment and engagement, though reduces perceived safety and trust, suggesting a structured trade-off across the multi-dimensional aspects. Our findings demonstrate that playful deception can be a valuable design strategy for interactive robots in entertainment and engagement-focused contexts, while underscoring the importance of deliberate consideration of its complex trade-offs. You can find more information, including demonstration videos, on https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .
<div id='section'>PaperID: <span id='pid'>116, <a href='https://arxiv.org/pdf/2509.21651.pdf' target='_blank'>https://arxiv.org/pdf/2509.21651.pdf</a></span>   <span><a href='https://asimov-benchmark.github.io/v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Abhishek Jindal, Dmitry Kalashnikov, Oscar Chang, Divya Garikapati, Anirudha Majumdar, Pierre Sermanet, Vikas Sindhwani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21651">Can AI Perceive Physical Danger and Intervene?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2
<div id='section'>PaperID: <span id='pid'>117, <a href='https://arxiv.org/pdf/2509.21377.pdf' target='_blank'>https://arxiv.org/pdf/2509.21377.pdf</a></span>   <span><a href='https://github.com/zzzmmm-svg/DMTF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yinfeng Yu, Hailong Zhang, Meiling Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21377">Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at https://github.com/zzzmmm-svg/DMTF.
<div id='section'>PaperID: <span id='pid'>118, <a href='https://arxiv.org/pdf/2509.20414.pdf' target='_blank'>https://arxiv.org/pdf/2509.20414.pdf</a></span>   <span><a href='https://scene-weaver.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yandan Yang, Baoxiong Jia, Shujie Zhang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20414">SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.
<div id='section'>PaperID: <span id='pid'>119, <a href='https://arxiv.org/pdf/2509.19936.pdf' target='_blank'>https://arxiv.org/pdf/2509.19936.pdf</a></span>   <span><a href='https://github.com/toukapy/capsStare' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Miren Samaniego, Igor Rodriguez, Elena Lazkano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19936">CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze estimation that integrates a ConvNeXt backbone, capsule formation with attention routing, and dual GRU decoders specialized for slow and rapid gaze dynamics. This modular design enables efficient part-whole reasoning and disentangled temporal modeling, achieving state-of-the-art performance on ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference (< 10 ms). The model also generalizes well to unconstrained conditions in Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76), outperforming or matching existing methods with fewer parameters and greater interpretability. These results demonstrate that CapStARE offers a practical and robust solution for real-time gaze estimation in interactive systems. The related code and results for this article can be found on: https://github.com/toukapy/capsStare
<div id='section'>PaperID: <span id='pid'>120, <a href='https://arxiv.org/pdf/2509.17430.pdf' target='_blank'>https://arxiv.org/pdf/2509.17430.pdf</a></span>   <span><a href='https://gchhablani.github.io/embodied-splat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Gunjan Chhablani, Xiaomeng Ye, Muhammad Zubair Irshad, Zsolt Kira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17430">EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.
<div id='section'>PaperID: <span id='pid'>121, <a href='https://arxiv.org/pdf/2509.15333.pdf' target='_blank'>https://arxiv.org/pdf/2509.15333.pdf</a></span>   <span><a href='https://github.com/LeapLabTHU/AdaptiveNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yulin Wang, Yang Yue, Yang Yue, Huanqian Wang, Haojun Jiang, Yizeng Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao, Zhuofan Xia, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15333">Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.
<div id='section'>PaperID: <span id='pid'>122, <a href='https://arxiv.org/pdf/2509.14687.pdf' target='_blank'>https://arxiv.org/pdf/2509.14687.pdf</a></span>   <span><a href='https://terminators2025.github.io/RealMirror.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Cong Tai, Zhaoyu Zheng, Haixu Long, Hansheng Wu, Haodong Xiang, Zhengbin Long, Jun Xiong, Rong Shi, Shizhuang Zhang, Gang Qiu, He Wang, Ruifeng Li, Jun Huang, Bin Chang, Shuai Feng, Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14687">RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io
<div id='section'>PaperID: <span id='pid'>123, <a href='https://arxiv.org/pdf/2509.12594.pdf' target='_blank'>https://arxiv.org/pdf/2509.12594.pdf</a></span>   <span><a href='https://liauto-research.github.io/LightVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan, Peng Jia, Yahui Liu, Sheng Sun, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12594">The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.
<div id='section'>PaperID: <span id='pid'>124, <a href='https://arxiv.org/pdf/2509.12129.pdf' target='_blank'>https://arxiv.org/pdf/2509.12129.pdf</a></span>   <span><a href='https://pku-epic.github.io/NavFoM-Web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12129">Embodied Navigation Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.
<div id='section'>PaperID: <span id='pid'>125, <a href='https://arxiv.org/pdf/2509.11895.pdf' target='_blank'>https://arxiv.org/pdf/2509.11895.pdf</a></span>   <span><a href='https://github.com/m4renz/incremental-scene-graph-prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Marian Renz, Felix Igelbrink, Martin Atzmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11895">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D semantic scene graphs (3DSSG) provide compact structured representations of environments by explicitly modeling objects, attributes, and relationships. While 3DSSGs have shown promise in robotics and embodied AI, many existing methods rely mainly on sensor data, not integrating further information from semantically rich environments. Additionally, most methods assume access to complete scene reconstructions, limiting their applicability in real-world, incremental settings. This paper introduces a novel heterogeneous graph model for incremental 3DSSG prediction that integrates additional, multi-modal information, such as prior observations, directly into the message-passing process. Utilizing multiple layers, the model flexibly incorporates global and local scene representations without requiring specialized modules or full scene reconstructions. We evaluate our approach on the 3DSSG dataset, showing that GNNs enriched with multi-modal information such as semantic embeddings (e.g., CLIP) and prior observations offer a scalable and generalizable solution for complex, real-world environments. The full source code of the presented architecture will be made available at https://github.com/m4renz/incremental-scene-graph-prediction.
<div id='section'>PaperID: <span id='pid'>126, <a href='https://arxiv.org/pdf/2509.11839.pdf' target='_blank'>https://arxiv.org/pdf/2509.11839.pdf</a></span>   <span><a href='https://jiachengliu3.github.io/TrajBooster/' target='_blank'>  GitHub</a></span> <span><a href='https://jiachengliu3.github.io/TrajBooster/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiacheng Liu, Pengxiang Ding, Qihang Zhou, Yuxuan Wu, Da Huang, Zimian Peng, Wei Xiao, Weinan Zhang, Lixin Yang, Cewu Lu, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11839">TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action models show potential to generalize across embodiments but struggle to quickly align with a new robot's action space when high-quality demonstrations are scarce, especially for bipedal humanoids. We present TrajBooster, a cross-embodiment framework that leverages abundant wheeled-humanoid data to boost bipedal VLA. Our key idea is to use end-effector trajectories as a morphology-agnostic interface. TrajBooster (i) extracts 6D dual-arm end-effector trajectories from real-world wheeled humanoids, (ii) retargets them in simulation to Unitree G1 with a whole-body controller trained via a heuristic-enhanced harmonized online DAgger to lift low-dimensional trajectory references into feasible high-dimensional whole-body actions, and (iii) forms heterogeneous triplets that couple source vision/language with target humanoid-compatible actions to post-pre-train a VLA, followed by only 10 minutes of teleoperation data collection on the target humanoid domain. Deployed on Unitree G1, our policy achieves beyond-tabletop household tasks, enabling squatting, cross-height manipulation, and coordinated whole-body motion with markedly improved robustness and generalization. Results show that TrajBooster allows existing wheeled-humanoid data to efficiently strengthen bipedal humanoid VLA performance, reducing reliance on costly same-embodiment data while enhancing action space understanding and zero-shot skill transfer capabilities. For more details, For more details, please refer to our \href{https://jiachengliu3.github.io/TrajBooster/}.
<div id='section'>PaperID: <span id='pid'>127, <a href='https://arxiv.org/pdf/2509.11617.pdf' target='_blank'>https://arxiv.org/pdf/2509.11617.pdf</a></span>   <span><a href='https://github.com/cristina304/AssemMate.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qi Zheng, Chaoran Zhang, Zijian Liang, EnTe Lin, Shubo Cui, Qinghongbing Xie, Zhaobo Xu, Long Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11617">AssemMate: Graph-Based LLM for Robotic Assembly Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM)-based robotic assembly assistance has gained significant research attention. It requires the injection of domain-specific knowledge to guide the assembly process through natural language interaction with humans. Despite some progress, existing methods represent knowledge in the form of natural language text. Due to the long context and redundant content, they struggle to meet the robots' requirements for real-time and precise reasoning. In order to bridge this gap, we present AssemMate, which utilizes the graph\textemdash a concise and accurate form of knowledge representation\textemdash as input. This graph-based LLM enables knowledge graph question answering (KGQA), supporting human-robot interaction and assembly task planning for specific products. Beyond interactive QA, AssemMate also supports sensing stacked scenes and executing grasping to assist with assembly. Specifically, a self-supervised Graph Convolutional Network (GCN) encodes knowledge graph entities and relations into a latent space and aligns them with LLM's representation, enabling the LLM to understand graph information. In addition, a vision-enhanced strategy is employed to address stacked scenes in grasping. Through training and evaluation, AssemMate outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster inference, and 28 times shorter context length, while demonstrating strong generalization ability on random graphs. And our approach further demonstrates superiority through robotic grasping experiments in both simulated and real-world settings. More details can be found on the project page: https://github.com/cristina304/AssemMate.git
<div id='section'>PaperID: <span id='pid'>128, <a href='https://arxiv.org/pdf/2509.10884.pdf' target='_blank'>https://arxiv.org/pdf/2509.10884.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/Nav-R1' target='_blank'>  GitHub</a></span> <span><a href='https://aigeeksgroup.github.io/Nav-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10884">Nav-R1: Reasoning and Navigation in Embodied Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.
<div id='section'>PaperID: <span id='pid'>129, <a href='https://arxiv.org/pdf/2509.08699.pdf' target='_blank'>https://arxiv.org/pdf/2509.08699.pdf</a></span>   <span><a href='https://github.com/podgorki/TANGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08699">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.
<div id='section'>PaperID: <span id='pid'>130, <a href='https://arxiv.org/pdf/2509.07962.pdf' target='_blank'>https://arxiv.org/pdf/2509.07962.pdf</a></span>   <span><a href='https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zongzheng Zhang, Haobo Xu, Zhuo Yang, Chenghao Yue, Zehao Lin, Huan-ang Gao, Ziwei Wang, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07962">TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.
<div id='section'>PaperID: <span id='pid'>131, <a href='https://arxiv.org/pdf/2509.06951.pdf' target='_blank'>https://arxiv.org/pdf/2509.06951.pdf</a></span>   <span><a href='https://aopolin-lv.github.io/F1-VLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06951">F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.
<div id='section'>PaperID: <span id='pid'>132, <a href='https://arxiv.org/pdf/2509.06819.pdf' target='_blank'>https://arxiv.org/pdf/2509.06819.pdf</a></span>   <span><a href='https://utiasDSL.github.io/crisp_controllers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniel San JosÃ© Pro, Oliver HausdÃ¶rfer, Ralf RÃ¶mer, Maximilian DÃ¶sch, Martin Schuck, Angela P. SchÃ¶llig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06819">CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.
<div id='section'>PaperID: <span id='pid'>133, <a href='https://arxiv.org/pdf/2509.05031.pdf' target='_blank'>https://arxiv.org/pdf/2509.05031.pdf</a></span>   <span><a href='https://github.com/lucamuellercode/MMITF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Luca MÃ¼ller, Hassan Ali, Philipp Allgeuer, LukÃ¡Å¡ GajdoÅ¡ech, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05031">Pointing-Guided Target Estimation via Transformer-Based Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.
<div id='section'>PaperID: <span id='pid'>134, <a href='https://arxiv.org/pdf/2509.03383.pdf' target='_blank'>https://arxiv.org/pdf/2509.03383.pdf</a></span>   <span><a href='https://github.com/RLCLab/Annie' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03383">ANNIE: Be Careful of Your Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.
<div id='section'>PaperID: <span id='pid'>135, <a href='https://arxiv.org/pdf/2509.01106.pdf' target='_blank'>https://arxiv.org/pdf/2509.01106.pdf</a></span>   <span><a href='https://robix-seed.github.io/robix/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01106">Robix: A Unified Model for Robot Interaction, Reasoning and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.
<div id='section'>PaperID: <span id='pid'>136, <a href='https://arxiv.org/pdf/2508.19236.pdf' target='_blank'>https://arxiv.org/pdf/2508.19236.pdf</a></span>   <span><a href='https://shihao1895.github.io/MemoryVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19236">MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA
<div id='section'>PaperID: <span id='pid'>137, <a href='https://arxiv.org/pdf/2508.18269.pdf' target='_blank'>https://arxiv.org/pdf/2508.18269.pdf</a></span>   <span><a href='https://irpn-lab.github.io/FlowVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, Haoang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18269">FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/
<div id='section'>PaperID: <span id='pid'>138, <a href='https://arxiv.org/pdf/2508.15874.pdf' target='_blank'>https://arxiv.org/pdf/2508.15874.pdf</a></span>   <span><a href='https://plantpotatoonmoon.github.io/SpatialPolicy/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yijun Liu, Yuwei Liu, Yuan Meng, Jieheng Zhang, Yuwei Zhou, Ye Li, Jiacheng Jiang, Kangye Ji, Shijia Ge, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15874">Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-centric hierarchical embodied models have demonstrated strong potential. However, existing methods lack spatial awareness capabilities, limiting their effectiveness in bridging visual plans to actionable control in complex environments. To address this problem, we propose Spatial Policy (SP), a unified spatial-aware visuomotor robotic manipulation framework via explicit spatial modeling and reasoning. Specifically, we first design a spatial-conditioned embodied video generation module to model spatially guided predictions through the spatial plan table. Then, we propose a flow-based action prediction module to infer executable actions with coordination. Finally, we propose a spatial reasoning feedback policy to refine the spatial plan table via dual-stage replanning. Extensive experiments show that SP substantially outperforms state-of-the-art baselines, achieving over 33% improvement on Meta-World and over 25% improvement on iTHOR, demonstrating strong effectiveness across 23 embodied control tasks. We additionally evaluate SP in real-world robotic experiments to verify its practical viability. SP enhances the practicality of embodied models for robotic control applications. Code and checkpoints are maintained at https://plantpotatoonmoon.github.io/SpatialPolicy/.
<div id='section'>PaperID: <span id='pid'>139, <a href='https://arxiv.org/pdf/2508.15769.pdf' target='_blank'>https://arxiv.org/pdf/2508.15769.pdf</a></span>   <span><a href='https://mengmouxu.github.io/SceneGen' target='_blank'>  GitHub</a></span> <span><a href='https://mengmouxu.github.io/SceneGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15769">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
<div id='section'>PaperID: <span id='pid'>140, <a href='https://arxiv.org/pdf/2508.15354.pdf' target='_blank'>https://arxiv.org/pdf/2508.15354.pdf</a></span>   <span><a href='https://github.com/Franky-X/Awesome-Embodied-Navigation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chaoran Xiong, Yulong Huang, Fangwen Yu, Changhao Chen, Yue Wang, Songpengchen Xia, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15354">Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation (EN) advances traditional navigation by enabling robots to perform complex egocentric tasks through sensing, social, and motion intelligence. In contrast to classic methodologies that rely on explicit localization and pre-defined maps, EN leverages egocentric perception and human-like interaction strategies. This survey introduces a comprehensive EN formulation structured into five stages: Transition, Observation, Fusion, Reward-policy construction, and Action (TOFRA). The TOFRA framework serves to synthesize the current state of the art, provide a critical review of relevant platforms and evaluation metrics, and identify critical open research challenges. A list of studies is available at https://github.com/Franky-X/Awesome-Embodied-Navigation.
<div id='section'>PaperID: <span id='pid'>141, <a href='https://arxiv.org/pdf/2508.13073.pdf' target='_blank'>https://arxiv.org/pdf/2508.13073.pdf</a></span>   <span><a href='https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13073">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation
<div id='section'>PaperID: <span id='pid'>142, <a href='https://arxiv.org/pdf/2508.09621.pdf' target='_blank'>https://arxiv.org/pdf/2508.09621.pdf</a></span>   <span><a href='https://github.com/snt-arg/robot_suite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ingrid MaÃ©va Chekam, Ines Pastor-Martinez, Ali Tourani, Jose Andres Millan-Romera, Laura Ribeiro, Pedro Miguel Bastos Soares, Holger Voos, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09621">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at https://github.com/snt-arg/robot_suite.
<div id='section'>PaperID: <span id='pid'>143, <a href='https://arxiv.org/pdf/2508.09547.pdf' target='_blank'>https://arxiv.org/pdf/2508.09547.pdf</a></span>   <span><a href='https://github.com/F1y1113/GoViG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09547">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.
<div id='section'>PaperID: <span id='pid'>144, <a href='https://arxiv.org/pdf/2508.09071.pdf' target='_blank'>https://arxiv.org/pdf/2508.09071.pdf</a></span>   <span><a href='https://linsun449.github.io/GeoVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09071">GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.
<div id='section'>PaperID: <span id='pid'>145, <a href='https://arxiv.org/pdf/2508.09032.pdf' target='_blank'>https://arxiv.org/pdf/2508.09032.pdf</a></span>   <span><a href='https://ampiromax.github.io/ST-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Maxim A. Patratskiy, Alexey K. Kovalev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09032">Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at https://ampiromax.github.io/ST-VLA.
<div id='section'>PaperID: <span id='pid'>146, <a href='https://arxiv.org/pdf/2508.08252.pdf' target='_blank'>https://arxiv.org/pdf/2508.08252.pdf</a></span>   <span><a href='https://github.com/heshuting555/ReferSplat' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/heshuting555/ReferSplat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08252">ReferSplat: Referring Segmentation in 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.
<div id='section'>PaperID: <span id='pid'>147, <a href='https://arxiv.org/pdf/2508.08189.pdf' target='_blank'>https://arxiv.org/pdf/2508.08189.pdf</a></span>   <span><a href='https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08189">Reinforcement Learning in Vision: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.
<div id='section'>PaperID: <span id='pid'>148, <a href='https://arxiv.org/pdf/2508.07770.pdf' target='_blank'>https://arxiv.org/pdf/2508.07770.pdf</a></span>   <span><a href='https://yizhengzhang1.github.io/agent_world/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yizheng Zhang, Zhenjun Yu, Jiaxin Lai, Cewu Lu, Lei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07770">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment. The code, datasets will be available at https://yizhengzhang1.github.io/agent_world/
<div id='section'>PaperID: <span id='pid'>149, <a href='https://arxiv.org/pdf/2508.06206.pdf' target='_blank'>https://arxiv.org/pdf/2508.06206.pdf</a></span>   <span><a href='https://github.com/hq-King/Affordance-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06206">Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance grounding focuses on predicting the specific regions of objects that are associated with the actions to be performed by robots. It plays a vital role in the fields of human-robot interaction, human-object interaction, embodied manipulation, and embodied perception. Existing models often neglect the affordance shared among different objects because they lack the Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD) generalization and explicit reasoning capabilities. To address these challenges, we propose Affordance-R1, the first unified affordance grounding framework that integrates cognitive CoT guided Group Relative Policy Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we designed a sophisticated affordance function, which contains format, perception, and cognition rewards to effectively guide optimization directions. Furthermore, we constructed a high-quality affordance-centric reasoning dataset, ReasonAff, to support training. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Affordance-R1 achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Comprehensive experiments demonstrate that our model outperforms well-established methods and exhibits open-world generalization. To the best of our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with reasoning into affordance reasoning. The code of our method and our dataset is released on https://github.com/hq-King/Affordance-R1.
<div id='section'>PaperID: <span id='pid'>150, <a href='https://arxiv.org/pdf/2508.05614.pdf' target='_blank'>https://arxiv.org/pdf/2508.05614.pdf</a></span>   <span><a href='https://zju-real.github.io/OmniEmbodied' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ZJU-REAL/OmniEmbodied' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05614">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.
<div id='section'>PaperID: <span id='pid'>151, <a href='https://arxiv.org/pdf/2508.04598.pdf' target='_blank'>https://arxiv.org/pdf/2508.04598.pdf</a></span>   <span><a href='https://NavigationA3.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingfeng Zhang, Xiaoshuai Hao, Yingbo Tang, Haoxiang Fu, Xinyu Zheng, Pengwei Wang, Zhongyuan Wang, Wenbo Ding, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04598">$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation is a fundamental capability of embodied intelligence, enabling robots to move and interact within physical environments. However, existing navigation tasks primarily focus on predefined object navigation or instruction following, which significantly differs from human needs in real-world scenarios involving complex, open-ended scenes. To bridge this gap, we introduce a challenging long-horizon navigation task that requires understanding high-level human instructions and performing spatial-aware object navigation in real-world environments. Existing embodied navigation methods struggle with such tasks due to their limitations in comprehending high-level human instructions and localizing objects with an open vocabulary. In this paper, we propose $NavA^3$, a hierarchical framework divided into two stages: global and local policies. In the global policy, we leverage the reasoning capabilities of Reasoning-VLM to parse high-level human instructions and integrate them with global 3D scene views. This allows us to reason and navigate to regions most likely to contain the goal object. In the local policy, we have collected a dataset of 1.0 million samples of spatial-aware object affordances to train the NaviAfford model (PointingVLM), which provides robust open-vocabulary object localization and spatial awareness for precise goal identification and navigation in complex environments. Extensive experiments demonstrate that $NavA^3$ achieves SOTA results in navigation performance and can successfully complete longhorizon navigation tasks across different robot embodiments in real-world settings, paving the way for universal embodied navigation. The dataset and code will be made available. Project website: https://NavigationA3.github.io/.
<div id='section'>PaperID: <span id='pid'>152, <a href='https://arxiv.org/pdf/2508.02046.pdf' target='_blank'>https://arxiv.org/pdf/2508.02046.pdf</a></span>   <span><a href='https://iron-boyy.github.io/navimaster/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhihao Luo, Wentao Yan abd Jingyu Gong, Min Wang, Zhizhong Zhang, Xuhong Wang, Yuan Xie, Xin Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02046">NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Graphical User Interface (GUI) and embodied navigation have driven significant progress, yet these domains have largely evolved in isolation, with disparate datasets and training paradigms. In this paper, we observe that both tasks can be formulated as Markov Decision Processes (MDP), suggesting a foundational principle for their unification. Hence, we present NaviMaster, the first unified agent capable of seamlessly integrating GUI navigation and embodied navigation within a single framework. Specifically, NaviMaster (i) proposes a visual-target trajectory collection pipeline that generates trajectories for both GUI and embodied tasks in one formulation. (ii) employs a unified reinforcement learning framework on the mix data for better generalization. (iii) designs a novel distance-aware reward to ensure efficient learning from the trajectories. Through extensive experiments on out-of-domain benchmarks, NaviMaster is shown to outperform state-of-the-art agents in GUI navigation, spatial affordance prediction, and embodied navigation. Ablation studies further confirm the efficacy of our unified training strategy, data mixing strategy, and reward design.
<div id='section'>PaperID: <span id='pid'>153, <a href='https://arxiv.org/pdf/2508.01766.pdf' target='_blank'>https://arxiv.org/pdf/2508.01766.pdf</a></span>   <span><a href='https://github.com/farlit/VPN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuo Feng, Zihan Wang, Yuchen Li, Rui Kong, Hengyi Cai, Shuaiqiang Wang, Gim Hee Lee, Piji Li, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01766">VPN: Visual Prompt Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While natural language is commonly used to guide embodied agents, the inherent ambiguity and verbosity of language often hinder the effectiveness of language-guided navigation in complex environments. To this end, we propose Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate using only user-provided visual prompts within 2D top-view maps. This visual prompt primarily focuses on marking the visual navigation trajectory on a top-down view of a scene, offering intuitive and spatially grounded guidance without relying on language instructions. It is more friendly for non-expert users and reduces interpretive ambiguity. We build VPN tasks in both discrete and continuous navigation settings, constructing two new datasets, R2R-VP and R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network to handle the VPN tasks, with two data augmentation strategies: view-level augmentation (altering initial headings and prompt orientations) and trajectory-level augmentation (incorporating diverse trajectories from large-scale 3D scenes), to enhance navigation performance. Extensive experiments evaluate how visual prompt forms, top-view map formats, and data augmentation strategies affect the performance of visual prompt navigation. The code is available at https://github.com/farlit/VPN.
<div id='section'>PaperID: <span id='pid'>154, <a href='https://arxiv.org/pdf/2508.01126.pdf' target='_blank'>https://arxiv.org/pdf/2508.01126.pdf</a></span>   <span><a href='https://chaitanya100100.github.io/UniEgoMotion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01126">UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.
<div id='section'>PaperID: <span id='pid'>155, <a href='https://arxiv.org/pdf/2508.00823.pdf' target='_blank'>https://arxiv.org/pdf/2508.00823.pdf</a></span>   <span><a href='https://gwxuan.github.io/IGL-Nav/' target='_blank'>  GitHub</a></span> <span><a href='https://gwxuan.github.io/IGL-Nav/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenxuan Guo, Xiuwei Xu, Hang Yin, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00823">IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.
<div id='section'>PaperID: <span id='pid'>156, <a href='https://arxiv.org/pdf/2508.00400.pdf' target='_blank'>https://arxiv.org/pdf/2508.00400.pdf</a></span>   <span><a href='https://github.com/upeee/sari-sandbox-env' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Janika Deborah Gajo, Gerarld Paul Merales, Jerome Escarcha, Brenden Ashley Molina, Gian Nartea, Emmanuel G. Maminta, Juan Carlos Roldan, Rowel O. Atienza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00400">Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks. Addressing a gap in retail-specific sim environments for embodied agent training, Sari Sandbox features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. We also introduce SariBench, a dataset of annotated human demonstrations across varied task difficulties. Our sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance. We conclude with benchmarks, performance analysis, and recommendations for enhancing realism and scalability. The source code can be accessed via https://github.com/upeee/sari-sandbox-env.
<div id='section'>PaperID: <span id='pid'>157, <a href='https://arxiv.org/pdf/2508.00097.pdf' target='_blank'>https://arxiv.org/pdf/2508.00097.pdf</a></span>   <span><a href='https://github.com/XR-Robotics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhigen Zhao, Liuchuan Yu, Ke Jing, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00097">XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.
<div id='section'>PaperID: <span id='pid'>158, <a href='https://arxiv.org/pdf/2507.22522.pdf' target='_blank'>https://arxiv.org/pdf/2507.22522.pdf</a></span>   <span><a href='https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyi Wang, Peiming Li, Hong Liu, Zhichao Deng, Can Wang, Jun Liu, Junsong Yuan, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22522">Recognizing Actions from Robotic View for Natural Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural Human-Robot Interaction (N-HRI) requires robots to recognize human actions at varying distances and states, regardless of whether the robot itself is in motion or stationary. This setup is more flexible and practical than conventional human action recognition tasks. However, existing benchmarks designed for traditional action recognition fail to address the unique complexities in N-HRI due to limited data, modalities, task categories, and diversity of subjects and environments. To address these challenges, we introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored specifically for perception-centric robotic views prevalent in mobile service robots. ACTIVE comprises 30 composite action categories, 80 participants, and 46,868 annotated video instances, covering both RGB and point cloud modalities. Participants performed various human actions in diverse environments at distances ranging from 3m to 50m, while the camera platform was also mobile, simulating real-world scenarios of robot perception with varying camera heights due to uneven ground. This comprehensive and challenging benchmark aims to advance action and attribute recognition research in N-HRI. Furthermore, we propose ACTIVE-PC, a method that accurately perceives human actions at long distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic Ellipse Query, and precise decoupling of kinematic interference from human actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our code is available at: https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.
<div id='section'>PaperID: <span id='pid'>159, <a href='https://arxiv.org/pdf/2507.21045.pdf' target='_blank'>https://arxiv.org/pdf/2507.21045.pdf</a></span>   <span><a href='https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowen Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21045">Reconstructing 4D Spatial Intelligence: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.
<div id='section'>PaperID: <span id='pid'>160, <a href='https://arxiv.org/pdf/2507.19684.pdf' target='_blank'>https://arxiv.org/pdf/2507.19684.pdf</a></span>   <span><a href='https://rosielab.github.io/compas3d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Bermet Burkanova, Payam Jome Yazdian, Chuxuan Zhang, Trinity Evans, Paige TuttÃ¶sÃ­, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19684">Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.
<div id='section'>PaperID: <span id='pid'>161, <a href='https://arxiv.org/pdf/2507.17294.pdf' target='_blank'>https://arxiv.org/pdf/2507.17294.pdf</a></span>   <span><a href='https://github.com/jxbi1010/VLA-Touch' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jxbi1010/VLA-Touch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianxin Bi, Kevin Yuchen Ma, Ce Hao, Mike Zheng Shou, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17294">VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile feedback is generally recognized to be crucial for effective interaction with the physical world. However, state-of-the-art Vision-Language-Action (VLA) models lack the ability to interpret and use tactile signals, limiting their effectiveness in contact-rich tasks. Incorporating tactile feedback into these systems is challenging due to the absence of large multi-modal datasets. We present VLA-Touch, an approach that enhances generalist robot policies with tactile sensing \emph{without fine-tuning} the base VLA. Our method introduces two key innovations: (1) a pipeline that leverages a pretrained tactile-language model that provides semantic tactile feedback for high-level task planning, and (2) a diffusion-based controller that refines VLA-generated actions with tactile signals for contact-rich manipulation. Through real-world experiments, we demonstrate that our dual-level integration of tactile feedback improves task planning efficiency while enhancing execution precision. Code is open-sourced at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.
<div id='section'>PaperID: <span id='pid'>162, <a href='https://arxiv.org/pdf/2507.16815.pdf' target='_blank'>https://arxiv.org/pdf/2507.16815.pdf</a></span>   <span><a href='https://jasper0314-huang.github.io/thinkact-vla/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16815">ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.
<div id='section'>PaperID: <span id='pid'>163, <a href='https://arxiv.org/pdf/2507.14049.pdf' target='_blank'>https://arxiv.org/pdf/2507.14049.pdf</a></span>   <span><a href='https://github.com/kscalelabs/evla' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/kscalelabs/evla ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>PaweÅ Budzianowski, Wesley Maa, Matthew Freed, Jingxiang Mo, Winston Hsiao, Aaron Xie, Tomasz MÅoduchowski, Viraj Tipnis, Benjamin Bolte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14049">EdgeVLA: Efficient Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have emerged as a promising approach to address the data scarcity challenge in robotics, enabling the development of generalizable visuomotor control policies. While models like OpenVLA showcase the potential of this paradigm, deploying large-scale VLMs on resource-constrained mobile manipulation systems remains a significant hurdle. This paper introduces Edge VLA (EVLA), a novel approach designed to significantly enhance the inference speed of Vision-Language-Action (VLA) models. EVLA maintains the representational power of these models while enabling real-time performance on edge devices. We achieve this through two key innovations: 1) Eliminating the autoregressive requirement for end-effector position prediction, leading to a 7x speedup in inference, and 2) Leveraging the efficiency of Small Language Models (SLMs), demonstrating comparable training performance to larger models with significantly reduced computational demands. Our early results demonstrate that EVLA achieves comparable training characteristics to OpenVLA while offering substantial gains in inference speed and memory efficiency. We release our model checkpoints and training \href{https://github.com/kscalelabs/evla }{codebase} to foster further research.
<div id='section'>PaperID: <span id='pid'>164, <a href='https://arxiv.org/pdf/2507.12440.pdf' target='_blank'>https://arxiv.org/pdf/2507.12440.pdf</a></span>   <span><a href='https://rchalyang.github.io/EgoVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Xuxin Cheng, Ri-Zhao Qiu, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12440">EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: https://rchalyang.github.io/EgoVLA
<div id='section'>PaperID: <span id='pid'>165, <a href='https://arxiv.org/pdf/2507.08496.pdf' target='_blank'>https://arxiv.org/pdf/2507.08496.pdf</a></span>   <span><a href='https://github.com/sunshibo1234/LLaPa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shibo Sun, Xue Li, Donglin Di, Mingjie Wei, Lanshun Nie, Wei-Nan Zhang, Dechen Zhan, Yang Song, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08496">LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) have advanced procedural planning for embodied AI systems through strong reasoning abilities, the integration of multimodal inputs and counterfactual reasoning remains underexplored. To tackle these challenges, we introduce LLaPa, a vision-language model framework designed for multimodal procedural planning. LLaPa generates executable action sequences from textual task descriptions and visual environmental images using vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary modules to improve procedural planning. The first module, the Task-Environment Reranker (TER), leverages task-oriented segmentation to create a task-sensitive feature space, aligning textual descriptions with visual environments and emphasizing critical regions for procedural execution. The second module, the Counterfactual Activities Retriever (CAR), identifies and emphasizes potential counterfactual conditions, enhancing the model's reasoning capability in counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED benchmarks demonstrate that LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models. The code and models are available https://github.com/sunshibo1234/LLaPa.
<div id='section'>PaperID: <span id='pid'>166, <a href='https://arxiv.org/pdf/2507.07781.pdf' target='_blank'>https://arxiv.org/pdf/2507.07781.pdf</a></span>   <span><a href='https://github.com/liziwennba/SUPRISE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaxin Huang, Ziwen Li, Hanlve Zhang, Runnan Chen, Xiao He, Yandong Guo, Wenping Wang, Tongliang Liu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07781">SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of language and 3D perception is critical for embodied AI and robotic systems to perceive, understand, and interact with the physical world. Spatial reasoning, a key capability for understanding spatial relationships between objects, remains underexplored in current 3D vision-language research. Existing datasets often mix semantic cues (e.g., object name) with spatial context, leading models to rely on superficial shortcuts rather than genuinely interpreting spatial relationships. To address this gap, we introduce S\textsc{urprise}3D, a novel dataset designed to evaluate language-guided spatial reasoning segmentation in complex 3D scenes. S\textsc{urprise}3D consists of more than 200k vision language pairs across 900+ detailed indoor scenes from ScanNet++ v2, including more than 2.8k unique object classes. The dataset contains 89k+ human-annotated spatial queries deliberately crafted without object name, thereby mitigating shortcut biases in spatial understanding. These queries comprehensively cover various spatial reasoning skills, such as relative position, narrative perspective, parametric perspective, and absolute distance reasoning. Initial benchmarks demonstrate significant challenges for current state-of-the-art expert 3D visual grounding methods and 3D-LLMs, underscoring the necessity of our dataset and the accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite. S\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially aware AI, paving the way for effective embodied interaction and robotic planning. The code and datasets can be found in https://github.com/liziwennba/SUPRISE.
<div id='section'>PaperID: <span id='pid'>167, <a href='https://arxiv.org/pdf/2507.05763.pdf' target='_blank'>https://arxiv.org/pdf/2507.05763.pdf</a></span>   <span><a href='https://dream-art-0.github.io/DreamArt/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05763">DreamArt: Generating Interactable Articulated Objects from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.
<div id='section'>PaperID: <span id='pid'>168, <a href='https://arxiv.org/pdf/2507.05555.pdf' target='_blank'>https://arxiv.org/pdf/2507.05555.pdf</a></span>   <span><a href='https://uiuckimlab.github.io/paprle-pages' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Obin Kwon, Sankalp Yamsani, Noboru Myers, Sean Taylor, Jooyoung Hong, Kyungseo Park, Alex Alspach, Joohyung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05555">PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular ecosystem that enables flexible placement and control of robotic limbs. With PAPRLE, a user can change the arrangement of the robotic limbs, and control them using a variety of input devices, including puppeteers, gaming controllers, and VR-based interfaces. This versatility supports a wide range of teleoperation scenarios and promotes adaptability to different task requirements. To further enhance configurability, we introduce a pluggable puppeteer device that can be easily mounted and adapted to match the target robot configurations. PAPRLE supports bilateral teleoperation through these puppeteer devices, agnostic to the type or configuration of the follower robot. By supporting both joint-space and task-space control, the system provides real-time force feedback, improving user fidelity and physical interaction awareness. The modular design of PAPRLE facilitates novel spatial arrangements of the limbs and enables scalable data collection, thereby advancing research in embodied AI and learning-based control. We validate PAPRLE in various real-world settings, demonstrating its versatility across diverse combinations of leader devices and follower robots. The system will be released as open source, including both hardware and software components, to support broader adoption and community-driven extension. Additional resources and demonstrations are available at the project website: https://uiuckimlab.github.io/paprle-pages
<div id='section'>PaperID: <span id='pid'>169, <a href='https://arxiv.org/pdf/2507.04789.pdf' target='_blank'>https://arxiv.org/pdf/2507.04789.pdf</a></span>   <span><a href='https://t2-vlm.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yinuo Zhao, Jiale Yuan, Zhiyuan Xu, Xiaoshuai Hao, Xinyi Zhang, Kun Wu, Zhengping Che, Chi Harold Liu, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04789">Training-free Generation of Temporally Consistent Rewards from VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language models (VLMs) have significantly improved performance in embodied tasks such as goal decomposition and visual comprehension. However, providing accurate rewards for robotic manipulation without fine-tuning VLMs remains challenging due to the absence of domain-specific robotic knowledge in pre-trained datasets and high computational costs that hinder real-time applicability. To address this, we propose $\mathrm{T}^2$-VLM, a novel training-free, temporally consistent framework that generates accurate rewards through tracking the status changes in VLM-derived subgoals. Specifically, our method first queries the VLM to establish spatially aware subgoals and an initial completion estimate before each round of interaction. We then employ a Bayesian tracking algorithm to update the goal completion status dynamically, using subgoal hidden states to generate structured rewards for reinforcement learning (RL) agents. This approach enhances long-horizon decision-making and improves failure recovery capabilities with RL. Extensive experiments indicate that $\mathrm{T}^2$-VLM achieves state-of-the-art performance in two robot manipulation benchmarks, demonstrating superior reward accuracy with reduced computation consumption. We believe our approach not only advances reward generation techniques but also contributes to the broader field of embodied AI. Project website: https://t2-vlm.github.io/.
<div id='section'>PaperID: <span id='pid'>170, <a href='https://arxiv.org/pdf/2507.01016.pdf' target='_blank'>https://arxiv.org/pdf/2507.01016.pdf</a></span>   <span><a href='https://xiaoxiao0406.github.io/vqvla.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01016">VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io
<div id='section'>PaperID: <span id='pid'>171, <a href='https://arxiv.org/pdf/2507.00917.pdf' target='_blank'>https://arxiv.org/pdf/2507.00917.pdf</a></span>   <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00917">A Survey: Learning Embodied Intelligence from Physical Simulators and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.
<div id='section'>PaperID: <span id='pid'>172, <a href='https://arxiv.org/pdf/2506.24044.pdf' target='_blank'>https://arxiv.org/pdf/2506.24044.pdf</a></span>   <span><a href='https://github.com/JohnsonJiang1996/Awesome-VLA4AD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/JohnsonJiang1996/Awesome-VLA4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Sicong Jiang, Zilin Huang, Kangan Qian, Ziang Luo, Tianze Zhu, Yang Zhong, Yihong Tang, Menglin Kong, Yunlong Wang, Siwen Jiao, Hao Ye, Zihao Sheng, Xin Zhao, Tuopu Wen, Zheng Fu, Sikai Chen, Kun Jiang, Diange Yang, Seongjin Choi, Lijun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24044">A Survey on Vision-Language-Action Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.
<div id='section'>PaperID: <span id='pid'>173, <a href='https://arxiv.org/pdf/2506.23852.pdf' target='_blank'>https://arxiv.org/pdf/2506.23852.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/RGC-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianing Jin, Jiangyong Ying, Huiyu Duan, Liu Yang, Sijing Wu, Yunhao Li, Yushuo Zheng, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23852">RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As camera-equipped robotic platforms become increasingly integrated into daily life, robotic-generated videos have begun to appear on streaming media platforms, enabling us to envision a future where humans and robots coexist. We innovatively propose the concept of Robotic-Generated Content (RGC) to term these videos generated from egocentric perspective of robots. The perceptual quality of RGC videos is critical in human-robot interaction scenarios, and RGC videos exhibit unique distortions and visual requirements that differ markedly from those of professionally-generated content (PGC) videos and user-generated content (UGC) videos. However, dedicated research on quality assessment of RGC videos is still lacking. To address this gap and to support broader robotic applications, we establish the first Robotic-Generated Content Database (RGCD), which contains a total of 2,100 videos drawn from three robot categories and sourced from diverse platforms. A subjective VQA experiment is conducted subsequently to assess human visual perception of robotic-generated videos. Finally, we conduct a benchmark experiment to evaluate the performance of 11 state-of-the-art VQA models on our database. Experimental results reveal significant limitations in existing VQA models when applied to complex, robotic-generated content, highlighting a critical need for RGC-specific VQA models. Our RGCD is publicly available at: https://github.com/IntMeGroup/RGC-VQA.
<div id='section'>PaperID: <span id='pid'>174, <a href='https://arxiv.org/pdf/2506.23351.pdf' target='_blank'>https://arxiv.org/pdf/2506.23351.pdf</a></span>   <span><a href='https://robotwin-benchmark.github.io/cvpr-2025-challenge/' target='_blank'>  GitHub</a></span> <span><a href='https://robotwin-benchmark.github.io/cvpr-2025-challenge/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianxing Chen, Kaixuan Wang, Zhaohui Yang, Yuhao Zhang, Zanxin Chen, Baijun Chen, Wanxi Dong, Ziyuan Liu, Dong Chen, Tianshuo Yang, Haibao Yu, Xiaokang Yang, Yusen Qin, Zhiqiang Xie, Yao Mu, Ping Luo, Tian Nian, Weiliang Deng, Yiheng Ge, Yibin Liu, Zixuan Li, Dehui Wang, Zhixuan Liang, Haohui Xie, Rijie Zeng, Yunfei Ge, Peiqing Cong, Guannan He, Zhaoming Han, Ruocheng Yin, Jingxiang Guo, Lunkai Lin, Tianling Xu, Hongzhe Bi, Xuewu Lin, Tianwei Lin, Shujie Luo, Keyu Li, Ziyan Zhao, Ke Fan, Heyang Xu, Bo Peng, Wenlong Gao, Dongjiang Li, Feng Jin, Hui Shen, Jinming Li, Chaowei Cui, Yu Chen, Yaxin Peng, Lingdong Zeng, Wenlong Dong, Tengfei Li, Weijie Ke, Jun Chen, Erdemt Bao, Tian Lan, Tenglong Liu, Jin Yang, Huiping Zhuang, Baozhi Jia, Shuai Zhang, Zhengfeng Zou, Fangheng Guan, Tianyi Jia, Ke Zhou, Hongjiu Zhang, Yating Han, Cheng Fang, Yixian Zou, Chongyang Xu, Qinglun Zhang, Shen Cheng, Xiaohe Wang, Ping Tan, Haoqiang Fan, Shuaicheng Liu, Jiaheng Chen, Chuxuan Huang, Chengliang Lin, Kaijun Luo, Boyu Yue, Yi Liu, Jinyu Chen, Zichang Tan, Liming Deng, Shuo Xu, Zijian Cai, Shilong Yin, Hao Wang, Hongshan Liu, Tianyang Li, Long Shi, Ran Xu, Huilin Xu, Zhengquan Zhang, Congsheng Xu, Jinchang Yang, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23351">Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.
<div id='section'>PaperID: <span id='pid'>175, <a href='https://arxiv.org/pdf/2506.23135.pdf' target='_blank'>https://arxiv.org/pdf/2506.23135.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/RoboScape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23135">RoboScape: Physics-informed Embodied World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.
<div id='section'>PaperID: <span id='pid'>176, <a href='https://arxiv.org/pdf/2506.20566.pdf' target='_blank'>https://arxiv.org/pdf/2506.20566.pdf</a></span>   <span><a href='https://github.com/interaction-lab/HRIBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhonghao Shi, Enyu Zhao, Nathaniel Dennler, Jingzhen Wang, Xinyang Xu, Kaleen Shrestha, Mengxue Fu, Daniel Seita, Maja MatariÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20566">HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time human perception is crucial for effective human-robot interaction (HRI). Large vision-language models (VLMs) offer promising generalizable perceptual capabilities but often suffer from high latency, which negatively impacts user experience and limits VLM applicability in real-world scenarios. To systematically study VLM capabilities in human perception for HRI and performance-latency trade-offs, we introduce HRIBench, a visual question-answering (VQA) benchmark designed to evaluate VLMs across a diverse set of human perceptual tasks critical for HRI. HRIBench covers five key domains: (1) non-verbal cue understanding, (2) verbal instruction understanding, (3) human-robot object relationship understanding, (4) social navigation, and (5) person identification. To construct HRIBench, we collected data from real-world HRI environments to curate questions for non-verbal cue understanding, and leveraged publicly available datasets for the remaining four domains. We curated 200 VQA questions for each domain, resulting in a total of 1000 questions for HRIBench. We then conducted a comprehensive evaluation of both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench. Our results show that, despite their generalizability, current VLMs still struggle with core perceptual capabilities essential for HRI. Moreover, none of the models within our experiments demonstrated a satisfactory performance-latency trade-off suitable for real-time deployment, underscoring the need for future research on developing smaller, low-latency VLMs with improved human perception capabilities. HRIBench and our results can be found in this Github repository: https://github.com/interaction-lab/HRIBench.
<div id='section'>PaperID: <span id='pid'>177, <a href='https://arxiv.org/pdf/2506.20487.pdf' target='_blank'>https://arxiv.org/pdf/2506.20487.pdf</a></span>   <span><a href='https://github.com/yuanmingqi/awesome-bfm-papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingqi Yuan, Tao Yu, Wenqi Ge, Xiuyong Yao, Huijiang Wang, Jiayu Chen, Xin Jin, Bo Li, Hua Chen, Wei Zhang, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20487">A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and long-term list of BFM papers and projects to facilitate more subsequent research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.
<div id='section'>PaperID: <span id='pid'>178, <a href='https://arxiv.org/pdf/2506.20066.pdf' target='_blank'>https://arxiv.org/pdf/2506.20066.pdf</a></span>   <span><a href='https://github.com/hsiangwei0903/ToSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hsiang-Wei Huang, Wenhao Chai, Kuang-Ming Chen, Cheng-Yen Yang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20066">ToSA: Token Merging with Spatial Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Token merging has emerged as an effective strategy to accelerate Vision Transformers (ViT) by reducing computational costs. However, existing methods primarily rely on the visual token's feature similarity for token merging, overlooking the potential of integrating spatial information, which can serve as a reliable criterion for token merging in the early layers of ViT, where the visual tokens only possess weak visual information. In this paper, we propose ToSA, a novel token merging method that combines both semantic and spatial awareness to guide the token merging process. ToSA leverages the depth image as input to generate pseudo spatial tokens, which serve as auxiliary spatial information for the visual token merging process. With the introduced spatial awareness, ToSA achieves a more informed merging strategy that better preserves critical scene structure. Experimental results demonstrate that ToSA outperforms previous token merging methods across multiple benchmarks on visual and embodied question answering while largely reducing the runtime of the ViT, making it an efficient solution for ViT acceleration. The code will be available at: https://github.com/hsiangwei0903/ToSA
<div id='section'>PaperID: <span id='pid'>179, <a href='https://arxiv.org/pdf/2506.18904.pdf' target='_blank'>https://arxiv.org/pdf/2506.18904.pdf</a></span>   <span><a href='https://github.com/Linketic/TC-Light' target='_blank'>  GitHub</a></span> <span><a href='https://dekuliutesla.github.io/tclight/' target='_blank'>  GitHub</a></span> <span><a href='https://dekuliutesla.github.io/tclight/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Liu, Chuanchen Luo, Zimo Tang, Yingyan Li, Yuran Yang, Yuanyong Ning, Lue Fan, Zhaoxiang Zhang, Junran Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18904">TC-Light: Temporally Coherent Generative Rendering for Realistic World Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Illumination and texture editing are critical dimensions for world-to-world transfer, which is valuable for applications including sim2real and real2real visual data scaling up for embodied AI. Existing techniques generatively re-render the input video to realize the transfer, such as video relighting models and conditioned world generation models. Nevertheless, these models are predominantly limited to the domain of training data (e.g., portrait) or fall into the bottleneck of temporal consistency and computation efficiency, especially when the input video involves complex dynamics and long durations. In this paper, we propose TC-Light, a novel generative renderer to overcome these problems. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible re-rendering results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.
<div id='section'>PaperID: <span id='pid'>180, <a href='https://arxiv.org/pdf/2506.18448.pdf' target='_blank'>https://arxiv.org/pdf/2506.18448.pdf</a></span>   <span><a href='https://zquang2202.github.io/GraspMAS/' target='_blank'>  GitHub</a></span> <span><a href='https://zquang2202.github.io/GraspMAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Quang Nguyen, Tri Le, Huy Nguyen, Thieu Vo, Tung D. Ta, Baoru Huang, Minh N. Vu, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18448">GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach. Our project page is available at https://zquang2202.github.io/GraspMAS
<div id='section'>PaperID: <span id='pid'>181, <a href='https://arxiv.org/pdf/2506.16402.pdf' target='_blank'>https://arxiv.org/pdf/2506.16402.pdf</a></span>   <span><a href='https://github.com/AI45Lab/IS-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoya Lu, Zeren Chen, Xuhao Hu, Yijin Zhou, Weichen Zhang, Dongrui Liu, Lu Sheng, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16402">IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under [this https URL](https://github.com/AI45Lab/IS-Bench).
<div id='section'>PaperID: <span id='pid'>182, <a href='https://arxiv.org/pdf/2506.16012.pdf' target='_blank'>https://arxiv.org/pdf/2506.16012.pdf</a></span>   <span><a href='https://github.com/ds199895/DualTHOR.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Boyu Li, Siyuan He, Hang Xu, Haoqi Yuan, Yu Zang, Liwei Hu, Junpeng Yue, Zhenxiong Jiang, Pengbo Hu, BÃ¶rje F. Karlsson, Yehui Tang, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16012">DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains a fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present a physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, a task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce a contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables a more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git.
<div id='section'>PaperID: <span id='pid'>183, <a href='https://arxiv.org/pdf/2506.15610.pdf' target='_blank'>https://arxiv.org/pdf/2506.15610.pdf</a></span>   <span><a href='https://lanlan96.github.io/BoxFusion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqing Lan, Chenyang Zhu, Zhirui Gao, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15610">BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary 3D object detection has gained significant interest due to its critical applications in autonomous driving and embodied AI. Existing detection methods, whether offline or online, typically rely on dense point cloud reconstruction, which imposes substantial computational overhead and memory constraints, hindering real-time deployment in downstream tasks. To address this, we propose a novel reconstruction-free online framework tailored for memory-efficient and real-time 3D detection. Specifically, given streaming posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual foundation model (VFM) for single-view 3D object detection by bounding boxes, coupled with CLIP to capture open-vocabulary semantics of detected objects. To fuse all detected bounding boxes across different views into a unified one, we employ an association module for correspondences of multi-views and an optimization module to fuse the 3D bounding boxes of the same instance predicted in multi-views. The association module utilizes 3D Non-Maximum Suppression (NMS) and a box correspondence matching module, while the optimization module uses an IoU-guided efficient random optimization technique based on particle filtering to enforce multi-view consistency of the 3D bounding boxes while minimizing computational complexity. Extensive experiments on ScanNetV2 and CA-1M datasets demonstrate that our method achieves state-of-the-art performance among online methods. Benefiting from this novel reconstruction-free paradigm for 3D object detection, our method exhibits great generalization abilities in various scenarios, enabling real-time perception even in environments exceeding 1000 square meters.
<div id='section'>PaperID: <span id='pid'>184, <a href='https://arxiv.org/pdf/2506.14317.pdf' target='_blank'>https://arxiv.org/pdf/2506.14317.pdf</a></span>   <span><a href='https://clutterdexgrasp.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zeyuan Chen, Qiyang Yan, Yuanpei Chen, Tianhao Wu, Jiyao Zhang, Zihan Ding, Jinzhou Li, Yaodong Yang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14317">ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a geometry and spatially-embedded scene representation and a novel comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/.
<div id='section'>PaperID: <span id='pid'>185, <a href='https://arxiv.org/pdf/2506.13751.pdf' target='_blank'>https://arxiv.org/pdf/2506.13751.pdf</a></span>   <span><a href='https://ember-lab-berkeley.github.io/LeVERB-Website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoru Xue, Xiaoyu Huang, Dantong Niu, Qiayuan Liao, Thomas Kragerud, Jan Tommy Gravdahl, Xue Bin Peng, Guanya Shi, Trevor Darrell, Koushil Sreenath, Shankar Sastry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13751">LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action "vocabulary" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.
<div id='section'>PaperID: <span id='pid'>186, <a href='https://arxiv.org/pdf/2506.13045.pdf' target='_blank'>https://arxiv.org/pdf/2506.13045.pdf</a></span>   <span><a href='https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13045">Continual Learning for Generative AI: From LLMs to MLLMs and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative models has empowered modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models are fundamentally constrained by \emph{catastrophic forgetting}, \ie~a persistent challenge where models experience performance degradation on previously learned tasks when adapting to new tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative AI in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative AI models, encompassing large language models, multimodal large language models, vision-language-action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, thereby providing deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.
<div id='section'>PaperID: <span id='pid'>187, <a href='https://arxiv.org/pdf/2506.10600.pdf' target='_blank'>https://arxiv.org/pdf/2506.10600.pdf</a></span>   <span><a href='https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinjie Wang, Liu Liu, Yu Cao, Ruiqi Wu, Wenkang Qin, Dehui Wang, Wei Sui, Zhizhong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10600">EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.
<div id='section'>PaperID: <span id='pid'>188, <a href='https://arxiv.org/pdf/2506.10389.pdf' target='_blank'>https://arxiv.org/pdf/2506.10389.pdf</a></span>   <span><a href='https://github.com/UNITES-Lab/EQA-RM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhang Chen, Zhen Tan, Tianlong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10389">EQA-RM: A Generative Embodied Reward Model with Test-time Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward Models (RMs), vital for large model alignment, are underexplored for complex embodied tasks like Embodied Question Answering (EQA) where nuanced evaluation of agents' spatial, temporal, and logical understanding is critical yet not considered by generic approaches. We introduce EQA-RM, a novel generative multimodal reward model specifically architected for EQA, trained via our innovative Contrastive Group Relative Policy Optimization (C-GRPO) strategy to learn fine-grained behavioral distinctions. The generative nature of EQA-RM provides interpretable, structured reward feedback (beyond simple scalars), uniquely enabling test-time scaling to dynamically adjust evaluation granularity, from concise scores to detailed critiques of reasoning and grounding, at inference without retraining. Concurrently, we introduce EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700 samples, outperforming strong proprietary baselines, including Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art models such as RoVRM and VisualPRM. The code and dataset can be found here https://github.com/UNITES-Lab/EQA-RM.
<div id='section'>PaperID: <span id='pid'>189, <a href='https://arxiv.org/pdf/2506.09937.pdf' target='_blank'>https://arxiv.org/pdf/2506.09937.pdf</a></span>   <span><a href='https://vla-safe.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiao Gu, Yuanliang Ju, Shengxiang Sun, Igor Gilitschenski, Haruki Nishimura, Masha Itkina, Florian Shkurti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09937">SAFE: Multitask Failure Detection for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $Ï_0$, and $Ï_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.
<div id='section'>PaperID: <span id='pid'>190, <a href='https://arxiv.org/pdf/2506.09930.pdf' target='_blank'>https://arxiv.org/pdf/2506.09930.pdf</a></span>   <span><a href='https://ai4ce.github.io/INT-ACT/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Irving Fang, Juexiao Zhang, Shengbang Tong, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09930">From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/
<div id='section'>PaperID: <span id='pid'>191, <a href='https://arxiv.org/pdf/2506.09623.pdf' target='_blank'>https://arxiv.org/pdf/2506.09623.pdf</a></span>   <span><a href='https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lipei Xie, Yingxin Li, Huiping Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09623">Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied foundation models are crucial for Artificial Intelligence (AI) interacting with the physical world by integrating multi-modal inputs, such as proprioception, vision and language, to understand human intentions and generate actions to control robots. While these models demonstrate strong generalization and few-shot learning capabilities, they face significant challenges in continually acquiring new skills without forgetting previously learned skills, a problem known as catastrophic forgetting. To address this issue, we propose the Analytic Task Scheduler (ATS), a novel framework for continual learning in embodied foundation models. ATS consists of a task-specific model library, where each model is fine-tuned independently on a single task, and an analytic scheduler trained using recursive least squares (RLS) to learn the mapping between language instructions and task-specific models. This architecture enables accurate task recognition and dynamic model selection while fundamentally avoiding parameter interference across tasks. The scheduler updates its parameters incrementally using only statistics (autocorrelation and cross-correlation matrices), enabling forgetting-resistant learning without the need to revisit historical data. We validate ATS on a real-world robot platform (RM65B), demonstrating superior resistance to forgetting and strong adaptability to task variations. The results highlight ATS as an effective, scalable, and deployable solution for continual learning in embodied foundation models operating in complex, dynamic environments. Our code will be available at https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler
<div id='section'>PaperID: <span id='pid'>192, <a href='https://arxiv.org/pdf/2506.09176.pdf' target='_blank'>https://arxiv.org/pdf/2506.09176.pdf</a></span>   <span><a href='https://github.com/metadriverse/AIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyuan Cai, Zhenghao Peng, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09176">Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency. Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM.
<div id='section'>PaperID: <span id='pid'>193, <a href='https://arxiv.org/pdf/2506.09049.pdf' target='_blank'>https://arxiv.org/pdf/2506.09049.pdf</a></span>   <span><a href='https://faceong.github.io/VIKI-R/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09049">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.
<div id='section'>PaperID: <span id='pid'>194, <a href='https://arxiv.org/pdf/2506.08334.pdf' target='_blank'>https://arxiv.org/pdf/2506.08334.pdf</a></span>   <span><a href='https://3dlg-hcvc.github.io/video2articulation/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weikun Peng, Jun Lv, Cewu Lu, Manolis Savva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08334">Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Articulated objects are prevalent in daily life. Understanding their kinematic structure and reconstructing them have numerous applications in embodied AI and robotics. However, current methods require carefully captured data for training or inference, preventing practical, scalable, and generalizable reconstruction of articulated objects. We focus on reconstruction of an articulated object from a casually captured RGBD video shot with a hand-held camera. A casually captured video of an interaction with an articulated object is easy to acquire at scale using smartphones. However, this setting is quite challenging, as the object and camera move simultaneously and there are significant occlusions as the person interacts with the object. To tackle these challenges, we introduce a coarse-to-fine framework that infers joint parameters and segments movable parts of the object from a dynamic RGBD video. To evaluate our method under this new setting, we build a 20$\times$ larger synthetic dataset of 784 videos containing 284 objects across 11 categories. We compare our approach with existing methods that also take video as input. Experiments show that our method can reconstruct synthetic and real articulated objects across different categories from dynamic RGBD videos, outperforming existing methods significantly.
<div id='section'>PaperID: <span id='pid'>195, <a href='https://arxiv.org/pdf/2506.08006.pdf' target='_blank'>https://arxiv.org/pdf/2506.08006.pdf</a></span>   <span><a href='https://metadriverse.github.io/dreamland/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Sicheng Mo, Ziyang Leng, Leon Liu, Weizhen Wang, Honglin He, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08006">Dreamland: Controllable World Creation with Simulator and Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.
<div id='section'>PaperID: <span id='pid'>196, <a href='https://arxiv.org/pdf/2506.07530.pdf' target='_blank'>https://arxiv.org/pdf/2506.07530.pdf</a></span>   <span><a href='https://github.com/ustcwhy/BitVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongyu Wang, Chuyan Xiong, Ruiping Wang, Xilin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07530">BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.
<div id='section'>PaperID: <span id='pid'>197, <a href='https://arxiv.org/pdf/2506.03834.pdf' target='_blank'>https://arxiv.org/pdf/2506.03834.pdf</a></span>   <span><a href='https://airlab-sogang.github.io/CARE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Joonkyung Kim, Joonyeol Sim, Woojun Kim, Katia Sycara, Changjoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03834">CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose CARE (Collision Avoidance via Repulsive Estimation) to improve the robustness of learning-based visual navigation methods. Recently, visual navigation models, particularly foundation models, have demonstrated promising performance by generating viable trajectories using only RGB images. However, these policies can generalize poorly to environments containing out-of-distribution (OOD) scenes characterized by unseen objects or different camera setups (e.g., variations in field of view, camera pose, or focal length). Without fine-tuning, such models could produce trajectories that lead to collisions, necessitating substantial efforts in data collection and additional training. To address this limitation, we introduce CARE, an attachable module that enhances the safety of visual navigation without requiring additional range sensors or fine-tuning of pretrained models. CARE can be integrated seamlessly into any RGB-based navigation model that generates local robot trajectories. It dynamically adjusts trajectories produced by a pretrained model using repulsive force vectors computed from depth images estimated directly from RGB inputs. We evaluate CARE by integrating it with state-of-the-art visual navigation models across diverse robot platforms. Real-world experiments show that CARE significantly reduces collisions (up to 100%) without compromising navigation performance in goal-conditioned navigation, and further improves collision-free travel distance (up to 10.7x) in exploration tasks. Project page: https://airlab-sogang.github.io/CARE/
<div id='section'>PaperID: <span id='pid'>198, <a href='https://arxiv.org/pdf/2506.03613.pdf' target='_blank'>https://arxiv.org/pdf/2506.03613.pdf</a></span>   <span><a href='https://github.com/airs-admin/HEAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoshan Liu, Fan Wang, Hongjun Zhou, Yuanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03613">Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While theory and practice are often seen as separate domains, this article shows that theoretical insight is essential for overcoming real-world engineering barriers. We begin with a practical challenge: training a cross-morphology embodied AI policy that generalizes across diverse robot morphologies. We formalize this as the Heterogeneous Embodied Agent Training (HEAT) problem and prove it reduces to a structured Partially Observable Markov Decision Process (POMDP) that is PSPACE-complete. This result explains why current reinforcement learning pipelines break down under morphological diversity, due to sequential training constraints, memory-policy coupling, and data incompatibility. We further explore Collective Adaptation, a distributed learning alternative inspired by biological systems. Though NEXP-complete in theory, it offers meaningful scalability and deployment benefits in practice. This work illustrates how computational theory can illuminate system design trade-offs and guide the development of more robust, scalable embodied AI. For practitioners and researchers to explore this problem, the implementation code of this work has been made publicly available at https://github.com/airs-admin/HEAT
<div id='section'>PaperID: <span id='pid'>199, <a href='https://arxiv.org/pdf/2506.03350.pdf' target='_blank'>https://arxiv.org/pdf/2506.03350.pdf</a></span>   <span><a href='https://github.com/eliotjones1/robogcg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03350">Adversarial Attacks on Robotic Vision Language Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .
<div id='section'>PaperID: <span id='pid'>200, <a href='https://arxiv.org/pdf/2506.03097.pdf' target='_blank'>https://arxiv.org/pdf/2506.03097.pdf</a></span>   <span><a href='https://github.com/adityavavre/VidEgoVLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ashwin Vinod, Shrey Pandit, Aditya Vavre, Linshen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03097">EgoVLM: Policy Optimization for Egocentric Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning.
<div id='section'>PaperID: <span id='pid'>201, <a href='https://arxiv.org/pdf/2506.02112.pdf' target='_blank'>https://arxiv.org/pdf/2506.02112.pdf</a></span>   <span><a href='https://uva-computer-vision-lab.github.io/sab3r/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuweiyi Chen, Tian Xia, Sihan Xu, Jianing Yang, Joyce Chai, Zezhou Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02112">SAB3R: Semantic-Augmented Backbone in 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new task, Map and Locate, which unifies the traditionally distinct objectives of open-vocabulary segmentation - detecting and segmenting object instances based on natural language queries - and 3D reconstruction, the process of estimating a scene's 3D structure from visual inputs. Specifically, Map and Locate involves generating a point cloud from an unposed video and segmenting object instances based on open-vocabulary queries. This task serves as a critical step toward real-world embodied AI applications and introduces a practical task that bridges reconstruction, recognition and reorganization. To tackle this task, we introduce a simple yet effective baseline, which we denote as SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer vision, and incorporates a lightweight distillation strategy. This method transfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP and DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary frozen networks, our model generates per-pixel semantic features and constructs cohesive point maps in a single forward pass. Compared to separately deploying MASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the Map and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic segmentation and 3D tasks to comprehensively validate its effectiveness.
<div id='section'>PaperID: <span id='pid'>202, <a href='https://arxiv.org/pdf/2506.01551.pdf' target='_blank'>https://arxiv.org/pdf/2506.01551.pdf</a></span>   <span><a href='https://github.com/expectorlin/EvolveNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01551">EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.
<div id='section'>PaperID: <span id='pid'>203, <a href='https://arxiv.org/pdf/2506.00927.pdf' target='_blank'>https://arxiv.org/pdf/2506.00927.pdf</a></span>   <span><a href='https://github.com/Alice01010101/TASU' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Alice01010101/TASU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianrui Pan, Jie Liu, Zewen Huang, Jie Tang, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00927">In-the-wild Audio Spatialization with Flexible Text-guided Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enhance immersive experiences, binaural audio offers spatial awareness of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes flexible text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of premium and large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio. It outperforms existing methods on both simulated and real-recorded datasets, demonstrating superior generalization and accuracy. Besides, we develop an assessment model based on Llama-3.1-8B, which evaluates the spatial semantic coherence between our generated binaural audio and text prompts through a spatial reasoning task. Results demonstrate that text prompts provide flexible and interactive control to generate binaural audio with excellent quality and semantic consistency in spatial locations. Dataset is available at \href{https://github.com/Alice01010101/TASU}
<div id='section'>PaperID: <span id='pid'>204, <a href='https://arxiv.org/pdf/2505.23757.pdf' target='_blank'>https://arxiv.org/pdf/2505.23757.pdf</a></span>   <span><a href='https://github.com/ahydchh/Impromptu-VLA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ahydchh/Impromptu-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23757">Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.
<div id='section'>PaperID: <span id='pid'>205, <a href='https://arxiv.org/pdf/2505.23189.pdf' target='_blank'>https://arxiv.org/pdf/2505.23189.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-web' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23189">TrackVLA: Embodied Visual Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.
<div id='section'>PaperID: <span id='pid'>206, <a href='https://arxiv.org/pdf/2505.21500.pdf' target='_blank'>https://arxiv.org/pdf/2505.21500.pdf</a></span>   <span><a href='https://zju-real.github.io/ViewSpatial-Page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21500">ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.
<div id='section'>PaperID: <span id='pid'>207, <a href='https://arxiv.org/pdf/2505.20129.pdf' target='_blank'>https://arxiv.org/pdf/2505.20129.pdf</a></span>   <span><a href='https://spatctxvlm.github.io/project_page/' target='_blank'>  GitHub</a></span> <span><a href='https://spatctxvlm.github.io/project_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20129">Agentic 3D Scene Generation with Spatially Contextualized VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advances in multimodal content generation enabled by vision-language models (VLMs), their ability to reason about and generate structured 3D scenes remains largely underexplored. This limitation constrains their utility in spatially grounded tasks such as embodied AI, immersive simulations, and interactive 3D applications. We introduce a new paradigm that enables VLMs to generate, understand, and edit complex 3D environments by injecting a continually evolving spatial context. Constructed from multimodal input, this context consists of three components: a scene portrait that provides a high-level semantic blueprint, a semantically labeled point cloud capturing object-level geometry, and a scene hypergraph that encodes rich spatial relationships, including unary, binary, and higher-order constraints. Together, these components provide the VLM with a structured, geometry-aware working memory that integrates its inherent multimodal reasoning capabilities with structured 3D understanding for effective spatial reasoning. Building on this foundation, we develop an agentic 3D scene generation pipeline in which the VLM iteratively reads from and updates the spatial context. The pipeline features high-quality asset generation with geometric restoration, environment setup with automatic verification, and ergonomic adjustment guided by the scene hypergraph. Experiments show that our framework can handle diverse and challenging inputs, achieving a level of generalization not observed in prior work. Further results demonstrate that injecting spatial context enables VLMs to perform downstream tasks such as interactive scene editing and path planning, suggesting strong potential for spatially intelligent systems in computer graphics, 3D vision, and embodied applications. Project page: https://spatctxvlm.github.io/project_page/.
<div id='section'>PaperID: <span id='pid'>208, <a href='https://arxiv.org/pdf/2505.19510.pdf' target='_blank'>https://arxiv.org/pdf/2505.19510.pdf</a></span>   <span><a href='https://github.com/docworlds/tsg-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19510">LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs' ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs' ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://github.com/docworlds/tsg-bench.
<div id='section'>PaperID: <span id='pid'>209, <a href='https://arxiv.org/pdf/2505.18078.pdf' target='_blank'>https://arxiv.org/pdf/2505.18078.pdf</a></span>   <span><a href='https://DanceTog.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Junhao Chen, Mingjin Chen, Jianjin Xu, Xiang Li, Junting Dong, Mingze Sun, Puhua Jiang, Hongxiang Li, Yuhang Yang, Hao Zhao, Xiaoxiao Long, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18078">DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds "who" and "how" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/.
<div id='section'>PaperID: <span id='pid'>210, <a href='https://arxiv.org/pdf/2505.17016.pdf' target='_blank'>https://arxiv.org/pdf/2505.17016.pdf</a></span>   <span><a href='https://ariostgx.github.io/ript_vla/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuhan Tan, Kairan Dou, Yue Zhao, Philipp KrÃ¤henbÃ¼hl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17016">Interactive Post-Training for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.
<div id='section'>PaperID: <span id='pid'>211, <a href='https://arxiv.org/pdf/2505.16815.pdf' target='_blank'>https://arxiv.org/pdf/2505.16815.pdf</a></span>   <span><a href='https://github.com/lcysyzxdxc/EmbodiedIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16815">Perceptual Quality Assessment for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: https://github.com/lcysyzxdxc/EmbodiedIQA
<div id='section'>PaperID: <span id='pid'>212, <a href='https://arxiv.org/pdf/2505.16663.pdf' target='_blank'>https://arxiv.org/pdf/2505.16663.pdf</a></span>   <span><a href='https://oceanhao.github.io/CoNav/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haihong Hao, Mingfei Han, Changlin Li, Zhihui Li, Xiaojun Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16663">CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation demands comprehensive scene understanding and precise spatial reasoning. While image-text models excel at interpreting pixel-level color and lighting cues, 3D-text models capture volumetric structure and spatial relationships. However, unified fusion approaches that jointly fuse 2D images, 3D point clouds, and textual instructions face challenges in limited availability of triple-modality data and difficulty resolving conflicting beliefs among modalities. In this work, we introduce CoNav, a collaborative cross-modal reasoning framework where a pretrained 3D-text model explicitly guides an image-text navigation agent by providing structured spatial-semantic knowledge to resolve ambiguities during navigation. Specifically, we introduce Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance by simply sharing textual hypotheses from the 3D-text model to the navigation agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the navigation agent learns to integrate visual cues with spatial-semantic knowledge derived from the 3D-text model, enabling effective reasoning in embodied navigation. CoNav achieves significant improvements on four standard embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success Rate, CoNav often generates shorter paths compared to other methods (as measured by SPL), showcasing the potential and challenges of fusing data from different modalities in embodied navigation. Project Page: https://oceanhao.github.io/CoNav/
<div id='section'>PaperID: <span id='pid'>213, <a href='https://arxiv.org/pdf/2505.16640.pdf' target='_blank'>https://arxiv.org/pdf/2505.16640.pdf</a></span>   <span><a href='https://badvla-project.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16640">BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have advanced robotic control by enabling end-to-end decision-making directly from multimodal inputs. However, their tightly coupled architectures expose novel security vulnerabilities. Unlike traditional adversarial perturbations, backdoor attacks represent a stealthier, persistent, and practically significant threat-particularly under the emerging Training-as-a-Service paradigm-but remain largely unexplored in the context of VLA models. To address this gap, we propose BadVLA, a backdoor attack method based on Objective-Decoupled Optimization, which for the first time exposes the backdoor vulnerabilities of VLA models. Specifically, it consists of a two-stage process: (1) explicit feature-space separation to isolate trigger representations from benign inputs, and (2) conditional control deviations that activate only in the presence of the trigger, while preserving clean-task performance. Empirical results on multiple VLA benchmarks demonstrate that BadVLA consistently achieves near-100% attack success rates with minimal impact on clean task accuracy. Further analyses confirm its robustness against common input perturbations, task transfers, and model fine-tuning, underscoring critical security vulnerabilities in current VLA deployments. Our work offers the first systematic investigation of backdoor vulnerabilities in VLA models, highlighting an urgent need for secure and trustworthy embodied model design practices. We have released the project page at https://badvla-project.github.io/.
<div id='section'>PaperID: <span id='pid'>214, <a href='https://arxiv.org/pdf/2505.16278.pdf' target='_blank'>https://arxiv.org/pdf/2505.16278.pdf</a></span>   <span><a href='https://thinklab-sjtu.github.io/DriveMoE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16278">DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $Ï_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$Ï_0$. Specifically, we add Vision MoE to Drive-$Ï_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$Ï_0$.
<div id='section'>PaperID: <span id='pid'>215, <a href='https://arxiv.org/pdf/2505.15197.pdf' target='_blank'>https://arxiv.org/pdf/2505.15197.pdf</a></span>   <span><a href='https://andypinxinliu.github.io/Intentional-Gesture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Pinxin Liu, Haiyang Liu, Luchuan Song, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15197">Intentional Gesture: Deliver Your Intentions with Gestures for Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When humans speak, gestures help convey communicative intentions, such as adding emphasis or describing concepts. However, current co-speech gesture generation methods rely solely on superficial linguistic cues (\textit{e.g.} speech audio or text transcripts), neglecting to understand and leverage the communicative intention that underpins human gestures. This results in outputs that are rhythmically synchronized with speech but are semantically shallow. To address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework that casts gesture generation as an intention-reasoning task grounded in high-level communicative functions. % First, we curate the \textbf{InG} dataset by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text sentences summarizing intentions), which are automatically annotated using large vision-language models. Next, we introduce the \textbf{Intentional Gesture Motion Tokenizer} to leverage these intention annotations. It injects high-level communicative functions (\textit{e.g.}, intentions) into tokenized motion representations to enable intention-aware gesture synthesis that are both temporally aligned and semantically meaningful, achieving new state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a modular foundation for expressive gesture generation in digital humans and embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture
<div id='section'>PaperID: <span id='pid'>216, <a href='https://arxiv.org/pdf/2505.14866.pdf' target='_blank'>https://arxiv.org/pdf/2505.14866.pdf</a></span>   <span><a href='https://nisarganc.github.io/UPTor-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Nisarga Nilavadi, Andrey Rudenko, Timm Linder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14866">UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset ``DARKO'' with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/
<div id='section'>PaperID: <span id='pid'>217, <a href='https://arxiv.org/pdf/2505.13888.pdf' target='_blank'>https://arxiv.org/pdf/2505.13888.pdf</a></span>   <span><a href='https://Koorye.github.io/proj/Inspire' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ji Zhang, Shihan Wu, Xu Luo, Hao Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13888">InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging pretrained Vision-Language Models (VLMs) to map language instruction and visual observations to raw low-level actions, Vision-Language-Action models (VLAs) hold great promise for achieving general-purpose robotic systems. Despite their advancements, existing VLAs tend to spuriously correlate task-irrelevant visual features with actions, limiting their generalization capacity beyond the training data. To tackle this challenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet effective approach that mitigates the adverse effects of spurious correlations by boosting the spatial reasoning ability of VLAs. Specifically, InSpire redirects the VLA's attention to task-relevant factors by prepending the question "In which direction is the [object] relative to the robot?" to the language instruction and aligning the answer "right/left/up/down/front/back/grasped" and predicted actions with the ground-truth. Notably, InSpire can be used as a plugin to enhance existing autoregressive VLAs, requiring no extra training data or interaction with other large models. Extensive experimental results in both simulation and real-world environments demonstrate the effectiveness and flexibility of our approach. Our code, pretrained models and demos are publicly available at: https://Koorye.github.io/proj/Inspire.
<div id='section'>PaperID: <span id='pid'>218, <a href='https://arxiv.org/pdf/2505.13441.pdf' target='_blank'>https://arxiv.org/pdf/2505.13441.pdf</a></span>   <span><a href='https://abhaybd.github.io/GraspMolmo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Abhay Deshpande, Yuquan Deng, Arijit Ray, Jordi Salvador, Winson Han, Jiafei Duan, Kuo-Hao Zeng, Yuke Zhu, Ranjay Krishna, Rose Hendrix
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13441">GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given "pour me some tea", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from PRISM, a novel large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. GraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot. We release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation, which, along with videos, are available at https://abhaybd.github.io/GraspMolmo/.
<div id='section'>PaperID: <span id='pid'>219, <a href='https://arxiv.org/pdf/2505.12278.pdf' target='_blank'>https://arxiv.org/pdf/2505.12278.pdf</a></span>   <span><a href='https://zhengyiluo.github.io/PDC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhengyi Luo, Chen Tessler, Toru Lin, Ye Yuan, Tairan He, Wenli Xiao, Yunrong Guo, Gal Chechik, Kris Kitani, Linxi Fan, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12278">Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human behavior is fundamentally shaped by visual perception -- our ability to interact with the world depends on actively gathering relevant information and adapting our movements accordingly. Behaviors like searching for objects, reaching, and hand-eye coordination naturally emerge from the structure of our sensory system. Inspired by these principles, we introduce Perceptive Dexterous Control (PDC), a framework for vision-driven dexterous whole-body control with simulated humanoids. PDC operates solely on egocentric vision for task specification, enabling object search, target placement, and skill selection through visual cues, without relying on privileged state information (e.g., 3D object positions and geometries). This perception-as-interface paradigm enables learning a single policy to perform multiple household tasks, including reaching, grasping, placing, and articulated object manipulation. We also show that training from scratch with reinforcement learning can produce emergent behaviors such as active search. These results demonstrate how vision-driven control and complex tasks induce human-like behaviors and can serve as the key ingredients in closing the perception-action loop for animation, robotics, and embodied AI.
<div id='section'>PaperID: <span id='pid'>220, <a href='https://arxiv.org/pdf/2505.10010.pdf' target='_blank'>https://arxiv.org/pdf/2505.10010.pdf</a></span>   <span><a href='https://github.com/LAMDA-RL/ImagineBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10010">ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge in reinforcement learning (RL) is its dependence on extensive real-world interaction data to learn task-specific policies. While recent work demonstrates that large language models (LLMs) can mitigate this limitation by generating synthetic experience (noted as imaginary rollouts) for mastering novel tasks, progress in this emerging field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ImagineBench, the first comprehensive benchmark for evaluating offline RL algorithms that leverage both real rollouts and LLM-imaginary rollouts. The key features of ImagineBench include: (1) datasets comprising environment-collected and LLM-imaginary rollouts; (2) diverse domains of environments covering locomotion, robotic manipulation, and navigation tasks; and (3) natural language task instructions with varying complexity levels to facilitate language-conditioned policy learning. Through systematic evaluation of state-of-the-art offline RL algorithms, we observe that simply applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving 35.44% success rate in hard tasks in contrast to 64.37% of method training on real rollouts for hard tasks. This result highlights the need for algorithm advancements to better leverage LLM-imaginary rollouts. Additionally, we identify key opportunities for future research: including better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks. Our code is publicly available at https://github.com/LAMDA-RL/ImagineBench.
<div id='section'>PaperID: <span id='pid'>221, <a href='https://arxiv.org/pdf/2505.09694.pdf' target='_blank'>https://arxiv.org/pdf/2505.09694.pdf</a></span>   <span><a href='https://github.com/AgibotTech/EWMBench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AgibotTech/EWMBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, Guanghui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09694">EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench.
<div id='section'>PaperID: <span id='pid'>222, <a href='https://arxiv.org/pdf/2505.08854.pdf' target='_blank'>https://arxiv.org/pdf/2505.08854.pdf</a></span>   <span><a href='https://github.com/taco-group/GenAI4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuping Wang, Shuo Xing, Cui Can, Renjie Li, Hongyuan Hua, Kexin Tian, Zhaobin Mo, Xiangbo Gao, Keshu Wu, Sulong Zhou, Hengxu You, Juntong Peng, Junge Zhang, Zehao Wang, Rui Song, Mingxuan Yan, Walter Zimmer, Xingcheng Zhou, Peiran Li, Zhaohan Lu, Chia-Ju Chen, Yue Huang, Ryan A. Rossi, Lichao Sun, Hongkai Yu, Zhiwen Fan, Frank Hao Yang, Yuhao Kang, Ross Greer, Chenxi Liu, Eun Hak Lee, Xuan Di, Xinyue Ye, Liu Ren, Alois Knoll, Xiaopeng Li, Shuiwang Ji, Masayoshi Tomizuka, Marco Pavone, Tianbao Yang, Jing Du, Ming-Hsuan Yang, Hua Wei, Ziran Wang, Yang Zhou, Jiachen Li, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08854">Generative AI for Autonomous Driving: Frontiers and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.
<div id='section'>PaperID: <span id='pid'>223, <a href='https://arxiv.org/pdf/2505.07446.pdf' target='_blank'>https://arxiv.org/pdf/2505.07446.pdf</a></span>   <span><a href='https://medlartea.github.io/tpt-bench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanjing Ye, Yu Zhan, Weixi Situ, Guangcheng Chen, Jingwen Yu, Ziqi Zhao, Kuanqi Cai, Arash Ajoudani, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07446">TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking a target person from robot-egocentric views is crucial for developing autonomous robots that provide continuous personalized assistance or collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most existing target person tracking (TPT) benchmarks are limited to controlled laboratory environments with few distractions, clean backgrounds, and short-term occlusions. In this paper, we introduce a large-scale dataset designed for TPT in crowded and unstructured environments, demonstrated through a robot-person following task. The dataset is collected by a human pushing a sensor-equipped cart while following a target person, capturing human-like following behavior and emphasizing long-term tracking challenges, including frequent occlusions and the need for re-identification from numerous pedestrians. It includes multi-modal data streams, including odometry, 3D LiDAR, IMU, panoramic images, and RGB-D images, along with exhaustively annotated 2D bounding boxes of the target person across 48 sequences, both indoors and outdoors. Using this dataset and visual annotations, we perform extensive experiments with existing SOTA TPT methods, offering a thorough analysis of their limitations and suggesting future research directions.
<div id='section'>PaperID: <span id='pid'>224, <a href='https://arxiv.org/pdf/2505.05622.pdf' target='_blank'>https://arxiv.org/pdf/2505.05622.pdf</a></span>   <span><a href='https://github.com/VinceOuti/CityNavAgent' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/VinceOuti/CityNavAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weichen Zhang, Chen Gao, Shiquan Yu, Ruiying Peng, Baining Zhao, Qian Zhang, Jinqiang Cui, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05622">CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \href{https://github.com/VinceOuti/CityNavAgent}{link}.
<div id='section'>PaperID: <span id='pid'>225, <a href='https://arxiv.org/pdf/2505.05474.pdf' target='_blank'>https://arxiv.org/pdf/2505.05474.pdf</a></span>   <span><a href='https://github.com/hzxie/Awesome-3D-Scene-Generation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hzxie/Awesome-3D-Scene-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05474">3D Scene Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.
<div id='section'>PaperID: <span id='pid'>226, <a href='https://arxiv.org/pdf/2505.05074.pdf' target='_blank'>https://arxiv.org/pdf/2505.05074.pdf</a></span>   <span><a href='https://apicis.github.io/aff-survey/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tommaso Apicella, Alessio Xompero, Andrea Cavallaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05074">Visual Affordances: Enabling Robots to Understand Object Functionality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. Predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. In this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. To address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. To favour transparency, we introduce the Affordance Sheet, a document to detail the proposed solution, the datasets, and the validation. As the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. Using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. Our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.
<div id='section'>PaperID: <span id='pid'>227, <a href='https://arxiv.org/pdf/2505.03792.pdf' target='_blank'>https://arxiv.org/pdf/2505.03792.pdf</a></span>   <span><a href='https://github.com/langfengQ/CoSo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, Bo An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03792">Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo.
<div id='section'>PaperID: <span id='pid'>228, <a href='https://arxiv.org/pdf/2505.02388.pdf' target='_blank'>https://arxiv.org/pdf/2505.02388.pdf</a></span>   <span><a href='https://meta-scenes.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02388">MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
<div id='section'>PaperID: <span id='pid'>229, <a href='https://arxiv.org/pdf/2505.01862.pdf' target='_blank'>https://arxiv.org/pdf/2505.01862.pdf</a></span>   <span><a href='https://linusnep.github.io/ReLI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Linus Nwankwo, Bjoern Ellensohn, Ozan Ãzdenizci, Elmar Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01862">ReLI: A Language-Agnostic Approach to Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at https://linusnep.github.io/ReLI/.
<div id='section'>PaperID: <span id='pid'>230, <a href='https://arxiv.org/pdf/2504.21769.pdf' target='_blank'>https://arxiv.org/pdf/2504.21769.pdf</a></span>   <span><a href='https://github.com/Tubicor/LLM-iTeach' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonas Werner, Kun Chu, Cornelius Weber, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21769">LLM-based Interactive Imitation Learning for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in machine learning provide methods to train autonomous agents capable of handling the increasing complexity of sequential decision-making in robotics. Imitation Learning (IL) is a prominent approach, where agents learn to control robots based on human demonstrations. However, IL commonly suffers from violating the independent and identically distributed (i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL) achieves improved performance by allowing agents to learn from interactive feedback from human teachers. Despite these improvements, both approaches come with significant costs due to the necessity of human involvement. Leveraging the emergent capabilities of Large Language Models (LLMs) in reasoning and generating human-like responses, we introduce LLM-iTeach -- a novel IIL framework that utilizes an LLM as an interactive teacher to enhance agent performance while alleviating the dependence on human resources. Firstly, LLM-iTeach uses a hierarchical prompting strategy that guides the LLM in generating a policy in Python code. Then, with a designed similarity-based feedback mechanism, LLM-iTeach provides corrective and evaluative feedback interactively during the agent's training. We evaluate LLM-iTeach against baseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a state-of-the-art IIL method using a human teacher, on various robotic manipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the success rate and achieves or even outscores that of CEILing, highlighting the potential of LLMs as cost-effective, human-like teachers in interactive learning environments. We further demonstrate the method's potential for generalization by evaluating it on additional tasks. The code and prompts are provided at: https://github.com/Tubicor/LLM-iTeach.
<div id='section'>PaperID: <span id='pid'>231, <a href='https://arxiv.org/pdf/2504.20041.pdf' target='_blank'>https://arxiv.org/pdf/2504.20041.pdf</a></span>   <span><a href='https://go2heart.github.io/streamformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20041">Learning Streaming Video Representation via Multitask Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding continuous video streams plays a fundamental role in real-time applications including embodied AI and autonomous driving. Unlike offline video understanding, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions. To address these challenges, our main contributions are three-fold. (i) We develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability. (ii) To train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) We conduct extensive experiments on online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive results while maintaining efficiency, demonstrating its potential for real-time applications.
<div id='section'>PaperID: <span id='pid'>232, <a href='https://arxiv.org/pdf/2504.18317.pdf' target='_blank'>https://arxiv.org/pdf/2504.18317.pdf</a></span>   <span><a href='https://github.com/fangzr/TOC-Edge-Aerial' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18317">Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support the Low Altitude Economy (LAE), it is essential to achieve precise localization of unmanned aerial vehicles (UAVs) in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available at: github.com/fangzr/TOC-Edge-Aerial.
<div id='section'>PaperID: <span id='pid'>233, <a href='https://arxiv.org/pdf/2504.16649.pdf' target='_blank'>https://arxiv.org/pdf/2504.16649.pdf</a></span>   <span><a href='https://peilin-666.github.io/projects/PP-Tac/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Pei Lin, Yuzhe Huang, Wanlin Li, Jianpeng Ma, Chenxi Xiao, Ziyuan Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16649">PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly envisioned as human companions, assisting with everyday tasks that often involve manipulating deformable objects. Although recent advances in robotic hardware and embodied AI have expanded their capabilities, current systems still struggle with handling thin, flat, and deformable objects such as paper and fabric. This limitation arises from the lack of suitable perception techniques for robust state estimation under diverse object appearances, as well as the absence of planning techniques for generating appropriate grasp motions. To bridge these gaps, this paper introduces PP-Tac, a robotic system for picking up paper-like objects. PP-Tac features a multi-fingered robotic hand with high-resolution omnidirectional tactile sensors \sensorname. This hardware configuration enables real-time slip detection and online frictional force control that mitigates such slips. Furthermore, grasp motion generation is achieved through a trajectory synthesis pipeline, which first constructs a dataset of finger's pinching motions. Based on this dataset, a diffusion-based policy is trained to control the hand-arm robotic system. Experiments demonstrate that PP-Tac can effectively grasp paper-like objects of varying material, thickness, and stiffness, achieving an overall success rate of 87.5\%. To our knowledge, this work is the first attempt to grasp paper-like deformable objects using a tactile dexterous hand. Our project webpage can be found at: https://peilin-666.github.io/projects/PP-Tac/
<div id='section'>PaperID: <span id='pid'>234, <a href='https://arxiv.org/pdf/2504.10808.pdf' target='_blank'>https://arxiv.org/pdf/2504.10808.pdf</a></span>   <span><a href='https://github.com/hasan-rakibul/TFMPathy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Md Rakibul Hasan, Md Zakir Hossain, Aneesh Krishna, Shafin Rahman, Tom Gedeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10808">TFMPathy: Tabular Foundation Model for Privacy-Aware, Generalisable Empathy Detection from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting empathy from video interactions is an emerging area of research, particularly in healthcare and social robotics. However, privacy and ethical concerns often prevent the release of raw video data, with many datasets instead shared as pre-extracted tabular features. Previous work on such datasets has established classical tree-based models as the state of the art. Motivated by recent successes of large-scale foundation models for text, we investigate the potential of tabular foundation models (TFMs) for empathy detection from video-derived tabular data. Our proposed system, TFMPathy, is demonstrated with two recent TFMs (TabPFN v2 and TabICL) under both in-context learning and fine-tuning paradigms. On a public human-robot interaction benchmark, TFMPathy significantly improves empathy detection accuracy reported in the literature. While the established evaluation protocol in the literature does not ensure cross-subject generalisation, our evaluation scheme also captures such generalisation. We show that TFMPathy under a fine-tuning setup has better cross-subject generalisation capacity over baseline methods (accuracy: $0.590 \rightarrow 0.730$; AUC: $0.564 \rightarrow 0.669$). Given the ongoing privacy and ethical constraints around raw video sharing, the proposed TFMPathy system provides a practical and scalable path toward building AI systems dependent on human-centred video datasets. Our code is publicly available at https://github.com/hasan-rakibul/TFMPathy (will be made available upon acceptance of this paper).
<div id='section'>PaperID: <span id='pid'>235, <a href='https://arxiv.org/pdf/2504.10414.pdf' target='_blank'>https://arxiv.org/pdf/2504.10414.pdf</a></span>   <span><a href='https://jiaxin-lu.github.io/humoto/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya, Qixing Huang, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10414">HUMOTO: A 4D Dataset of Mocap Human Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of human-object interactions for motion generation, computer vision, and robotics applications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO captures interactions with 63 precisely modeled objects and 72 articulated parts. Our innovations include a scene-driven LLM scripting pipeline creating complete, purposeful tasks with natural progression, and a mocap-and-camera recording setup to effectively handle occlusions. Spanning diverse activities from cooking to outdoor picnics, HUMOTO preserves both physical accuracy and logical task flow. Professional artists rigorously clean and verify each sequence, minimizing foot sliding and object penetrations. We also provide benchmarks compared to other datasets. HUMOTO's comprehensive full-body motion and simultaneous multi-object interactions address key data-capturing challenges and provide opportunities to advance realistic human-object interaction modeling across research domains with practical applications in animation, robotics, and embodied AI systems. Project: https://jiaxin-lu.github.io/humoto/ .
<div id='section'>PaperID: <span id='pid'>236, <a href='https://arxiv.org/pdf/2504.10041.pdf' target='_blank'>https://arxiv.org/pdf/2504.10041.pdf</a></span>   <span><a href='https://github.com/hren20/NaiviBridger' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Ren, Yiming Zeng, Zetong Bi, Zhaoliang Wan, Junlong Huang, Hui Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10041">Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion-based imitation learning, which show impressive performance in modeling multimodal distributions and training stability, have led to substantial progress in various robot learning tasks. In visual navigation, previous diffusion-based policies typically generate action sequences by initiating from denoising Gaussian noise. However, the target action distribution often diverges significantly from Gaussian noise, leading to redundant denoising steps and increased learning complexity. Additionally, the sparsity of effective action distributions makes it challenging for the policy to generate accurate actions without guidance. To address these issues, we propose a novel, unified visual navigation framework leveraging the denoising diffusion bridge models named NaviBridger. This approach enables action generation by initiating from any informative prior actions, enhancing guidance and efficiency in the denoising process. We explore how diffusion bridges can enhance imitation learning in visual navigation tasks and further examine three source policies for generating prior actions. Extensive experiments in both simulated and real-world indoor and outdoor scenarios demonstrate that NaviBridger accelerates policy inference and outperforms the baselines in generating target action sequences. Code is available at https://github.com/hren20/NaiviBridger.
<div id='section'>PaperID: <span id='pid'>237, <a href='https://arxiv.org/pdf/2504.10003.pdf' target='_blank'>https://arxiv.org/pdf/2504.10003.pdf</a></span>   <span><a href='https://github.com/SYSU-RoboticsLab/NaviD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiming Zeng, Hao Ren, Shuhang Wang, Junlong Huang, Hui Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10003">NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation, a fundamental challenge in mobile robotics, demands versatile policies to handle diverse environments. Classical methods leverage geometric solutions to minimize specific costs, offering adaptability to new scenarios but are prone to system errors due to their multi-modular design and reliance on hand-crafted rules. Learning-based methods, while achieving high planning success rates, face difficulties in generalizing to unseen environments beyond the training data and often require extensive training. To address these limitations, we propose a hybrid approach that combines the strengths of learning-based methods and classical approaches for RGB-only visual navigation. Our method first trains a conditional diffusion model on diverse path-RGB observation pairs. During inference, it integrates the gradients of differentiable scene-specific and task-level costs, guiding the diffusion model to generate valid paths that meet the constraints. This approach alleviates the need for retraining, offering a plug-and-play solution. Extensive experiments in both indoor and outdoor settings, across simulated and real-world scenarios, demonstrate zero-shot transfer capability of our approach, achieving higher success rates and fewer collisions compared to baseline methods. Code will be released at https://github.com/SYSU-RoboticsLab/NaviD.
<div id='section'>PaperID: <span id='pid'>238, <a href='https://arxiv.org/pdf/2504.08646.pdf' target='_blank'>https://arxiv.org/pdf/2504.08646.pdf</a></span>   <span><a href='https://github.com/RISELabPurdue/MBE-ARI/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ian Noronha, Advait Prasad Jawaji, Juan Camilo Soto, Jiajun An, Yan Gu, Upinder Kaur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08646">MBE-ARI: A Multimodal Dataset Mapping Bi-directional Engagement in Animal-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animal-robot interaction (ARI) remains an unexplored challenge in robotics, as robots struggle to interpret the complex, multimodal communication cues of animals, such as body language, movement, and vocalizations. Unlike human-robot interaction, which benefits from established datasets and frameworks, animal-robot interaction lacks the foundational resources needed to facilitate meaningful bidirectional communication. To bridge this gap, we present the MBE-ARI (Multimodal Bidirectional Engagement in Animal-Robot Interaction), a novel multimodal dataset that captures detailed interactions between a legged robot and cows. The dataset includes synchronized RGB-D streams from multiple viewpoints, annotated with body pose and activity labels across interaction phases, offering an unprecedented level of detail for ARI research. Additionally, we introduce a full-body pose estimation model tailored for quadruped animals, capable of tracking 39 keypoints with a mean average precision (mAP) of 92.7%, outperforming existing benchmarks in animal pose estimation. The MBE-ARI dataset and our pose estimation framework lay a robust foundation for advancing research in animal-robot interaction, providing essential tools for developing perception, reasoning, and interaction frameworks needed for effective collaboration between robots and animals. The dataset and resources are publicly available at https://github.com/RISELabPurdue/MBE-ARI/, inviting further exploration and development in this critical area.
<div id='section'>PaperID: <span id='pid'>239, <a href='https://arxiv.org/pdf/2504.03800.pdf' target='_blank'>https://arxiv.org/pdf/2504.03800.pdf</a></span>   <span><a href='https://wei-nijuan.github.io/DecisionSpikeFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei Huang, Qinying Gu, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03800">Decision SpikeFormer: Spike-Driven Transformer for Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) enables policy training solely on pre-collected data, avoiding direct environment interaction - a crucial benefit for energy-constrained embodied AI applications. Although Artificial Neural Networks (ANN)-based methods perform well in offline RL, their high computational and energy demands motivate exploration of more efficient alternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given their low power consumption. In this work, we introduce DSFormer, the first spike-driven transformer model designed to tackle offline RL via sequence modeling. Unlike existing SNN transformers focused on spatial dimensions for vision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional Spiking Self-Attention (PSSA) in DSFormer to capture the temporal and positional dependencies essential for sequence modeling in RL. Additionally, we propose Progressive Threshold-dependent Batch Normalization (PTBN), which combines the benefits of LayerNorm and BatchNorm to preserve temporal dependencies while maintaining the spiking nature of SNNs. Comprehensive results in the D4RL benchmark show DSFormer's superiority over both SNN and ANN counterparts, achieving 78.4% energy savings, highlighting DSFormer's advantages not only in energy efficiency but also in competitive performance. Code and models are public at https://wei-nijuan.github.io/DecisionSpikeFormer.
<div id='section'>PaperID: <span id='pid'>240, <a href='https://arxiv.org/pdf/2504.01301.pdf' target='_blank'>https://arxiv.org/pdf/2504.01301.pdf</a></span>   <span><a href='https://mertcookimg.github.io/bi-lat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Takumi Kobayashi, Masato Kobayashi, Thanpimon Buamanee, Yuki Uranishi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01301">Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Bi-LAT, a novel imitation learning framework that unifies bilateral control with natural language processing to achieve precise force modulation in robotic manipulation. Bi-LAT leverages joint position, velocity, and torque data from leader-follower teleoperation while also integrating visual and linguistic cues to dynamically adjust applied force. By encoding human instructions such as "softly grasp the cup" or "strongly twist the sponge" through a multimodal Transformer-based model, Bi-LAT learns to distinguish nuanced force requirements in real-world tasks. We demonstrate Bi-LAT's performance in (1) unimanual cup-stacking scenario where the robot accurately modulates grasp force based on language commands, and (2) bimanual sponge-twisting task that requires coordinated force control. Experimental results show that Bi-LAT effectively reproduces the instructed force levels, particularly when incorporating SigLIP among tested language encoders. Our findings demonstrate the potential of integrating natural language cues into imitation learning, paving the way for more intuitive and adaptive human-robot interaction. For additional material, please visit: https://mertcookimg.github.io/bi-lat/
<div id='section'>PaperID: <span id='pid'>241, <a href='https://arxiv.org/pdf/2504.00954.pdf' target='_blank'>https://arxiv.org/pdf/2504.00954.pdf</a></span>   <span><a href='https://github.com/BwLiu01/IDMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, Chaochao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00954">IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal retrieval systems are becoming increasingly vital for cutting-edge AI technologies, such as embodied AI and AI-driven digital content industries. However, current multimodal retrieval tasks lack sufficient complexity and demonstrate limited practical application value. It spires us to design Instance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires models to retrieve images containing the same instance as a query image while matching a text-described scenario. Unlike existing retrieval tasks focused on global image similarity or category-level matching, IDMR demands fine-grained instance-level consistency across diverse contexts. To benchmark this capability, we develop IDMR-bench using real-world object tracking and first-person video data. Addressing the scarcity of training data, we propose a cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets. Our Multimodal Large Language Model (MLLM) based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and our zero-shot IDMR-bench. Experimental results demonstrate previous models' limitations in instance-aware retrieval and highlight the potential of MLLM for advanced retrieval applications. The whole training dataset, codes and models, with wide ranges of sizes, are available at https://github.com/BwLiu01/IDMR.
<div id='section'>PaperID: <span id='pid'>242, <a href='https://arxiv.org/pdf/2504.00167.pdf' target='_blank'>https://arxiv.org/pdf/2504.00167.pdf</a></span>   <span><a href='https://TS-Robotics.github.io/pHRI-DIGI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Teresa Sinico, Giovanni Boschetti, Pedro Neto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00167">Enhancing Physical Human-Robot Interaction: Recognizing Digits via Intrinsic Robot Tactile Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction (pHRI) remains a key challenge for achieving intuitive and safe interaction with robots. Current advancements often rely on external tactile sensors as interface, which increase the complexity of robotic systems. In this study, we leverage the intrinsic tactile sensing capabilities of collaborative robots to recognize digits drawn by humans on an uninstrumented touchpad mounted to the robot's flange. We propose a dataset of robot joint torque signals along with corresponding end-effector (EEF) forces and moments, captured from the robot's integrated torque sensors in each joint, as users draw handwritten digits (0-9) on the touchpad. The pHRI-DIGI-TACT dataset was collected from different users to capture natural variations in handwriting. To enhance classification robustness, we developed a data augmentation technique to account for reversed and rotated digits inputs. A Bidirectional Long Short-Term Memory (Bi-LSTM) network, leveraging the spatiotemporal nature of the data, performs online digit classification with an overall accuracy of 94\% across various test scenarios, including those involving users who did not participate in training the system. This methodology is implemented on a real robot in a fruit delivery task, demonstrating its potential to assist individuals in everyday life. Dataset and video demonstrations are available at: https://TS-Robotics.github.io/pHRI-DIGI/.
<div id='section'>PaperID: <span id='pid'>243, <a href='https://arxiv.org/pdf/2503.20220.pdf' target='_blank'>https://arxiv.org/pdf/2503.20220.pdf</a></span>   <span><a href='https://analysis-by-synthesis.github.io/DINeMo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20220">DINeMo: Learning Neural Mesh Models with no 3D Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.
<div id='section'>PaperID: <span id='pid'>244, <a href='https://arxiv.org/pdf/2503.19901.pdf' target='_blank'>https://arxiv.org/pdf/2503.19901.pdf</a></span>   <span><a href='https://liangpan99.github.io/TokenHSI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, Jingbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19901">TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/
<div id='section'>PaperID: <span id='pid'>245, <a href='https://arxiv.org/pdf/2503.19851.pdf' target='_blank'>https://arxiv.org/pdf/2503.19851.pdf</a></span>   <span><a href='https://github.com/Sampson-Lee/OnlineMMSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, Yapeng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19851">Towards Online Multi-Modal Social Interaction Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal social interaction understanding (MMSI) is critical in human-robot interaction systems. In real-world scenarios, AI agents are required to provide real-time feedback. However, existing models often depend on both past and future contexts, which hinders them from applying to real-world problems. To bridge this gap, we propose an online MMSI setting, where the model must resolve MMSI tasks using only historical information, such as recorded dialogues and video streams. To address the challenges of missing the useful future context, we develop a novel framework, named Online-MMSI-VLM, that leverages two complementary strategies: multi-party conversation forecasting and social-aware visual prompting with multi-modal large language models. First, to enrich linguistic context, the multi-party conversation forecasting simulates potential future utterances in a coarse-to-fine manner, anticipating upcoming speaker turns and then generating fine-grained conversational details. Second, to effectively incorporate visual social cues like gaze and gesture, social-aware visual prompting highlights the social dynamics in video with bounding boxes and body keypoints for each person and frame. Extensive experiments on three tasks and two datasets demonstrate that our method achieves state-of-the-art performance and significantly outperforms baseline models, indicating its effectiveness on Online-MMSI. The code and pre-trained models will be publicly released at: https://github.com/Sampson-Lee/OnlineMMSI.
<div id='section'>PaperID: <span id='pid'>246, <a href='https://arxiv.org/pdf/2503.19397.pdf' target='_blank'>https://arxiv.org/pdf/2503.19397.pdf</a></span>   <span><a href='https://github.com/clee-jaist/QFAAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenghao Li, Razvan Beuran, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19397">Quality-focused Active Adversarial Policy for Safe Grasping in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-guided robot grasping methods based on Deep Neural Networks (DNNs) have achieved remarkable success in handling unknown objects, attributable to their powerful generalizability. However, these methods with this generalizability tend to recognize the human hand and its adjacent objects as graspable targets, compromising safety during Human-Robot Interaction (HRI). In this work, we propose the Quality-focused Active Adversarial Policy (QFAAP) to solve this problem. Specifically, the first part is the Adversarial Quality Patch (AQP), wherein we design the adversarial quality patch loss and leverage the grasp dataset to optimize a patch with high quality scores. Next, we construct the Projected Quality Gradient Descent (PQGD) and integrate it with the AQP, which contains only the hand region within each real-time frame, endowing the AQP with fast adaptability to the human hand shape. Through AQP and PQGD, the hand can be actively adversarial with the surrounding objects, lowering their quality scores. Therefore, further setting the quality score of the hand to zero will reduce the grasping priority of both the hand and its adjacent objects, enabling the robot to grasp other objects away from the hand without emergency stops. We conduct extensive experiments on the benchmark datasets and a cobot, showing the effectiveness of QFAAP. Our code and demo videos are available here: https://github.com/clee-jaist/QFAAP.
<div id='section'>PaperID: <span id='pid'>247, <a href='https://arxiv.org/pdf/2503.18525.pdf' target='_blank'>https://arxiv.org/pdf/2503.18525.pdf</a></span>   <span><a href='https://yvfengzhong.github.io/RoboTron-Nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yufeng Zhong, Chengjian Feng, Feng Yan, Fanfan Liu, Liming Zheng, Lin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18525">RoboTron-Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In language-guided visual navigation, agents locate target objects in unseen environments using natural language instructions. For reliable navigation in unfamiliar scenes, agents should possess strong perception, planning, and prediction capabilities. Additionally, when agents revisit previously explored areas during long-term navigation, they may retain irrelevant and redundant historical perceptions, leading to suboptimal results. In this work, we propose RoboTron-Nav, a unified framework that integrates perception, planning, and prediction capabilities through multitask collaborations on navigation and embodied question answering tasks, thereby enhancing navigation performances. Furthermore, RoboTron-Nav employs an adaptive 3D-aware history sampling strategy to effectively and efficiently utilize historical observations. By leveraging large language model, RoboTron-Nav comprehends diverse commands and complex visual scenes, resulting in appropriate navigation actions. RoboTron-Nav achieves an 81.1% success rate in object goal navigation on the $\mathrm{CHORES}$-$\mathbb{S}$ benchmark, setting a new state-of-the-art performance. Project page: https://yvfengzhong.github.io/RoboTron-Nav
<div id='section'>PaperID: <span id='pid'>248, <a href='https://arxiv.org/pdf/2503.16408.pdf' target='_blank'>https://arxiv.org/pdf/2503.16408.pdf</a></span>   <span><a href='https://iranqin.github.io/robofactory/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiran Qin, Li Kang, Xiufeng Song, Zhenfei Yin, Xiaohong Liu, Xihui Liu, Ruimao Zhang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16408">RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.
<div id='section'>PaperID: <span id='pid'>249, <a href='https://arxiv.org/pdf/2503.13446.pdf' target='_blank'>https://arxiv.org/pdf/2503.13446.pdf</a></span>   <span><a href='https://gary3410.github.io/momanipVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhenyu Wu, Yuheng Zhou, Xiuwei Xu, Ziwei Wang, Haibin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13446">MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and environments because of the lack of large-scale training. In contrast, recent advances in vision-language-action (VLA) models have shown impressive generalization capabilities, but these foundation models are developed for fixed-base manipulation tasks. Therefore, we propose an efficient policy adaptation framework named MoManipVLA to transfer pre-trained VLA models of fix-base manipulation to mobile manipulation, so that high generalization ability across tasks and environments can be achieved in mobile manipulation policy. Specifically, we utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability. We design motion planning objectives for the mobile base and the robot arm, which aim at maximizing the physical feasibility of the trajectory. Finally, we present an efficient bi-level objective optimization framework for trajectory generation, where the upper-level optimization predicts waypoints for base movement to enhance the manipulator policy space, and the lower-level optimization selects the optimal end-effector trajectory to complete the manipulation task. In this way, MoManipVLA can adjust the position of the robot base in a zero-shot manner, thus making the waypoints predicted from the fixed-base VLA models feasible. Extensive experimental results on OVMM and the real world demonstrate that MoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile manipulation, and only requires 50 training cost for real world deployment due to the strong generalization ability in the pre-trained VLA models.
<div id='section'>PaperID: <span id='pid'>250, <a href='https://arxiv.org/pdf/2503.13424.pdf' target='_blank'>https://arxiv.org/pdf/2503.13424.pdf</a></span>   <span><a href='https://github.com/Intern-Nexus/Infinite-Mobility' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinyu Lian, Zichao Yu, Ruiming Liang, Yitong Wang, Li Ray Luo, Kaixu Chen, Yuanzhen Zhou, Qihong Tang, Xudong Xu, Zhaoyang Lyu, Bo Dai, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13424">Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility
<div id='section'>PaperID: <span id='pid'>251, <a href='https://arxiv.org/pdf/2503.12955.pdf' target='_blank'>https://arxiv.org/pdf/2503.12955.pdf</a></span>   <span><a href='https://github.com/ZJHTerry18/HumanInScene' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahe Zhao, Ruibing Hou, Zejie Tian, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12955">HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new task to benchmark human-in-scene understanding for embodied agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors, reason about its surrounding environment, and answer human-related questions within the scene. To support this new task, we present HIS-Bench, a multimodal benchmark that systematically evaluates HIS understanding across a broad spectrum, from basic perception to commonsense reasoning and planning. Our evaluation of various vision-language models on HIS-Bench reveals significant limitations in their ability to handle HIS-QA tasks. To this end, we propose HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates 3D scene context and human motion dynamics into large language models while incorporating specialized mechanisms to capture human-scene interactions. Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on HIS-QA tasks. We hope this work inspires future research on human behavior analysis in 3D scenes, advancing embodied AI and world models. The codes and data: https://github.com/ZJHTerry18/HumanInScene.
<div id='section'>PaperID: <span id='pid'>252, <a href='https://arxiv.org/pdf/2503.11081.pdf' target='_blank'>https://arxiv.org/pdf/2503.11081.pdf</a></span>   <span><a href='https://momakitchen.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://momakitchen.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11081">MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mobile manipulation, navigation and manipulation are often treated as separate problems, resulting in a significant gap between merely approaching an object and engaging with it effectively. Many navigation approaches primarily define success by proximity to the target, often overlooking the necessity for optimal positioning that facilitates subsequent manipulation. To address this, we introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples that provide training data for models to learn optimal final navigation positions for seamless transition to manipulation. Our dataset includes affordance-grounded floor labels collected from diverse kitchen environments, in which robotic mobile manipulators of different models attempt to grasp target objects amidst clutter. Using a fully automated pipeline, we simulate diverse real-world scenarios and generate affordance labels for optimal manipulation positions. Visual data are collected from RGB-D inputs captured by a first-person view camera mounted on the robotic arm, ensuring consistency in viewpoint during data collection. We also develop a lightweight baseline model, NavAff, for navigation affordance grounding that demonstrates promising performance on the MoMa-Kitchen benchmark. Our approach enables models to learn affordance-based final positioning that accommodates different arm types and platform heights, thereby paving the way for more robust and generalizable integration of navigation and manipulation in embodied AI. Project page: \href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.
<div id='section'>PaperID: <span id='pid'>253, <a href='https://arxiv.org/pdf/2503.10307.pdf' target='_blank'>https://arxiv.org/pdf/2503.10307.pdf</a></span>   <span><a href='https://ponimatkin.github.io/wildpose/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Georgy Ponimatkin, Martin CÃ­fka, TomÃ¡Å¡ SouÄek, MÃ©dÃ©ric Fourmy, Yann LabbÃ©, Vladimir Petrik, Josef Sivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10307">6D Object Pose Tracking in Internet Videos for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We seek to extract a temporally consistent 6D pose trajectory of a manipulated object from an Internet instructional video. This is a challenging set-up for current 6D pose estimation methods due to uncontrolled capturing conditions, subtle but dynamic object motions, and the fact that the exact mesh of the manipulated object is not known. To address these challenges, we present the following contributions. First, we develop a new method that estimates the 6D pose of any object in the input image without prior knowledge of the object itself. The method proceeds by (i) retrieving a CAD model similar to the depicted object from a large-scale model database, (ii) 6D aligning the retrieved CAD model with the input image, and (iii) grounding the absolute scale of the object with respect to the scene. Second, we extract smooth 6D object trajectories from Internet videos by carefully tracking the detected objects across video frames. The extracted object trajectories are then retargeted via trajectory optimization into the configuration space of a robotic manipulator. Third, we thoroughly evaluate and ablate our 6D pose estimation method on YCB-V and HOPE-Video datasets as well as a new dataset of instructional videos manually annotated with approximate 6D object trajectories. We demonstrate significant improvements over existing state-of-the-art RGB 6D pose estimation methods. Finally, we show that the 6D object motion estimated from Internet videos can be transferred to a 7-axis robotic manipulator both in a virtual simulator as well as in a real world set-up. We also successfully apply our method to egocentric videos taken from the EPIC-KITCHENS dataset, demonstrating potential for Embodied AI applications.
<div id='section'>PaperID: <span id='pid'>254, <a href='https://arxiv.org/pdf/2503.09527.pdf' target='_blank'>https://arxiv.org/pdf/2503.09527.pdf</a></span>   <span><a href='https://combatvla.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Peng Chen, Pi Bu, Yingyao Wang, Xinyi Wang, Ziming Wang, Jie Guo, Yingxiu Zhao, Qi Zhu, Jun Song, Siran Yang, Jiamang Wang, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09527">CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will open-source all resources, including the action tracker, dataset, benchmark, model weights, training code, and the implementation of the framework at https://combatvla.github.io/.
<div id='section'>PaperID: <span id='pid'>255, <a href='https://arxiv.org/pdf/2503.09335.pdf' target='_blank'>https://arxiv.org/pdf/2503.09335.pdf</a></span>   <span><a href='https://github.com/laiyuzhi/NVP-HRI.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Thomas Weber, Matthias RÃ¤tsch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09335">NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective Human-Robot Interaction (HRI) is crucial for future service robots in aging societies. Existing solutions are biased toward only well-trained objects, creating a gap when dealing with new objects. Currently, HRI systems using predefined gestures or language tokens for pretrained objects pose challenges for all individuals, especially elderly ones. These challenges include difficulties in recalling commands, memorizing hand gestures, and learning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI paradigm that combines voice commands and deictic posture. NVP-HRI utilizes the Segment Anything Model (SAM) to analyze visual cues and depth data, enabling precise structural object representation. Through a pre-trained SAM network, NVP-HRI allows interaction with new objects via zero-shot prediction, even without prior knowledge. NVP-HRI also integrates with a large language model (LLM) for multimodal commands, coordinating them with object selection and scene distribution in real time for collision-free trajectory solutions. We also regulate the action sequence with the essential control syntax to reduce LLM hallucination risks. The evaluation of diverse real-world tasks using a Universal Robot showcased up to 59.2\% efficiency improvement over traditional gesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our code and design will be openly available at https://github.com/laiyuzhi/NVP-HRI.git.
<div id='section'>PaperID: <span id='pid'>256, <a href='https://arxiv.org/pdf/2503.04163.pdf' target='_blank'>https://arxiv.org/pdf/2503.04163.pdf</a></span>   <span><a href='https://aoqunjin.github.io/Expert-VLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04163">VLA Model-Expert Collaboration for Bi-directional Manipulation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. Although these models have achieved significant improvements, their generalization in multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces expert workload relative to manual operation while simultaneously improving the reliability and generalization of VLA models. Furthermore, manipulation data collected during collaboration can further refine the VLA model, while human participants concurrently enhance their skills. This bi-directional learning loop boosts the overall performance of the collaboration system. Experimental results across various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and learning, as evidenced by improved success rates across tasks. Additionally, validation using a brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of low-speed action systems by involving VLA model during manipulation. These promising results pave the way for advancing human-robot interaction in the era of foundation models for robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)
<div id='section'>PaperID: <span id='pid'>257, <a href='https://arxiv.org/pdf/2503.03984.pdf' target='_blank'>https://arxiv.org/pdf/2503.03984.pdf</a></span>   <span><a href='https://github.com/Qianzhong-Chen/grad_nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03984">GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous visual navigation is an essential element in robot autonomy. Reinforcement learning (RL) offers a promising policy training paradigm. However existing RL methods suffer from high sample complexity, poor sim-to-real transfer, and limited runtime adaptability to navigation scenarios not seen during training. These problems are particularly challenging for drones, with complex nonlinear and unstable dynamics, and strong dynamic coupling between control and perception. In this paper, we propose a novel framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep reinforcement learning (DDRL) to train vision-based drone navigation policies. By leveraging high-fidelity 3D scene representations and differentiable simulation, our method improves sample efficiency and sim-to-real transfer. Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt to environmental variations at runtime. Moreover, by curriculum training in a mixture of different surrounding environments, we achieve in-task generalization, the ability to solve new instances of a task not seen during training. Drone hardware experiments demonstrate our method's high training efficiency compared to state-of-the-art RL methods, zero shot sim-to-real transfer for real robot deployment without fine tuning, and ability to adapt to new instances within the same task class (e.g. to fly through a gate at different locations with different distractors in the environment). Our simulator and training framework are open-sourced at: https://github.com/Qianzhong-Chen/grad_nav.
<div id='section'>PaperID: <span id='pid'>258, <a href='https://arxiv.org/pdf/2503.02247.pdf' target='_blank'>https://arxiv.org/pdf/2503.02247.pdf</a></span>   <span><a href='https://b0b8k1ng.github.io/WMNav/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02247">WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
<div id='section'>PaperID: <span id='pid'>259, <a href='https://arxiv.org/pdf/2503.00778.pdf' target='_blank'>https://arxiv.org/pdf/2503.00778.pdf</a></span>   <span><a href='https://eqcy.github.io/affordgrasp/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yingbo Tang, Shuaike Zhang, Xiaoshuai Hao, Pengwei Wang, Jianlong Wu, Zhongyuan Wang, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00778">AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring the affordance of an object and grasping it in a task-oriented manner is crucial for robots to successfully complete manipulation tasks. Affordance indicates where and how to grasp an object by taking its functionality into account, serving as the foundation for effective task-oriented grasping. However, current task-oriented methods often depend on extensive training data that is confined to specific tasks and objects, making it difficult to generalize to novel objects and complex scenes. In this paper, we introduce AffordGrasp, a novel open-vocabulary grasping framework that leverages the reasoning capabilities of vision-language models (VLMs) for in-context affordance reasoning. Unlike existing methods that rely on explicit task and object specifications, our approach infers tasks directly from implicit user instructions, enabling more intuitive and seamless human-robot interaction in everyday scenarios. Building on the reasoning outcomes, our framework identifies task-relevant objects and grounds their part-level affordances using a visual grounding module. This allows us to generate task-oriented grasp poses precisely within the affordance regions of the object, ensuring both functional and context-aware robotic manipulation. Extensive experiments demonstrate that AffordGrasp achieves state-of-the-art performance in both simulation and real-world scenarios, highlighting the effectiveness of our method. We believe our approach advances robotic manipulation techniques and contributes to the broader field of embodied AI. Project website: https://eqcy.github.io/affordgrasp/.
<div id='section'>PaperID: <span id='pid'>260, <a href='https://arxiv.org/pdf/2502.19645.pdf' target='_blank'>https://arxiv.org/pdf/2502.19645.pdf</a></span>   <span><a href='https://openvla-oft.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Moo Jin Kim, Chelsea Finn, Percy Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19645">Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26$\times$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs ($Ï_0$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.
<div id='section'>PaperID: <span id='pid'>261, <a href='https://arxiv.org/pdf/2502.19090.pdf' target='_blank'>https://arxiv.org/pdf/2502.19090.pdf</a></span>   <span><a href='https://github.com/TianCuteQY/EndoMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19090">EndoMamba: An Efficient Foundation Model for Endoscopic Videos via Hierarchical Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Endoscopic video-based tasks, such as visual navigation and surgical phase recognition, play a crucial role in minimally invasive surgeries by providing real-time assistance. While recent video foundation models have shown promise, their applications are hindered by (1) computational inefficiencies and (2) suboptimal performance caused by limited data for pre-training in endoscopy. To address these issues, we present EndoMamba, a foundation model designed for real-time inference while learning generalized spatiotemporal representations. First, to mitigate computational inefficiencies, we propose the EndoMamba backbone, optimized for real-time inference. Inspired by recent advancements in state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial modeling within individual frames and vanilla Mamba blocks for past-to-present reasoning across the temporal domain. This design enables both strong spatiotemporal modeling and efficient inference in online video streams. Second, we propose a self-supervised hierarchical pre-training diagram to enhance EndoMamba's representation learning using endoscopic videos and incorporating general video domain knowledge. Specifically, our approach combines masked reconstruction with auxiliary supervision, leveraging low-level reconstruction to capture spatial-temporal structures and high-level alignment to transfer broader knowledge from a pretrained general-video domain foundation model. Extensive experiments on four downstream tasks--classification, segmentation, surgical phase recognition, and localization--demonstrate that EndoMamba outperforms existing foundation models and task-specific methods while maintaining real-time inference speed. The source code is available at https://github.com/TianCuteQY/EndoMamba.
<div id='section'>PaperID: <span id='pid'>262, <a href='https://arxiv.org/pdf/2502.12861.pdf' target='_blank'>https://arxiv.org/pdf/2502.12861.pdf</a></span>   <span><a href='https://github.com/icleveston/InstructRobot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Iury Cleveston, Alana C. Santana, Paula D. P. Costa, Ricardo R. Gudwin, Alexandre S. SimÃµes, Esther L. Colombini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12861">InstructRobot: A Model-Free Framework for Mapping Natural Language Instructions into Robot Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to communicate with robots using natural language is a significant step forward in human-robot interaction. However, accurately translating verbal commands into physical actions is promising, but still presents challenges. Current approaches require large datasets to train the models and are limited to robots with a maximum of 6 degrees of freedom. To address these issues, we propose a framework called InstructRobot that maps natural language instructions into robot motion without requiring the construction of large datasets or prior knowledge of the robot's kinematics model. InstructRobot employs a reinforcement learning algorithm that enables joint learning of language representations and inverse kinematics model, simplifying the entire learning process. The proposed framework is validated using a complex robot with 26 revolute joints in object manipulation tasks, demonstrating its robustness and adaptability in realistic environments. The framework can be applied to any task or domain where datasets are scarce and difficult to create, making it an intuitive and accessible solution to the challenges of training robots using linguistic communication. Open source code for the InstructRobot framework and experiments can be accessed at https://github.com/icleveston/InstructRobot.
<div id='section'>PaperID: <span id='pid'>263, <a href='https://arxiv.org/pdf/2502.12532.pdf' target='_blank'>https://arxiv.org/pdf/2502.12532.pdf</a></span>   <span><a href='https://github.com/BiluYong/CityEQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12532">CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.
<div id='section'>PaperID: <span id='pid'>264, <a href='https://arxiv.org/pdf/2502.12449.pdf' target='_blank'>https://arxiv.org/pdf/2502.12449.pdf</a></span>   <span><a href='https://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Gang Yang, Miao Wang, Quan Zhou, Jiangchuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12449">YUNet: Improved YOLOv11 Network for Skyline Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skyline detection plays an important role in geolocalizaion, flight control, visual navigation, port security, etc. The appearance of the sky and non-sky areas are variable, because of different weather or illumination environment, which brings challenges to skyline detection. In this research, we proposed the YUNet algorithm, which improved the YOLOv11 architecture to segment the sky region and extract the skyline in complicated and variable circumstances. To improve the ability of multi-scale and large range contextual feature fusion, the YOLOv11 architecture is extended as an UNet-like architecture, consisting of an encoder, neck and decoder submodule. The encoder extracts the multi-scale features from the given images. The neck makes fusion of these multi-scale features. The decoder applies the fused features to complete the prediction rebuilding. To validate the proposed approach, the YUNet was tested on Skyfinder and CH1 datasets for segmentation and skyline detection respectively. Our test shows that the IoU of YUnet segmentation can reach 0.9858, and the average error of YUnet skyline detection is just 1.36 pixels. The implementation is published at https://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git.
<div id='section'>PaperID: <span id='pid'>265, <a href='https://arxiv.org/pdf/2502.09278.pdf' target='_blank'>https://arxiv.org/pdf/2502.09278.pdf</a></span>   <span><a href='https://onatsahin.github.io/ConsistentDreamer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Onat Åahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09278">ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View Gaussian Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have significantly improved 3D generation, enabling the use of assets generated from an image for embodied AI simulations. However, the one-to-many nature of the image-to-3D problem limits their use due to inconsistent content and quality across views. Previous models optimize a 3D model by sampling views from a view-conditioned diffusion prior, but diffusion models cannot guarantee view consistency. Instead, we present ConsistentDreamer, where we first generate a set of fixed multi-view prior images and sample random views between them with another diffusion model through a score distillation sampling (SDS) loss. Thereby, we limit the discrepancies between the views guided by the SDS loss and ensure a consistent rough shape. In each iteration, we also use our generated multi-view prior images for fine-detail reconstruction. To balance between the rough shape and the fine-detail optimizations, we introduce dynamic task-dependent weights based on homoscedastic uncertainty, updated automatically in each iteration. Additionally, we employ opacity, depth distortion, and normal alignment losses to refine the surface for mesh extraction. Our method ensures better view consistency and visual quality compared to the state-of-the-art.
<div id='section'>PaperID: <span id='pid'>266, <a href='https://arxiv.org/pdf/2502.07380.pdf' target='_blank'>https://arxiv.org/pdf/2502.07380.pdf</a></span>   <span><a href='https://uwrobotlearning.github.io/WheeledLab/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tyler Han, Preet Shah, Sidharth Rajagopal, Yanda Bao, Sanghun Jung, Sidharth Talia, Gabriel Guo, Bryan Xu, Bhaumik Mehta, Emma Romig, Rosario Scalise, Byron Boots
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07380">Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) has been pivotal in recent robotics milestones and is poised to play a prominent role in the future. However, these advances can rely on proprietary simulators, expensive hardware, and a daunting range of tools and skills. As a result, broader communities are disconnecting from the state-of-the-art; education curricula are poorly equipped to teach indispensable modern robotics skills involving hardware, deployment, and iterative development. To address this gap between the broader and scientific communities, we contribute Wheeled Lab, an ecosystem which integrates accessible, open-source wheeled robots with Isaac Lab, an open-source robot learning and simulation framework, that is widely adopted in the state-of-the-art. To kickstart research and education, this work demonstrates three state-of-the-art zero-shot policies for small-scale RC cars developed through Wheeled Lab: controlled drifting, elevation traversal, and visual navigation. The full stack, from hardware to software, is low-cost and open-source. Videos and additional materials can be found at: https://uwrobotlearning.github.io/WheeledLab/
<div id='section'>PaperID: <span id='pid'>267, <a href='https://arxiv.org/pdf/2502.03822.pdf' target='_blank'>https://arxiv.org/pdf/2502.03822.pdf</a></span>   <span><a href='https://apollo-lab-yale.github.io/25-RSS-DRIFT-website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiatao Sun, Shuo Yang, Yinxing Chen, Francis Fan, Yiyan Liang, Daniel Rakita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03822">Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion policies trained via offline behavioral cloning have recently gained traction in robotic motion generation. While effective, these policies typically require a large number of trainable parameters. This model size affords powerful representations but also incurs high computational cost during training. Ideally, it would be beneficial to dynamically adjust the trainable portion as needed, balancing representational power with computational efficiency. For example, while overparameterization enables diffusion policies to capture complex robotic behaviors via offline behavioral cloning, the increased computational demand makes online interactive imitation learning impractical due to longer training time. To address this challenge, we present a framework, called DRIFT, that uses the Singular Value Decomposition to enable dynamic rank adjustment during diffusion policy training. We implement and demonstrate the benefits of this framework in DRIFT-DAgger, an imitation learning algorithm that can seamlessly slide between an offline bootstrapping phase and an online interactive phase. We perform extensive experiments to better understand the proposed framework, and demonstrate that DRIFT-DAgger achieves improved sample efficiency and faster training with minimal impact on model performance. The project website is available at: https://apollo-lab-yale.github.io/25-RSS-DRIFT-website/.
<div id='section'>PaperID: <span id='pid'>268, <a href='https://arxiv.org/pdf/2502.00392.pdf' target='_blank'>https://arxiv.org/pdf/2502.00392.pdf</a></span>   <span><a href='https://github.com/sunzc-sunny/refdrone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhichao Sun, Yepeng Liu, Huachao Zhu, Yuliang Gu, Yuda Zou, Zelong Liu, Gui-Song Xia, Bo Du, Yongchao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00392">RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code are be publicly at https://github.com/sunzc-sunny/refdrone.
<div id='section'>PaperID: <span id='pid'>269, <a href='https://arxiv.org/pdf/2502.00379.pdf' target='_blank'>https://arxiv.org/pdf/2502.00379.pdf</a></span>   <span><a href='https://github.com/dunnolab/laom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00379">Latent Action Learning Requires Supervision in the Presence of Distractors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.
<div id='section'>PaperID: <span id='pid'>270, <a href='https://arxiv.org/pdf/2501.16899.pdf' target='_blank'>https://arxiv.org/pdf/2501.16899.pdf</a></span>   <span><a href='https://github.com/shadynasrat/RDMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shady Nasrat, Myungsu Kim, Seonil Lee, Jiho Lee, Yeoncheol Jang, Seung-joon Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16899">RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with Enhanced Contextual Awareness in Specific Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) represent a significant advancement in integrating physical robots with AI-driven systems. We showcase the capabilities of our framework within the context of the real-world household competition. This research introduces a framework that utilizes RDMM (Robotics Decision-Making Models), which possess the capacity for decision-making within domain-specific contexts, as well as an awareness of their personal knowledge and capabilities. The framework leverages information to enhance the autonomous decision-making of the system. In contrast to other approaches, our focus is on real-time, on-device solutions, successfully operating on hardware with as little as 8GB of memory. Our framework incorporates visual perception models equipping robots with understanding of their environment. Additionally, the framework has integrated real-time speech recognition capabilities, thus enhancing the human-robot interaction experience. Experimental results demonstrate that the RDMM framework can plan with an 93\% accuracy. Furthermore, we introduce a new dataset consisting of 27k planning instances, as well as 1.3k text-image annotated samples derived from the competition. The framework, benchmarks, datasets, and models developed in this work are publicly available on our GitHub repository at https://github.com/shadynasrat/RDMM.
<div id='section'>PaperID: <span id='pid'>271, <a href='https://arxiv.org/pdf/2501.13416.pdf' target='_blank'>https://arxiv.org/pdf/2501.13416.pdf</a></span>   <span><a href='https://github.com/AbrarAnwar/masked-social-signals/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiming Tang, Abrar Anwar, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13416">M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding social signals in multi-party conversations is important for human-robot interaction and artificial social intelligence. Social signals include body pose, head pose, speech, and context-specific activities like acquiring and taking bites of food when dining. Past work in multi-party interaction tends to build task-specific models for predicting social signals. In this work, we address the challenge of predicting multimodal social signals in multi-party settings in a single model. We introduce M3PT, a causal transformer architecture with modality and temporal blockwise attention masking to simultaneously process multiple social cues across multiple participants and their temporal interactions. We train and evaluate M3PT on the Human-Human Commensality Dataset (HHCD), and demonstrate that using multiple modalities improves bite timing and speaking status prediction. Source code: https://github.com/AbrarAnwar/masked-social-signals/.
<div id='section'>PaperID: <span id='pid'>272, <a href='https://arxiv.org/pdf/2501.11858.pdf' target='_blank'>https://arxiv.org/pdf/2501.11858.pdf</a></span>   <span><a href='https://github.com/thunlp/EmbodiedEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11858">EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.
<div id='section'>PaperID: <span id='pid'>273, <a href='https://arxiv.org/pdf/2501.10105.pdf' target='_blank'>https://arxiv.org/pdf/2501.10105.pdf</a></span>   <span><a href='https://github.com/2toinf/UniAct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10105">Universal Actions for Enhanced Embodied Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training on diverse, internet-scale data is a key factor in the success of recent large foundation models. Yet, using the same recipe for building embodied agents has faced noticeable difficulties. Despite the availability of many crowd-sourced embodied datasets, their action spaces often exhibit significant heterogeneity due to distinct physical embodiment and control interfaces for different robots, causing substantial challenges in developing embodied foundation models using cross-domain data. In this paper, we introduce UniAct, a new embodied foundation modeling framework operating in a Universal Action Space. Our learned universal actions capture the generic atomic behaviors across diverse robots by exploiting their shared structural features, and enable enhanced cross-domain data utilization and cross-embodiment generalizations by eliminating the notorious heterogeneity. The universal actions can be efficiently translated back to heterogeneous actionable commands by simply adding embodiment-specific details, from which fast adaptation to new robots becomes simple and straightforward. Our 0.5B instantiation of UniAct outperforms 14X larger SOTA embodied foundation models in extensive evaluations on various real-world and simulation robots, showcasing exceptional cross-embodiment control and adaptation capability, highlighting the crucial benefit of adopting universal actions. Project page: https://github.com/2toinf/UniAct
<div id='section'>PaperID: <span id='pid'>274, <a href='https://arxiv.org/pdf/2501.09167.pdf' target='_blank'>https://arxiv.org/pdf/2501.09167.pdf</a></span>   <span><a href='https://metadriverse.github.io/metavqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weizhen Wang, Chenda Duan, Zhenghao Peng, Yuxin Liu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09167">Embodied Scene Understanding for Vision Language Models via MetaVQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present MetaVQA: a comprehensive benchmark designed to assess and enhance VLMs' understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations. MetaVQA leverages Set-of-Mark prompting and top-down view ground-truth annotations from nuScenes and Waymo datasets to automatically generate extensive question-answer pairs based on diverse real-world traffic scenarios, ensuring object-centric and context-rich instructions. Our experiments show that fine-tuning VLMs with the MetaVQA dataset significantly improves their spatial reasoning and embodied scene comprehension in safety-critical simulations, evident not only in improved VQA accuracies but also in emerging safety-aware driving maneuvers. In addition, the learning demonstrates strong transferability from simulation to real-world observation. Code and data will be publicly available at https://metadriverse.github.io/metavqa .
<div id='section'>PaperID: <span id='pid'>275, <a href='https://arxiv.org/pdf/2501.07051.pdf' target='_blank'>https://arxiv.org/pdf/2501.07051.pdf</a></span>   <span><a href='https://github.com/CHRI-Lab/ROSAnnotator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yan Zhang, Haoqi Li, Ramtin Tabatabaei, Wafa Johal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07051">ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) is an interdisciplinary field that utilises both quantitative and qualitative methods. While ROSBags, a file format within the Robot Operating System (ROS), offer an efficient means of collecting temporally synched multimodal data in empirical studies with real robots, there is a lack of tools specifically designed to integrate qualitative coding and analysis functions with ROSBags. To address this gap, we developed ROSAnnotator, a web-based application that incorporates a multimodal Large Language Model (LLM) to support both manual and automated annotation of ROSBag data. ROSAnnotator currently facilitates video, audio, and transcription annotations and provides an open interface for custom ROS messages and tools. By using ROSAnnotator, researchers can streamline the qualitative analysis process, create a more cohesive analysis pipeline, and quickly access statistical summaries of annotations, thereby enhancing the overall efficiency of HRI data analysis. https://github.com/CHRI-Lab/ROSAnnotator
<div id='section'>PaperID: <span id='pid'>276, <a href='https://arxiv.org/pdf/2501.06693.pdf' target='_blank'>https://arxiv.org/pdf/2501.06693.pdf</a></span>   <span><a href='https://metadriverse.github.io/vid2sim/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06693">Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim-to-real gap has long posed a significant challenge for robot learning in simulation, preventing the deployment of learned models in the real world. Previous work has primarily focused on domain randomization and system identification to mitigate this gap. However, these methods are often limited by the inherent constraints of the simulation and graphics engines. In this work, we propose Vid2Sim, a novel framework that effectively bridges the sim2real gap through a scalable and cost-efficient real2sim pipeline for neural 3D scene reconstruction and simulation. Given a monocular video as input, Vid2Sim can generate photorealistic and physically interactable 3D simulation environments to enable the reinforcement learning of visual navigation agents in complex urban environments. Extensive experiments demonstrate that Vid2Sim significantly improves the performance of urban navigation in the digital twins and real world by 31.2% and 68.3% in success rate compared with agents trained with prior simulation methods.
<div id='section'>PaperID: <span id='pid'>277, <a href='https://arxiv.org/pdf/2412.18774.pdf' target='_blank'>https://arxiv.org/pdf/2412.18774.pdf</a></span>   <span><a href='https://github.com/Jianbo-maker/EPD_benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianbo Zhang, Chunyi Li, Jie Hao, Jun Jia, Huiyu Duan, Guoquan Zheng, Liang Yuan, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18774">Embodied Image Quality Assessment for Robotic Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) of User-Generated Content (UGC) is a critical technique for human Quality of Experience (QoE). However, does the the image quality of Robot-Generated Content (RGC) demonstrate traits consistent with the Moravec paradox, potentially conflicting with human perceptual norms? Human subjective scoring is more based on the attractiveness of the image. Embodied agent are required to interact and perceive in the environment, and finally perform specific tasks. Visual images as inputs directly influence downstream tasks. In this paper, we explore the perception mechanism of embodied robots for image quality. We propose the first Embodied Preference Database (EPD), which contains 12,500 distorted image annotations. We establish assessment metrics based on the downstream tasks of robot. In addition, there is a gap between UGC and RGC. To address this, we propose a novel Multi-scale Attention Embodied Image Quality Assessment called MA-EIQA. For the proposed EPD dataset, this is the first no-reference IQA model designed for embodied robot. Finally, the performance of mainstream IQA algorithms on EPD dataset is verified. The experiments demonstrate that quality assessment of embodied images is different from that of humans. We sincerely hope that the EPD can contribute to the development of embodied AI by focusing on image quality assessment. The benchmark is available at https://github.com/Jianbo-maker/EPD_benchmark.
<div id='section'>PaperID: <span id='pid'>278, <a href='https://arxiv.org/pdf/2412.18600.pdf' target='_blank'>https://arxiv.org/pdf/2412.18600.pdf</a></span>   <span><a href='https://awfuact.github.io/zerohsi/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18600">ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-scene interaction (HSI) generation is crucial for applications in embodied AI, virtual reality, and robotics. Yet, existing methods cannot synthesize interactions in unseen environments such as in-the-wild scenes or reconstructed scenes, as they rely on paired 3D scenes and captured human motion data for training, which are unavailable for unseen environments. We present ZeroHSI, a novel approach that enables zero-shot 4D human-scene interaction synthesis, eliminating the need for training on any MoCap data. Our key insight is to distill human-scene interactions from state-of-the-art video generation models, which have been trained on vast amounts of natural human movements and interactions, and use differentiable rendering to reconstruct human-scene interactions. ZeroHSI can synthesize realistic human motions in both static scenes and environments with dynamic objects, without requiring any ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different types of various indoor and outdoor scenes with different interaction prompts, demonstrating its ability to generate diverse and contextually appropriate human-scene interactions.
<div id='section'>PaperID: <span id='pid'>279, <a href='https://arxiv.org/pdf/2412.18335.pdf' target='_blank'>https://arxiv.org/pdf/2412.18335.pdf</a></span>   <span><a href='https://gauleejx.github.io/flona/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaxin Li, Weiqi Huang, Zan Wang, Wei Liang, Huijun Di, Feng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18335">FloNa: Floor Plan Guided Embodied Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally rely on floor plans to navigate in unfamiliar environments, as they are readily available, reliable, and provide rich geometrical guidance. However, existing visual navigation settings overlook this valuable prior knowledge, leading to limited efficiency and accuracy. To eliminate this gap, we introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the first attempt to incorporate floor plan into embodied visual navigation. While the floor plan offers significant advantages, two key challenges emerge: (1) handling the spatial inconsistency between the floor plan and the actual scene layout for collision-free navigation, and (2) aligning observed images with the floor plan sketch despite their distinct modalities. To address these challenges, we propose FloDiff, a novel diffusion policy framework incorporating a localization module to facilitate alignment between the current observation and the floor plan. We further collect $20k$ navigation episodes across $117$ scenes in the iGibson simulator to support the training and evaluation. Extensive experiments demonstrate the effectiveness and efficiency of our framework in unfamiliar scenes using floor plan knowledge. Project website: https://gauleejx.github.io/flona/.
<div id='section'>PaperID: <span id='pid'>280, <a href='https://arxiv.org/pdf/2412.14480.pdf' target='_blank'>https://arxiv.org/pdf/2412.14480.pdf</a></span>   <span><a href='https://saumyasaxena.github.io/grapheqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Saumya Saxena, Blake Buchanan, Chris Paxton, Peiqi Liu, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14480">GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment to answer a situated question with confidence. This problem remains challenging in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient planning and exploration. To address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantics-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps. We further demonstrate GraphEQA in multiple real-world home and office environments.
<div id='section'>PaperID: <span id='pid'>281, <a href='https://arxiv.org/pdf/2412.13729.pdf' target='_blank'>https://arxiv.org/pdf/2412.13729.pdf</a></span>   <span><a href='https://github.com/tmralmeida/thor-magni-actions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tiago Rodrigues de Almeida, Tim Schreiter, Andrey Rudenko, Luigi Palmieiri, Johannes A. Stork, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13729">THÃR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the THÃR-MAGNI Act dataset, a substantial extension of the THÃR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. THÃR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided THÃR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of THÃR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of THÃR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.
<div id='section'>PaperID: <span id='pid'>282, <a href='https://arxiv.org/pdf/2412.09585.pdf' target='_blank'>https://arxiv.org/pdf/2412.09585.pdf</a></span>   <span><a href='https://praeclarumjj3.github.io/visper_lm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, Jianwei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09585">Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent times, the standard practice for developing MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. This approach often causes models to lean towards language comprehension and undermine the rich visual perception signals present in the data, which are critical for tasks involving spatial reasoning in the domain of embodied AI and robotics. Is it possible to optimize both at the same time? In this work, we propose VisPer-LM, the first approach that infuses visual perception knowledge from expert vision encoders into the LLM's (of an MLLM) hidden representations. We start by investigating MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Given this insight, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next (text) token prediction. Moreover, through extensive probing, we observe improved visual representation quality due to embedding optimization, underscoring the effectiveness of our probing setup. We demonstrate that our VisPer-LM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. In particular, VisPer-LM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench.
<div id='section'>PaperID: <span id='pid'>283, <a href='https://arxiv.org/pdf/2412.08467.pdf' target='_blank'>https://arxiv.org/pdf/2412.08467.pdf</a></span>   <span><a href='https://github.com/wz0919/VLN-SRDF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08467">Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.
<div id='section'>PaperID: <span id='pid'>284, <a href='https://arxiv.org/pdf/2412.06224.pdf' target='_blank'>https://arxiv.org/pdf/2412.06224.pdf</a></span>   <span><a href='https://pku-epic.github.io/Uni-NaVid/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06224">Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments. Uni-NaVid achieves this by harmonizing the input and output data configurations for all commonly used embodied navigation tasks and thereby integrating all tasks in one model. For training Uni-NaVid, we collect 3.6 million navigation data samples in total from four essential navigation sub-tasks and foster synergy in learning across them. Extensive experiments on comprehensive navigation benchmarks clearly demonstrate the advantages of unification modeling in Uni-NaVid and show it achieves state-of-the-art performance. Additionally, real-world experiments confirm the model's effectiveness and efficiency, shedding light on its strong generalizability.
<div id='section'>PaperID: <span id='pid'>285, <a href='https://arxiv.org/pdf/2412.01663.pdf' target='_blank'>https://arxiv.org/pdf/2412.01663.pdf</a></span>   <span><a href='https://rlc-lab.github.io/dadu-e/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenhao Sun, Sai Hou, Zixuan Wang, Bo Yu, Shaoshan Liu, Xu Yang, Shuai Liang, Yiming Gan, Yinhe Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01663">DaDu-E: Rethinking the Role of Large Language Model in Robotic Computing Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing complex tasks in open environments remains challenging for robots, even when using large language models (LLMs) as the core planner. Many LLM-based planners are inefficient due to their large number of parameters and prone to inaccuracies because they operate in open-loop systems. We think the reason is that only applying LLMs as planners is insufficient. In this work, we propose DaDu-E, a robust closed-loop planning framework for embodied AI robots. Specifically, DaDu-E is equipped with a relatively lightweight LLM, a set of encapsulated robot skill instructions, a robust feedback system, and memory augmentation. Together, these components enable DaDu-E to (i) actively perceive and adapt to dynamic environments, (ii) optimize computational costs while maintaining high performance, and (iii) recover from execution failures using its memory and feedback mechanisms. Extensive experiments on real-world and simulated tasks show that DaDu-E achieves task success rates comparable to embodied AI robots with larger models as planners like COME-Robot, while reducing computational requirements by $6.6 \times$. Users are encouraged to explore our system at: \url{https://rlc-lab.github.io/dadu-e/}.
<div id='section'>PaperID: <span id='pid'>286, <a href='https://arxiv.org/pdf/2412.01179.pdf' target='_blank'>https://arxiv.org/pdf/2412.01179.pdf</a></span>   <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tao Tang, Hong Liu, Yingxuan You, Ti Wang, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01179">Dual-Branch Graph Transformer Network for 3D Human Mesh Reconstruction from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Mesh Reconstruction (HMR) from monocular video plays an important role in human-robot interaction and collaboration. However, existing video-based human mesh reconstruction methods face a trade-off between accurate reconstruction and smooth motion. These methods design networks based on either RNNs or attention mechanisms to extract local temporal correlations or global temporal dependencies, but the lack of complementary long-term information and local details limits their performance. To address this problem, we propose a \textbf{D}ual-branch \textbf{G}raph \textbf{T}ransformer network for 3D human mesh \textbf{R}econstruction from video, named DGTR. DGTR employs a dual-branch network including a Global Motion Attention (GMA) branch and a Local Details Refine (LDR) branch to parallelly extract long-term dependencies and local crucial information, helping model global human motion and local human details (e.g., local motion, tiny movement). Specifically, GMA utilizes a global transformer to model long-term human motion. LDR combines modulated graph convolutional networks and the transformer framework to aggregate local information in adjacent frames and extract crucial information of human details. Experiments demonstrate that our DGTR outperforms state-of-the-art video-based methods in reconstruction accuracy and maintains competitive motion smoothness. Moreover, DGTR utilizes fewer parameters and FLOPs, which validate the effectiveness and efficiency of the proposed DGTR. Code is publicly available at \href{https://github.com/TangTao-PKU/DGTR}{\textcolor{myBlue}{https://github.com/TangTao-PKU/DGTR}}.
<div id='section'>PaperID: <span id='pid'>287, <a href='https://arxiv.org/pdf/2411.17820.pdf' target='_blank'>https://arxiv.org/pdf/2411.17820.pdf</a></span>   <span><a href='https://ai4ce.github.io/CityWalker/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinhao Liu, Jintong Li, Yicheng Jiang, Niranjan Sujay, Zhicheng Yang, Juexiao Zhang, John Abanes, Jing Zhang, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17820">CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating dynamic urban environments presents significant challenges for embodied agents, requiring advanced spatial reasoning and adherence to common-sense norms. Despite progress, existing visual navigation methods struggle in map-free or off-street settings, limiting the deployment of autonomous agents like last-mile delivery robots. To overcome these obstacles, we propose a scalable, data-driven approach for human-like urban navigation by training agents on thousands of hours of in-the-wild city walking and driving videos sourced from the web. We introduce a simple and scalable data processing pipeline that extracts action supervision from these videos, enabling large-scale imitation learning without costly annotations. Our model learns sophisticated navigation policies to handle diverse challenges and critical scenarios. Experimental results show that training on large-scale, diverse datasets significantly enhances navigation performance, surpassing current methods. This work shows the potential of using abundant online video data to develop robust navigation policies for embodied agents in dynamic urban settings. Project homepage is at https://ai4ce.github.io/CityWalker/.
<div id='section'>PaperID: <span id='pid'>288, <a href='https://arxiv.org/pdf/2411.16310.pdf' target='_blank'>https://arxiv.org/pdf/2411.16310.pdf</a></span>   <span><a href='https://tev-fbk.github.io/fun3du/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jaime Corsetti, Francesco Giuliari, Alice Fasoli, Davide Boscaini, Fabio Poiesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16310">Functionality understanding and segmentation in 3D scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like 'turn on the ceiling light', an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description. To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a vision and language model. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Project page: https://tev-fbk.github.io/fun3du/
<div id='section'>PaperID: <span id='pid'>289, <a href='https://arxiv.org/pdf/2411.15714.pdf' target='_blank'>https://arxiv.org/pdf/2411.15714.pdf</a></span>   <span><a href='https://github.com/harrytea/ROOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yonghui Wang, Shi-Yong Chen, Zhenxing Zhou, Siyi Li, Haoran Li, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15714">ROOT: VLM based System for Indoor Scene Understanding and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vision Language Models (VLMs) have experienced significant advancements, yet these models still face challenges in spatial hierarchical reasoning within indoor scenes. In this study, we introduce ROOT, a VLM-based system designed to enhance the analysis of indoor scenes. Specifically, we first develop an iterative object perception algorithm using GPT-4V to detect object entities within indoor scenes. This is followed by employing vision foundation models to acquire additional meta-information about the scene, such as bounding boxes. Building on this foundational data, we propose a specialized VLM, SceneVLM, which is capable of generating spatial hierarchical scene graphs and providing distance information for objects within indoor environments. This information enhances our understanding of the spatial arrangement of indoor scenes. To train our SceneVLM, we collect over 610,000 images from various public indoor datasets and implement a scene data generation pipeline with a semi-automated technique to establish relationships and estimate distances among indoor objects. By utilizing this enriched data, we conduct various training recipes and finish SceneVLM. Our experiments demonstrate that \rootname facilitates indoor scene understanding and proves effective in diverse downstream applications, such as 3D scene generation and embodied AI. The code will be released at \url{https://github.com/harrytea/ROOT}.
<div id='section'>PaperID: <span id='pid'>290, <a href='https://arxiv.org/pdf/2411.13674.pdf' target='_blank'>https://arxiv.org/pdf/2411.13674.pdf</a></span>   <span><a href='https://github.com/knowledgetechnologyuhh/FabuLight-ASD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hugo Carneiro, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13674">FabuLight-ASD: Unveiling Speech Activity via Body Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active speaker detection (ASD) in multimodal environments is crucial for various applications, from video conferencing to human-robot interaction. This paper introduces FabuLight-ASD, an advanced ASD model that integrates facial, audio, and body pose information to enhance detection accuracy and robustness. Our model builds upon the existing Light-ASD framework by incorporating human pose data, represented through skeleton graphs, which minimises computational overhead. Using the Wilder Active Speaker Detection (WASD) dataset, renowned for reliable face and body bounding box annotations, we demonstrate FabuLight-ASD's effectiveness in real-world scenarios. Achieving an overall mean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD, which has an overall mAP of 93.7% across various challenging scenarios. The incorporation of body pose information shows a particularly advantageous impact, with notable improvements in mAP observed in scenarios with speech impairment, face occlusion, and human voice background noise. Furthermore, efficiency analysis indicates only a modest increase in parameter count (27.3%) and multiply-accumulate operations (up to 2.4%), underscoring the model's efficiency and feasibility. These findings validate the efficacy of FabuLight-ASD in enhancing ASD performance through the integration of body pose data. FabuLight-ASD's code and model weights are available at https://github.com/knowledgetechnologyuhh/FabuLight-ASD.
<div id='section'>PaperID: <span id='pid'>291, <a href='https://arxiv.org/pdf/2411.13587.pdf' target='_blank'>https://arxiv.org/pdf/2411.13587.pdf</a></span>   <span><a href='https://github.com/William-wAng618/roboticAttack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Taowen Wang, Cheng Han, James Chenhao Liang, Wenhao Yang, Dongfang Liu, Luna Xinyu Zhang, Qifan Wang, Jiebo Luo, Ruixiang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13587">Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. Despite their significant capabilities, VLA models introduce new attack surfaces. This paper systematically evaluates their robustness. Recognizing the unique demands of robotic execution, our attack objectives target the inherent spatial and functional characteristics of robotic systems. In particular, we introduce two untargeted attack objectives that leverage spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory. Additionally, we design an adversarial patch generation approach that places a small, colorful patch within the camera's view, effectively executing the attack in both digital and physical environments. Our evaluation reveals a marked degradation in task success rates, with up to a 100\% reduction across a suite of simulated robotic tasks, highlighting critical security gaps in current VLA architectures. By unveiling these vulnerabilities and proposing actionable evaluation metrics, we advance both the understanding and enhancement of safety for VLA-based robotic systems, underscoring the necessity for continuously developing robust defense strategies prior to physical-world deployments.
<div id='section'>PaperID: <span id='pid'>292, <a href='https://arxiv.org/pdf/2411.05902.pdf' target='_blank'>https://arxiv.org/pdf/2411.05902.pdf</a></span>   <span><a href='https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05902">Autoregressive Models in Vision: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.
<div id='section'>PaperID: <span id='pid'>293, <a href='https://arxiv.org/pdf/2411.02236.pdf' target='_blank'>https://arxiv.org/pdf/2411.02236.pdf</a></span>   <span><a href='https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Artem Sokolov, Swapnil Bhosale, Xiatian Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02236">3D Audio-Visual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing the sounding objects in scenes is a longstanding objective in embodied AI, with diverse applications in robotics and AR/VR/MR. To that end, Audio-Visual Segmentation (AVS), taking as condition an audio signal to identify the masks of the target sounding objects in an input image with synchronous camera and microphone sensors, has been recently advanced. However, this paradigm is still insufficient for real-world operation, as the mapping from 2D images to 3D scenes is missing. To address this fundamental limitation, we introduce a novel research problem, 3D Audio-Visual Segmentation, extending the existing AVS to the 3D output space. This problem poses more challenges due to variations in camera extrinsics, audio scattering, occlusions, and diverse acoustics across sounding object categories. To facilitate this research, we create the very first simulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D scene environments with grounded spatial audio under single-instance and multi-instance settings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator to generate comprehensive annotations of sounding object locations and corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet, characterized by integrating the ready-to-use knowledge from pretrained 2D audio-visual foundation models synergistically with 3D visual scene representation through spatial audio-aware mask alignment and refinement. Extensive experiments demonstrate that EchoSegnet can effectively segment sounding objects in 3D space on our new benchmark, representing a significant advancement in the field of embodied AI. Project page: https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/
<div id='section'>PaperID: <span id='pid'>294, <a href='https://arxiv.org/pdf/2410.22997.pdf' target='_blank'>https://arxiv.org/pdf/2410.22997.pdf</a></span>   <span><a href='https://github.com/AIS-Bonn/Prompt_Engineering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonas Bode, Bastian PÃ¤tzold, Raphael Memmesheimer, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22997">A Comparison of Prompt Engineering Techniques for Task Planning and Execution in Service Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in LLM have been instrumental in autonomous robot control and human-robot interaction by leveraging their vast general knowledge and capabilities to understand and reason across a wide range of tasks and scenarios. Previous works have investigated various prompt engineering techniques for improving the performance of LLM to accomplish tasks, while others have proposed methods that utilize LLMs to plan and execute tasks based on the available functionalities of a given robot platform. In this work, we consider both lines of research by comparing prompt engineering techniques and combinations thereof within the application of high-level task planning and execution in service robotics. We define a diverse set of tasks and a simple set of functionalities in simulation, and measure task completion accuracy and execution time for several state-of-the-art models.
<div id='section'>PaperID: <span id='pid'>295, <a href='https://arxiv.org/pdf/2410.20263.pdf' target='_blank'>https://arxiv.org/pdf/2410.20263.pdf</a></span>   <span><a href='https://github.com/chengkaiAcademyCity/EfficientEQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Kai Cheng, Zhengyuan Li, Xingpeng Sun, Byung-Cheol Min, Amrit Singh Bedi, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20263">EfficientEQA: An Efficient Approach to Open-Vocabulary Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) is an essential yet challenging task for robot assistants. Large vision-language models (VLMs) have shown promise for EQA, but existing approaches either treat it as static video question answering without active exploration or restrict answers to a closed set of choices. These limitations hinder real-world applicability, where a robot must explore efficiently and provide accurate answers in open-vocabulary settings. To overcome these challenges, we introduce EfficientEQA, a novel framework that couples efficient exploration with free-form answer generation. EfficientEQA features three key innovations: (1) Semantic-Value-Weighted Frontier Exploration (SFE) with Verbalized Confidence (VC) from a black-box VLM to prioritize semantically important areas to explore, enabling the agent to gather relevant information faster; (2) a BLIP relevancy-based mechanism to stop adaptively by flagging highly relevant observations as outliers to indicate whether the agent has collected enough information; and (3) a Retrieval-Augmented Generation (RAG) method for the VLM to answer accurately based on pertinent images from the agent's observation history without relying on predefined choices. Our experimental results show that EfficientEQA achieves over 15% higher answer accuracy and requires over 20% fewer exploration steps than state-of-the-art methods. Our code is available at: https://github.com/chengkaiAcademyCity/EfficientEQA
<div id='section'>PaperID: <span id='pid'>296, <a href='https://arxiv.org/pdf/2410.18373.pdf' target='_blank'>https://arxiv.org/pdf/2410.18373.pdf</a></span>   <span><a href='https://pi3-141592653.github.io/UGotMe/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Peizhen Li, Longbing Cao, Xiao-Ming Wu, Xiaohan Yu, Runze Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18373">UGotMe: An Embodied System for Affective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multiparty conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, realtime response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve realtime response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://pi3-141592653.github.io/UGotMe/.
<div id='section'>PaperID: <span id='pid'>297, <a href='https://arxiv.org/pdf/2410.18195.pdf' target='_blank'>https://arxiv.org/pdf/2410.18195.pdf</a></span>   <span><a href='https://aimagelab.github.io/pin/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Luca Barsellotti, Roberto Bigazzi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18195">Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.
<div id='section'>PaperID: <span id='pid'>298, <a href='https://arxiv.org/pdf/2410.16411.pdf' target='_blank'>https://arxiv.org/pdf/2410.16411.pdf</a></span>   <span><a href='https://github.com/clmoro/Robotics-RL-FMs-Integration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Angelo Moroncelli, Vishal Soni, Marco Forgione, Dario Piga, Blerina Spahiu, Loris Roveda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16411">The Duality of Generative AI and Reinforcement Learning in Robotics: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.
  Lastly, we identify open challenges accounting for model scalability, adaptation and grounding, giving recommendations and insights on future research directions. We reflect on which generative AI models best fit the RL tasks and why. On the other side, we reflect on important issues inherent to RL-enhanced generative policies, such as safety concerns and failure modes, and what are the limitations of current methods. A curated collection of relevant research papers is maintained on our GitHub repository, serving as a resource for ongoing research and development in this field: https://github.com/clmoro/Robotics-RL-FMs-Integration.
<div id='section'>PaperID: <span id='pid'>299, <a href='https://arxiv.org/pdf/2410.08208.pdf' target='_blank'>https://arxiv.org/pdf/2410.08208.pdf</a></span>   <span><a href='https://haoyizhu.github.io/spa/' target='_blank'>  GitHub</a></span> <span><a href='https://haoyizhu.github.io/spa/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyi Zhu, Honghui Yang, Yating Wang, Jiange Yang, Limin Wang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08208">SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present the most comprehensive evaluation of embodied representation learning to date, covering 268 tasks across 8 simulators with diverse policies in both single-task and language-conditioned multi-task scenarios. The results are compelling: SPA consistently outperforms more than 10 state-of-the-art representation methods, including those specifically designed for embodied AI, vision-centric tasks, and multi-modal applications, while using less training data. Furthermore, we conduct a series of real-world experiments to confirm its effectiveness in practical scenarios. These results highlight the critical role of 3D spatial awareness for embodied representation learning. Our strongest model takes more than 6000 GPU hours to train and we are committed to open-sourcing all code and model weights to foster future research in embodied representation learning. Project Page: https://haoyizhu.github.io/spa/.
<div id='section'>PaperID: <span id='pid'>300, <a href='https://arxiv.org/pdf/2410.02751.pdf' target='_blank'>https://arxiv.org/pdf/2410.02751.pdf</a></span>   <span><a href='https://github.com/aielawady/relic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ahmad Elawady, Gunjan Chhablani, Ram Ramrakhya, Karmesh Yadav, Dhruv Batra, Zsolt Kira, Andrew Szot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02751">ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called "partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic
<div id='section'>PaperID: <span id='pid'>301, <a href='https://arxiv.org/pdf/2410.01971.pdf' target='_blank'>https://arxiv.org/pdf/2410.01971.pdf</a></span>   <span><a href='https://aasherh.github.io/byovla/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Asher J. Hancock, Allen Z. Ren, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01971">Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models trained on large-scale internet data and robot demonstrations have the potential to serve as generalist robot policies. However, despite their large-scale training, VLAs are often brittle to task-irrelevant visual details such as distractor objects or background colors. We introduce Bring Your Own VLA (BYOVLA): a run-time intervention scheme that (1) dynamically identifies regions of the input image that the model is sensitive to, and (2) minimally alters task-irrelevant regions to reduce the model's sensitivity using automated image editing tools. Our approach is compatible with any off the shelf VLA without model fine-tuning or access to the model's weights. Hardware experiments on language-instructed manipulation tasks demonstrate that BYOVLA enables state-of-the-art VLA models to nearly retain their nominal performance in the presence of distractor objects and backgrounds, which otherwise degrade task success rates by up to 40%. Website with additional information, videos, and code: https://aasherh.github.io/byovla/ .
<div id='section'>PaperID: <span id='pid'>302, <a href='https://arxiv.org/pdf/2410.01273.pdf' target='_blank'>https://arxiv.org/pdf/2410.01273.pdf</a></span>   <span><a href='https://worv-ai.github.io/canvas' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Suhwan Choi, Yongjun Cho, Minchan Kim, Jaeyoon Jung, Myunchul Joe, Yubeen Park, Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, Jiwan Chung, Youngjae Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01273">CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.
<div id='section'>PaperID: <span id='pid'>303, <a href='https://arxiv.org/pdf/2409.20188.pdf' target='_blank'>https://arxiv.org/pdf/2409.20188.pdf</a></span>   <span><a href='https://github.com/bigzen/Active-Listener' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Bishal Ghosh, Emma Li, Tanaya Guha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20188">Active Listener: Continuous Generation of Listener's Head Motion Response in Dyadic Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key component of dyadic spoken interactions is the contextually relevant non-verbal gestures, such as head movements that reflect a listener's response to the interlocutor's speech. Although significant progress has been made in the context of generating co-speech gestures, generating listener's response has remained a challenge. We introduce the task of generating continuous head motion response of a listener in response to the speaker's speech in real time. To this end, we propose a graph-based end-to-end crossmodal model that takes interlocutor's speech audio as input and directly generates head pose angles (roll, pitch, yaw) of the listener in real time. Different from previous work, our approach is completely data-driven, does not require manual annotations or oversimplify head motion to merely nods and shakes. Extensive evaluation on the dyadic interaction sessions on the IEMOCAP dataset shows that our model produces a low overall error (4.5 degrees) and a high frame rate, thereby indicating its deployability in real-world human-robot interaction systems. Our code is available at - https://github.com/bigzen/Active-Listener
<div id='section'>PaperID: <span id='pid'>304, <a href='https://arxiv.org/pdf/2409.17004.pdf' target='_blank'>https://arxiv.org/pdf/2409.17004.pdf</a></span>   <span><a href='https://github.com/IrmakDogan/ExpressionDataset' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/IrmakDogan/ExpressionDataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Fethiye Irmak Dogan, Maithili Patel, Weiyu Liu, Iolanda Leite, Sonia Chernova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17004">A Model-Agnostic Approach for Semantically Driven Disambiguation in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ambiguities are inevitable in human-robot interaction, especially when a robot follows user instructions in a large, shared space. For example, if a user asks the robot to find an object in a home environment with underspecified instructions, the object could be in multiple locations depending on missing factors. For instance, a bowl might be in the kitchen cabinet or on the dining room table, depending on whether it is clean or dirty, full or empty, and the presence of other objects around it. Previous works on object search have assumed that the queried object is immediately visible to the robot or have predicted object locations using one-shot inferences, which are likely to fail for ambiguous or partially understood instructions. This paper focuses on these gaps and presents a novel model-agnostic approach leveraging semantically driven clarifications to enhance the robot's ability to locate queried objects in fewer attempts. Specifically, we leverage different knowledge embedding models, and when ambiguities arise, we propose an informative clarification method, which follows an iterative prediction process. The user experiment evaluation of our method shows that our approach is applicable to different custom semantic encoders as well as LLMs, and informative clarifications improve performances, enabling the robot to locate objects on its first attempts. The user experiment data is publicly available at https://github.com/IrmakDogan/ExpressionDataset.
<div id='section'>PaperID: <span id='pid'>305, <a href='https://arxiv.org/pdf/2409.15560.pdf' target='_blank'>https://arxiv.org/pdf/2409.15560.pdf</a></span>   <span><a href='https://github.com/exponentialR/QUB-PHEO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Samuel Adebayo, SeÃ¡n McLoone, Joost C. Dessing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15560">QUB-PHEO: A Visual-Based Dyadic Multi-View Dataset for Intention Inference in Collaborative Assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>QUB-PHEO introduces a visual-based, dyadic dataset with the potential of advancing human-robot interaction (HRI) research in assembly operations and intention inference. This dataset captures rich multimodal interactions between two participants, one acting as a 'robot surrogate,' across a variety of assembly tasks that are further broken down into 36 distinct subtasks. With rich visual annotations, such as facial landmarks, gaze, hand movements, object localization, and more for 70 participants, QUB-PHEO offers two versions: full video data for 50 participants and visual cues for all 70. Designed to improve machine learning models for HRI, QUB-PHEO enables deeper analysis of subtle interaction cues and intentions, promising contributions to the field. The dataset will be available at https://github.com/exponentialR/QUB-PHEO subject to an End-User License Agreement (EULA).
<div id='section'>PaperID: <span id='pid'>306, <a href='https://arxiv.org/pdf/2409.14908.pdf' target='_blank'>https://arxiv.org/pdf/2409.14908.pdf</a></span>   <span><a href='https://github.com/WZX0Swarm0Robotics/KARMA/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14908">KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates long-term and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. KARMA distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dual-memory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data. Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by 1.3x and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by 3.4x and 62.7x. Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms.Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient. The experimental videos from the work can be found at https://youtu.be/4BT7fnw9ehs. Our code is available at https://github.com/WZX0Swarm0Robotics/KARMA/tree/master.
<div id='section'>PaperID: <span id='pid'>307, <a href='https://arxiv.org/pdf/2409.11635.pdf' target='_blank'>https://arxiv.org/pdf/2409.11635.pdf</a></span>   <span><a href='https://damtien444.github.io/paindf/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Quang Tien Dam, Tri Tung Nguyen Nguyen, Yuki Endo, Dinh Tuan Tran, Joo-Ho Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11635">PainDiffusion: Learning to Express Pain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate pain expression synthesis is essential for improving clinical training and human-robot interaction. Current Robotic Patient Simulators (RPSs) lack realistic pain facial expressions, limiting their effectiveness in medical training. In this work, we introduce PainDiffusion, a generative model that synthesizes naturalistic facial pain expressions. Unlike traditional heuristic or autoregressive methods, PainDiffusion operates in a continuous latent space, ensuring smoother and more natural facial motion while supporting indefinite-length generation via diffusion forcing. Our approach incorporates intrinsic characteristics such as pain expressiveness and emotion, allowing for personalized and controllable pain expression synthesis. We train and evaluate our model using the BioVid HeatPain Database. Additionally, we integrate PainDiffusion into a robotic system to assess its applicability in real-time rehabilitation exercises. Qualitative studies with clinicians reveal that PainDiffusion produces realistic pain expressions, with a 31.2% (std 4.8%) preference rate against ground-truth recordings. Our results suggest that PainDiffusion can serve as a viable alternative to real patients in clinical training and simulation, bridging the gap between synthetic and naturalistic pain expression. Code and videos are available at: https://damtien444.github.io/paindf/
<div id='section'>PaperID: <span id='pid'>308, <a href='https://arxiv.org/pdf/2409.10071.pdf' target='_blank'>https://arxiv.org/pdf/2409.10071.pdf</a></span>   <span><a href='https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Meng Chen, Jiawei Tu, Chao Qi, Yonghao Dang, Feng Zhou, Wei Wei, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10071">Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant advancements in embodied vision navigation have raised concerns about its susceptibility to adversarial attacks exploiting deep neural networks. Investigating the adversarial robustness of embodied vision navigation is crucial, especially given the threat of 3D physical attacks that could pose risks to human safety. However, existing attack methods for embodied vision navigation often lack physical feasibility due to challenges in transferring digital perturbations into the physical world. Moreover, current physical attacks for object detection struggle to achieve both multi-view effectiveness and visual naturalness in navigation scenarios. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches to objects, where both opacity and textures are learnable. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which optimizes the patch's texture based on feedback from the vision-based perception model used in navigation. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, in which opacity is fine-tuned after texture optimization. Experimental results demonstrate that our adversarial patches decrease the navigation success rate by an average of 22.39%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav
<div id='section'>PaperID: <span id='pid'>309, <a href='https://arxiv.org/pdf/2409.09016.pdf' target='_blank'>https://arxiv.org/pdf/2409.09016.pdf</a></span>   <span><a href='https://github.com/OpenDriveLab/CLOVER' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/OpenDriveLab/CLOVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09016">Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.
<div id='section'>PaperID: <span id='pid'>310, <a href='https://arxiv.org/pdf/2409.05655.pdf' target='_blank'>https://arxiv.org/pdf/2409.05655.pdf</a></span>   <span><a href='https://github.com/DLR-RM/interactive-incremental-learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Markus Knauer, Alin Albu-SchÃ¤ffer, Freek Stulp, JoÃ£o SilvÃ©rio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05655">Interactive incremental learning of generalizable skills with local trajectory modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of generalization in learning from demonstration (LfD) has received considerable attention over the years, particularly within the context of movement primitives, where a number of approaches have emerged. Recently, two important approaches have gained recognition. While one leverages via-points to adapt skills locally by modulating demonstrated trajectories, another relies on so-called task-parameterized models that encode movements with respect to different coordinate systems, using a product of probabilities for generalization. While the former are well-suited to precise, local modulations, the latter aim at generalizing over large regions of the workspace and often involve multiple objects. Addressing the quality of generalization by leveraging both approaches simultaneously has received little attention. In this work, we propose an interactive imitation learning framework that simultaneously leverages local and global modulations of trajectory distributions. Building on the kernelized movement primitives (KMP) framework, we introduce novel mechanisms for skill modulation from direct human corrective feedback. Our approach particularly exploits the concept of via-points to incrementally and interactively 1) improve the model accuracy locally, 2) add new objects to the task during execution and 3) extend the skill into regions where demonstrations were not provided. We evaluate our method on a bearing ring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.
<div id='section'>PaperID: <span id='pid'>311, <a href='https://arxiv.org/pdf/2409.05583.pdf' target='_blank'>https://arxiv.org/pdf/2409.05583.pdf</a></span>   <span><a href='https://github.com/gmuraleekrishna/SAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Muraleekrishna Gopinathan, Martin Masek, Jumana Abu-Khalaf, David Suter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05583">Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI aims to develop robots that can \textit{understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or \textit{Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at \url{https://github.com/gmuraleekrishna/SAS}.
<div id='section'>PaperID: <span id='pid'>312, <a href='https://arxiv.org/pdf/2409.04965.pdf' target='_blank'>https://arxiv.org/pdf/2409.04965.pdf</a></span>   <span><a href='https://hsacllm.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Congcong Wen, Yifan Liu, Geeta Chandra Raju Bethala, Shuaihang Yuan, Hao Huang, Yu Hao, Mengyu Wang, Yu-Shen Liu, Anthony Tzes, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04965">Socially-Aware Robot Navigation Enhanced by Bidirectional Natural Language Conversations Using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot navigation is crucial across various domains, yet traditional methods focus on efficiency and obstacle avoidance, often overlooking human behavior in shared spaces. With the rise of service robots, socially aware navigation has gained prominence. However, existing approaches primarily predict pedestrian movements or issue alerts, lacking true human-robot interaction. We introduce Hybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), a novel framework for socially aware navigation. By integrating deep reinforcement learning with large language models, HSAC-LLM enables bidirectional natural language interactions, predicting both continuous and discrete navigation actions. When potential collisions arise, the robot proactively communicates with pedestrians to determine avoidance strategies. Experiments in 2D simulation, Gazebo, and real-world environments demonstrate that HSAC-LLM outperforms state-of-the-art DRL methods in interaction, navigation, and obstacle avoidance. This paradigm advances effective human-robot interactions in dynamic settings. Videos are available at https://hsacllm.github.io/.
<div id='section'>PaperID: <span id='pid'>313, <a href='https://arxiv.org/pdf/2409.01559.pdf' target='_blank'>https://arxiv.org/pdf/2409.01559.pdf</a></span>   <span><a href='https://github.com/pr2-humanoid/PR2-Platform,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hangxin Liu, Qi Xie, Zeyu Zhang, Tao Yuan, Song Wang, Zaijin Wang, Xiaokun Leng, Lining Sun, Jingwen Zhang, Zhicheng He, Yao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01559">PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in Competition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the development of a Physics-realistic and Photo-realistic humanoid robot testbed, PR2, to facilitate collaborative research between Embodied Artificial Intelligence (Embodied AI) and robotics. PR2 offers high-quality scene rendering and robot dynamic simulation, enabling (i) the creation of diverse scenes using various digital assets, (ii) the integration of advanced perception or foundation models, and (iii) the implementation of planning and control algorithms for dynamic humanoid robot behaviors based on environmental feedback. The beta version of PR2 has been deployed for the simulation track of a nationwide full-size humanoid robot competition for college students, attracting 137 teams and over 400 participants within four months. This competition covered traditional tasks in bipedal walking, as well as novel challenges in loco-manipulation and language-instruction-based object search, marking a first for public college robotics competitions. A retrospective analysis of the competition suggests that future events should emphasize the integration of locomotion with manipulation and perception. By making the PR2 testbed publicly available at https://github.com/pr2-humanoid/PR2-Platform, we aim to further advance education and training in humanoid robotics. Video demonstration: https://pr2-humanoid.github.io/
<div id='section'>PaperID: <span id='pid'>314, <a href='https://arxiv.org/pdf/2506.19613.pdf' target='_blank'>https://arxiv.org/pdf/2506.19613.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sha Zhang, Suorong Yang, Tong Xie, Xiangyuan Xue, Zixuan Hu, Rui Li, Wenxi Qu, Zhenfei Yin, Tianfan Fu, Di Hu, Andres M Bran, Nian Ran, Bram Hoex, Wangmeng Zuo, Philippe Schwaller, Wanli Ouyang, Lei Bai, Yanyong Zhang, Lingyu Duan, Shixiang Tang, Dongzhan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19613">Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific discovery has long been constrained by human limitations in expertise, physical capability, and sleep cycles. The recent rise of AI scientists and automated laboratories has accelerated both the cognitive and operational aspects of research. However, key limitations persist: AI systems are often confined to virtual environments, while automated laboratories lack the flexibility and autonomy to adaptively test new hypotheses in the physical world. Recent advances in embodied AI, such as generalist robot foundation models, diffusion-based action policies, fine-grained manipulation learning, and sim-to-real transfer, highlight the promise of integrating cognitive and embodied intelligence. This convergence opens the door to closed-loop systems that support iterative, autonomous experimentation and the possibility of serendipitous discovery. In this position paper, we propose the paradigm of Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework that deeply integrates cognitive and embodied intelligence. ISLs unify foundation models for scientific reasoning, agent-based workflow orchestration, and embodied agents for robust physical experimentation. We argue that such systems are essential for overcoming the current limitations of scientific discovery and for realizing the full transformative potential of AI-driven science.
<div id='section'>PaperID: <span id='pid'>315, <a href='https://arxiv.org/pdf/2512.11612.pdf' target='_blank'>https://arxiv.org/pdf/2512.11612.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunyi Li, Rui Qing, Jianbo Zhang, Yuan Tian, Xiangyang Zhu, Zicheng Zhang, Xiaohong Liu, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11612">Embodied Image Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.
<div id='section'>PaperID: <span id='pid'>316, <a href='https://arxiv.org/pdf/2508.06553.pdf' target='_blank'>https://arxiv.org/pdf/2508.06553.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahao Xiao, Jianbo Zhang, BoWen Yan, Shengyu Guo, Tongrui Ye, Kaiwei Zhang, Zicheng Zhang, Xiaohong Liu, Zhengxue Cheng, Lei Fan, Chuyi Li, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06553">Static and Plugged: Make Embodied Evaluation Simple</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied intelligence is advancing rapidly, driving the need for efficient evaluation. Current benchmarks typically rely on interactive simulated environments or real-world setups, which are costly, fragmented, and hard to scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play benchmark that enables unified evaluation using static scene representations. Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and comprehensive assessment through a simple interface. Furthermore, we evaluate 19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs), establishing the first unified static leaderboard for Embodied intelligence. Moreover, we release a subset of 200 samples from our benchmark to accelerate the development of embodied intelligence.
<div id='section'>PaperID: <span id='pid'>317, <a href='https://arxiv.org/pdf/2502.09624.pdf' target='_blank'>https://arxiv.org/pdf/2502.09624.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiawen Kang, Jiana Liao, Runquan Gao, Jinbo Wen, Huawei Huang, Maomao Zhang, Changyan Yi, Tao Zhang, Dusit Niyato, Zibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09624">Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By synergistically integrating mobile networks and embodied artificial intelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of vehicular messages or vulnerability to malicious tampering, potentially causing severe traffic accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the miner trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.
<div id='section'>PaperID: <span id='pid'>318, <a href='https://arxiv.org/pdf/2410.01176.pdf' target='_blank'>https://arxiv.org/pdf/2410.01176.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yue Zhong, Jiawen Kang, Jinbo Wen, Dongdong Ye, Jiangtian Nie, Dusit Niyato, Xiaozheng Gao, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01176">Generative Diffusion-based Contract Design for Efficient AI Twins Migration in Vehicular Embodied AI Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is a rapidly advancing field that bridges the gap between cyberspace and physical space, enabling a wide range of applications. This evolution has led to the development of the Vehicular Embodied AI NETwork (VEANET), where advanced AI capabilities are integrated into vehicular systems to enhance autonomous operations and decision-making. Embodied agents, such as Autonomous Vehicles (AVs), are autonomous entities that can perceive their environment and take actions to achieve specific goals, actively interacting with the physical world. Embodied twins are digital models of these embodied agents, with various embodied AI twins for intelligent applications in cyberspace. In VEANET, embodied AI twins act as in-vehicle AI assistants to perform diverse tasks supporting autonomous driving using generative AI models. Due to limited computational resources of AVs, these AVs often offload computationally intensive tasks, such as constructing and updating embodied AI twins, to nearby RSUs. However, since the rapid mobility of AVs and the limited provision coverage of a single RSU, embodied AI twins require dynamic migrations from current RSU to other RSUs in real-time, resulting in the challenge of selecting suitable RSUs for efficient embodied AI twins migrations. Given information asymmetry, AVs cannot know the detailed information of RSUs. To this end, in this paper, we construct a multi-dimensional contract theoretical model between AVs and alternative RSUs. Considering that AVs may exhibit irrational behavior, we utilize prospect theory instead of expected utility theory to model the actual utilities of AVs. Finally, we employ a generative diffusion model-based algorithm to identify the optimal contract designs. Compared with traditional deep reinforcement learning algorithms, numerical results demonstrate the effectiveness of the proposed scheme.
<div id='section'>PaperID: <span id='pid'>319, <a href='https://arxiv.org/pdf/2501.01141.pdf' target='_blank'>https://arxiv.org/pdf/2501.01141.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruichen Zhang, Changyuan Zhao, Hongyang Du, Dusit Niyato, Jiacheng Wang, Suttinee Sawadsitang, Xuemin Shen, Dong In Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01141">Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language Models and Reinforcement Learning Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating large language models (LLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90\% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model's focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36\% compared to DDPG and accelerates convergence by reducing required steps by up to 47\% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4\% improvement in QoE when scaling from 4 to 8 vehicles.
<div id='section'>PaperID: <span id='pid'>320, <a href='https://arxiv.org/pdf/2412.19996.pdf' target='_blank'>https://arxiv.org/pdf/2412.19996.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yaoqi Yang, Yong Chen, Jiacheng Wang, Geng Sun, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19996">Embodied AI-empowered Low Altitude Economy: Integrated Sensing, Communications, Computation, and Control (ISC3)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low altitude economy (LAE) holds immense potential to drive urban development across various sectors. However, LAE also faces challenges in data collection and processing efficiency, flight control precision, and network performance. The challenges could be solved by realizing an integration of sensing, communications, computation, and control (ISC3) for LAE. In this regard, embodied artificial intelligence (EAI), with its unique perception, planning, and decision-making capabilities, offers a promising solution to realize ISC3. Specifically, this paper investigates an application of EAI into ISC3 to support LAE, exploring potential research focuses, solutions, and case study. We begin by outlining rationales and benefits of introducing EAI into LAE, followed by reviewing research directions and solutions for EAI in ISC3. We then propose a framework of an EAI-enabled ISC3 for LAE. The framework's effectiveness is evaluated through a case study of express delivery utilizing an EAI-enabled UAV. Finally, we discuss several future research directions for advancing EAI-enabled LAE.
<div id='section'>PaperID: <span id='pid'>321, <a href='https://arxiv.org/pdf/2511.17411.pdf' target='_blank'>https://arxiv.org/pdf/2511.17411.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nikolay Nikolov, Giuliano Albanese, Sombit Dey, Aleksandar Yanev, Luc Van Gool, Jan-Nico Zaech, Danda Pani Paudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17411">SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.
<div id='section'>PaperID: <span id='pid'>322, <a href='https://arxiv.org/pdf/2509.12989.pdf' target='_blank'>https://arxiv.org/pdf/2509.12989.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12989">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.
<div id='section'>PaperID: <span id='pid'>323, <a href='https://arxiv.org/pdf/2505.11907.pdf' target='_blank'>https://arxiv.org/pdf/2505.11907.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zihao Dongfang, Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Danda Pani Paudel, Luc Van Gool, Kailun Yang, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11907">Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 180x360 omnidirectional field of view captured by 360-degree cameras enables their use in a wide range of applications such as embodied AI and virtual reality. Although recent advances in multimodal large language models (MLLMs) have shown promise in visual-spatial reasoning, most studies focus on standard pinhole-view images, leaving omnidirectional perception largely unexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial reasoning? To investigate this, we introduce OSR-Bench, the first benchmark specifically designed for this setting. OSR-Bench includes over 153,000 diverse question-answer pairs grounded in high-fidelity panoramic indoor scene maps. It covers key reasoning types including object counting, relative distance, and direction. We also propose a negative sampling strategy that inserts non-existent objects into prompts to evaluate hallucination and grounding robustness. For fine-grained analysis, we design a two-stage evaluation framework assessing both cognitive map generation and QA accuracy using rotation-invariant matching and a combination of rule-based and LLM-based metrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5 Pro, and leading open-source models under zero-shot settings. Results show that current models struggle with spatial reasoning in panoramic contexts, highlighting the need for more perceptually grounded MLLMs. OSR-Bench and code will be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench
<div id='section'>PaperID: <span id='pid'>324, <a href='https://arxiv.org/pdf/2412.01398.pdf' target='_blank'>https://arxiv.org/pdf/2412.01398.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anna-Maria Halacheva, Yang Miao, Jan-Nico Zaech, Xi Wang, Luc Van Gool, Danda Pani Paudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01398">Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding is a long-standing challenge in computer vision and a key component in enabling mixed reality, wearable computing, and embodied AI. Providing a solution to these applications requires a multifaceted approach that covers scene-centric, object-centric, as well as interaction-centric capabilities. While there exist numerous datasets and algorithms approaching the former two problems, the task of understanding interactable and articulated objects is underrepresented and only partly covered in the research field. In this work, we address this shortcoming by introducing: (1) Articulate3D, an expertly curated 3D dataset featuring high-quality manual annotations on 280 indoor scenes. Articulate3D provides 8 types of annotations for articulated objects, covering parts and detailed motion information, all stored in a standardized scene representation format designed for scalable 3D content creation, exchange and seamless integration into simulation environments. (2) USDNet, a novel unified framework capable of simultaneously predicting part segmentation along with a full specification of motion attributes for articulated objects. We evaluate USDNet on Articulate3D as well as two existing datasets, demonstrating the advantage of our unified dense prediction approach. Furthermore, we highlight the value of Articulate3D through cross-dataset and cross-domain evaluations and showcase its applicability in downstream tasks such as scene editing through LLM prompting and robotic policy training for articulated object manipulation. We provide open access to our dataset, benchmark, and method's source code.
<div id='section'>PaperID: <span id='pid'>325, <a href='https://arxiv.org/pdf/2409.15250.pdf' target='_blank'>https://arxiv.org/pdf/2409.15250.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15250">ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in large language models and access to large-scale robotic datasets has sparked a paradigm shift in robotics models transforming them into generalists able to adapt to various tasks, scenes, and robot modalities. A large step for the community are open Vision Language Action models which showcase strong performance in a wide variety of tasks. In this work, we study the visual generalization capabilities of three existing robotic foundation models, and propose a corresponding evaluation framework. Our study shows that the existing models do not exhibit robustness to visual out-of-domain scenarios. This is potentially caused by limited variations in the training data and/or catastrophic forgetting, leading to domain limitations in the vision foundation models. We further explore OpenVLA, which uses two pre-trained vision foundation models and is, therefore, expected to generalize to out-of-domain experiments. However, we showcase catastrophic forgetting by DINO-v2 in OpenVLA through its failure to fulfill the task of depth regression. To overcome the aforementioned issue of visual catastrophic forgetting, we propose a gradual backbone reversal approach founded on model merging. This enables OpenVLA -- which requires the adaptation of the visual backbones during initial training -- to regain its visual generalization ability. Regaining this capability enables our ReVLA model to improve over OpenVLA by a factor of 77\% and 66\% for grasping and lifting in visual OOD tasks. Comprehensive evaluations, episode rollouts and model weights are available on the ReVLA Page
<div id='section'>PaperID: <span id='pid'>326, <a href='https://arxiv.org/pdf/2602.07837.pdf' target='_blank'>https://arxiv.org/pdf/2602.07837.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongzhi Zang, Shu'ang Yu, Hao Lin, Tianxing Zhou, Zefang Huang, Zhen Guo, Xin Xu, Jiakai Zhou, Yuze Sheng, Shizhe Zhang, Feng Gao, Wenhao Tang, Yufeng Yue, Quanlu Zhang, Xinlei Chen, Chao Yu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07837">RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.
<div id='section'>PaperID: <span id='pid'>327, <a href='https://arxiv.org/pdf/2510.25889.pdf' target='_blank'>https://arxiv.org/pdf/2510.25889.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang, Chao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25889">$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with $π_{\text{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $π_{\text{RL}}$ implements two RL algorithms: (1) {Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) {Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO, $π_{\text{RL}}$ boosts few-shot SFT models $π_0$ and $π_{0.5}$ from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train $π_{\text{RL}}$ in 320 parallel environments, improving $π_0$ from 41.6% to 85.7% and $π_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks, demonstrating scalable multitask RL under heterogeneous simulation. Overall, $π_{\text{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.
<div id='section'>PaperID: <span id='pid'>328, <a href='https://arxiv.org/pdf/2507.22424.pdf' target='_blank'>https://arxiv.org/pdf/2507.22424.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Songsheng Wang, Rucheng Yu, Zhihang Yuan, Chao Yu, Feng Gao, Yu Wang, Derek F. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22424">Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.
<div id='section'>PaperID: <span id='pid'>329, <a href='https://arxiv.org/pdf/2505.19789.pdf' target='_blank'>https://arxiv.org/pdf/2505.19789.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19789">What Can RL Bring to VLA Generalization? An Empirical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io
<div id='section'>PaperID: <span id='pid'>330, <a href='https://arxiv.org/pdf/2409.13573.pdf' target='_blank'>https://arxiv.org/pdf/2409.13573.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weizheng Wang, Chao Yu, Yu Wang, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13573">Human-Robot Cooperative Distribution Coupling for Hamiltonian-Constrained Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating in human-filled public spaces is a critical challenge for deploying autonomous robots in real-world environments. This paper introduces NaviDIFF, a novel Hamiltonian-constrained socially-aware navigation framework designed to address the complexities of human-robot interaction and socially-aware path planning. NaviDIFF integrates a port-Hamiltonian framework to model dynamic physical interactions and a diffusion model to manage uncertainty in human-robot cooperation. The framework leverages a spatial-temporal transformer to capture social and temporal dependencies, enabling more accurate spatial-temporal environmental dynamics understanding and port-Hamiltonian physical interactive process construction. Additionally, reinforcement learning from human feedback is employed to fine-tune robot policies, ensuring adaptation to human preferences and social norms. Extensive experiments demonstrate that NaviDIFF outperforms state-of-the-art methods in social navigation tasks, offering improved stability, efficiency, and adaptability.
<div id='section'>PaperID: <span id='pid'>331, <a href='https://arxiv.org/pdf/2601.04266.pdf' target='_blank'>https://arxiv.org/pdf/2601.04266.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ji Guo, Wenbo Jiang, Yansong Lin, Yijing Liu, Ruichen Zhang, Guomin Lu, Aiguo Chen, Xinshuo Han, Hongwei Li, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04266">State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are widely deployed in safety-critical embodied AI applications such as robotics. However, their complex multimodal interactions also expose new security vulnerabilities. In this paper, we investigate a backdoor threat in VLA models, where malicious inputs cause targeted misbehavior while preserving performance on clean data. Existing backdoor methods predominantly rely on inserting visible triggers into visual modality, which suffer from poor robustness and low insusceptibility in real-world settings due to environmental variability. To overcome these limitations, we introduce the State Backdoor, a novel and practical backdoor attack that leverages the robot arm's initial state as the trigger. To optimize trigger for insusceptibility and effectiveness, we design a Preference-guided Genetic Algorithm (PGA) that efficiently searches the state space for minimal yet potent triggers. Extensive experiments on five representative VLA models and five real-world tasks show that our method achieves over 90% attack success rate without affecting benign task performance, revealing an underexplored vulnerability in embodied AI systems.
<div id='section'>PaperID: <span id='pid'>332, <a href='https://arxiv.org/pdf/2509.19870.pdf' target='_blank'>https://arxiv.org/pdf/2509.19870.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Wang, Jie Li, Zejia Weng, Yixu Wang, Yifeng Gao, Tianyu Pang, Chao Du, Yan Teng, Yingchun Wang, Zuxuan Wu, Xingjun Ma, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19870">FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.
<div id='section'>PaperID: <span id='pid'>333, <a href='https://arxiv.org/pdf/2509.02055.pdf' target='_blank'>https://arxiv.org/pdf/2509.02055.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Zhang, Chenwei Wang, Ouyang Lu, Yuan Zhao, Yunfei Ge, Zhenglong Sun, Xiu Li, Chi Zhang, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02055">Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce \textbf{Align-Then-stEer (\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation framework. \texttt{ATE} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to \textbf{9.8\%} in simulation and achieves a striking \textbf{32\% success rate gain} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.
<div id='section'>PaperID: <span id='pid'>334, <a href='https://arxiv.org/pdf/2409.11279.pdf' target='_blank'>https://arxiv.org/pdf/2409.11279.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11279">P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.
<div id='section'>PaperID: <span id='pid'>335, <a href='https://arxiv.org/pdf/2512.13030.pdf' target='_blank'>https://arxiv.org/pdf/2512.13030.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, Hongyan Zhao, Hanyu Liu, Zhizhong Su, Lei Ma, Hang Su, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13030">Motus: A Unified Latent Action World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.
<div id='section'>PaperID: <span id='pid'>336, <a href='https://arxiv.org/pdf/2506.05204.pdf' target='_blank'>https://arxiv.org/pdf/2506.05204.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanbo Wang, Ziyi Wang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05204">OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing semantic-aware 3D scenes from sparse views is a challenging yet essential research direction, driven by the demands of emerging applications such as virtual reality and embodied AI. Existing per-scene optimization methods require dense input views and incur high computational costs, while generalizable approaches often struggle to reconstruct regions outside the input view cone. In this paper, we propose OGGSplat, an open Gaussian growing method that expands the field-of-view in generalizable 3D reconstruction. Our key insight is that the semantic attributes of open Gaussians provide strong priors for image extrapolation, enabling both semantic consistency and visual plausibility. Specifically, once open Gaussians are initialized from sparse views, we introduce an RGB-semantic consistent inpainting module applied to selected rendered views. This module enforces bidirectional control between an image diffusion model and a semantic diffusion model. The inpainted regions are then lifted back into 3D space for efficient and progressive Gaussian parameter optimization. To evaluate our method, we establish a Gaussian Outpainting (GO) benchmark that assesses both semantic and generative quality of reconstructed open-vocabulary scenes. OGGSplat also demonstrates promising semantic-aware scene reconstruction capabilities when provided with two view images captured directly from a smartphone camera.
<div id='section'>PaperID: <span id='pid'>337, <a href='https://arxiv.org/pdf/2506.05171.pdf' target='_blank'>https://arxiv.org/pdf/2506.05171.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Linxuan He, Qing-Shan Jia, Ang Li, Hongyan Sang, Ling Wang, Jiwen Lu, Tao Zhang, Jie Zhou, Yi Zhang, Yisen Wang, Peng Wei, Zhongyuan Wang, Henry X. Liu, Shuo Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05171">Towards provable probabilistic safety for scalable embodied AI systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems, comprising AI models and physical plants, are increasingly prevalent across various applications. Due to the rarity of system failures, ensuring their safety in complex operating environments remains a major challenge, which severely hinders their large-scale deployment in safety-critical domains, such as autonomous vehicles, medical devices, and robotics. While achieving provable deterministic safety--verifying system safety across all possible scenarios--remains theoretically ideal, the rarity and complexity of corner cases make this approach impractical for scalable embodied AI systems. Instead, empirical safety evaluation is employed as an alternative, but the absence of provable guarantees imposes significant limitations. To address these issues, we argue for a paradigm shift to provable probabilistic safety that integrates provable guarantees with progressive achievement toward a probabilistic safety boundary on overall system performance. The new paradigm better leverages statistical methods to enhance feasibility and scalability, and a well-defined probabilistic safety boundary enables embodied AI systems to be deployed at scale. In this Perspective, we outline a roadmap for provable probabilistic safety, along with corresponding challenges and potential solutions. By bridging the gap between theoretical safety assurance and practical deployment, this Perspective offers a pathway toward safer, large-scale adoption of embodied AI systems in safety-critical applications.
<div id='section'>PaperID: <span id='pid'>338, <a href='https://arxiv.org/pdf/2504.21853.pdf' target='_blank'>https://arxiv.org/pdf/2504.21853.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21853">A Survey of Interactive Generative Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.
<div id='section'>PaperID: <span id='pid'>339, <a href='https://arxiv.org/pdf/2602.08971.pdf' target='_blank'>https://arxiv.org/pdf/2602.08971.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Shang, Zhuohang Li, Yiding Ma, Weikang Su, Xin Jin, Ziyou Wang, Lei Jin, Xin Zhang, Yinzhou Tang, Haisheng Su, Chen Gao, Wei Wu, Xihui Liu, Dhruv Shah, Zhaoxiang Zhang, Zhibo Chen, Jun Zhu, Yonghong Tian, Tat-Seng Chua, Wenwu Zhu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.08971">WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://world-arena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.
<div id='section'>PaperID: <span id='pid'>340, <a href='https://arxiv.org/pdf/2601.14339.pdf' target='_blank'>https://arxiv.org/pdf/2601.14339.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14339">CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.
<div id='section'>PaperID: <span id='pid'>341, <a href='https://arxiv.org/pdf/2507.15428.pdf' target='_blank'>https://arxiv.org/pdf/2507.15428.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaao Li, Kaiyuan Li, Chen Gao, Yong Li, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15428">EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egomotion videos are first-person recordings where the view changes continuously due to the agent's movement. As they serve as the primary visual input for embodied AI agents, making egomotion video reasoning more efficient is therefore essential for real-world deployment. Recent advances in vision-language models have enabled strong multimodal reasoning capabilities, but their computational cost remains prohibitive for long, redundant video inputs. Existing token pruning methods, typically designed for third-person videos, fail to leverage the spatiotemporal continuity and motion constraints inherent in egomotion settings. To address this, we propose EgoPrune, a training-free token pruning method tailored for egomotion video reasoning. EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR for temporally efficient sampling; Perspective-Aware Redundancy Filtering (PARF), which aligns visual tokens using perspective transformations and removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token selector that jointly considers visual-text relevance and intra-frame diversity. Experiments on two egomotion video benchmarks show that EgoPrune consistently outperforms prior training-free methods across various pruning ratios while significantly reducing FLOPs, memory usage, and latency. Moreover, we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB edge device, demonstrating its real-world efficiency and suitability for on-device egomotion video reasoning.
<div id='section'>PaperID: <span id='pid'>342, <a href='https://arxiv.org/pdf/2506.18385.pdf' target='_blank'>https://arxiv.org/pdf/2506.18385.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, Tong He, Wenqi Shao, Kaipeng Zhang, Yi Wang, Botian Shi, Yanting Zhang, Jifeng Dai, Yu Qiao, Hongjie Zhang, Wenhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18385">InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent benchmarks and datasets have been proposed to improve spatial reasoning in vision-language models (VLMs), yet existing open resources remain limited in scale, visual diversity, and instruction expressiveness. In this work, we introduce InternSpatial, the largest open-source dataset for spatial reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation benchmark designed to assess spatial understanding under diverse instruction formats. InternSpatial comprises 12 million QA pairs spanning both single-view and multi-view settings, drawn from diverse visual environments and supporting 19 instruction formats that reflect varied query styles. For evaluation, we propose InternSpatial-Bench for single-view tasks and expand multi-view reasoning by introducing a novel rotation angle prediction task that has not been explored in prior work. Experimental results show that models trained on InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining strong performance on general-purpose benchmarks. We hope these resources will support the development of spatially capable VLMs in practical applications such as robotics and embodied AI.
<div id='section'>PaperID: <span id='pid'>343, <a href='https://arxiv.org/pdf/2504.09587.pdf' target='_blank'>https://arxiv.org/pdf/2504.09587.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haotian Xu, Yue Hu, Chen Gao, Zhengqiu Zhu, Yong Zhao, Yong Li, Quanjun Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09587">GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.
<div id='section'>PaperID: <span id='pid'>344, <a href='https://arxiv.org/pdf/2502.11859.pdf' target='_blank'>https://arxiv.org/pdf/2502.11859.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11859">Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.
<div id='section'>PaperID: <span id='pid'>345, <a href='https://arxiv.org/pdf/2503.13882.pdf' target='_blank'>https://arxiv.org/pdf/2503.13882.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhengsheng Guo, Linwei Zheng, Xinyang Chen, Xuefeng Bai, Kehai Chen, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13882">MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented Generation for Embodied AI Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While human cognition inherently retrieves information from diverse and specialized knowledge sources during decision-making processes, current Retrieval-Augmented Generation (RAG) systems typically operate through single-source knowledge retrieval, leading to a cognitive-algorithmic discrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG framework that implements a mixture of knowledge paths enhanced retrieval mechanism through functional partitioning of a large language model (LLM) corpus into distinct sections, enabling retrieval from multiple specialized knowledge paths. Applied to the generation of 3D simulated environments, our proposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into distinct sections and organizing them based on a hierarchical knowledge tree structure. Different from previous methods that only use manual evaluation, we pioneered the introduction of automated evaluation methods for 3D scenes. Both automatic and human evaluations in our experiments demonstrate that MoK-RAG3D can assist Embodied AI agents in generating diverse scenes.
<div id='section'>PaperID: <span id='pid'>346, <a href='https://arxiv.org/pdf/2509.07496.pdf' target='_blank'>https://arxiv.org/pdf/2509.07496.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ayano Miyamichi, Moju Zhao, Kazuki Sugihara, Junichiro Sugihara, Masanori Konishi, Kunio Kojima, Kei Okada, Masayuki Inaba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07496">Flexible Morphing Aerial Robot with Inflatable Structure for Perching-based Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Birds in nature perform perching not only for rest but also for interaction with human such as the relationship with falconers. Recently, researchers achieve perching-capable aerial robots as a way to save energy, and deformable structure demonstrate significant advantages in efficiency of perching and compactness of configuration. However, ensuring flight stability remains challenging for deformable aerial robots due to the difficulty of controlling flexible arms. Furthermore, perching for human interaction requires high compliance along with safety. Thus, this study aims to develop a deformable aerial robot capable of perching on humans with high flexibility and grasping ability. To overcome the challenges of stability of both flight and perching, we propose a hybrid morphing structure that combines a unilateral flexible arm and a pneumatic inflatable actuators. This design allows the robot's arms to remain rigid during flight and soft while perching for more effective grasping. We also develop a pneumatic control system that optimizes pressure regulation while integrating shock absorption and adjustable grasping forces, enhancing interaction capabilities and energy efficiency. Besides, we focus on the structural characteristics of the unilateral flexible arm and identify sufficient conditions under which standard quadrotor modeling and control remain effective in terms of flight stability. Finally, the developed prototype demonstrates the feasibility of compliant perching maneuvers on humans, as well as the robust recovery even after arm deformation caused by thrust reductions during flight. To the best of our knowledge, this work is the first to achieve an aerial robot capable of perching on humans for interaction.
<div id='section'>PaperID: <span id='pid'>347, <a href='https://arxiv.org/pdf/2412.18194.pdf' target='_blank'>https://arxiv.org/pdf/2412.18194.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18194">VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh\&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks.
<div id='section'>PaperID: <span id='pid'>348, <a href='https://arxiv.org/pdf/2409.16019.pdf' target='_blank'>https://arxiv.org/pdf/2409.16019.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhenghao Qi, Shenghai Yuan, Fen Liu, Haozhi Cao, Tianchen Deng, Jianfei Yang, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16019">AIR-Embodied: An Efficient Active 3DGS-based Interaction and Reconstruction Framework with Embodied Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D reconstruction and neural rendering have enhanced the creation of high-quality digital assets, yet existing methods struggle to generalize across varying object shapes, textures, and occlusions. While Next Best View (NBV) planning and Learning-based approaches offer solutions, they are often limited by predefined criteria and fail to manage occlusions with human-like common sense. To address these problems, we present AIR-Embodied, a novel framework that integrates embodied AI agents with large-scale pretrained multi-modal language models to improve active 3DGS reconstruction. AIR-Embodied utilizes a three-stage process: understanding the current reconstruction state via multi-modal prompts, planning tasks with viewpoint selection and interactive actions, and employing closed-loop reasoning to ensure accurate execution. The agent dynamically refines its actions based on discrepancies between the planned and actual outcomes. Experimental evaluations across virtual and real-world environments demonstrate that AIR-Embodied significantly enhances reconstruction efficiency and quality, providing a robust solution to challenges in active 3D reconstruction.
<div id='section'>PaperID: <span id='pid'>349, <a href='https://arxiv.org/pdf/2512.15411.pdf' target='_blank'>https://arxiv.org/pdf/2512.15411.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhenhan Yin, Xuanhan Wang, Jiahao Jiang, Kaiyuan Deng, Pengqi Chen, Shuangle Li, Chong Liu, Xing Xu, Jingkuan Song, Lianli Gao, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15411">MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.
<div id='section'>PaperID: <span id='pid'>350, <a href='https://arxiv.org/pdf/2512.03438.pdf' target='_blank'>https://arxiv.org/pdf/2512.03438.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Reuben Tan, Baolin Peng, Zhengyuan Yang, Hao Cheng, Oier Mees, Theodore Zhao, Andrea Tupini, Isar Meijier, Qianhui Wu, Yuncong Yang, Lars Liden, Yu Gu, Sheng Zhang, Xiaodong Liu, Lijuan Wang, Marc Pollefeys, Yong Jae Lee, Jianfeng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03438">Multimodal Reinforcement Learning with Agentic Verifier for AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.
<div id='section'>PaperID: <span id='pid'>351, <a href='https://arxiv.org/pdf/2510.24795.pdf' target='_blank'>https://arxiv.org/pdf/2510.24795.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24795">A Survey on Efficient Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/
<div id='section'>PaperID: <span id='pid'>352, <a href='https://arxiv.org/pdf/2505.05456.pdf' target='_blank'>https://arxiv.org/pdf/2505.05456.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, Boqing Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05456">SITE: towards Spatial Intelligence Thorough Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task.
<div id='section'>PaperID: <span id='pid'>353, <a href='https://arxiv.org/pdf/2509.26642.pdf' target='_blank'>https://arxiv.org/pdf/2509.26642.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26642">MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla
<div id='section'>PaperID: <span id='pid'>354, <a href='https://arxiv.org/pdf/2505.21200.pdf' target='_blank'>https://arxiv.org/pdf/2505.21200.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xudong Tan, Yaoxin Yang, Peng Ye, Jialin Zheng, Bizhe Bai, Xinyi Wang, Jia Hao, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21200">Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a powerful paradigm for general-purpose robot control through natural language instructions. However, their high inference cost-stemming from large-scale token computation and autoregressive decoding-poses significant challenges for real-time deployment and edge applications. While prior work has primarily focused on architectural optimization, we take a different perspective by identifying a dual form of redundancy in VLA models: (i) high similarity across consecutive action steps, and (ii) substantial redundancy in visual tokens. Motivated by these observations, we propose FlashVLA, the first training-free and plug-and-play acceleration framework that enables action reuse in VLA models. FlashVLA improves inference efficiency through a token-aware action reuse mechanism that avoids redundant decoding across stable action steps, and an information-guided visual token selection strategy that prunes low-contribution tokens. Extensive experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7% and latency by 36.0%, with only a 0.7% drop in task success rate. These results demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency VLA inference without retraining.
<div id='section'>PaperID: <span id='pid'>355, <a href='https://arxiv.org/pdf/2505.04769.pdf' target='_blank'>https://arxiv.org/pdf/2505.04769.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ranjan Sapkota, Yang Cao, Konstantinos I. Roumeliotis, Manoj Karkee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04769">Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models
<div id='section'>PaperID: <span id='pid'>356, <a href='https://arxiv.org/pdf/2509.25032.pdf' target='_blank'>https://arxiv.org/pdf/2509.25032.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ryosuke Takanami, Petr Khrapchenkov, Shu Morikuni, Jumpei Arima, Yuta Takaba, Shunsuke Maeda, Takuya Okubo, Genki Sano, Satoshi Sekioka, Aoi Kadoya, Motonari Kambara, Naoya Nishiura, Haruto Suzuki, Takanori Yoshimoto, Koya Sakamoto, Shinnosuke Ono, Hu Yang, Daichi Yashima, Aoi Horo, Tomohiro Motoda, Kensuke Chiyoma, Hiroshi Ito, Koki Fukuda, Akihito Goto, Kazumi Morinaga, Yuya Ikeda, Riko Kawada, Masaki Yoshikawa, Norio Kosuge, Yuki Noguchi, Kei Ota, Tatsuya Matsushima, Yusuke Iwasawa, Yutaka Matsuo, Tetsuya Ogata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25032">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .
<div id='section'>PaperID: <span id='pid'>357, <a href='https://arxiv.org/pdf/2509.10813.pdf' target='_blank'>https://arxiv.org/pdf/2509.10813.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weipeng Zhong, Peizhou Cao, Yichen Jin, Li Luo, Wenzhe Cai, Jingli Lin, Hanqing Wang, Zhaoyang Lyu, Tai Wang, Bo Dai, Xudong Xu, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10813">InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.
<div id='section'>PaperID: <span id='pid'>358, <a href='https://arxiv.org/pdf/2507.17520.pdf' target='_blank'>https://arxiv.org/pdf/2507.17520.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17520">InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.
<div id='section'>PaperID: <span id='pid'>359, <a href='https://arxiv.org/pdf/2506.19816.pdf' target='_blank'>https://arxiv.org/pdf/2506.19816.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19816">CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.
<div id='section'>PaperID: <span id='pid'>360, <a href='https://arxiv.org/pdf/2503.19757.pdf' target='_blank'>https://arxiv.org/pdf/2503.19757.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19757">Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.
<div id='section'>PaperID: <span id='pid'>361, <a href='https://arxiv.org/pdf/2410.15959.pdf' target='_blank'>https://arxiv.org/pdf/2410.15959.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15959">Diffusion Transformer Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large vision-language-action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict individual discretized or continuous action by a small action head, which limits the ability in handling diverse action spaces. In contrast, we model the continuous action sequence with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head for action embedding. By leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. Extensive experiments demonstrate the effectiveness and generalization of Diffusion Transformer Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world Franka arm, achieving consistent better performance on Real-to-Sim benchmark SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo. Specifically, without bells and whistles, the proposed approach achieves state-of-the-art performance with only a single third-view camera stream in the Calvin task ABC->D, improving the average number of tasks completed in a row of 5 to 3.6, and the pretraining stage significantly facilitates the success sequence length on the Calvin by over 1.2.
<div id='section'>PaperID: <span id='pid'>362, <a href='https://arxiv.org/pdf/2511.12149.pdf' target='_blank'>https://arxiv.org/pdf/2511.12149.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiayu Li, Yunhan Zhao, Xiang Zheng, Zonghuan Xu, Yige Li, Xingjun Ma, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12149">AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.
<div id='section'>PaperID: <span id='pid'>363, <a href='https://arxiv.org/pdf/2510.10932.pdf' target='_blank'>https://arxiv.org/pdf/2510.10932.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10932">TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.
<div id='section'>PaperID: <span id='pid'>364, <a href='https://arxiv.org/pdf/2506.15757.pdf' target='_blank'>https://arxiv.org/pdf/2506.15757.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruoyu Wang, Tong Yu, Junda Wu, Yao Liu, Julian McAuley, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15757">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.
<div id='section'>PaperID: <span id='pid'>365, <a href='https://arxiv.org/pdf/2409.13174.pdf' target='_blank'>https://arxiv.org/pdf/2409.13174.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Cheng, Erjia Xiao, Yichi Wang, Chengyuan Yu, Mengshu Sun, Qiang Zhang, Yijie Guo, Kaidi Xu, Jize Zhang, Chao Shen, Philip Torr, Jindong Gu, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13174">Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, driven by advancements in Multimodal Large Language Models (MLLMs), Vision Language Action Models (VLAMs) are being proposed to achieve better performance in open-vocabulary scenarios for robotic manipulation tasks. Since manipulation tasks involve direct interaction with the physical world, ensuring robustness and safety during the execution of this task is always a very critical issue. In this paper, by synthesizing current safety research on MLLMs and the specific application scenarios of the manipulation task in the physical world, we comprehensively evaluate VLAMs in the face of potential physical threats. Specifically, we propose the Physical Vulnerability Evaluating Pipeline (PVEP) that can incorporate as many visual modal physical threats as possible for evaluating the physical robustness of VLAMs. The physical threats in PVEP specifically include Out-of-Distribution, Typography-based Visual Prompt, and Adversarial Patch Attacks. By comparing the performance fluctuations of VLAMs before and after being attacked, we provide generalizable \textbf{\textit{Analyses}} of how VLAMs respond to different physical threats.
<div id='section'>PaperID: <span id='pid'>366, <a href='https://arxiv.org/pdf/2601.20742.pdf' target='_blank'>https://arxiv.org/pdf/2601.20742.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Jin, Jinming Liu, Yuntao Wei, Junyan Lin, Zhicheng Wang, Jianguo Huang, Xudong Yang, Yanxiao Liu, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20742">Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.
<div id='section'>PaperID: <span id='pid'>367, <a href='https://arxiv.org/pdf/2601.12428.pdf' target='_blank'>https://arxiv.org/pdf/2601.12428.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Baorui Peng, Wenyao Zhang, Liang Xu, Zekun Qi, Jiazhao Zhang, Hongsi Liu, Wenjun Zeng, Xin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12428">ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.
<div id='section'>PaperID: <span id='pid'>368, <a href='https://arxiv.org/pdf/2502.01376.pdf' target='_blank'>https://arxiv.org/pdf/2502.01376.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lu Chen, Lipeng Chen, Xiangchi Chen, Haojian Lu, Yu Zheng, Jun Wu, Yue Wang, Zhengyou Zhang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01376">Compliance while resisting: a shear-thickening fluid controller for physical human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction (pHRI) is widely needed in many fields, such as industrial manipulation, home services, and medical rehabilitation, and puts higher demands on the safety of robots. Due to the uncertainty of the working environment, the pHRI may receive unexpected impact interference, which affects the safety and smoothness of the task execution. The commonly used linear admittance control (L-AC) can cope well with high-frequency small-amplitude noise, but for medium-frequency high-intensity impact, the effect is not as good. Inspired by the solid-liquid phase change nature of shear-thickening fluid, we propose a Shear-thickening Fluid Control (SFC) that can achieve both an easy human-robot collaboration and resistance to impact interference. The SFC's stability, passivity, and phase trajectory are analyzed in detail, the frequency and time domain properties are quantified, and parameter constraints in discrete control and coupled stability conditions are provided. We conducted simulations to compare the frequency and time domain characteristics of L-AC, nonlinear admittance controller (N-AC), and SFC, and validated their dynamic properties. In real-world experiments, we compared the performance of L-AC, N-AC, and SFC in both fixed and mobile manipulators. L-AC exhibits weak resistance to impact. N-AC can resist moderate impacts but not high-intensity ones, and may exhibit self-excited oscillations. In contrast, SFC demonstrated superior impact resistance and maintained stable collaboration, enhancing comfort in cooperative water delivery tasks. Additionally, a case study was conducted in a factory setting, further affirming the SFC's capability in facilitating human-robot collaborative manipulation and underscoring its potential in industrial applications.
<div id='section'>PaperID: <span id='pid'>369, <a href='https://arxiv.org/pdf/2511.13648.pdf' target='_blank'>https://arxiv.org/pdf/2511.13648.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziang Cao, Fangzhou Hong, Zhaoxi Chen, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13648">PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.
<div id='section'>PaperID: <span id='pid'>370, <a href='https://arxiv.org/pdf/2507.12465.pdf' target='_blank'>https://arxiv.org/pdf/2507.12465.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziang Cao, Zhaoxi Chen, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12465">PhysX-3D: Physical-Grounded 3D Asset Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \textbf{PhysX-3D}, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.
<div id='section'>PaperID: <span id='pid'>371, <a href='https://arxiv.org/pdf/2510.14902.pdf' target='_blank'>https://arxiv.org/pdf/2510.14902.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14902">VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.
<div id='section'>PaperID: <span id='pid'>372, <a href='https://arxiv.org/pdf/2505.11214.pdf' target='_blank'>https://arxiv.org/pdf/2505.11214.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11214">Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.
<div id='section'>PaperID: <span id='pid'>373, <a href='https://arxiv.org/pdf/2503.08007.pdf' target='_blank'>https://arxiv.org/pdf/2503.08007.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, Pengxiang Ding, Xuelian Cheng, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08007">MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing versatile quadruped robots that can smoothly perform various actions and tasks in real-world environments remains a significant challenge. This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL) for fine-tuning large-scale VLA models with a large amount of mixed-quality data. MoRE integrates multiple low-rank adaptation modules as distinct experts within a dense multi-modal large language model (MLLM), forming a sparse-activated mixture-of-experts model. This design enables the model to effectively adapt to a wide array of downstream tasks. Moreover, we employ a reinforcement learning-based training objective to train our model as a Q-function after deeply exploring the structural properties of our tasks. Effective learning from automatically collected mixed-quality data enhances data efficiency and model performance. Extensive experiments demonstrate that MoRE outperforms all baselines across six different skills and exhibits superior generalization capabilities in out-of-distribution scenarios. We further validate our method in real-world scenarios, confirming the practicality of our approach and laying a solid foundation for future research on multi-task learning in quadruped robots.
<div id='section'>PaperID: <span id='pid'>374, <a href='https://arxiv.org/pdf/2502.13508.pdf' target='_blank'>https://arxiv.org/pdf/2502.13508.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13508">VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have become increasingly popular in robot manipulation for their end-to-end design and remarkable performance. However, existing VLAs rely heavily on vision-language models (VLMs) that only support text-based instructions, neglecting the more natural speech modality for human-robot interaction. Traditional speech integration methods usually involves a separate speech recognition system, which complicates the model and introduces error propagation. Moreover, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which may be crucial for robots to successfully complete customized tasks. To overcome above challenges, we propose VLAS, a novel end-to-end VLA that integrates speech recognition directly into the robot policy model. VLAS allows the robot to understand spoken commands through inner speech-text alignment and produces corresponding actions to fulfill the task. We also present two new datasets, SQA and CSI, to support a three-stage tuning process for speech instructions, which empowers VLAS with the ability of multimodal interaction across text, image, speech, and robot actions. Taking a step further, a voice retrieval-augmented generation (RAG) paradigm is designed to enable our model to effectively handle tasks that require individual-specific knowledge. Our extensive experiments show that VLAS can effectively accomplish robot manipulation tasks with diverse speech commands, offering a seamless and customized interaction experience.
<div id='section'>PaperID: <span id='pid'>375, <a href='https://arxiv.org/pdf/2410.15461.pdf' target='_blank'>https://arxiv.org/pdf/2410.15461.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15461">EVA: An Embodied World Model for Future Video Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have made significant progress in simulating future states, showcasing their potential as world simulators in embodied scenarios. However, existing models often lack robust understanding, limiting their ability to perform multi-step predictions or handle Out-of-Distribution (OOD) scenarios. To address this challenge, we propose the Reflection of Generation (RoG), a set of intermediate reasoning strategies designed to enhance video prediction. It leverages the complementary strengths of pre-trained vision-language and video generation models, enabling them to function as a world model in embodied scenarios. To support RoG, we introduce Embodied Video Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates embodied world models across diverse tasks and scenarios, utilizing both in-domain and OOD datasets. Building on this foundation, we devise a world model, Embodied Video Anticipator (EVA), that follows a multistage training paradigm to generate high-fidelity video frames and apply an autoregressive strategy to enable adaptive generalization for longer video sequences. Extensive experiments demonstrate the efficacy of EVA in various downstream tasks like video generation and robotics, thereby paving the way for large-scale pre-trained models in real-world video prediction applications. The video demos are available at \hyperlink{https://sites.google.com/view/icml-eva}{https://sites.google.com/view/icml-eva}.
<div id='section'>PaperID: <span id='pid'>376, <a href='https://arxiv.org/pdf/2602.04411.pdf' target='_blank'>https://arxiv.org/pdf/2602.04411.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tongtong Feng, Xin Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04411">Self-evolving Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence.
<div id='section'>PaperID: <span id='pid'>377, <a href='https://arxiv.org/pdf/2509.20021.pdf' target='_blank'>https://arxiv.org/pdf/2509.20021.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tongtong Feng, Xin Wang, Yu-Gang Jiang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20021">Embodied AI: From LLMs to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.
<div id='section'>PaperID: <span id='pid'>378, <a href='https://arxiv.org/pdf/2508.17481.pdf' target='_blank'>https://arxiv.org/pdf/2508.17481.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Priyanka Prakash Surve, Asaf Shabtai, Yuval Elovici
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17481">SoK: Cybersecurity Assessment of Humanoid Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoids are progressing toward practical deployment across healthcare, industrial, defense, and service sectors. While typically considered cyber-physical systems (CPSs), their dependence on traditional networked software stacks (e.g., Linux operating systems), robot operating system (ROS) middleware, and over-the-air update channels, creates a distinct security profile that exposes them to vulnerabilities conventional CPS models do not fully address. Prior studies have mainly examined specific threats, such as LiDAR spoofing or adversarial machine learning (AML). This narrow focus overlooks how an attack targeting one component can cascade harm throughout the robot's interconnected systems. We address this gap through a systematization of knowledge (SoK) that takes a comprehensive approach, consolidating fragmented research from robotics, CPS, and network security domains. We introduce a seven-layer security model for humanoid robots, organizing 39 known attacks and 35 defenses across the humanoid ecosystem-from hardware to human-robot interaction. Building on this security model, we develop a quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated through Monte Carlo analysis. We demonstrate our method by evaluating three real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed varying security maturity levels, with scores ranging from 39.9% to 79.5% across the platforms. This work introduces a structured, evidence-based assessment method that enables systematic security evaluation, supports cross-platform benchmarking, and guides prioritization of security investments in humanoid robotics.
<div id='section'>PaperID: <span id='pid'>379, <a href='https://arxiv.org/pdf/2505.20640.pdf' target='_blank'>https://arxiv.org/pdf/2505.20640.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20640">IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Embodied Question Answering (EQA) benchmarks primarily focus on household environments, often overlooking safety-critical aspects and reasoning processes pertinent to industrial settings. This drawback limits the evaluation of agent readiness for real-world industrial applications. To bridge this, we introduce IndustryEQA, the first benchmark dedicated to evaluating embodied agent capabilities within safety-critical warehouse scenarios. Built upon the NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets, dynamic human agents, and carefully designed hazardous situations inspired by real-world safety guidelines. The benchmark includes rich annotations covering six categories: equipment safety, human safety, object recognition, attribute recognition, temporal understanding, and spatial understanding. Besides, it also provides extra reasoning evaluation based on these categories. Specifically, it comprises 971 question-answer pairs generated from small warehouse and 373 pairs from large ones, incorporating scenarios with and without human. We further propose a comprehensive evaluation framework, including various baseline models, to assess their general perception and reasoning abilities in industrial environments. IndustryEQA aims to steer EQA research towards developing more robust, safety-aware, and practically applicable embodied agents for complex industrial environments. Benchmark and codes are available.
<div id='section'>PaperID: <span id='pid'>380, <a href='https://arxiv.org/pdf/2510.21307.pdf' target='_blank'>https://arxiv.org/pdf/2510.21307.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bingchen Miao, Rong Wei, Zhiqi Ge, Xiaoquan sun, Shiqi Gao, Jingzhe Zhu, Renhan Wang, Siliang Tang, Jun Xiao, Rui Tang, Juncheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21307">Towards Physically Executable 3D Gaussian for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. The data and code will be available soon.
<div id='section'>PaperID: <span id='pid'>381, <a href='https://arxiv.org/pdf/2601.14352.pdf' target='_blank'>https://arxiv.org/pdf/2601.14352.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Huajie Tan, Enshen Zhou, Zhiyu Li, Yijie Xu, Yuheng Ji, Xiansheng Chen, Cheng Chi, Pengwei Wang, Huizhu Jia, Yulong Ao, Mingyu Cao, Sixiang Chen, Zhe Li, Mengzhen Liu, Zixiao Wang, Shanyu Rong, Yaoxu Lyu, Zhongxia Zhao, Peterson Co, Yibo Li, Yi Han, Shaoxuan Xie, Guocai Yao, Songjing Wang, Leiduo Zhang, Xi Yang, Yance Jiao, Donghai Shi, Kunchang Xie, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14352">RoboBrain 2.5: Depth in Sight, Time in Mind</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io
<div id='section'>PaperID: <span id='pid'>382, <a href='https://arxiv.org/pdf/2507.09876.pdf' target='_blank'>https://arxiv.org/pdf/2507.09876.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yongheng Zhang, Xu Liu, Ruihan Tao, Qiguang Chen, Hao Fei, Wanxiang Che, Libo Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09876">ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.
<div id='section'>PaperID: <span id='pid'>383, <a href='https://arxiv.org/pdf/2507.02029.pdf' target='_blank'>https://arxiv.org/pdf/2507.02029.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Mengfei Du, Cheng Chi, Mengdi Zhao, Xiaoshuai Hao, Junkai Zhao, Xiaojie Zhang, Shanyu Rong, Huaihai Lyu, Zhengliang Cai, Yankai Fu, Ning Chen, Bolun Zhang, Lingfeng Zhang, Shuyi Zhang, Dong Liu, Xi Feng, Songjing Wang, Xiaodan Liu, Yance Jiao, Mengsi Lyu, Zhuo Chen, Chenrui He, Yulong Ao, Xue Sun, Zheqi He, Jingshu Zheng, Xi Yang, Donghai Shi, Kunchang Xie, Bochao Zhang, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02029">RoboBrain 2.0 Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.
<div id='section'>PaperID: <span id='pid'>384, <a href='https://arxiv.org/pdf/2506.06535.pdf' target='_blank'>https://arxiv.org/pdf/2506.06535.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Vineet Bhat, Naman Patel, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06535">MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation of unseen objects via natural language commands remains challenging. Language driven robotic grasping (LDRG) predicts stable grasp poses from natural language queries and RGB-D images. We propose MapleGrasp, a novel framework that leverages mask-guided feature pooling for efficient vision-language driven grasping. Our two-stage training first predicts segmentation masks from CLIP-based vision-language features. The second stage pools features within these masks to generate pixel-level grasp predictions, improving efficiency, and reducing computation. Incorporating mask pooling results in a 7% improvement over prior approaches on the OCID-VLG benchmark. Furthermore, we introduce RefGraspNet, an open-source dataset eight times larger than existing alternatives, significantly enhancing model generalization for open-vocabulary grasping. MapleGrasp scores a strong grasping accuracy of 89\% when compared with competing methods in the RefGraspNet benchmark. Our method achieves comparable performance to larger Vision-Language-Action models on the LIBERO benchmark, and shows significantly better generalization to unseen tasks. Real-world experiments on a Franka arm demonstrate 73% success rate with unseen objects, surpassing competitive baselines by 11%. Code is provided in our github repository.
<div id='section'>PaperID: <span id='pid'>385, <a href='https://arxiv.org/pdf/2505.16055.pdf' target='_blank'>https://arxiv.org/pdf/2505.16055.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Patanjali Maithani, Aliasghar Arab, Farshad Khorrami, Prashanth Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16055">Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In collaborative human-robot environments, the unpredictable and dynamic nature of human motion can lead to situations where collisions become unavoidable. In such cases, it is essential for the robotic system to proactively mitigate potential harm through intelligent control strategies. This paper presents a hierarchical control framework based on Control Barrier Functions (CBFs) designed to ensure safe and adaptive operation of autonomous robotic manipulators during close-proximity human-robot interaction. The proposed method introduces a relaxation variable that enables real-time prioritization of safety constraints, allowing the robot to dynamically manage collision risks based on the criticality of different parts of the human body. A secondary constraint mechanism is incorporated to resolve infeasibility by increasing the priority of imminent threats. The framework is experimentally validated on a Franka Research 3 robot equipped with a ZED2i AI camera for real-time human pose and body detection. Experimental results confirm that the CBF-based controller, integrated with depth sensing, facilitates responsive and safe human-robot collaboration, while providing detailed risk analysis and maintaining robust performance in highly dynamic settings.
<div id='section'>PaperID: <span id='pid'>386, <a href='https://arxiv.org/pdf/2505.05800.pdf' target='_blank'>https://arxiv.org/pdf/2505.05800.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Vineet Bhat, Yu-Hsiang Lan, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05800">3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1$\%$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io
<div id='section'>PaperID: <span id='pid'>387, <a href='https://arxiv.org/pdf/2502.13451.pdf' target='_blank'>https://arxiv.org/pdf/2502.13451.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13451">MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.
<div id='section'>PaperID: <span id='pid'>388, <a href='https://arxiv.org/pdf/2411.17662.pdf' target='_blank'>https://arxiv.org/pdf/2411.17662.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Raktim Gautam Goswami, Prashanth Krishnamurthy, Yann LeCun, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17662">RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.
<div id='section'>PaperID: <span id='pid'>389, <a href='https://arxiv.org/pdf/2509.11480.pdf' target='_blank'>https://arxiv.org/pdf/2509.11480.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amir Taherin, Juyi Lin, Arash Akbari, Arman Akbari, Pu Zhao, Weiwei Chen, David Kaeli, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11480">Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.
<div id='section'>PaperID: <span id='pid'>390, <a href='https://arxiv.org/pdf/2511.09119.pdf' target='_blank'>https://arxiv.org/pdf/2511.09119.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahao Xiao, Bowen Yan, Jianbo Zhang, Jia Wang, Chunyi Li, Zhengxue Cheng, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09119">Data Assessment for Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.
<div id='section'>PaperID: <span id='pid'>391, <a href='https://arxiv.org/pdf/2505.21432.pdf' target='_blank'>https://arxiv.org/pdf/2505.21432.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21432">Hume: Introducing System-2 Thinking in Visual-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments.
<div id='section'>PaperID: <span id='pid'>392, <a href='https://arxiv.org/pdf/2502.18041.pdf' target='_blank'>https://arxiv.org/pdf/2502.18041.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18041">OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) aims to guide agents by leveraging language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising various rendering engines, a versatile toolchain, and a large-scale benchmark for aerial VLN. Firstly, we integrate diverse rendering engines and advanced techniques for environment simulation, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of our environments. Secondly, we develop a highly automated toolchain for aerial VLN data collection, streamlining point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Thirdly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. Moreover, we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key observations during flight. For benchmarking, extensive experiments and analyses are conducted, evaluating several recent VLN methods and showcasing the superiority of our OpenFly platform and agent. The toolchain, dataset, and codes will be open-sourced.
<div id='section'>PaperID: <span id='pid'>393, <a href='https://arxiv.org/pdf/2508.21112.pdf' target='_blank'>https://arxiv.org/pdf/2508.21112.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21112">EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.
<div id='section'>PaperID: <span id='pid'>394, <a href='https://arxiv.org/pdf/2510.20310.pdf' target='_blank'>https://arxiv.org/pdf/2510.20310.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingliang Zhai, Hansheng Liang, Xiaomeng Fan, Zhi Gao, Chuanhao Li, Che Sun, Xu Bin, Yuwei Wu, Yunde Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20310">Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) requires agents to explore 3D environments to obtain observations and answer questions related to the scene. Existing methods leverage VLMs to directly explore the environment and answer questions without explicit thinking or planning, which limits their reasoning ability and results in excessive or inefficient exploration as well as ineffective responses. In this paper, we introduce ToolEQA, an agent that integrates external tools with multi-step reasoning, where external tools can provide more useful information for completing the task, helping the model derive better exploration directions in the next step of reasoning and thus obtaining additional effective information. This enables ToolEQA to generate more accurate responses with a shorter exploration distance. To enhance the model's ability for tool-usage and multi-step reasoning, we further design a novel EQA data generation pipeline that automatically constructs large-scale EQA tasks with reasoning trajectories and corresponding answers. Based on the pipeline, we collect the EQA-RT dataset that contains about 18K tasks, divided into a training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping with the training set) and EQA-RT-Unseen (novel scenes). Experiments on EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by 9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot ToolEQA by 10% in success rate. In addition, ToolEQA also achieves state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench datasets, demonstrating its generality. Our homepage see https://tooleqa.github.io.
<div id='section'>PaperID: <span id='pid'>395, <a href='https://arxiv.org/pdf/2510.12693.pdf' target='_blank'>https://arxiv.org/pdf/2510.12693.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanyang Chen, Mark Zhao, Rui Yang, Qinwei Ma, Ke Yang, Jiarui Yao, Kangrui Wang, Hao Bai, Zhenhailong Wang, Rui Pan, Mengchao Zhang, Jose Barreiros, Aykut Onol, ChengXiang Zhai, Heng Ji, Manling Li, Huan Zhang, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12693">ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present \textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, \textit{Embodied Prior Learning}, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.
<div id='section'>PaperID: <span id='pid'>396, <a href='https://arxiv.org/pdf/2510.04401.pdf' target='_blank'>https://arxiv.org/pdf/2510.04401.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuyang Guo, Zekai Huang, Zhenmei Shi, Zhao Song, Jiahao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04401">Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have become a central focus of today's AI community, owing to their impressive abilities gained from training on large-scale vision-language data from the Web. These models have demonstrated strong performance across diverse tasks, including image understanding, video understanding, complex visual reasoning, and embodied AI. Despite these noteworthy successes, a fundamental question remains: Can VLMs count objects correctly? In this paper, we introduce a simple yet effective benchmark, VLMCountBench, designed under a minimalist setting with only basic geometric shapes (e.g., triangles, circles) and their compositions, focusing exclusively on counting tasks without interference from other factors. We adopt strict independent variable control and systematically study the effects of simple properties such as color, size, and prompt refinement in a controlled ablation. Our empirical results reveal that while VLMs can count reliably when only one shape type is present, they exhibit substantial failures when multiple shape types are combined (i.e., compositional counting). This highlights a fundamental empirical limitation of current VLMs and motivates important directions for future research.
<div id='section'>PaperID: <span id='pid'>397, <a href='https://arxiv.org/pdf/2507.21407.pdf' target='_blank'>https://arxiv.org/pdf/2507.21407.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yixin Liu, Guibin Zhang, Kun Wang, Shiyuan Li, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21407">Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.
<div id='section'>PaperID: <span id='pid'>398, <a href='https://arxiv.org/pdf/2505.13948.pdf' target='_blank'>https://arxiv.org/pdf/2505.13948.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingliang Zhai, Zhi Gao, Yuwei Wu, Yunde Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13948">Memory-Centric Embodied Question Answer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) requires agents to autonomously explore and understand the environment to answer context-dependent questions. Existing frameworks typically center around the planner, which guides the stopping module, memory module, and answering module for reasoning. In this paper, we propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric EQA models where the memory module cannot fully interact with other modules, MemoryEQA flexible feeds memory information into all modules, thereby enhancing efficiency and accuracy in handling complex tasks, such as those involving multiple targets across different regions. Specifically, we establish a multi-modal hierarchical memory mechanism, which is divided into global memory that stores language-enhanced scene maps, and local memory that retains historical observations and state information. When performing EQA tasks, the multi-modal large language model is leveraged to convert memory information into the required input formats for injection into different modules. To evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset based on HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 19.8% performance gain on MT-HM3D compared to baseline model further underscores memory capability's pivotal role in resolving complex tasks.
<div id='section'>PaperID: <span id='pid'>399, <a href='https://arxiv.org/pdf/2601.16394.pdf' target='_blank'>https://arxiv.org/pdf/2601.16394.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yihao Wang, Jusheng Zhang, Ziyi Tang, Keze Wang, Meng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16394">ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \textbf{\model}, a novel RES framework integrating \textbf{E}ntropy-\textbf{B}ased Point \textbf{D}iscovery (\textbf{EBD}) and \textbf{V}ision-\textbf{B}ased \textbf{R}easoning (\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts.
<div id='section'>PaperID: <span id='pid'>400, <a href='https://arxiv.org/pdf/2510.09507.pdf' target='_blank'>https://arxiv.org/pdf/2510.09507.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixin Zhang, Kanghao Chen, Xingwang Lin, Lutao Jiang, Xu Zheng, Yuanhuiyi Lyu, Litao Guo, Yinchuan Li, Ying-Cong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09507">PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.
<div id='section'>PaperID: <span id='pid'>401, <a href='https://arxiv.org/pdf/2508.02106.pdf' target='_blank'>https://arxiv.org/pdf/2508.02106.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02106">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.
<div id='section'>PaperID: <span id='pid'>402, <a href='https://arxiv.org/pdf/2503.18016.pdf' target='_blank'>https://arxiv.org/pdf/2503.18016.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Lutao Jiang, Haiwei Xue, Bin Ren, Danda Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18016">Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) has emerged as a pivotal technique in artificial intelligence (AI), particularly in enhancing the capabilities of large language models (LLMs) by enabling access to external, reliable, and up-to-date knowledge sources. In the context of AI-Generated Content (AIGC), RAG has proven invaluable by augmenting model outputs with supplementary, relevant information, thus improving their quality. Recently, the potential of RAG has extended beyond natural language processing, with emerging methods integrating retrieval-augmented strategies into the computer vision (CV) domain. These approaches aim to address the limitations of relying solely on internal model knowledge by incorporating authoritative external knowledge bases, thereby improving both the understanding and generation capabilities of vision models. This survey provides a comprehensive review of the current state of retrieval-augmented techniques in CV, focusing on two main areas: (I) visual understanding and (II) visual generation. In the realm of visual understanding, we systematically review tasks ranging from basic image recognition to complex applications such as medical report generation and multimodal question answering. For visual content generation, we examine the application of RAG in tasks related to image, video, and 3D generation. Furthermore, we explore recent advancements in RAG for embodied AI, with a particular focus on applications in planning, task execution, multimodal perception, interaction, and specialized domains. Given that the integration of retrieval-augmented techniques in CV is still in its early stages, we also highlight the key limitations of current approaches and propose future research directions to drive the development of this promising area.
<div id='section'>PaperID: <span id='pid'>403, <a href='https://arxiv.org/pdf/2503.22020.pdf' target='_blank'>https://arxiv.org/pdf/2503.22020.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, Tsung-Yi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22020">CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Project website: https://cot-vla.github.io/
<div id='section'>PaperID: <span id='pid'>404, <a href='https://arxiv.org/pdf/2505.02152.pdf' target='_blank'>https://arxiv.org/pdf/2505.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02152">Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great promise for generalist robotic manipulation in the physical world. However, existing models are restricted to robot observations and text-only instructions, lacking the flexibility of interleaved multimodal instructions enabled by recent advances in foundation models in the digital world. In this paper, we present Interleave-VLA, the first framework capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world. It offers a flexible, model-agnostic paradigm that extends state-of-the-art VLA models with minimal modifications and strong zero-shot generalization. A key challenge in realizing Interleave-VLA is the absence of large-scale interleaved embodied datasets. To bridge this gap, we develop an automatic pipeline that converts text-only instructions from real-world datasets in Open X-Embodiment into interleaved image-text instructions, resulting in the first large-scale real-world interleaved embodied dataset with 210k episodes. Through comprehensive evaluation on simulation benchmarks and real-robot experiments, we demonstrate that Interleave-VLA offers significant benefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x compared to state-of-the-art baselines, 2) supports flexible task interfaces, and 3) handles diverse user-provided image instructions in a zero-shot manner, such as hand-drawn sketches. We further analyze the factors behind Interleave-VLA's strong zero-shot performance, showing that the interleaved paradigm effectively leverages heterogeneous datasets and diverse instruction images, including those from the Internet, which demonstrates strong potential for scaling up. Our model and dataset will be open-sourced.
<div id='section'>PaperID: <span id='pid'>405, <a href='https://arxiv.org/pdf/2602.02864.pdf' target='_blank'>https://arxiv.org/pdf/2602.02864.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo, Yue Wang, Wenhao Ding, Boyi Li, Heng Yang, Boris Ivanovic, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.02864">Accelerating Structured Chain-of-Thought in Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning.
<div id='section'>PaperID: <span id='pid'>406, <a href='https://arxiv.org/pdf/2601.19233.pdf' target='_blank'>https://arxiv.org/pdf/2601.19233.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zeyu Xiao, Mingyang Sun, Yimin Cong, Lintao Wang, Dongliang Kou, Zhenyi Wu, Dingkang Yang, Peng Zhai, Zeyu Wang, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19233">UniMGS: Unifying Mesh and 3D Gaussian Splatting with Single-Pass Rasterization and Proxy-Based Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint rendering and deformation of mesh and 3D Gaussian Splatting (3DGS) have significant value as both representa tions offer complementary advantages for graphics applica tions. However, due to differences in representation and ren dering pipelines, existing studies render meshes and 3DGS separately, making it difficult to accurately handle occlusions and transparency. Moreover, the deformed 3DGS still suffers from visual artifacts due to the sensitivity to the topology quality of the proxy mesh. These issues pose serious obsta cles to the joint use of 3DGS and meshes, making it diffi cult to adapt 3DGS to conventional mesh-oriented graphics pipelines. We propose UniMGS, the first unified framework for rasterizing mesh and 3DGS in a single-pass anti-aliased manner, with a novel binding strategy for 3DGS deformation based on proxy mesh. Our key insight is to blend the col ors of both triangle and Gaussian fragments by anti-aliased α-blending in a single pass, achieving visually coherent re sults with precise handling of occlusion and transparency. To improve the visual appearance of the deformed 3DGS, our Gaussian-centric binding strategy employs a proxy mesh and spatially associates Gaussians with the mesh faces, signifi cantly reducing rendering artifacts. With these two compo nents, UniMGS enables the visualization and manipulation of 3D objects represented by mesh or 3DGS within a unified framework, opening up new possibilities in embodied AI, vir tual reality, and gaming. We will release our source code to facilitate future research.
<div id='section'>PaperID: <span id='pid'>407, <a href='https://arxiv.org/pdf/2511.19861.pdf' target='_blank'>https://arxiv.org/pdf/2511.19861.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>GigaWorld Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jiagang Zhu, Kerui Li, Mengyuan Xu, Qiuping Deng, Siting Wang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yankai Wang, Yu Cao, Yifan Chang, Yuan Xu, Yun Ye, Yang Wang, Yukun Zhou, Zhengyuan Zhang, Zhehao Dong, Zheng Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19861">GigaWorld-0: World Models as Data Engine to Empower Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.
<div id='section'>PaperID: <span id='pid'>408, <a href='https://arxiv.org/pdf/2509.22407.pdf' target='_blank'>https://arxiv.org/pdf/2509.22407.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, Xinze Chen, Yun Ye, Guan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22407">EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.
<div id='section'>PaperID: <span id='pid'>409, <a href='https://arxiv.org/pdf/2507.05198.pdf' target='_blank'>https://arxiv.org/pdf/2507.05198.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05198">EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91\%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.
<div id='section'>PaperID: <span id='pid'>410, <a href='https://arxiv.org/pdf/2505.11191.pdf' target='_blank'>https://arxiv.org/pdf/2505.11191.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11191">Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: multi-modal multi-task foundation models (M3T-FMs) provide a pathway toward generalization across tasks and modalities, whereas federated learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied AI environments. In this vision paper, we introduce multi-modal multi-task federated foundation models (M3T-FFMs) for embodied AI, a new paradigm that unifies the strengths of M3T-FMs with the privacy-preserving distributed training nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of M3T-FFMs in embodied AI ecosystems under a unified framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying M3T-FFMs in embodied AI systems, along with the associated trade-offs. Finally, we present a prototype implementation of M3T-FFMs and evaluate their energy and latency performance.
<div id='section'>PaperID: <span id='pid'>411, <a href='https://arxiv.org/pdf/2601.08325.pdf' target='_blank'>https://arxiv.org/pdf/2601.08325.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhenyang Liu, Yongchong Gu, Yikai Wang, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08325">ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.
<div id='section'>PaperID: <span id='pid'>412, <a href='https://arxiv.org/pdf/2512.22539.pdf' target='_blank'>https://arxiv.org/pdf/2512.22539.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22539">VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Vision-Language-Action models (VLAs) are rapidly advancing towards generalist robot policies, it remains difficult to quantitatively understand their limits and failure modes. To address this, we introduce a comprehensive benchmark called VLA-Arena. We propose a novel structured task design framework to quantify difficulty across three orthogonal axes: (1) Task Structure, (2) Language Command, and (3) Visual Observation. This allows us to systematically design tasks with fine-grained difficulty levels, enabling a precise measurement of model capability frontiers. For Task Structure, VLA-Arena's 170 tasks are grouped into four dimensions: Safety, Distractor, Extrapolation, and Long Horizon. Each task is designed with three difficulty levels (L0-L2), with fine-tuning performed exclusively on L0 to assess general capability. Orthogonal to this, language (W0-W4) and visual (V0-V4) perturbations can be applied to any task to enable a decoupled analysis of robustness. Our extensive evaluation of state-of-the-art VLAs reveals several critical limitations, including a strong tendency toward memorization over generalization, asymmetric robustness, a lack of consideration for safety constraints, and an inability to compose learned skills for long-horizon tasks. To foster research addressing these challenges and ensure reproducibility, we provide the complete VLA-Arena framework, including an end-to-end toolchain from task definition to automated evaluation and the VLA-Arena-S/M/L datasets for fine-tuning. Our benchmark, data, models, and leaderboard are available at https://vla-arena.github.io.
<div id='section'>PaperID: <span id='pid'>413, <a href='https://arxiv.org/pdf/2511.17225.pdf' target='_blank'>https://arxiv.org/pdf/2511.17225.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shanshan Li, Da Huang, Yu He, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17225">TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.
<div id='section'>PaperID: <span id='pid'>414, <a href='https://arxiv.org/pdf/2510.11687.pdf' target='_blank'>https://arxiv.org/pdf/2510.11687.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11687">Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating an object's 6D pose, size, and shape from visual input is a fundamental problem in computer vision, with critical applications in robotic grasping and manipulation. Existing methods either rely on object-specific priors such as CAD models or templates, or suffer from limited generalization across categories due to pose-shape entanglement and multi-stage pipelines. In this work, we propose a unified, category-agnostic framework that simultaneously predicts 6D pose, size, and dense shape from a single RGB-D image, without requiring templates, CAD models, or category labels at test time. Our model fuses dense 2D features from vision foundation models with partial 3D point clouds using a Transformer encoder enhanced by a Mixture-of-Experts, and employs parallel decoders for pose-size estimation and shape reconstruction, achieving real-time inference at 28 FPS. Trained solely on synthetic data from 149 categories in the SOPE dataset, our framework is evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL, spanning over 300 categories. It achieves state-of-the-art accuracy on seen categories while demonstrating remarkably strong zero-shot generalization to unseen real-world objects, establishing a new standard for open-set 6D understanding in robotics and embodied AI.
<div id='section'>PaperID: <span id='pid'>415, <a href='https://arxiv.org/pdf/2509.26375.pdf' target='_blank'>https://arxiv.org/pdf/2509.26375.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zichao Shen, Chen Gao, Jiaqi Yuan, Tianchen Zhu, Xingcheng Fu, Qingyun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26375">SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.However, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.
<div id='section'>PaperID: <span id='pid'>416, <a href='https://arxiv.org/pdf/2507.06719.pdf' target='_blank'>https://arxiv.org/pdf/2507.06719.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhenyang Liu, Sixiao Zheng, Siyu Chen, Cairong Zhao, Longfei Liang, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06719">A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.
<div id='section'>PaperID: <span id='pid'>417, <a href='https://arxiv.org/pdf/2503.03480.pdf' target='_blank'>https://arxiv.org/pdf/2503.03480.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03480">SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, this exploration yields an 83.58% safety improvement compared to the current state-of-the-art method, while also maintaining task performance (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.
<div id='section'>PaperID: <span id='pid'>418, <a href='https://arxiv.org/pdf/2512.06387.pdf' target='_blank'>https://arxiv.org/pdf/2512.06387.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhang Huang, Junchao Li, Boyang Ma, Xuelong Dai, Minghui Xu, Kaidi Xu, Yue Zhang, Jianping Wang, Xiuzhen Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06387">Beyond Model Jailbreak: Systematic Dissection of the "Ten DeadlySins" in Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the "Ten Sins of Embodied AI Security." Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.
<div id='section'>PaperID: <span id='pid'>419, <a href='https://arxiv.org/pdf/2507.04047.pdf' target='_blank'>https://arxiv.org/pdf/2507.04047.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyu Zhu, Xilin Wang, Yixuan Li, Zhuofan Zhang, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Wei Liang, Qian Yu, Zhidong Deng, Siyuan Huang, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04047">Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \underline{\textbf{M}}ove \underline{\textbf{t}}o \underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework that integrates active perception with \underline{\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%, and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.
<div id='section'>PaperID: <span id='pid'>420, <a href='https://arxiv.org/pdf/2412.09634.pdf' target='_blank'>https://arxiv.org/pdf/2412.09634.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jesse Atuhurra, Hidetaka Kamigaito, Hiroki Ouchi, Hiroyuki Shindo, Taro Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09634">NERsocial: Efficient Named Entity Recognition Dataset Construction for Human-Robot Interaction Utilizing RapidNER</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting named entity recognition (NER) methods to new domains poses significant challenges. We introduce RapidNER, a framework designed for the rapid deployment of NER systems through efficient dataset construction. RapidNER operates through three key steps: (1) extracting domain-specific sub-graphs and triples from a general knowledge graph, (2) collecting and leveraging texts from various sources to build the NERsocial dataset, which focuses on entities typical in human-robot interaction, and (3) implementing an annotation scheme using Elasticsearch (ES) to enhance efficiency. NERsocial, validated by human annotators, includes six entity types, 153K tokens, and 99.4K sentences, demonstrating RapidNER's capability to expedite dataset creation.
<div id='section'>PaperID: <span id='pid'>421, <a href='https://arxiv.org/pdf/2409.02389.pdf' target='_blank'>https://arxiv.org/pdf/2409.02389.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02389">Multi-modal Situated Reasoning in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.
<div id='section'>PaperID: <span id='pid'>422, <a href='https://arxiv.org/pdf/2602.00780.pdf' target='_blank'>https://arxiv.org/pdf/2602.00780.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuting Huang, Leilei Ding, Zhipeng Tang, Zenghuan Zhu, Jiajun Deng, Xinrui Lin, Shuo Liu, Haojie Ren, Jianmin Ji, Yanyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00780">Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.
<div id='section'>PaperID: <span id='pid'>423, <a href='https://arxiv.org/pdf/2511.22963.pdf' target='_blank'>https://arxiv.org/pdf/2511.22963.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhirui Liu, Kaiyang Ji, Ke Yang, Jingyi Yu, Ye Shi, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22963">Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to follow free-form language commands is critical for seamless human-robot interaction, collaborative task execution, and general-purpose embodied intelligence. While recent advances have improved low-level humanoid locomotion and robot manipulation, language-conditioned whole-body control remains a significant challenge. Existing methods are often limited to simple instructions and sacrifice either motion diversity or physical plausibility. To address this, we introduce Humanoid-LLA, a Large Language Action Model that maps expressive language commands to physically executable whole-body actions for humanoid robots. Our approach integrates three core components: a unified motion vocabulary that aligns human and humanoid motion primitives into a shared discrete space; a vocabulary-directed controller distilled from a privileged policy to ensure physical feasibility; and a physics-informed fine-tuning stage using reinforcement learning with dynamics-aware rewards to enhance robustness and stability. Extensive evaluations in simulation and on a real-world Unitree G1 humanoid show that Humanoid-LLA delivers strong language generalization while maintaining high physical fidelity, outperforming existing language-conditioned controllers in motion naturalness, stability, and execution success rate.
<div id='section'>PaperID: <span id='pid'>424, <a href='https://arxiv.org/pdf/2511.15669.pdf' target='_blank'>https://arxiv.org/pdf/2511.15669.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cheng Yin, Yankai Lin, Wang Xu, Sikyuen Tam, Xiangrui Zeng, Zhiyuan Liu, Zhouping Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15669">DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.
<div id='section'>PaperID: <span id='pid'>425, <a href='https://arxiv.org/pdf/2511.03992.pdf' target='_blank'>https://arxiv.org/pdf/2511.03992.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03992">CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.
<div id='section'>PaperID: <span id='pid'>426, <a href='https://arxiv.org/pdf/2504.08581.pdf' target='_blank'>https://arxiv.org/pdf/2504.08581.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08581">FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.
<div id='section'>PaperID: <span id='pid'>427, <a href='https://arxiv.org/pdf/2506.05287.pdf' target='_blank'>https://arxiv.org/pdf/2506.05287.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqian Yuan, Ronghao Dang, Long Li, Wentong Li, Dian Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05287">EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.
<div id='section'>PaperID: <span id='pid'>428, <a href='https://arxiv.org/pdf/2506.10100.pdf' target='_blank'>https://arxiv.org/pdf/2506.10100.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10100">EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.
<div id='section'>PaperID: <span id='pid'>429, <a href='https://arxiv.org/pdf/2505.06575.pdf' target='_blank'>https://arxiv.org/pdf/2505.06575.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06575">GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
<div id='section'>PaperID: <span id='pid'>430, <a href='https://arxiv.org/pdf/2602.03793.pdf' target='_blank'>https://arxiv.org/pdf/2602.03793.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yixiang Chen, Peiyan Li, Jiabing Yang, Keji He, Xiangnan Wu, Yuan Xu, Kai Wang, Jing Liu, Nianfeng Liu, Yan Huang, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03793">BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .
<div id='section'>PaperID: <span id='pid'>431, <a href='https://arxiv.org/pdf/2509.09560.pdf' target='_blank'>https://arxiv.org/pdf/2509.09560.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shulai Zhang, Ao Xu, Quan Chen, Han Zhao, Weihao Cui, Ningxin Zheng, Haibin Lin, Xin Liu, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09560">Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary "thinking" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.
<div id='section'>PaperID: <span id='pid'>432, <a href='https://arxiv.org/pdf/2602.15864.pdf' target='_blank'>https://arxiv.org/pdf/2602.15864.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuzhuo Ao, Anbang Wang, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15864">ReasonNavi: Human-Inspired Global Map Reasoning for Zero-Shot Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents often struggle with efficient navigation because they rely primarily on partial egocentric observations, which restrict global foresight and lead to inefficient exploration. In contrast, humans plan using maps: we reason globally first, then act locally. We introduce ReasonNavi, a human-inspired framework that operationalizes this reason-then-act paradigm by coupling Multimodal Large Language Models (MLLMs) with deterministic planners. ReasonNavi converts a top-down map into a discrete reasoning space by room segmentation and candidate target nodes sampling. An MLLM is then queried in a multi-stage process to identify the candidate most consistent with the instruction (object, image, or text goal), effectively leveraging the model's semantic reasoning ability while sidestepping its weakness in continuous coordinate prediction. The selected waypoint is grounded into executable trajectories using a deterministic action planner over an online-built occupancy map, while pretrained object detectors and segmenters ensure robust recognition at the goal. This yields a unified zero-shot navigation framework that requires no MLLM fine-tuning, circumvents the brittleness of RL-based policies and scales naturally with foundation model improvements. Across three navigation tasks, ReasonNavi consistently outperforms prior methods that demand extensive training or heavy scene modeling, offering a scalable, interpretable, and globally grounded solution to embodied navigation. Project page: https://reasonnavi.github.io/
<div id='section'>PaperID: <span id='pid'>433, <a href='https://arxiv.org/pdf/2502.09649.pdf' target='_blank'>https://arxiv.org/pdf/2502.09649.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhang Dong, Haizhou Ge, Yupei Zeng, Jiangning Zhang, Beiwen Tian, Hongrui Zhu, Yufei Jia, Ruixiang Wang, Zhucun Xue, Guyue Zhou, Longhua Ma, Guanzhong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09649">ImitDiff: Transferring Foundation-Model Priors for Distraction Robust Visuomotor Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visuomotor imitation learning policies enable robots to efficiently acquire manipulation skills from visual demonstrations. However, as scene complexity and visual distractions increase, policies that perform well in simple settings often experience substantial performance degradation. To address this challenge, we propose ImitDiff, a diffusion-based imitation learning policy guided by fine-grained semantics within a dual-resolution workflow. Leveraging pretrained priors of vision-language foundation models, our method transforms high-level instructions into pixel-level visual semantic masks. These masks guide a dual-resolution perception pipeline that captures both global context (e.g., overall layout) from low-resolution observation and fine-grained local features (e.g., geometric details) from high-resolution observation, enabling the policy to focus on task-relevant regions. Additionally, we introduce a consistency-driven diffusion transformer action head that bridges visual semantic conditions and real-time action generation. Extensive experiments demonstrate that ImitDiff outperforms state-of-the-art vision-language manipulation frameworks, as well as visuomotor imitation learning policies, particularly under increased scene complexity and visual distractions. Notably, ImitDiff exhibits strong generalization in zero-shot settings involving novel objects and visual distractions. Furthermore, our consistency-driven action head achieves an order-of-magnitude improvement in inference speed while maintaining competitive success rates.
<div id='section'>PaperID: <span id='pid'>434, <a href='https://arxiv.org/pdf/2602.08236.pdf' target='_blank'>https://arxiv.org/pdf/2602.08236.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.08236">When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.
<div id='section'>PaperID: <span id='pid'>435, <a href='https://arxiv.org/pdf/2512.02982.pdf' target='_blank'>https://arxiv.org/pdf/2512.02982.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiang Xu, Ao Liang, Youquan Liu, Linfeng Li, Lingdong Kong, Ziwei Liu, Qingshan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02982">U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.
<div id='section'>PaperID: <span id='pid'>436, <a href='https://arxiv.org/pdf/2506.03574.pdf' target='_blank'>https://arxiv.org/pdf/2506.03574.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Meng Li, Zhen Zhao, Zhengping Che, Fei Liao, Kun Wu, Zhiyuan Xu, Pei Ren, Zhao Jin, Ning Liu, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03574">SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in dynamic environments must be able to not only follow diverse language instructions but flexibly adapt when user intent changes mid-execution. While recent Vision-Language-Action (VLA) models have advanced multi-task learning and instruction following, they typically assume static task intent, failing to respond when new instructions arrive during ongoing execution. This limitation hinders natural and robust interaction in dynamic settings, such as retail or household environments, where real-time intent changes are common. We propose SwitchVLA, a unified, execution-aware framework that enables smooth and reactive task switching without external planners or additional switch-specific data. We model task switching as a behavior modulation problem conditioned on execution state and instruction context. Expert demonstrations are segmented into temporally grounded contact phases, allowing the policy to infer task progress and adjust its behavior accordingly. A multi-behavior conditional policy is then trained to generate flexible action chunks under varying behavior modes through conditioned trajectory modeling. Experiments in both simulation and real-world robotic manipulation demonstrate that SwitchVLA enables robust instruction adherence, fluid task switching, and strong generalization-outperforming prior VLA baselines in both task success rate and interaction naturalness.
<div id='section'>PaperID: <span id='pid'>437, <a href='https://arxiv.org/pdf/2505.11350.pdf' target='_blank'>https://arxiv.org/pdf/2505.11350.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Derek Ming Siang Tan, Shailesh, Boyang Liu, Alok Raj, Qi Xuan Ang, Weiheng Dai, Tanishq Duhan, Jimmy Chiun, Yuhong Cao, Florian Shkurti, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11350">Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To perform outdoor autonomous visual navigation and search, a robot may leverage satellite imagery as a prior map. This can help inform high-level search and exploration strategies, even when such images lack sufficient resolution to allow for visual recognition of targets. However, there are limited training datasets of satellite images with annotated targets that are not directly visible. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework with a flexible plug-and-play interface compatible with various input modalities (e.g. image, text, sound) and planning methods. First, we pretrain a satellite image encoder to align with CLIP's visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP's predictions during search using a test-time adaptation mechanism. Through a novel feedback loop inspired by Spatial Poisson Point Processes, uncertainty-weighted gradient updates are used to correct potentially inaccurate predictions and improve search performance. To train and evaluate Search-TTA, we curate AVS-Bench, a visual search dataset based on internet-scale ecological data that contains up to 380k training and 8k validation images (in- and out-domain). We find that Search-TTA improves planner performance by up to 30.0%, particularly in cases with poor initial CLIP predictions due to limited training data. It also performs comparably with significantly larger VLMs, and achieves zero-shot generalization to unseen modalities. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.
<div id='section'>PaperID: <span id='pid'>438, <a href='https://arxiv.org/pdf/2601.04137.pdf' target='_blank'>https://arxiv.org/pdf/2601.04137.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chun-Kai Fan, Xiaowei Chi, Xiaozhu Ju, Hao Li, Yong Bao, Yu-Kai Wang, Lizhang Chen, Zhiyuan Jiang, Kuangzhi Ge, Ying Li, Weishi Mi, Qingpo Wuwu, Peidong Jia, Yulin Luo, Kevin Zhang, Zhiyuan Qin, Yong Dai, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04137">Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.
<div id='section'>PaperID: <span id='pid'>439, <a href='https://arxiv.org/pdf/2510.09976.pdf' target='_blank'>https://arxiv.org/pdf/2510.09976.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingyang Lyu, Yinqian Sun, Erliang Lin, Huangrui Li, Ruolin Chen, Feifei Zhao, Yi Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09976">Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $π_0$ have shown strong generalization by leveraging large-scale demonstrations, yet their performance is still fundamentally constrained by the quality and coverage of supervised data. Reinforcement learning (RL) provides a promising path for improving and fine-tuning VLAs through online interaction. However, conventional policy gradient methods are computationally infeasible in the context of flow-matching based models due to the intractability of the importance sampling process, which requires explicit computation of policy ratios. To overcome this limitation, we propose Flow Policy Optimization (FPO) algorithm, which reformulates importance sampling by leveraging per-sample changes in the conditional flow-matching objective. Furthermore, FPO achieves stable and scalable online reinforcement fine-tuning of the $π_0$ model by integrating structure-aware credit assignment to enhance gradient efficiency, clipped surrogate objectives to stabilize optimization, multi-step latent exploration to encourage diverse policy updates, and a Q-ensemble mechanism to provide robust value estimation. We evaluate FPO on the LIBERO benchmark and the ALOHA simulation task against supervised, preference-aligned, diffusion-based, autoregressive online RL, and $π_0$-FAST baselines, observing consistent improvements over the imitation prior and strong alternatives with stable learning under sparse rewards. In addition, ablation studies and analyses of the latent space dynamics further highlight the contributions of individual components within FPO, validating the effectiveness of the proposed computational modules and the stable convergence of the conditional flow-matching objective during online RL.
<div id='section'>PaperID: <span id='pid'>440, <a href='https://arxiv.org/pdf/2507.12026.pdf' target='_blank'>https://arxiv.org/pdf/2507.12026.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rongtao Xu, Han Gao, Mingming Yu, Dong An, Shunpeng Chen, Changwei Wang, Li Guo, Xiaodan Liang, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12026">3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the growing need for diverse and scalable data in indoor scene tasks, such as question answering and dense captioning, we propose 3D-MoRe, a novel paradigm designed to generate large-scale 3D-language datasets by leveraging the strengths of foundational models. The framework integrates key components, including multi-modal embedding, cross-modal interaction, and a language model decoder, to process natural language instructions and 3D scene data. This approach facilitates enhanced reasoning and response generation in complex 3D environments. Using the ScanNet 3D scene dataset, along with text annotations from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs and 73,000 object descriptions across 1,513 scenes. We also employ various data augmentation techniques and implement semantic filtering to ensure high-quality data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms state-of-the-art baselines, with the CIDEr score improving by 2.15\%. Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5 by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated datasets will be publicly released to benefit the community, and both can be accessed on the https://3D-MoRe.github.io.
<div id='section'>PaperID: <span id='pid'>441, <a href='https://arxiv.org/pdf/2504.11218.pdf' target='_blank'>https://arxiv.org/pdf/2504.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11218">3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.
<div id='section'>PaperID: <span id='pid'>442, <a href='https://arxiv.org/pdf/2503.11117.pdf' target='_blank'>https://arxiv.org/pdf/2503.11117.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kaixuan Jiang, Yang Liu, Weixing Chen, Jingzhou Luo, Ziliang Chen, Ling Pan, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11117">Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) is a challenging task in embodied intelligence that requires agents to dynamically explore 3D environments, actively gather visual information, and perform multi-step reasoning to answer questions. However, current EQA approaches suffer from critical limitations in exploration efficiency, dataset design, and evaluation metrics. Moreover, existing datasets often introduce biases or prior knowledge, leading to disembodied reasoning, while frontier-based exploration strategies struggle in cluttered environments and fail to ensure fine-grained exploration of task-relevant areas. To address these challenges, we construct the EXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the largest dataset designed specifically to evaluate both exploration and reasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories and 2,044 question-trajectory pairs. To improve exploration efficiency, we propose Fine-EQA, a hybrid exploration model that integrates frontier-based and goal-oriented navigation to guide agents toward task-relevant regions more effectively. Additionally, we introduce a novel evaluation metric, Exploration-Answer Consistency (EAC), which ensures faithful assessment by measuring the alignment between answer grounding and exploration reliability. Extensive experimental comparisons with state-of-the-art EQA models demonstrate the effectiveness of our EXPRESS-Bench in advancing embodied exploration and question reasoning.
<div id='section'>PaperID: <span id='pid'>443, <a href='https://arxiv.org/pdf/2501.18232.pdf' target='_blank'>https://arxiv.org/pdf/2501.18232.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenshuo Chen, Haozhe Jia, Songning Lai, Lei Wang, Yuqi Lin, Hongru Xiao, Lijie Hu, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18232">Free-T2M: Robust Text-to-Motion Generation for Humanoid Robots via Frequency-Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to synthesize complex, physically coherent motions from natural language commands is a cornerstone of autonomous robotics and human-robot interaction. While diffusion models have shown promise in this text-to-motion (T2M) task, they often generate semantically flawed or unstable motions, limiting their applicability to real-world robots. This paper reframes the T2M problem from a frequency-domain perspective, revealing that the generative process mirrors a hierarchical control paradigm. We identify two critical phases: a semantic planning stage, where low-frequency components establish the global motion trajectory, and a fine-grained execution stage, where high-frequency details refine the movement. To address the distinct challenges of each phase, we introduce Frequency enhanced text-to-motion (Free-T2M), a framework incorporating stage-specific frequency-domain consistency alignment. We design a frequency-domain temporal-adaptive module to modulate the alignment effects of different frequency bands. These designs enforce robustness in the foundational semantic plan and enhance the accuracy of detailed execution. Extensive experiments show our method dramatically improves motion quality and semantic correctness. Notably, when applied to the StableMoFusion baseline, Free-T2M reduces the FID from 0.152 to 0.060, establishing a new state-of-the-art within diffusion architectures. These findings underscore the critical role of frequency-domain insights for generating robust and reliable motions, paving the way for more intuitive natural language control of robots.
<div id='section'>PaperID: <span id='pid'>444, <a href='https://arxiv.org/pdf/2512.02834.pdf' target='_blank'>https://arxiv.org/pdf/2512.02834.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyuan Yang, Yang Zhang, Haoran He, Ling Pan, Xiu Li, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02834">Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.
<div id='section'>PaperID: <span id='pid'>445, <a href='https://arxiv.org/pdf/2509.19041.pdf' target='_blank'>https://arxiv.org/pdf/2509.19041.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuning Zhang, Hong Jia, Simin Li, Ting Dang, Yongquan `Owen' Hu, Xin Yi, Hewu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19041">Position: Human-Robot Interaction in Embodied Intelligence Demands a Shift From Static Privacy Controls to Dynamic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reasoning capabilities of embodied agents introduce a critical, under-explored inferential privacy challenge, where the risk of an agent generate sensitive conclusions from ambient data. This capability creates a fundamental tension between an agent's utility and user privacy, rendering traditional static controls ineffective. To address this, this position paper proposes a framework that reframes privacy as a dynamic learning problem grounded in theory of Contextual Integrity (CI). Our approach enables agents to proactively learn and adapt to individual privacy norms through interaction, outlining a research agenda to develop embodied agents that are both capable and function as trustworthy safeguards of user privacy.
<div id='section'>PaperID: <span id='pid'>446, <a href='https://arxiv.org/pdf/2501.09783.pdf' target='_blank'>https://arxiv.org/pdf/2501.09783.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09783">GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GeoManip, a framework to enable generalist robots to leverage essential conditions derived from object and part relationships, as geometric constraints, for robot manipulation. For example, cutting the carrot requires adhering to a geometric constraint: the blade of the knife should be perpendicular to the carrot's direction. By interpreting these constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, enabling greater generalizability across diverse even unseen tasks, objects, and scenarios. Unlike vision-language-action models that require extensive training, operates training-free by utilizing large foundational models: a constraint generation module that predicts stage-specific geometric constraints and a geometry parser that identifies object parts involved in these constraints. A solver then optimizes trajectories to satisfy inferred constraints from task descriptions and the scene. Furthermore, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip's state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.
<div id='section'>PaperID: <span id='pid'>447, <a href='https://arxiv.org/pdf/2510.11760.pdf' target='_blank'>https://arxiv.org/pdf/2510.11760.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Wang, Yinfeng Yu, Fuchun Sun, Liejun Wang, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11760">Audio-Guided Visual Perception for Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.
<div id='section'>PaperID: <span id='pid'>448, <a href='https://arxiv.org/pdf/2509.25652.pdf' target='_blank'>https://arxiv.org/pdf/2509.25652.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hailong Zhang, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25652">Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual navigation represents a significant area of research in which intelligent agents utilize egocentric visual and auditory perceptions to identify audio targets. Conventional navigation methodologies typically adopt a staged modular design, which involves first executing feature fusion, then utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally making decisions through reinforcement learning. While this modular approach has demonstrated effectiveness, it may also lead to redundant information processing and inconsistencies in information transmission between the various modules during the feature fusion and GRU sequence modeling phases. This paper presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for Audiovisual Navigation), an end-to-end framework that integrates multimodal information fusion and sequence modeling within a unified IRCAM module, thereby replacing the traditional separate components for fusion and GRU. This innovative mechanism employs a multi-level residual design that concatenates initial multimodal sequences with processed information sequences. This methodological shift progressively optimizes the feature extraction process while reducing model bias and enhancing the model's stability and generalization capabilities. Empirical results indicate that intelligent agents employing the iterative residual cross-attention mechanism exhibit superior navigation performance.
<div id='section'>PaperID: <span id='pid'>449, <a href='https://arxiv.org/pdf/2509.22698.pdf' target='_blank'>https://arxiv.org/pdf/2509.22698.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hailong Zhang, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22698">Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent agents often require collaborative strategies to achieve complex tasks beyond individual capabilities in real-world scenarios. While existing audio-visual navigation (AVN) research mainly focuses on single-agent systems, their limitations emerge in dynamic 3D environments where rapid multi-agent coordination is critical, especially for time-sensitive applications like emergency response. This paper introduces MASTAVN (Multi-Agent Scalable Transformer Audio-Visual Navigation), a scalable framework enabling two agents to collaboratively localize and navigate toward an audio target in shared 3D environments. By integrating cross-agent communication protocols and joint audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal synchronization. Through rigorous evaluation in photorealistic 3D simulators (Replica and Matterport3D), MASTAVN achieves significant reductions in task completion time and notable improvements in navigation success rates compared to single-agent and non-collaborative baselines. This highlights the essential role of spatiotemporal coordination in multi-agent systems. Our findings validate MASTAVN's effectiveness in time-sensitive emergency scenarios and establish a paradigm for advancing scalable multi-agent embodied intelligence in complex 3D environments.
<div id='section'>PaperID: <span id='pid'>450, <a href='https://arxiv.org/pdf/2509.16924.pdf' target='_blank'>https://arxiv.org/pdf/2509.16924.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jia Li, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16924">Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention \textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.
<div id='section'>PaperID: <span id='pid'>451, <a href='https://arxiv.org/pdf/2412.05789.pdf' target='_blank'>https://arxiv.org/pdf/2412.05789.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05789">InfiniteWorld: A Unified Scalable Simulation Framework for General Visual-Language Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realizing scaling laws in embodied AI has become a focus. However, previous work has been scattered across diverse simulation platforms, with assets and models lacking unified interfaces, which has led to inefficiencies in research. To address this, we introduce InfiniteWorld, a unified and scalable simulator for general vision-language robot interaction built on Nvidia Isaac Sim. InfiniteWorld encompasses a comprehensive set of physics asset construction methods and generalized free robot interaction benchmarks. Specifically, we first built a unified and scalable simulation framework for embodied learning that integrates a series of improvements in generation-driven 3D asset construction, Real2Sim, automated annotation framework, and unified 3D asset processing. This framework provides a unified and scalable platform for robot interaction and learning. In addition, to simulate realistic robot interaction, we build four new general benchmarks, including scene graph collaborative exploration and open-world social mobile manipulation. The former is often overlooked as an important task for robots to explore the environment and build scene knowledge, while the latter simulates robot interaction tasks with different levels of knowledge agents based on the former. They can more comprehensively evaluate the embodied agent's capabilities in environmental understanding, task planning and execution, and intelligent interaction. We hope that this work can provide the community with a systematic asset interface, alleviate the dilemma of the lack of high-quality assets, and provide a more comprehensive evaluation of robot interactions.
<div id='section'>PaperID: <span id='pid'>452, <a href='https://arxiv.org/pdf/2410.11758.pdf' target='_blank'>https://arxiv.org/pdf/2410.11758.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11758">Latent Action Pretraining from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.
<div id='section'>PaperID: <span id='pid'>453, <a href='https://arxiv.org/pdf/2505.03238.pdf' target='_blank'>https://arxiv.org/pdf/2505.03238.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Liam Boyle, Nicolas Baumann, Paviththiren Sivasothilingam, Michele Magno, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03238">RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Future robotic systems operating in real-world environments will require on-board embodied intelligence without continuous cloud connection, balancing capabilities with constraints on computational power and memory. This work presents an extension of the R1-zero approach, which enables the usage of low parameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero approach was originally developed to enable mathematical reasoning in LLMs using static datasets. We extend it to the robotics domain through integration in a closed-loop Reinforcement Learning (RL) framework. This extension enhances reasoning in Embodied Artificial Intelligence (Embodied AI) settings without relying solely on distillation of large models through Supervised Fine-Tuning (SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which enables tasks that previously required significantly larger models. In an autonomous driving setting, a performance gain of 20.2%-points over the SFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3% control adaptability score, surpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These results highlight that practical, on-board deployment of small LLMs is not only feasible but can outperform larger models if trained through environmental feedback, underscoring the importance of an interactive learning framework for robotic Embodied AI, one grounded in practical experience rather than static supervision.
<div id='section'>PaperID: <span id='pid'>454, <a href='https://arxiv.org/pdf/2512.22414.pdf' target='_blank'>https://arxiv.org/pdf/2512.22414.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22414">Emergence of Human to Robot Transfer in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.
<div id='section'>PaperID: <span id='pid'>455, <a href='https://arxiv.org/pdf/2511.22025.pdf' target='_blank'>https://arxiv.org/pdf/2511.22025.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Joel Alberto Santos, Zongwei Wu, Xavier Alameda-Pineda, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22025">Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.
<div id='section'>PaperID: <span id='pid'>456, <a href='https://arxiv.org/pdf/2502.19417.pdf' target='_blank'>https://arxiv.org/pdf/2502.19417.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19417">Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don't like that one") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and incorporate situated feedback during task execution ("that's not trash"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping. Videos are available at https://www.pi.website/research/hirobot
<div id='section'>PaperID: <span id='pid'>457, <a href='https://arxiv.org/pdf/2501.09747.pdf' target='_blank'>https://arxiv.org/pdf/2501.09747.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09747">FAST: Efficient Action Tokenization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.
<div id='section'>PaperID: <span id='pid'>458, <a href='https://arxiv.org/pdf/2501.00358.pdf' target='_blank'>https://arxiv.org/pdf/2501.00358.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yue Fan, Xiaojian Ma, Rongpeng Su, Jun Guo, Rujie Wu, Xi Chen, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00358">Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public.
<div id='section'>PaperID: <span id='pid'>459, <a href='https://arxiv.org/pdf/2511.23300.pdf' target='_blank'>https://arxiv.org/pdf/2511.23300.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yara Mahmoud, Jeffrin Sam, Nguyen Khang, Marcelino Fernando, Issatay Tokmurziyev, Miguel Altamirano Cabrera, Muhammad Haris Khan, Artem Lykov, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23300">SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.
<div id='section'>PaperID: <span id='pid'>460, <a href='https://arxiv.org/pdf/2509.15273.pdf' target='_blank'>https://arxiv.org/pdf/2509.15273.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fei Ni, Min Zhang, Pengyi Li, Yifu Yuan, Lingfeng Zhang, Yuecheng Liu, Peilong Han, Longxin Kou, Shaojin Ma, Jinbin Qiao, David Gamaliel Arcos Bravo, Yuening Wang, Xiao Hu, Zhanguang Zhang, Xianze Yao, Yutong Li, Zhao Zhang, Ying Wen, Ying-Cong Chen, Xiaodan Liang, Liang Lin, Bin He, Haitham Bou-Ammar, He Wang, Huazhe Xu, Jiankang Deng, Shan Luo, Shuqiang Jiang, Wei Pan, Yang Gao, Stefanos Zafeiriou, Jan Peters, Yuzheng Zhuang, Yingxue Zhang, Yan Zheng, Hongyao Tang, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15273">Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI development significantly lags behind large foundation models due to three critical challenges: (1) lack of systematic understanding of core capabilities needed for Embodied AI, making research lack clear objectives; (2) absence of unified and standardized evaluation systems, rendering cross-benchmark evaluation infeasible; and (3) underdeveloped automated and scalable acquisition methods for embodied data, creating critical bottlenecks for model scaling. To address these obstacles, we present Embodied Arena, a comprehensive, unified, and evolving evaluation platform for Embodied AI. Our platform establishes a systematic embodied capability taxonomy spanning three levels (perception, reasoning, task execution), seven core capabilities, and 25 fine-grained dimensions, enabling unified evaluation with systematic research objectives. We introduce a standardized evaluation system built upon unified infrastructure supporting flexible integration of 22 diverse benchmarks across three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced models from 20+ worldwide institutes. Additionally, we develop a novel LLM-driven automated generation pipeline ensuring scalable embodied evaluation data with continuous evolution for diversity and comprehensiveness. Embodied Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task Planning) with dual perspectives (benchmark view and capability view), providing comprehensive overviews of advanced model capabilities. Especially, we present nine findings summarized from the evaluation results on the leaderboards of Embodied Arena. This helps to establish clear research veins and pinpoint critical research problems, thereby driving forward progress in the field of Embodied AI.
<div id='section'>PaperID: <span id='pid'>461, <a href='https://arxiv.org/pdf/2508.13998.pdf' target='_blank'>https://arxiv.org/pdf/2508.13998.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13998">Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.
<div id='section'>PaperID: <span id='pid'>462, <a href='https://arxiv.org/pdf/2508.01361.pdf' target='_blank'>https://arxiv.org/pdf/2508.01361.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luis Francisco Moreno Fuentes, Muhammad Haris Khan, Miguel Altamirano Cabrera, Valerii Serpiva, Dmitri Iarchuk, Yara Mahmoud, Issatay Tokmurziyev, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01361">VLH: Vision-Language-Haptics Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VLH, a novel Visual-Language-Haptic Foundation Model that unifies perception, language, and tactile feedback in aerial robotics and virtual reality. Unlike prior work that treats haptics as a secondary, reactive channel, VLH synthesizes mid-air force and vibration cues as a direct consequence of contextual visual understanding and natural language commands. Our platform comprises an 8-inch quadcopter equipped with dual inverse five-bar linkage arrays for localized haptic actuation, an egocentric VR camera, and an exocentric top-down view. Visual inputs and language instructions are processed by a fine-tuned OpenVLA backbone - adapted via LoRA on a bespoke dataset of 450 multimodal scenarios - to output a 7-dimensional action vector (Vx, Vy, Vz, Hx, Hy, Hz, Hv). INT8 quantization and a high-performance server ensure real-time operation at 4-5 Hz. In human-robot interaction experiments (90 flights), VLH achieved a 56.7% success rate for target acquisition (mean reach time 21.3 s, pose error 0.24 m) and 100% accuracy in texture discrimination. Generalization tests yielded 70.0% (visual), 54.4% (motion), 40.0% (physical), and 35.0% (semantic) performance on novel tasks. These results demonstrate VLH's ability to co-evolve haptic feedback with perceptual reasoning and intent, advancing expressive, immersive human-robot interactions.
<div id='section'>PaperID: <span id='pid'>463, <a href='https://arxiv.org/pdf/2505.08548.pdf' target='_blank'>https://arxiv.org/pdf/2505.08548.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08548">From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both "seeing" and "doing," achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.
<div id='section'>PaperID: <span id='pid'>464, <a href='https://arxiv.org/pdf/2504.13898.pdf' target='_blank'>https://arxiv.org/pdf/2504.13898.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dong Won Lee, Yubin Kim, Denison Guvenoz, Sooyeon Jeong, Parker Malachowsky, Louis-Philippe Morency, Cynthia Breazeal, Hae Won Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13898">Social Human Robot Embodied Conversation (SHREC) Dataset: Benchmarking Foundational Models' Social Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our work focuses on the social reasoning capabilities of foundation models for real-world human-robot interactions. We introduce the Social Human Robot Embodied Conversation (SHREC) Dataset, a benchmark of $\sim$400 real-world human-robot interaction videos and over 10K annotations, capturing robot social errors, competencies, underlying rationales, and corrections. Unlike prior datasets focused on human-human interactions, the SHREC Dataset uniquely highlights the social challenges faced by real-world social robots such as emotion understanding, intention tracking, and conversational mechanics. Moreover, current foundation models struggle to recognize these deficits, which manifest as subtle, socially situated failures. To evaluate AI models' capacity for social reasoning, we define eight benchmark tasks targeting critical areas such as (1) detection of social errors and competencies, (2) identification of underlying social attributes, (3) comprehension of interaction flow, and (4) providing rationale and alternative correct actions. Experiments with state-of-the-art foundation models, alongside human evaluations, reveal substantial performance gaps -- underscoring the difficulty and providing directions in developing socially intelligent AI.
<div id='section'>PaperID: <span id='pid'>465, <a href='https://arxiv.org/pdf/2503.10070.pdf' target='_blank'>https://arxiv.org/pdf/2503.10070.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haiqin Cui, Yifu Yuan, Yan Zheng, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10070">AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation and manipulation in open-world environments remain unsolved challenges in the Embodied AI. The high cost of commercial mobile manipulation robots significantly limits research in real-world scenes. To address this issue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile manipulation robot system with a hardware cost of only $1,000 (excluding optional computational resources), which is less than 1/15 of the cost of popular mobile robots. The AhaRobot system consists of three components: (1) a novel low-cost hardware architecture primarily composed of off-the-shelf components, (2) an optimized control solution to enhance operational precision integrating dual-motor backlash control and static friction compensation, and (3) a simple remote teleoperation method RoboPilot. We use handles to control the dual arms and pedals for whole-body movement. The teleoperation process is low-burden and easy to operate, much like piloting. RoboPilot is designed for remote data collection in embodied scenarios. Experimental results demonstrate that RoboPilot significantly enhances data collection efficiency in complex manipulation tasks, achieving a 30% increase compared to methods using 3D mouse and leader-follower systems. It also excels at completing extremely long-horizon tasks in one go. Furthermore, AhaRobot can be used to learn end-to-end policies and autonomously perform complex manipulation tasks, such as pen insertion and cleaning up the floor. We aim to build an affordable yet powerful platform to promote the development of embodied tasks on real devices, advancing more robust and reliable embodied AI. All hardware and software systems are available at https://aha-robot.github.io.
<div id='section'>PaperID: <span id='pid'>466, <a href='https://arxiv.org/pdf/2502.10040.pdf' target='_blank'>https://arxiv.org/pdf/2502.10040.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shichao Fan, Quantao Yang, Yajie Liu, Kun Wu, Zhengping Che, Qingjie Liu, Min Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10040">Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vision-Language-Action models (VLA) have advanced robot imitation learning, but high data collection costs and limited demonstrations hinder generalization and current imitation learning methods struggle in out-of-distribution scenarios, especially for long-horizon tasks. A key challenge is how to mitigate compounding errors in imitation learning, which lead to cascading failures over extended trajectories. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates 2D trajectories through a diffusion model to guide policy learning for long-horizon tasks. By leveraging task-relevant trajectories, DTP provides trajectory-level guidance to reduce error accumulation. Our two-stage approach first trains a generative vision-language model to create diffusion-based trajectories, then refines the imitation policy using them. Experiments on the CALVIN benchmark show that DTP outperforms state-of-the-art baselines by 25% in success rate, starting from scratch without external pretraining. Moreover, DTP significantly improves real-world robot performance.
<div id='section'>PaperID: <span id='pid'>467, <a href='https://arxiv.org/pdf/2410.07166.pdf' target='_blank'>https://arxiv.org/pdf/2410.07166.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07166">Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.
<div id='section'>PaperID: <span id='pid'>468, <a href='https://arxiv.org/pdf/2601.23065.pdf' target='_blank'>https://arxiv.org/pdf/2601.23065.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xijie Yang, Mulin Yu, Changjian Jiang, Kerui Ren, Tao Lu, Jiangmiao Pang, Dahua Lin, Bo Dai, Linning Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.23065">EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent reconstruction methods based on radiance field such as NeRF and 3DGS reproduce indoor scenes with high visual fidelity, but break down under scene editing due to baked illumination and the lack of explicit light transport. In contrast, physically based inverse rendering relies on mesh representations and path tracing, which enforce correct light transport but place strong requirements on geometric fidelity, becoming a practical bottleneck for real indoor scenes. In this work, we propose Emission-Aware Gaussians and Path Tracing (EAG-PT), aiming for physically based light transport with a unified 2D Gaussian representation. Our design is based on three cores: (1) using 2D Gaussians as a unified scene representation and transport-friendly geometry proxy that avoids reconstructed mesh, (2) explicitly separating emissive and non-emissive components during reconstruction for further scene editing, and (3) decoupling reconstruction from final rendering by using efficient single-bounce optimization and high-quality multi-bounce path tracing after scene editing. Experiments on synthetic and real indoor scenes show that EAG-PT produces more natural and physically consistent renders after editing than radiant scene reconstructions, while preserving finer geometric detail and avoiding mesh-induced artifacts compared to mesh-based inverse path tracing. These results suggest promising directions for future use in interior design, XR content creation, and embodied AI.
<div id='section'>PaperID: <span id='pid'>469, <a href='https://arxiv.org/pdf/2601.18323.pdf' target='_blank'>https://arxiv.org/pdf/2601.18323.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin, Kuangzhi Ge, Kai Tang, Peidong Jia, Shanghang Zhang, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18323">TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.
<div id='section'>PaperID: <span id='pid'>470, <a href='https://arxiv.org/pdf/2506.05341.pdf' target='_blank'>https://arxiv.org/pdf/2506.05341.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xingjian Ran, Yixuan Li, Linning Xu, Mulin Yu, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05341">Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.
<div id='section'>PaperID: <span id='pid'>471, <a href='https://arxiv.org/pdf/2505.23450.pdf' target='_blank'>https://arxiv.org/pdf/2505.23450.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhejian Yang, Yongchao Chen, Xueyang Zhou, Jiangyue Yan, Dingjie Song, Yinuo Liu, Yuting Li, Yu Zhang, Pan Zhou, Hechang Chen, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23450">Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon robotic manipulation poses significant challenges for autonomous systems, requiring extended reasoning, precise execution, and robust error recovery across complex sequential tasks. Current approaches, whether based on static planning or end-to-end visuomotor policies, suffer from error accumulation and lack effective verification mechanisms during execution, limiting their reliability in real-world scenarios. We present Agentic Robot, a brain-inspired framework that addresses these limitations through Standardized Action Procedure (SAP)--a novel coordination protocol governing component interactions throughout manipulation tasks. Drawing inspiration from Standardized Operating Procedures (SOPs) in human organizations, SAP establishes structured workflows for planning, execution, and verification phases. Our architecture comprises three specialized components: (1) a large reasoning model that decomposes high-level instructions into semantically coherent subgoals, (2) a vision-language-action executor that generates continuous control commands from real-time visual inputs, and (3) a temporal verifier that enables autonomous progression and error recovery through introspective assessment. This SAP-driven closed-loop design supports dynamic self-verification without external supervision. On the LIBERO benchmark, Agentic Robot achieves state-of-the-art performance with an average success rate of 79.6%, outperforming SpatialVLA by 6.1% and OpenVLA by 7.4% on long-horizon tasks. These results demonstrate that SAP-driven coordination between specialized components enhances both performance and interpretability in sequential manipulation, suggesting significant potential for reliable autonomous systems. Project Github: https://agentic-robot.github.io.
<div id='section'>PaperID: <span id='pid'>472, <a href='https://arxiv.org/pdf/2505.05108.pdf' target='_blank'>https://arxiv.org/pdf/2505.05108.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhaohan Feng, Ruiqi Xue, Lei Yuan, Yang Yu, Ning Ding, Meiqin Liu, Bingzhao Gao, Jian Sun, Xinhu Zheng, Gang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05108">Multi-agent Embodied AI: Advances and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field.
<div id='section'>PaperID: <span id='pid'>473, <a href='https://arxiv.org/pdf/2502.13175.pdf' target='_blank'>https://arxiv.org/pdf/2502.13175.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenpeng Xing, Minghao Li, Mohan Li, Meng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13175">Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.
<div id='section'>PaperID: <span id='pid'>474, <a href='https://arxiv.org/pdf/2510.00441.pdf' target='_blank'>https://arxiv.org/pdf/2510.00441.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiyuan Pan, Yunzhe Xu, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00441">Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.
<div id='section'>PaperID: <span id='pid'>475, <a href='https://arxiv.org/pdf/2506.21117.pdf' target='_blank'>https://arxiv.org/pdf/2506.21117.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan Ackermann, Jonas Kulhanek, Shengqu Cai, Haofei Xu, Marc Pollefeys, Gordon Wetzstein, Leonidas Guibas, Songyou Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21117">CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.
<div id='section'>PaperID: <span id='pid'>476, <a href='https://arxiv.org/pdf/2411.03487.pdf' target='_blank'>https://arxiv.org/pdf/2411.03487.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yichen Wang, Qiming Liu, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03487">Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of visual navigation in unknown scenes, both "exploration" and "exploitation" are equally crucial. Robots must first establish environmental cognition through exploration and then utilize the cognitive information to accomplish target searches. However, most existing methods for image-goal navigation prioritize target search over the generation of exploratory behavior. To address this, we propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, which uses an implicit and compact scene representation, NeRF, as a cognitive structure. We estimate the uncertainty of NeRF and augment the exploratory ability by the uncertainty to in turn facilitate the construction of implicit representation. Simultaneously, we extract memory information from NeRF to enhance the robot's reasoning ability for determining the location of the target. Ultimately, we seamlessly combine the two generated abilities to produce navigational actions. Our pipeline is end-to-end, with the environmental cognitive structure being constructed online. Extensive experimental results on image-goal navigation demonstrate the capability of our pipeline to enhance exploratory behaviors, while also enabling a natural transition from the exploration to exploitation phase. This enables our model to outperform existing memory-based cognitive navigation structures in terms of navigation performance.
<div id='section'>PaperID: <span id='pid'>477, <a href='https://arxiv.org/pdf/2601.02125.pdf' target='_blank'>https://arxiv.org/pdf/2601.02125.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuoxiong Xu, Xuanchen Li, Yuhao Cheng, Fei Xu, Yichao Yan, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02125">SingingBot: An Avatar-Driven System for Robotic Face Singing Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.
<div id='section'>PaperID: <span id='pid'>478, <a href='https://arxiv.org/pdf/2510.08316.pdf' target='_blank'>https://arxiv.org/pdf/2510.08316.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08316">Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance segmentation aims to parse 3D objects into functionally distinct parts, bridging recognition and interaction for applications in robotic manipulation, embodied AI, and AR. While recent studies leverage visual or textual prompts to guide this process, they often rely on point cloud encoders as generic feature extractors, overlooking the intrinsic challenges of 3D data such as sparsity, noise, and geometric ambiguity. As a result, 3D features learned in isolation frequently lack clear and semantically consistent functional boundaries. To address this bottleneck, we propose a semantic-grounded learning paradigm that transfers rich semantic knowledge from large-scale 2D Vision Foundation Models (VFMs) into the 3D domain. Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training strategy that aligns a 3D encoder with lifted 2D semantics and jointly optimizes reconstruction, affinity, and diversity to yield semantically organized representations. Building on this backbone, we further design the Cross-modal Affordance Segmentation Transformer (CAST), which integrates multi-modal prompts with CMAT-pretrained features to generate precise, prompt-aware segmentation maps. Extensive experiments on standard benchmarks demonstrate that our framework establishes new state-of-the-art results for 3D affordance segmentation.
<div id='section'>PaperID: <span id='pid'>479, <a href='https://arxiv.org/pdf/2509.09332.pdf' target='_blank'>https://arxiv.org/pdf/2509.09332.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09332">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io
<div id='section'>PaperID: <span id='pid'>480, <a href='https://arxiv.org/pdf/2506.19769.pdf' target='_blank'>https://arxiv.org/pdf/2506.19769.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shulan Ruan, Rongwei Wang, Xuchen Shen, Huijie Liu, Baihui Xiao, Jun Shi, Kun Zhang, Zhenya Huang, Yu Liu, Enhong Chen, You He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19769">A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.
<div id='section'>PaperID: <span id='pid'>481, <a href='https://arxiv.org/pdf/2505.16517.pdf' target='_blank'>https://arxiv.org/pdf/2505.16517.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, Zhongzhi Li, Rui Yan, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16517">ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated training datasets, which limits their generalization and causes them to struggle in out-of-domain (OOD) scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.
<div id='section'>PaperID: <span id='pid'>482, <a href='https://arxiv.org/pdf/2502.14254.pdf' target='_blank'>https://arxiv.org/pdf/2502.14254.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, Haoping Xu, Guowei Huang, Zhanpeng Zhang, Tongtong Cao, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang, Yingxue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14254">Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.
<div id='section'>PaperID: <span id='pid'>483, <a href='https://arxiv.org/pdf/2501.10074.pdf' target='_blank'>https://arxiv.org/pdf/2501.10074.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Helong Huang, Guangjian Tian, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10074">SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning is an essential problem in embodied AI research. Efforts to enhance spatial reasoning abilities through supplementary spatial data and fine-tuning have proven limited and ineffective when addressing complex embodied tasks, largely due to their dependence on language-based outputs. While some approaches have introduced a point-based action space to mitigate this issue, they fall short in managing more intricate tasks within complex environments. This deficiency arises from their failure to fully exploit the inherent thinking and reasoning capabilities that are fundamental strengths of Vision-Language Models (VLMs). To address these limitations, we propose a novel approach named SpatialCoT, specifically designed to bolster the spatial reasoning capabilities of VLMs. Our approach comprises two stages: spatial coordinate bi-directional alignment, which aligns vision-language inputs with spatial coordinates, and chain-of-thought spatial grounding, which harnesses the reasoning capabilities of language models for advanced spatial reasoning. We evaluate SpatialCoT on challenging navigation and manipulation tasks, both in simulation and real-world settings. Experimental results demonstrate that our method significantly outperforms previous state-of-the-art approaches in both tasks.
<div id='section'>PaperID: <span id='pid'>484, <a href='https://arxiv.org/pdf/2410.14682.pdf' target='_blank'>https://arxiv.org/pdf/2410.14682.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingfeng Zhang, Yuening Wang, Hongjian Gu, Atia Hamidizadeh, Zhanguang Zhang, Yuecheng Liu, Yutong Wang, David Gamaliel Arcos Bravo, Junyi Dong, Shunbo Zhou, Tongtong Cao, Xingyue Quan, Yuzheng Zhuang, Yingxue Zhang, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14682">ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards Spatial-Temporal Cognition with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) have spurred numerous attempts to apply these technologies to embodied tasks, particularly focusing on high-level task planning and task decomposition. To further explore this area, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which specifically targets embodied task planning using LLMs. It features a controllable and diverse set of embodied tasks varying in different levels of difficulties and complexities, and is designed to evaluate two critical dimensions of LLMs' application in embodied task understanding: spatial (relation constraint, occlusion for target objects) and temporal & causal understanding of the sequence of actions in the environment. By using multi-source simulators as the backend simulator, it can provide immediate environment feedback to LLMs, which enables LLMs to interact dynamically with the environment and re-plan as necessary. We evaluated the state-of-the-art open source and closed source foundation models, including GPT-4, LLAMA and Mistral on our proposed benchmark. While they perform adequately well on simple navigation tasks, their performance can significantly deteriorate when faced with tasks that require a deeper understanding of spatial, temporal, and causal relationships. Thus, our benchmark distinguishes itself as a large-scale, quantifiable, highly automated, and fine-grained diagnostic framework that presents a significant challenge to the latest foundation models. We hope it can spark and drive further research in embodied task planning using foundation models.
<div id='section'>PaperID: <span id='pid'>485, <a href='https://arxiv.org/pdf/2602.17345.pdf' target='_blank'>https://arxiv.org/pdf/2602.17345.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Boyang Ma, Hechuan Guo, Peizhuo Lv, Minghui Xu, Xuelong Dai, YeChao Zhang, Yijun Yang, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.17345">What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.
<div id='section'>PaperID: <span id='pid'>486, <a href='https://arxiv.org/pdf/2601.04052.pdf' target='_blank'>https://arxiv.org/pdf/2601.04052.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhihao Zhan, Yuhao Chen, Jiaying Zhou, Qinhan Lv, Hao Liu, Keze Wang, Liang Lin, Guangrun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04052">Stable Language Guidance for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.
<div id='section'>PaperID: <span id='pid'>487, <a href='https://arxiv.org/pdf/2511.18929.pdf' target='_blank'>https://arxiv.org/pdf/2511.18929.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zijian Song, Xiaoxin Lin, Tao Pu, Zhenlong Yuan, Guangrun Wang, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18929">Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.
<div id='section'>PaperID: <span id='pid'>488, <a href='https://arxiv.org/pdf/2507.06404.pdf' target='_blank'>https://arxiv.org/pdf/2507.06404.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matteo Tiezzi, Tommaso Apicella, Carlos Cardenas-Perez, Giovanni Fregonese, Stefano Dafarra, Pietro Morerio, Daniele Pucci, Alessio Del Bue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06404">Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating and comparing the performance of autonomous Humanoid Robots is challenging, as success rate metrics are difficult to reproduce and fail to capture the complexity of robot movement trajectories, critical in Human-Robot Interaction and Collaboration (HRIC). To address these challenges, we propose a general evaluation framework that measures the quality of Imitation Learning (IL) methods by focusing on trajectory performance. We devise the Neural Meta Evaluator (NeME), a deep learning model trained to classify actions from robot joint trajectories. NeME serves as a meta-evaluator to compare the performance of robot control policies, enabling policy evaluation without requiring human involvement in the loop. We validate our framework on ergoCub, a humanoid robot, using teleoperation data and comparing IL methods tailored to the available platform. The experimental results indicate that our method is more aligned with the success rate obtained on the robot than baselines, offering a reproducible, systematic, and insightful means for comparing the performance of multimodal imitation learning approaches in complex HRI tasks.
<div id='section'>PaperID: <span id='pid'>489, <a href='https://arxiv.org/pdf/2507.01925.pdf' target='_blank'>https://arxiv.org/pdf/2507.01925.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01925">A Survey on Vision-Language-Action Models: An Action Tokenization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.
<div id='section'>PaperID: <span id='pid'>490, <a href='https://arxiv.org/pdf/2410.14993.pdf' target='_blank'>https://arxiv.org/pdf/2410.14993.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Wu, Donglin Bai, Shiqi Jiang, Qianxi Zhang, Yifan Yang, Xin Ding, Ting Cao, Yunxin Liu, Fengyuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14993">Making Every Frame Matter: Continuous Activity Recognition in Streaming Video via Adaptive Video Context Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video activity recognition has become increasingly important in robots and embodied AI. Recognizing continuous video activities poses considerable challenges due to the fast expansion of streaming video, which contains multi-scale and untrimmed activities. We introduce a novel system, CARS, to overcome these issues through adaptive video context modeling. Adaptive video context modeling refers to selectively maintaining activity-related features in temporal and spatial dimensions. CARS has two key designs. The first is an activity spatial feature extraction by eliminating irrelevant visual features while maintaining recognition accuracy. The second is an activity-aware state update introducing dynamic adaptability to better preserve the video context for multi-scale activity recognition. Our CARS runs at speeds $>$30 FPS on typical edge devices and outperforms all baselines by 1.2\% to 79.7\% in accuracy. Moreover, we explore applying CARS to a large video model as a video encoder. Experimental results show that our CARS can result in a 0.46-point enhancement (on a 5-point scale) on the in-distribution video activity dataset, and an improvement ranging from 1.19\% to 4\% on zero-shot video activity datasets.
<div id='section'>PaperID: <span id='pid'>491, <a href='https://arxiv.org/pdf/2512.19021.pdf' target='_blank'>https://arxiv.org/pdf/2512.19021.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19021">VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting "ghost" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.
<div id='section'>PaperID: <span id='pid'>492, <a href='https://arxiv.org/pdf/2505.10415.pdf' target='_blank'>https://arxiv.org/pdf/2505.10415.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuebo Ji, Zherong Pan, Xifeng Gao, Lei Yang, Xinxin Du, Kaiyun Li, Yongjin Liu, Wenping Wang, Changhe Tu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10415">Internal State Estimation in Groups via Active Information Gathering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating human internal states, such as personality traits or behavioral patterns, is critical for enhancing the effectiveness of human-robot interaction, particularly in group settings. These insights are key in applications ranging from social navigation to autism diagnosis. However, prior methods are limited by scalability and passive observation, making real-time estimation in complex, multi-human settings difficult. In this work, we propose a practical method for active human personality estimation in groups, with a focus on applications related to Autism Spectrum Disorder (ASD). Our method combines a personality-conditioned behavior model, based on the Eysenck 3-Factor theory, with an active robot information gathering policy that triggers human behaviors through a receding-horizon planner. The robot's belief about human personality is then updated via Bayesian inference. We demonstrate the effectiveness of our approach through simulations, user studies with typical adults, and preliminary experiments involving participants with ASD. Our results show that our method can scale to tens of humans and reduce personality prediction error by 29.2% and uncertainty by 79.9% in simulation. User studies with typical adults confirm the method's ability to generalize across complex personality distributions. Additionally, we explore its application in autism-related scenarios, demonstrating that the method can identify the difference between neurotypical and autistic behavior, highlighting its potential for diagnosing ASD. The results suggest that our framework could serve as a foundation for future ASD-specific interventions.
<div id='section'>PaperID: <span id='pid'>493, <a href='https://arxiv.org/pdf/2503.10480.pdf' target='_blank'>https://arxiv.org/pdf/2503.10480.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10480">World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.
<div id='section'>PaperID: <span id='pid'>494, <a href='https://arxiv.org/pdf/2503.05231.pdf' target='_blank'>https://arxiv.org/pdf/2503.05231.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuo Jiang, Haonan Li, Ruochen Ren, Yanmin Zhou, Zhipeng Wang, Bin He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05231">Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cutting-edge robot learning techniques including foundation models and imitation learning from humans all pose huge demands on large-scale and high-quality datasets which constitute one of the bottleneck in the general intelligent robot fields. This paper presents the Kaiwu multimodal dataset to address the missing real-world synchronized multimodal data problems in the sophisticated assembling scenario,especially with dynamics information and its fine-grained labelling. The dataset first provides an integration of human,environment and robot data collection framework with 20 subjects and 30 interaction objects resulting in totally 11,664 instances of integrated actions. For each of the demonstration,hand motions,operation pressures,sounds of the assembling process,multi-view videos, high-precision motion capture information,eye gaze with first-person videos,electromyography signals are all recorded. Fine-grained multi-level annotation based on absolute timestamp,and semantic segmentation labelling are performed. Kaiwu dataset aims to facilitate robot learning,dexterous manipulation,human intention investigation and human-robot collaboration research.
<div id='section'>PaperID: <span id='pid'>495, <a href='https://arxiv.org/pdf/2512.15773.pdf' target='_blank'>https://arxiv.org/pdf/2512.15773.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ye Li, Jiahe Feng, Yuan Meng, Kangye Ji, Chen Tang, Xinwan Wen, Shutao Xia, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15773">TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.
<div id='section'>PaperID: <span id='pid'>496, <a href='https://arxiv.org/pdf/2512.01629.pdf' target='_blank'>https://arxiv.org/pdf/2512.01629.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yumeng He, Ying Jiang, Jiayin Lu, Yin Yang, Chenfanfu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01629">SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling. Project page: https://heyumeng.com/SPARK/index.html.
<div id='section'>PaperID: <span id='pid'>497, <a href='https://arxiv.org/pdf/2512.00287.pdf' target='_blank'>https://arxiv.org/pdf/2512.00287.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuzheng Gao, Yuxing Long, Lei Kang, Yuchong Guo, Ziyan Yu, Shangqing Mao, Jiyao Zhang, Ruihai Wu, Dongjiang Li, Hui Shen, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00287">RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing appliance assets suffer from poor rendering, incomplete mechanisms, and misalignment with manuals, leading to simulation-reality gaps that hinder appliance manipulation development. In this work, we introduce the RealAppliance dataset, comprising 100 high-fidelity appliances with complete physical, electronic mechanisms, and program logic aligned with their manuals. Based on these assets, we propose the RealAppliance-Bench benchmark, which evaluates multimodal large language models and embodied manipulation planning models across key tasks in appliance manipulation planning: manual page retrieval, appliance part grounding, open-loop manipulation planning, and closed-loop planning adjustment. Our analysis of model performances on RealAppliance-Bench provides insights for advancing appliance manipulation research
<div id='section'>PaperID: <span id='pid'>498, <a href='https://arxiv.org/pdf/2510.27607.pdf' target='_blank'>https://arxiv.org/pdf/2510.27607.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27607">Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.
<div id='section'>PaperID: <span id='pid'>499, <a href='https://arxiv.org/pdf/2510.01711.pdf' target='_blank'>https://arxiv.org/pdf/2510.01711.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Taeyoung Kim, Jimin Lee, Myungkyu Koo, Dongyoung Kim, Kyungmin Lee, Changyeon Kim, Younggyo Seo, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01711">Contrastive Representation Regularization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs). However, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states. To address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals. In particular, RS-CL aligns the representations more closely with the robot's proprioceptive states, by using relative distances between the states as soft supervision. Complementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline. Our empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models; it pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing, and boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.
<div id='section'>PaperID: <span id='pid'>500, <a href='https://arxiv.org/pdf/2505.11409.pdf' target='_blank'>https://arxiv.org/pdf/2505.11409.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan VuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11409">Visual Planning: Let's Think Only with Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.
<div id='section'>PaperID: <span id='pid'>501, <a href='https://arxiv.org/pdf/2501.16698.pdf' target='_blank'>https://arxiv.org/pdf/2501.16698.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yueen Ma, Yuzheng Zhuang, Jianye Hao, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16698">3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose Diffusion via Rectified Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D vision and spatial reasoning have long been recognized as preferable for accurately perceiving our three-dimensional world, especially when compared with traditional visual reasoning based on 2D images. Due to the difficulties in collecting high-quality 3D data, research in this area has only recently gained momentum. With the advent of powerful large language models (LLMs), multi-modal LLMs for 3D vision have been developed over the past few years. However, most of these models focus primarily on the vision encoder for 3D data. In this paper, we propose converting existing densely activated LLMs into mixture-of-experts (MoE) models, which have proven effective for multi-modal data processing. In addition to leveraging these models' instruction-following capabilities, we further enable embodied task planning by attaching a diffusion head, Pose-DiT, that employs a novel rectified flow diffusion scheduler. Experimental results on 3D question answering and task-planning tasks demonstrate that our 3D-MoE framework achieves improved performance with fewer activated parameters.
<div id='section'>PaperID: <span id='pid'>502, <a href='https://arxiv.org/pdf/2501.07295.pdf' target='_blank'>https://arxiv.org/pdf/2501.07295.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Oleg Kobzarev, Artem Lykov, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07295">GestLLM: Advanced Hand Gesture Interpretation via Large Language Models for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces GestLLM, an advanced system for human-robot interaction that enables intuitive robot control through hand gestures. Unlike conventional systems, which rely on a limited set of predefined gestures, GestLLM leverages large language models and feature extraction via MediaPipe to interpret a diverse range of gestures. This integration addresses key limitations in existing systems, such as restricted gesture flexibility and the inability to recognize complex or unconventional gestures commonly used in human communication.
  By combining state-of-the-art feature extraction and language model capabilities, GestLLM achieves performance comparable to leading vision-language models while supporting gestures underrepresented in traditional datasets. For example, this includes gestures from popular culture, such as the ``Vulcan salute" from Star Trek, without any additional pretraining, prompt engineering, etc. This flexibility enhances the naturalness and inclusivity of robot control, making interactions more intuitive and user-friendly.
  GestLLM provides a significant step forward in gesture-based interaction, enabling robots to understand and respond to a wide variety of hand gestures effectively. This paper outlines its design, implementation, and evaluation, demonstrating its potential applications in advanced human-robot collaboration, assistive robotics, and interactive entertainment.
<div id='section'>PaperID: <span id='pid'>503, <a href='https://arxiv.org/pdf/2411.17347.pdf' target='_blank'>https://arxiv.org/pdf/2411.17347.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Filippo Ansalone, Flavio Maiorana, Daniele Affinita, Flavio Volpi, Eugenio Bugli, Francesco Petri, Michele Brienza, Valerio Spagnoli, Vincenzo Suriani, Daniele Nardi, Domenico D. Bloisi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17347">Real-Time Multimodal Signal Processing for HRI in RoboCup: Understanding a Human Referee</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancing human-robot communication is crucial for autonomous systems operating in dynamic environments, where accurate real-time interpretation of human signals is essential. RoboCup provides a compelling scenario for testing these capabilities, requiring robots to understand referee gestures and whistle with minimal network reliance. Using the NAO robot platform, this study implements a two-stage pipeline for gesture recognition through keypoint extraction and classification, alongside continuous convolutional neural networks (CCNNs) for efficient whistle detection. The proposed approach enhances real-time human-robot interaction in a competitive setting like RoboCup, offering some tools to advance the development of autonomous systems capable of cooperating with humans.
<div id='section'>PaperID: <span id='pid'>504, <a href='https://arxiv.org/pdf/2410.09054.pdf' target='_blank'>https://arxiv.org/pdf/2410.09054.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Viviane Potocnik, Alfio Di Mauro, Lorenzo Lamberti, Victor Kartsch, Moritz Scherer, Francesco Conti, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09054">Circuits and Systems for Embodied AI: Exploring uJ Multi-Modal Perception for Nano-UAVs on the Kraken Shield</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied artificial intelligence (AI) requires pushing complex multi-modal models to the extreme edge for time-constrained tasks such as autonomous navigation of robots and vehicles. On small form-factor devices, e.g., nano-sized unmanned aerial vehicles (UAVs), such challenges are exacerbated by stringent constraints on energy efficiency and weight. In this paper, we explore embodied multi-modal AI-based perception for Nano-UAVs with the Kraken shield, a 7g multi-sensor (frame-based and event-based imagers) board based on Kraken, a 22 nm SoC featuring multiple acceleration engines for multi-modal event and frame-based inference based on spiking (SNN) and ternary (TNN) neural networks, respectively. Kraken can execute SNN real-time inference for depth estimation at 1.02k inf/s, 18 Î¼J/inf, TNN real-time inference for object classification at 10k inf/s, 6 Î¼J/inf, and real-time inference for obstacle avoidance at 221 frame/s, 750 Î¼J/inf.
<div id='section'>PaperID: <span id='pid'>505, <a href='https://arxiv.org/pdf/2409.12514.pdf' target='_blank'>https://arxiv.org/pdf/2409.12514.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12514">TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.
<div id='section'>PaperID: <span id='pid'>506, <a href='https://arxiv.org/pdf/2512.18933.pdf' target='_blank'>https://arxiv.org/pdf/2512.18933.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18933">Point What You Mean: Visually Grounded Instruction Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.
<div id='section'>PaperID: <span id='pid'>507, <a href='https://arxiv.org/pdf/2510.11308.pdf' target='_blank'>https://arxiv.org/pdf/2510.11308.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weixi Situ, Hanjing Ye, Jianwei Peng, Yu Zhan, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11308">Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot person following (RPF) is a core capability in human-robot interaction, enabling robots to assist users in daily activities, collaborative work, and other service scenarios. However, achieving practical RPF remains challenging due to frequent occlusions, particularly in dynamic and crowded environments. Existing approaches often rely on fixed-point following or sparse candidate-point selection with oversimplified heuristics, which cannot adequately handle complex occlusions caused by moving obstacles such as pedestrians. To address these limitations, we propose an adaptive trajectory sampling method that generates dense candidate points within socially aware zones and evaluates them using a multi-objective cost function. Based on the optimal point, a person-following trajectory is estimated relative to the predicted motion of the target. We further design a prediction-aware model predictive path integral (MPPI) controller that simultaneously tracks this trajectory and proactively avoids collisions using predicted pedestrian motions. Extensive experiments show that our method outperforms state-of-the-art baselines in smoothness, safety, robustness, and human comfort, with its effectiveness further demonstrated on a mobile robot in real-world scenarios.
<div id='section'>PaperID: <span id='pid'>508, <a href='https://arxiv.org/pdf/2509.20109.pdf' target='_blank'>https://arxiv.org/pdf/2509.20109.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pengxiang Li, Yinan Zheng, Yue Wang, Huimin Wang, Hang Zhao, Jingjing Liu, Xianyuan Zhan, Kun Zhan, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20109">Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.
<div id='section'>PaperID: <span id='pid'>509, <a href='https://arxiv.org/pdf/2506.16211.pdf' target='_blank'>https://arxiv.org/pdf/2506.16211.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Puhao Li, Yingying Wu, Ziheng Xi, Wanlin Li, Yuzhe Huang, Zhiyuan Zhang, Yinghan Chen, Jianan Wang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16211">ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies. In real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7% success rate while requiring only 10-20 demonstrations -- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. Additional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.
<div id='section'>PaperID: <span id='pid'>510, <a href='https://arxiv.org/pdf/2505.11917.pdf' target='_blank'>https://arxiv.org/pdf/2505.11917.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11917">OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose robots capable of performing diverse tasks require synergistic reasoning and acting capabilities. However, recent dual-system approaches, which separate high-level reasoning from low-level acting, often suffer from challenges such as limited mutual understanding of capabilities between systems and latency issues. This paper introduces OneTwoVLA, a single unified vision-language-action model that can perform both acting (System One) and reasoning (System Two). Crucially, OneTwoVLA adaptively switches between two modes: explicitly reasoning at critical moments during task execution, and generating actions based on the most recent reasoning at other times. To further unlock OneTwoVLA's reasoning and generalization capabilities, we design a scalable pipeline for synthesizing embodied reasoning-centric vision-language data, used for co-training with robot data. We validate OneTwoVLA's effectiveness through extensive experiments, highlighting its superior performance across four key capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding, enabling the model to perform long-horizon, highly dexterous manipulation tasks such as making hotpot or mixing cocktails.
<div id='section'>PaperID: <span id='pid'>511, <a href='https://arxiv.org/pdf/2504.03602.pdf' target='_blank'>https://arxiv.org/pdf/2504.03602.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kai Lascheit, Daniel Barath, Marc Pollefeys, Leonidas Guibas, Francis Engelmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03602">Robust Human Registration with Body Part Segmentation on Noisy Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Registering human meshes to 3D point clouds is essential for applications such as augmented reality and human-robot interaction but often yields imprecise results due to noise and background clutter in real-world data. We introduce a hybrid approach that incorporates body-part segmentation into the mesh fitting process, enhancing both human pose estimation and segmentation accuracy. Our method first assigns body part labels to individual points, which then guide a two-step SMPL-X fitting: initial pose and orientation estimation using body part centroids, followed by global refinement of the point cloud alignment. Additionally, we demonstrate that the fitted human mesh can refine body part labels, leading to improved segmentation. Evaluations on the cluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that our approach significantly outperforms prior methods in both pose estimation and segmentation accuracy. Code and results are available on our project website: https://segfit.github.io
<div id='section'>PaperID: <span id='pid'>512, <a href='https://arxiv.org/pdf/2503.21860.pdf' target='_blank'>https://arxiv.org/pdf/2503.21860.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kailin Li, Puhao Li, Tengyu Liu, Yuyang Li, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21860">ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.
<div id='section'>PaperID: <span id='pid'>513, <a href='https://arxiv.org/pdf/2503.02916.pdf' target='_blank'>https://arxiv.org/pdf/2503.02916.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Zhan, Hanjing Ye, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02916">Monocular Person Localization under Camera Ego-motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localizing a person from a moving monocular camera is critical for Human-Robot Interaction (HRI). To estimate the 3D human position from a 2D image, existing methods either depend on the geometric assumption of a fixed camera or use a position regression model trained on datasets containing little camera ego-motion. These methods are vulnerable to fierce camera ego-motion, resulting in inaccurate person localization. We consider person localization as a part of a pose estimation problem. By representing a human with a four-point model, our method jointly estimates the 2D camera attitude and the person's 3D location through optimization. Evaluations on both public datasets and real robot experiments demonstrate our method outperforms baselines in person localization accuracy. Our method is further implemented into a person-following system and deployed on an agile quadruped robot.
<div id='section'>PaperID: <span id='pid'>514, <a href='https://arxiv.org/pdf/2512.21970.pdf' target='_blank'>https://arxiv.org/pdf/2512.21970.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21970">StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.
<div id='section'>PaperID: <span id='pid'>515, <a href='https://arxiv.org/pdf/2512.04446.pdf' target='_blank'>https://arxiv.org/pdf/2512.04446.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chang Liu, Sibo Tian, Sara Behdad, Xiao Liang, Minghui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04446">Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating disassembly of critical components from end-of-life (EoL) desktops, such as high-value items like RAM modules and CPUs, as well as sensitive parts like hard disk drives, remains challenging due to the inherent variability and uncertainty of these products. Moreover, their disassembly requires sequential, precise, and dexterous operations, further increasing the complexity of automation. Current robotic disassembly processes are typically divided into several stages: perception, sequence planning, task planning, motion planning, and manipulation. Each stage requires explicit modeling, which limits generalization to unfamiliar scenarios. Recent development of vision-language-action (VLA) models has presented an end-to-end approach for general robotic manipulation tasks. Although VLAs have demonstrated promising performance on simple tasks, the feasibility of applying such models to complex disassembly remains largely unexplored. In this paper, we collected a customized dataset for robotic RAM and CPU disassembly and used it to fine-tune two well-established VLA approaches, OpenVLA and OpenVLA-OFT, as a case study. We divided the whole disassembly task into several small steps, and our preliminary experimental results indicate that the fine-tuned VLA models can faithfully complete multiple early steps but struggle with certain critical subtasks, leading to task failure. However, we observed that a simple hybrid strategy that combines VLA with a rule-based controller can successfully perform the entire disassembly operation. These findings highlight the current limitations of VLA models in handling the dexterity and precision required for robotic EoL product disassembly. By offering a detailed analysis of the observed results, this study provides insights that may inform future research to address current challenges and advance end-to-end robotic automated disassembly.
<div id='section'>PaperID: <span id='pid'>516, <a href='https://arxiv.org/pdf/2511.22697.pdf' target='_blank'>https://arxiv.org/pdf/2511.22697.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chancharik Mitra, Yusen Luo, Raj Saravanan, Dantong Niu, Anirudh Pai, Jesse Thomason, Trevor Darrell, Abrar Anwar, Deva Ramanan, Roei Herzig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22697">Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.
<div id='section'>PaperID: <span id='pid'>517, <a href='https://arxiv.org/pdf/2511.16651.pdf' target='_blank'>https://arxiv.org/pdf/2511.16651.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Tian, Yuyin Yang, Yiman Xie, Zetao Cai, Xu Shi, Ning Gao, Hangxu Liu, Xuekun Jiang, Zherui Qiu, Feng Yuan, Yaping Li, Ping Wang, Junhao Cai, Jia Zeng, Hao Dong, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16651">InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.
<div id='section'>PaperID: <span id='pid'>518, <a href='https://arxiv.org/pdf/2510.08811.pdf' target='_blank'>https://arxiv.org/pdf/2510.08811.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiurun Song, Xiao Liang, Minghui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08811">Adaptive Motion Planning via Contact-Based Intent Inference for Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot collaboration (HRC) requires robots to adapt their motions to human intent to ensure safe and efficient cooperation in shared spaces. Although large language models (LLMs) provide high-level reasoning for inferring human intent, their application to reliable motion planning in HRC remains challenging. Physical human-robot interaction (pHRI) is intuitive but often relies on continuous kinesthetic guidance, which imposes burdens on operators. To address these challenges, a contact-informed adaptive motion-planning framework is introduced to infer human intent directly from physical contact and employ the inferred intent for online motion correction in HRC. First, an optimization-based force estimation method is proposed to infer human-intended contact forces and locations from joint torque measurements and a robot dynamics model, thereby reducing cost and installation complexity while enabling whole-body sensitivity. Then, a torque-based contact detection mechanism with link-level localization is introduced to reduce the optimization search space and to enable real-time estimation. Subsequently, a contact-informed adaptive motion planner is developed to infer human intent from contacts and to replan robot motion online, while maintaining smoothness and adapting to human corrections. Finally, experiments on a 7-DOF manipulator are conducted to demonstrate the accuracy of the proposed force estimation method and the effectiveness of the contact-informed adaptive motion planner under perception uncertainty in HRC.
<div id='section'>PaperID: <span id='pid'>519, <a href='https://arxiv.org/pdf/2509.16853.pdf' target='_blank'>https://arxiv.org/pdf/2509.16853.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinhao Wang, Nam Ling, Wei Wang, Wei Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16853">ISCS: Parameter-Guided Feature Pruning for Resource-Constrained Embodied Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior studies in embodied AI consistently show that robust perception is critical for human-robot interaction, yet deploying high-fidelity visual models on resource-constrained agents remains challenging due to limited on-device computation power and transmission latency. Exploiting the redundancy in latent representations could improve system efficiency, yet existing approaches often rely on costly dataset-specific ablation tests or heavy entropy models unsuitable for real-time edge-robot collaboration. We propose a generalizable, dataset-agnostic method to identify and selectively transmit structure-critical channels in pretrained encoders. Instead of brute-force empirical evaluations, our approach leverages intrinsic parameter statistics-weight variances and biases-to estimate channel importance. This analysis reveals a consistent organizational structure, termed the Invariant Salient Channel Space (ISCS), where Salient-Core channels capture dominant structures while Salient-Auxiliary channels encode fine visual details. Building on ISCS, we introduce a deterministic static pruning strategy that enables lightweight split-computing. Experiments across different datasets demonstrate that our method achieves a deterministic, ultra-low latency pipeline by bypassing heavy entropy modeling. Our method reduces end-to-end latency, providing a critical speed-accuracy trade-off for resource-constrained human-aware embodied systems.
<div id='section'>PaperID: <span id='pid'>520, <a href='https://arxiv.org/pdf/2508.16292.pdf' target='_blank'>https://arxiv.org/pdf/2508.16292.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wen-Han Hsieh, Elvis Hsieh, Dantong Niu, Trevor Darrell, Roei Herzig, David M. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16292">Do What? Teaching Vision-Language-Action Models to Reject the Impossible</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.
<div id='section'>PaperID: <span id='pid'>521, <a href='https://arxiv.org/pdf/2506.10357.pdf' target='_blank'>https://arxiv.org/pdf/2506.10357.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Weili Guan, Dongmei Jiang, Yaowei Wang, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10357">Optimus-3: Dual-Router Aligned Mixture-of-Experts Agent with Dual-Granularity Reasoning-Aware Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing generalist agents capable of solving open-ended tasks in visually rich, dynamic environments remains a core pursuit of embodied AI. While Minecraft has emerged as a compelling benchmark, existing agents often suffer from fragmented cognitive abilities, lacking the synergy between reflexive execution (System 1) and deliberative reasoning (System 2). In this paper, we introduce Optimus-3, a generalist agent that organically integrates these dual capabilities within a unified framework. To achieve this, we address three fundamental challenges. First, to overcome the scarcity of reasoning data, we propose a Knowledge-Enhanced Automated Data Generation Pipeline. It synthesizes high-quality System 2 reasoning traces from raw System 1 interaction trajectories, effectively mitigating hallucinations via injection of domain knowledge. We release the resulting dataset, \textbf{OptimusM$^{4}$}, to the community. Second, to reconcile the dichotomous computational requirements of the dual systems, we design a Dual-Router Aligned MoE Architecture. It employs a Task Router to prevent task interference via parameter decoupling, and a Layer Router to dynamically modulate reasoning depth, creating a computational ``Fast Path'' for System 1 and a ``Deep Path'' for System 2. Third, to activate the reasoning capabilities of System 2, we propose Dual-Granularity Reasoning-Aware Policy Optimization (DGRPO) algorithm. It enforces Process-Outcome Co-Supervision via dual-granularity dense rewards, ensuring consistency between the thought process and the answer. Extensive evaluations demonstrate that Optimus-3 surpasses existing state-of-the-art methods on both System~2 (21$\%$ on Planning, 66\% on Captioning, 76\% on Embodied QA, 3.4$\times$ on Grounding, and 18\% on Reflection) and System~1 (3\% on Long-Horizon Action) tasks, with a notable 60\% success rate on open-ended tasks.
<div id='section'>PaperID: <span id='pid'>522, <a href='https://arxiv.org/pdf/2505.03233.pdf' target='_blank'>https://arxiv.org/pdf/2505.03233.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, Heming Cui, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03233">GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.
<div id='section'>PaperID: <span id='pid'>523, <a href='https://arxiv.org/pdf/2503.08367.pdf' target='_blank'>https://arxiv.org/pdf/2503.08367.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Runling Long, Yunlong Wang, Jia Wan, Xiang Deng, Xinting Zhu, Weili Guan, Antoni B. Chan, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08367">Embodied Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occlusion is one of the fundamental challenges in crowd counting. In the community, various data-driven approaches have been developed to address this issue, yet their effectiveness is limited. This is mainly because most existing crowd counting datasets on which the methods are trained are based on passive cameras, restricting their ability to fully sense the environment. Recently, embodied navigation methods have shown significant potential in precise object detection in interactive scenes. These methods incorporate active camera settings, holding promise in addressing the fundamental issues in crowd counting. However, most existing methods are designed for indoor navigation, showing unknown performance in analyzing complex object distribution in large scale scenes, such as crowds. Besides, most existing embodied navigation datasets are indoor scenes with limited scale and object quantity, preventing them from being introduced into dense crowd analysis. Based on this, a novel task, Embodied Crowd Counting (ECC), is proposed. We first build up an interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables large scale scenes and large object quantity. A prior probability distribution that approximates realistic crowd distribution is introduced to generate crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method contains a MLLM driven coarse-to-fine navigation mechanism, enabling active Z-axis exploration, and a normal-line-based crowd distribution analysis method for fine counting. Experimental results against baselines show that the proposed method achieves the best trade-off between counting accuracy and navigation cost.
<div id='section'>PaperID: <span id='pid'>524, <a href='https://arxiv.org/pdf/2502.17894.pdf' target='_blank'>https://arxiv.org/pdf/2502.17894.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiheng Liu, Yuxuan Wan, Jilong Wang, Yuxuan Kuang, Wenbo Cui, Xuesong Shi, Haoran Li, Dongbin Zhao, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17894">FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable object fetching in cluttered scenes remains a fundamental and application-critical challenge in embodied AI. Closely packed objects cause inevitable occlusions, making safe action generation particularly difficult. Under such partial observability, effective policies must not only generalize across diverse objects and layouts but also reason about occlusion to avoid collisions. However, collecting large-scale real-world data for this task remains prohibitively expensive, leaving this problem largely unsolved. In this paper, we introduce FetchBot, a sim-to-real framework for this challenge. We first curate a large-scale synthetic dataset featuring 1M diverse scenes and 500k representative demonstrations. Based on this dataset, FetchBot employs a depth-conditioned method for action generation, which leverages structural cues to enable robust obstacle-aware action planning. However, depth is perfect in simulation but noisy in real-world environments. To address this sim-to-real gap, FetchBot predicts depth from RGB inputs using a foundation model and integrates local occupancy prediction as a pre-training task, providing a generalizable latent representation for sim-to-real transfer. Extensive experiments in simulation and real-world environments demonstrate the strong zero-shot sim-to-real transfer, effective clutter handling, and adaptability to novel scenarios. In cluttered environments, it achieves an average real-world success rate of 89.95%, significantly outperforming prior methods. Moreover, FetchBot demonstrates excellent robustness in challenging cases, such as fetching transparent, reflective, and irregular objects, highlighting its practical value.
<div id='section'>PaperID: <span id='pid'>525, <a href='https://arxiv.org/pdf/2410.16922.pdf' target='_blank'>https://arxiv.org/pdf/2410.16922.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mengxin Xu, Weiwei Wan, Hesheng Wang, Kensuke Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16922">Direction-Constrained Control for Efficient Physical Human-Robot Interaction under Hierarchical Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a control method to address the physical Human-Robot Interaction (pHRI) challenge in the context of hierarchical tasks. A common approach to managing hierarchical tasks is Hierarchical Quadratic Programming (HQP), which, however, cannot be directly applied to human interaction due to its allowance of arbitrary velocity direction adjustments. To resolve this limitation, we introduce the concept of directional constraints and develop a direction-constrained optimization algorithm to handle the nonlinearities induced by these constraints. The algorithm solves two sub-problems, minimizing the error and minimizing the deviation angle, in parallel, and combines the results of the two sub-problems to produce a final optimal outcome. The mutual influence between these two sub-problems is analyzed to determine the best parameter for combination. Additionally, the velocity objective in our control framework is computed using a variable admittance controller. Traditional admittance control does not account for constraints. To address this issue, we propose a variable admittance control method to adjust control objectives dynamically. The method helps reduce the deviation between robot velocity and human intention at the constraint boundaries, thereby enhancing interaction efficiency. We evaluate the proposed method in scenarios where a human operator physically interacts with a 7-degree-of-freedom robotic arm. The results highlight the importance of incorporating directional constraints in pHRI for hierarchical tasks. Compared to existing methods, our approach generates smoother robotic trajectories during interaction while avoiding interaction delays at the constraint boundaries.
<div id='section'>PaperID: <span id='pid'>526, <a href='https://arxiv.org/pdf/2410.01481.pdf' target='_blank'>https://arxiv.org/pdf/2410.01481.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kai Li, Wendi Sang, Chang Zeng, Runxuan Yang, Guo Chen, Xiaolin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01481">SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systematic evaluation of speech separation and enhancement models under moving sound source conditions requires extensive and diverse data. However, real-world datasets often lack sufficient data for training and evaluation, and synthetic datasets, while larger, lack acoustic realism. Consequently, neither effectively meets practical needs. To address this issue, we introduce SonicSim, a synthetic toolkit based on the embodied AI simulation platform Habitat-sim, designed to generate highly customizable data for moving sound sources. SonicSim supports multi-level adjustments, including scene-level, microphone-level, and source-level adjustments, enabling the creation of more diverse synthetic data. Leveraging SonicSim, we constructed a benchmark dataset called SonicSet, utilizing LibriSpeech, Freesound Dataset 50k (FSD50K), Free Music Archive (FMA), and 90 scenes from Matterport3D to evaluate speech separation and enhancement models. Additionally, to investigate the differences between synthetic and real-world data, we selected 5 hours of raw, non-reverberant data from the SonicSet validation set and recorded a real-world speech separation dataset, providing a reference for comparing SonicSet with other synthetic datasets. For speech enhancement, we utilized the real-world dataset RealMAN to validate the acoustic gap between SonicSet and existing synthetic datasets. The results indicate that models trained on SonicSet generalize better to real-world scenarios compared to other synthetic datasets. The code is publicly available at https://cslikai.cn/SonicSim/.
<div id='section'>PaperID: <span id='pid'>527, <a href='https://arxiv.org/pdf/2512.21243.pdf' target='_blank'>https://arxiv.org/pdf/2512.21243.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21243">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .
<div id='section'>PaperID: <span id='pid'>528, <a href='https://arxiv.org/pdf/2512.13609.pdf' target='_blank'>https://arxiv.org/pdf/2512.13609.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shweta Mahajan, Shreya Kadambi, Hoang Le, Munawar Hayat, Fatih Porikli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13609">Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.
<div id='section'>PaperID: <span id='pid'>529, <a href='https://arxiv.org/pdf/2510.00181.pdf' target='_blank'>https://arxiv.org/pdf/2510.00181.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luis Burbano, Diego Ortiz, Qi Sun, Siwei Yang, Haoqin Tu, Cihang Xie, Yinzhi Cao, Alvaro A Cardenas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00181">CHAI: Command Hijacking against embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.
<div id='section'>PaperID: <span id='pid'>530, <a href='https://arxiv.org/pdf/2509.09090.pdf' target='_blank'>https://arxiv.org/pdf/2509.09090.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hengyu Fang, Yijiang Liu, Yuan Du, Li Du, Huanrui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09090">SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct quantization or token pruning in an ad-hoc manner but fail to enable both for a holistic efficiency improvement due to an observed incompatibility. This work introduces SQAP-VLA, the first structured, training-free VLA inference acceleration framework that simultaneously enables state-of-the-art quantization and token pruning. We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on an aggressively quantized model while improving the quantizer design to enhance pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields significant gains in computational efficiency and inference speed while successfully preserving core model performance, achieving a $\times$1.93 speedup and up to a 4.5\% average success rate enhancement compared to the original model.
<div id='section'>PaperID: <span id='pid'>531, <a href='https://arxiv.org/pdf/2508.15663.pdf' target='_blank'>https://arxiv.org/pdf/2508.15663.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15663">Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks are crucial for evaluating progress in robotics and embodied AI. However, a significant gap exists between benchmarks designed for high-level language instruction following, which often assume perfect low-level execution, and those for low-level robot control, which rely on simple, one-step commands. This disconnect prevents a comprehensive evaluation of integrated systems where both task planning and physical execution are critical. To address this, we propose Kitchen-R, a novel benchmark that unifies the evaluation of task planning and low-level control within a simulated kitchen environment. Built as a digital twin using the Isaac Sim simulator and featuring more than 500 complex language instructions, Kitchen-R supports a mobile manipulator robot. We provide baseline methods for our benchmark, including a task-planning strategy based on a vision-language model and a low-level control policy based on diffusion policy. We also provide a trajectory collection system. Our benchmark offers a flexible framework for three evaluation modes: independent assessment of the planning module, independent assessment of the control policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R bridges a key gap in embodied AI research, enabling more holistic and realistic benchmarking of language-guided robotic agents.
<div id='section'>PaperID: <span id='pid'>532, <a href='https://arxiv.org/pdf/2506.19850.pdf' target='_blank'>https://arxiv.org/pdf/2506.19850.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19850">Unified Vision-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.
<div id='section'>PaperID: <span id='pid'>533, <a href='https://arxiv.org/pdf/2503.09956.pdf' target='_blank'>https://arxiv.org/pdf/2503.09956.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Qiao, Phuong-Nam Tran, Ji Su Yoon, Loc X. Nguyen, Eui-Nam Huh, Dusit Niyato, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09956">DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless Networks: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL)-based large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, have attracted widespread attention for their remarkable capabilities in multimodal data understanding. Meanwhile, the rapid expansion of information services has led to a growing demand for AI-enabled wireless networks. The open-source DeepSeek models are famous for their innovative designs, such as large-scale pure RL and cost-efficient training, which make them well-suited for practical deployment in wireless networks. By integrating DeepSeek-style LLMs with wireless infrastructures, a synergistic opportunity arises: the DeepSeek-style LLMs enhance network optimization with strong reasoning and decision-making abilities, while wireless infrastructure enables the broad deployment of these models. Motivated by this convergence, this survey presents a comprehensive DeepSeek-inspired exploration of RL-based LLMs in the context of wireless networks. We begin by reviewing key techniques behind network optimization to establish a foundation for understanding DeepSeek-style LLM integration. Next, we examine recent advancements in RL-based LLMs, using DeepSeek models as a representative example. Building on this, we explore the synergy between the two domains, highlighting motivations, challenges, and potential solutions. Finally, we highlight emerging directions for integrating LLMs with wireless networks, such as quantum, on-device, and neural-symbolic LLM models, as well as embodied AI agents. Overall, this survey offers a comprehensive examination of the interplay between DeepSeek-style LLMs and wireless networks, demonstrating how these domains can mutually enhance each other to drive innovation.
<div id='section'>PaperID: <span id='pid'>534, <a href='https://arxiv.org/pdf/2501.03841.pdf' target='_blank'>https://arxiv.org/pdf/2501.03841.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03841">OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.
<div id='section'>PaperID: <span id='pid'>535, <a href='https://arxiv.org/pdf/2602.07845.pdf' target='_blank'>https://arxiv.org/pdf/2602.07845.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yalcin Tur, Jalal Naghiyev, Haoquan Fang, Wei-Chuan Tsai, Jiafei Duan, Dieter Fox, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07845">Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/
<div id='section'>PaperID: <span id='pid'>536, <a href='https://arxiv.org/pdf/2602.04441.pdf' target='_blank'>https://arxiv.org/pdf/2602.04441.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang, Kaizhu Huang, Ning Gao, Peizhou Cao, Mingze Sun, Mulin Yu, Tao Lu, Linning Xu, Junting Dong, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04441">SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.
<div id='section'>PaperID: <span id='pid'>537, <a href='https://arxiv.org/pdf/2601.22714.pdf' target='_blank'>https://arxiv.org/pdf/2601.22714.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alexander Nikulin, Ilya Zisman, Albina Klepach, Denis Tarasov, Alexander Derevyagin, Andrei Polubarov, Lyubaykin Nikita, Vladislav Kurenkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.22714">Vision-Language Models Unlock Task-Centric Latent Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.
<div id='section'>PaperID: <span id='pid'>538, <a href='https://arxiv.org/pdf/2601.06748.pdf' target='_blank'>https://arxiv.org/pdf/2601.06748.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Changyu Liu, Yiyang Liu, Taowen Wang, Qiao Zhuang, James Chenhao Liang, Wenhao Yang, Renjing Xu, Qifan Wang, Dongfang Liu, Cheng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06748">On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic actions. Though popular, they are primarily trained via supervised fine-tuning or training-time reinforcement learning, requiring explicit fine-tuning phases, human interventions, or controlled data collection. Consequently, existing methods remain unsuitable for challenging simulated- or physical-world deployments, where robots must respond autonomously and flexibly to evolving environments. To address this limitation, we introduce a Test-Time Reinforcement Learning for VLAs (TT-VLA), a framework that enables on-the-fly policy adaptation during inference. TT-VLA formulates a dense reward mechanism that leverages step-by-step task-progress signals to refine action policies during test time while preserving the SFT/RL-trained priors, making it an effective supplement to current VLA models. Empirical results show that our approach enhances overall adaptability, stability, and task success in dynamic, previously unseen scenarios under simulated and real-world settings. We believe TT-VLA offers a principled step toward self-improving, deployment-ready VLAs.
<div id='section'>PaperID: <span id='pid'>539, <a href='https://arxiv.org/pdf/2512.09607.pdf' target='_blank'>https://arxiv.org/pdf/2512.09607.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09607">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.
<div id='section'>PaperID: <span id='pid'>540, <a href='https://arxiv.org/pdf/2510.13626.pdf' target='_blank'>https://arxiv.org/pdf/2510.13626.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13626">LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.
<div id='section'>PaperID: <span id='pid'>541, <a href='https://arxiv.org/pdf/2510.07778.pdf' target='_blank'>https://arxiv.org/pdf/2510.07778.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07778">IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $π_0$, achieving 18\% higher success rates with direct instructions and 28\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.
<div id='section'>PaperID: <span id='pid'>542, <a href='https://arxiv.org/pdf/2509.06932.pdf' target='_blank'>https://arxiv.org/pdf/2509.06932.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, Xiaoyan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06932">LLaDA-VLA: Vision Language Diffusion Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.
<div id='section'>PaperID: <span id='pid'>543, <a href='https://arxiv.org/pdf/2506.23127.pdf' target='_blank'>https://arxiv.org/pdf/2506.23127.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhaoye Fei, Li Ji, Siyin Wang, Junhao Shi, Jingjing Gong, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23127">Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they face significant challenges in embodied task planning scenarios that require continuous environmental understanding and action generation. Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments. We introduce Embodied Planner-R1, a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. Our framework incorporates three key innovations: (1) Without human annotations, we employ pure reinforcement learning with group rollout, incorporating in-environment interaction through parallel exploration; (2) completion-driven sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories. Across two challenging text-based Embodied planning benchmarks, Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments, evidencing strong generalization.
<div id='section'>PaperID: <span id='pid'>544, <a href='https://arxiv.org/pdf/2506.13456.pdf' target='_blank'>https://arxiv.org/pdf/2506.13456.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kangye Ji, Yuan Meng, Hanyun Cui, Ye Li, Shengjia Hua, Lei Chen, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13456">Block-wise Adaptive Caching for Accelerating Diffusion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Policy has demonstrated strong visuomotor modeling capabilities, but its high computational cost renders it impractical for real-time robotic control. Despite huge redundancy across repetitive denoising steps, existing diffusion acceleration techniques fail to generalize to Diffusion Policy due to fundamental architectural and data divergences. In this paper, we propose Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by caching intermediate action features. BAC achieves lossless action generation acceleration by adaptively updating and reusing cached features at the block level, based on a key observation that feature similarities vary non-uniformly across timesteps and locks. To operationalize this insight, we first propose the Adaptive Caching Scheduler, designed to identify optimal update timesteps by maximizing the global feature similarities between cached and skipped features. However, applying this scheduler for each block leads to signiffcant error surges due to the inter-block propagation of caching errors, particularly within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop the Bubbling Union Algorithm, which truncates these errors by updating the upstream blocks with signiffcant caching errors before downstream FFNs. As a training-free plugin, BAC is readily integrable with existing transformer-based Diffusion Policy and vision-language-action models. Extensive experiments on multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference speedup for free.
<div id='section'>PaperID: <span id='pid'>545, <a href='https://arxiv.org/pdf/2506.09839.pdf' target='_blank'>https://arxiv.org/pdf/2506.09839.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chen Gao, Liankai Jin, Xingyu Peng, Jiazhao Zhang, Yue Deng, Annan Li, He Wang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09839">OctoNav: Towards Generalist Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.
<div id='section'>PaperID: <span id='pid'>546, <a href='https://arxiv.org/pdf/2502.09680.pdf' target='_blank'>https://arxiv.org/pdf/2502.09680.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Albina Klepach, Alexander Nikulin, Ilya Zisman, Denis Tarasov, Alexander Derevyagin, Andrei Polubarov, Nikita Lyubaykin, Vladislav Kurenkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09680">Object-Centric Latent Action Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging vast amounts of unlabeled internet video data for embodied AI is currently bottlenecked by the lack of action labels and the presence of action-correlated visual distractors. Although recent latent action policy optimization (LAPO) has shown promise in inferring proxy-action labels from visual observations, its performance degrades significantly when distractors are present. To address this limitation, we propose a novel object-centric latent action learning framework that centers on objects rather than pixels. We leverage self-supervised object-centric pretraining to disentangle action-related and distracting dynamics. This allows LAPO to focus on task-relevant interactions, resulting in more robust proxy-action labels, enabling better imitation learning and efficient adaptation of the agent with just a few action-labeled trajectories. We evaluated our method in eight visually complex tasks across the Distracting Control Suite (DCS) and Distracting MetaWorld (DMW). Our results show that object-centric pretraining mitigates the negative effects of distractors by 50%, as measured by downstream task performance: average return (DCS) and success rate (DMW).
<div id='section'>PaperID: <span id='pid'>547, <a href='https://arxiv.org/pdf/2412.09624.pdf' target='_blank'>https://arxiv.org/pdf/2412.09624.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09624">GenEx: Generating an Explorable World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.
<div id='section'>PaperID: <span id='pid'>548, <a href='https://arxiv.org/pdf/2411.11844.pdf' target='_blank'>https://arxiv.org/pdf/2411.11844.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11844">Generative World Explorer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can $\textit{imagine}$ unseen parts of the world through a mental exploration and $\textit{revise}$ their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the $\textit{Generative World Explorer (Genex)}$, an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) $\textit{Genex}$ can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.
<div id='section'>PaperID: <span id='pid'>549, <a href='https://arxiv.org/pdf/2410.13825.pdf' target='_blank'>https://arxiv.org/pdf/2410.13825.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13825">AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent's observation/action representation and the pre-training data of the LLM it's based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.
<div id='section'>PaperID: <span id='pid'>550, <a href='https://arxiv.org/pdf/2602.04635.pdf' target='_blank'>https://arxiv.org/pdf/2602.04635.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Julia Kuhn, Francesco Verdoja, Tsvetomila Mihaylova, Ville Kyrki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04635">Relational Scene Graphs for Object Grounding of Natural Language Commands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.
<div id='section'>PaperID: <span id='pid'>551, <a href='https://arxiv.org/pdf/2601.18100.pdf' target='_blank'>https://arxiv.org/pdf/2601.18100.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18100">Spatial-Conditioned Reasoning in Long-Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.
<div id='section'>PaperID: <span id='pid'>552, <a href='https://arxiv.org/pdf/2503.19516.pdf' target='_blank'>https://arxiv.org/pdf/2503.19516.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Liming Zheng, Feng Yan, Fanfan Liu, Chengjian Feng, Yufeng Zhong, Lin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19516">Boosting Robotic Manipulation Generalization with Minimal Costly Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing adoption of Vision-Language-Action (VLA) models in embodied AI intensifies the demand for diverse manipulation demonstrations. However, high costs associated with data collection often result in insufficient data coverage across all scenarios, which limits the performance of the models. It is observed that the spatial reasoning phase (SRP) in large workspace dominates the failure cases. Fortunately, this data can be collected with low cost, underscoring the potential of leveraging inexpensive data to improve model performance. In this paper, we introduce the RoboTron-Craft, a stage-divided and cost-effective pipeline for realistic manipulation generation. Base on this, the RoboTron-Platter method is introduced, a framework that decouples training trajectories into distinct task stages and leverages abundant easily collectible SRP data to enhance VLA model's generalization. Through analysis we demonstrate that sub-task-specific training with additional SRP data with proper proportion can act as a performance catalyst for robot manipulation, maximizing the utilization of costly physical interaction phase (PIP) data. Experiments show that through introducing large proportion of cost-effective SRP trajectories into a limited set of PIP data, we can achieve a maximum improvement of 41\% on success rate in zero-shot scenes, while with the ability to transfer manipulation skill to novel targets. Project available at https://github.com/ notFoundThisPerson/RoboTron-Craft.
<div id='section'>PaperID: <span id='pid'>553, <a href='https://arxiv.org/pdf/2503.06241.pdf' target='_blank'>https://arxiv.org/pdf/2503.06241.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Koji Inoue, Yuki Okafuji, Jun Baba, Yoshiki Ohira, Katsuya Hyodo, Tatsuya Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06241">A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.
<div id='section'>PaperID: <span id='pid'>554, <a href='https://arxiv.org/pdf/2412.09867.pdf' target='_blank'>https://arxiv.org/pdf/2412.09867.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zi Haur Pang, Yahui Fu, Divesh Lala, Mikey Elmers, Koji Inoue, Tatsuya Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09867">Human-Like Embodied AI Interviewer: Employing Android ERICA in Real International Conference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the human-like embodied AI interviewer which integrates android robots equipped with advanced conversational capabilities, including attentive listening, conversational repairs, and user fluency adaptation. Moreover, it can analyze and present results post-interview. We conducted a real-world case study at SIGDIAL 2024 with 42 participants, of whom 69% reported positive experiences. This study demonstrated the system's effectiveness in conducting interviews just like a human and marked the first employment of such a system at an international conference. The demonstration video is available at https://youtu.be/jCuw9g99KuE.
<div id='section'>PaperID: <span id='pid'>555, <a href='https://arxiv.org/pdf/2602.09657.pdf' target='_blank'>https://arxiv.org/pdf/2602.09657.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaolou Sun, Wufei Si, Wenhui Ni, Yuntian Li, Dongming Wu, Fei Xie, Runwei Guan, He-Yang Xu, Henghui Ding, Yuan Wu, Yutao Yue, Yongming Huang, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.09657">AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.
<div id='section'>PaperID: <span id='pid'>556, <a href='https://arxiv.org/pdf/2601.11404.pdf' target='_blank'>https://arxiv.org/pdf/2601.11404.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, Guanghui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11404">ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.
<div id='section'>PaperID: <span id='pid'>557, <a href='https://arxiv.org/pdf/2511.19528.pdf' target='_blank'>https://arxiv.org/pdf/2511.19528.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rushuai Yang, Zhiyuan Feng, Tianxiang Zhang, Kaixin Wang, Chuheng Zhang, Li Zhao, Xiu Su, Yi Chen, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19528">Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.
<div id='section'>PaperID: <span id='pid'>558, <a href='https://arxiv.org/pdf/2511.06619.pdf' target='_blank'>https://arxiv.org/pdf/2511.06619.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chuheng Zhang, Rushuai Yang, Xiaoyu Chen, Kaixin Wang, Li Zhao, Yi Chen, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06619">How Do VLAs Effectively Inherit from VLMs?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.
<div id='section'>PaperID: <span id='pid'>559, <a href='https://arxiv.org/pdf/2511.03370.pdf' target='_blank'>https://arxiv.org/pdf/2511.03370.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunbo Long, Yuhan Liu, Alexandra Brintrup
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03370">EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of large language models (LLMs) in automated negotiation has set a high performance benchmark, but their computational cost and data privacy requirements render them unsuitable for many privacy-sensitive, on-device applications such as mobile assistants, embodied AI agents or private client interactions. While small language models (SLMs) offer a practical alternative, they suffer from a significant performance gap compared to LLMs in playing emotionally charged complex personas, especially for credit negotiation. This paper introduces EQ-Negotiator, a novel framework that bridges this capability gap using emotional personas. Its core is a reasoning system that integrates game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional states online, without pre-training. This allows EQ-Negotiator to equip SLMs with the strategic intelligence to counter manipulation while de-escalating conflict and upholding ethical standards. Through extensive agent-to-agent simulations across diverse credit negotiation scenarios, including adversarial debtor strategies like cheating, threatening, and playing the victim, we show that a 7B parameter language model with EQ-Negotiator achieves better debt recovery and negotiation efficiency than baseline LLMs more than 10 times its size. This work advances persona modeling from descriptive character profiles to dynamic emotional architectures that operate within privacy constraints. Besides, this paper establishes that strategic emotional intelligence, not raw model scale, is the critical factor for success in automated negotiation, paving the way for effective, ethical, and privacy-preserving AI negotiators that can operate on the edge.
<div id='section'>PaperID: <span id='pid'>560, <a href='https://arxiv.org/pdf/2510.17640.pdf' target='_blank'>https://arxiv.org/pdf/2510.17640.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuquan Xue, Guanxing Lu, Zhenyu Wu, Chuanrui Zhang, Bofang Jia, Zhengyi Gu, Yansong Tang, Ziwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17640">RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.
<div id='section'>PaperID: <span id='pid'>561, <a href='https://arxiv.org/pdf/2509.22643.pdf' target='_blank'>https://arxiv.org/pdf/2509.22643.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22643">VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.
<div id='section'>PaperID: <span id='pid'>562, <a href='https://arxiv.org/pdf/2509.15155.pdf' target='_blank'>https://arxiv.org/pdf/2509.15155.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15155">Self-Improving Embodied Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at https://self-improving-efms.github.io .
<div id='section'>PaperID: <span id='pid'>563, <a href='https://arxiv.org/pdf/2507.23682.pdf' target='_blank'>https://arxiv.org/pdf/2507.23682.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, Jianyu Chen, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23682">villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.
<div id='section'>PaperID: <span id='pid'>564, <a href='https://arxiv.org/pdf/2507.17220.pdf' target='_blank'>https://arxiv.org/pdf/2507.17220.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiansong Wan, Chengming Zhou, Jinkua Liu, Xiangge Huang, Xiaoyu Chen, Xiaohan Yi, Qisen Yang, Baiting Zhu, Xin-Qiang Cai, Lixing Liu, Rushuai Yang, Chuheng Zhang, Sherif Abdelfattah, Hayong Shin, Pushi Zhang, Li Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17220">PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have explored pretrained (foundation) models for vision-based robotic navigation, aiming to achieve generalizable navigation and positive transfer across diverse environments while enhancing zero-shot performance in unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal Navigation), a new approach that further investigates pretraining strategies for vision-based navigation models and contributes in two key areas. Model-wise, we identify two critical design choices that consistently improve the performance of pretrained navigation models: (1) integrating an early-fusion network structure to combine visual observations and goal images via appropriately pretrained Vision Transformer (ViT) image encoder, and (2) introducing suitable auxiliary tasks to enhance global navigation representation learning, thus further improving navigation performance. Dataset-wise, we propose a novel data preprocessing pipeline for efficiently labeling large-scale game video datasets for navigation model training. We demonstrate that augmenting existing open navigation datasets with diverse gameplay videos improves model performance. Our model achieves an average improvement of 22.6% in zero-shot settings and a 37.5% improvement in fine-tuning settings over existing visual navigation foundation models in two complex simulated environments and one real-world environment. These results advance the state-of-the-art in pretrained image-goal navigation models. Notably, our model maintains competitive performance while requiring significantly less fine-tuning data, highlighting its potential for real-world deployment with minimal labeled supervision.
<div id='section'>PaperID: <span id='pid'>565, <a href='https://arxiv.org/pdf/2505.14235.pdf' target='_blank'>https://arxiv.org/pdf/2505.14235.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yequan Wang, Aixin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14235">Toward Embodied AGI: A Review of Embodied AI and the Road Ahead</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial General Intelligence (AGI) is often envisioned as inherently embodied. With recent advances in robotics and foundational AI models, we stand at the threshold of a new era-one marked by increasingly generalized embodied AI systems. This paper contributes to the discourse by introducing a systematic taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing research and challenges at the foundational stages (L1-L2) and outline the key components required to achieve higher-level capabilities (L3-L5). Building on these insights and existing technologies, we propose a conceptual framework for an L3+ robotic brain, offering both a technical outlook and a foundation for future exploration.
<div id='section'>PaperID: <span id='pid'>566, <a href='https://arxiv.org/pdf/2503.07511.pdf' target='_blank'>https://arxiv.org/pdf/2503.07511.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07511">PointVLA: Injecting the 3D World into Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.
  Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments.
<div id='section'>PaperID: <span id='pid'>567, <a href='https://arxiv.org/pdf/2503.01058.pdf' target='_blank'>https://arxiv.org/pdf/2503.01058.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuo Chen, Ni Ou, Xuyang Zhang, Zhiyuan Wu, Yongqiang Zhao, Yupeng Wang, Nathan Lepora, Lorenzo Jamone, Jiankang Deng, Shan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01058">General Force Sensation for Tactile Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic tactile sensors, including vision-based and taxel-based sensors, enable agile manipulation and safe human-robot interaction through force sensation. However, variations in structural configurations, measured signals, and material properties create domain gaps that limit the transferability of learned force sensation across different tactile sensors. Here, we introduce GenForce, a general framework for achieving transferable force sensation across both homogeneous and heterogeneous tactile sensors in robotic systems. By unifying tactile signals into marker-based binary tactile images, GenForce enables the transfer of existing force labels to arbitrary target sensors using a marker-to-marker translation technique with a few paired data. This process equips uncalibrated tactile sensors with force prediction capabilities through spatiotemporal force prediction models trained on the transferred data. Extensive experimental results validate GenForce's generalizability, accuracy, and robustness across sensors with diverse marker patterns, structural designs, material properties, and sensing principles. The framework significantly reduces the need for costly and labor-intensive labeled data collection, enabling the rapid deployment of multiple tactile sensors on robotic hands requiring force sensing capabilities.
<div id='section'>PaperID: <span id='pid'>568, <a href='https://arxiv.org/pdf/2502.14420.pdf' target='_blank'>https://arxiv.org/pdf/2502.14420.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14420">ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.
<div id='section'>PaperID: <span id='pid'>569, <a href='https://arxiv.org/pdf/2412.20451.pdf' target='_blank'>https://arxiv.org/pdf/2412.20451.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinming Li, Yichen Zhu, Zhibin Tang, Junjie Wen, Minjie Zhu, Xiaoyu Liu, Chengmeng Li, Ran Cheng, Yaxin Peng, Yan Peng, Feifei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20451">CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot foundation models, particularly Vision-Language-Action (VLA) models, have garnered significant attention for their ability to enhance robot policy learning, greatly improving robot's generalization and robustness. OpenAI's recent model, O1, showcased impressive capabilities in solving complex problems by utilizing extensive reasoning chains. This prompts an important question: can robot models achieve better performance in multi-task , complex environments by reviewing prior observations and then providing task-specific reasoning to guide action prediction? In this paper, we introduce Chain-of-Affordance (CoA-VLA) , a novel approach to scaling robot models by incorporating reasoning in the format of sequential robot affordances to facilitate task completion. Specifically, we prompt the model to consider the following four types of affordances before taking action: (1) object affordance - what object to manipulate and where it is ; (2) grasp affordance - the specific object part to grasp ; (3) spatial affordance - the optimal space to place the object ; and (4) movement affordance-the collision - free path for movement. We further transform each affordance into two prompting formats: visual affordance and textual affordance. We introduce a novel vision-language co-injection module that integrates this knowledge into the policy network. This allows the robot to leverage essential contextual information during action inference, resulting in improved precision and robustness. Our experiments demonstrate that CoA-VLA outperforms state-of-the-art robot foundation models, including OpenVLA and Octo, on a variety of tasks. Furthermore, CoA-VLA exhibits strong generalization capabilities, including recognizing unseen object poses, identifying free space, and avoiding obstacles in novel environments.
<div id='section'>PaperID: <span id='pid'>570, <a href='https://arxiv.org/pdf/2411.00785.pdf' target='_blank'>https://arxiv.org/pdf/2411.00785.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00785">IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.
<div id='section'>PaperID: <span id='pid'>571, <a href='https://arxiv.org/pdf/2410.08901.pdf' target='_blank'>https://arxiv.org/pdf/2410.08901.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haosheng Li, Weixin Mao, Weipeng Deng, Chenyu Meng, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Hongan Wang, Xiaoming Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08901">SegGrasp: Zero-Shot Task-Oriented Grasping via Semantic and Geometric Guided Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping, which involves grasping specific parts of objects based on their functions, is crucial for developing advanced robotic systems capable of performing complex tasks in dynamic environments. In this paper, we propose a training-free framework that incorporates both semantic and geometric priors for zero-shot task-oriented grasp generation. The proposed framework, SegGrasp, first leverages the vision-language models like GLIP for coarse segmentation. It then uses detailed geometric information from convex decomposition to improve segmentation quality through a fusion policy named GeoFusion. An effective grasp pose can be generated by a grasping network with improved segmentation. We conducted the experiments on both segmentation benchmark and real-world robot grasping. The experimental results show that SegGrasp surpasses the baseline by more than 15\% in grasp and segmentation performance.
<div id='section'>PaperID: <span id='pid'>572, <a href='https://arxiv.org/pdf/2602.12636.pdf' target='_blank'>https://arxiv.org/pdf/2602.12636.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Liu, Yixuan Li, Yuhui Chen, Yuxing Qin, Haoran Li, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.12636">Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.
<div id='section'>PaperID: <span id='pid'>573, <a href='https://arxiv.org/pdf/2601.03309.pdf' target='_blank'>https://arxiv.org/pdf/2601.03309.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianke Zhang, Xiaoyu Chen, Qiuyue Wang, Mingsheng Li, Yanjiang Guo, Yucheng Hu, Jiajun Zhang, Shuai Bai, Junyang Lin, Jianyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03309">VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.
<div id='section'>PaperID: <span id='pid'>574, <a href='https://arxiv.org/pdf/2512.09619.pdf' target='_blank'>https://arxiv.org/pdf/2512.09619.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minghao Guo, Meng Cao, Jiachen Tao, Rongtao Xu, Yan Yan, Xiaodan Liang, Ivan Laptev, Xiaojun Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09619">GLaD: Geometric Latent Distillation for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.
<div id='section'>PaperID: <span id='pid'>575, <a href='https://arxiv.org/pdf/2511.17097.pdf' target='_blank'>https://arxiv.org/pdf/2511.17097.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuo Wang, Yucheng Wang, Guoxin Lian, Yongcai Wang, Maiyue Chen, Kaihui Wang, Bo Zhang, Zhizhong Su, Yutian Zhou, Wanting Li, Deying Li, Zhaoxin Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17097">Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.
<div id='section'>PaperID: <span id='pid'>576, <a href='https://arxiv.org/pdf/2511.16449.pdf' target='_blank'>https://arxiv.org/pdf/2511.16449.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16449">VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.
<div id='section'>PaperID: <span id='pid'>577, <a href='https://arxiv.org/pdf/2511.16203.pdf' target='_blank'>https://arxiv.org/pdf/2511.16203.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuping Yan, Yuhan Xie, Yixin Zhang, Lingjuan Lyu, Handing Wang, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16203">When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.
<div id='section'>PaperID: <span id='pid'>578, <a href='https://arxiv.org/pdf/2510.02851.pdf' target='_blank'>https://arxiv.org/pdf/2510.02851.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jeyoung Park, Yeonsub Lim, Seungeun Oh, Jihong Park, Jinho Choi, Seong-Lyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02851">Action Deviation-Aware Inference for Low-Latency Wireless Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support latency-sensitive AI applications ranging from autonomous driving to industrial robot manipulation, 6G envisions distributed ML, connecting distributed computational resources in edge and cloud over hyper-reliable low-latency communication (HRLLC). In this setting, speculative decoding can facilitate collaborative inference of models distributively deployed: an on-device draft model locally generates drafts and a remote server-based target model verifies and corrects them, resulting lower latency. However, unlike autoregressive text generation, behavior cloning policies, typically used for embodied AI applications like robot manipulation and autonomous driving, cannot parallelize verification and correction for multiple drafts as each action depends on observation which needs to be updated by a previous action. To this end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft model estimates an action's need for verification and correction by the target model and selectively skips communication and computation for server operations. Action deviation shows a strong correlation with action's rejection probability by the target model, enabling selective skipping. We derive the path deviation threshold that balances the transmission rate and the inference performance, and we empirically show that action deviation-aware hybrid inference reduces uplink transmission and server operation by 40%, while lowering end-to-end latency by 33.32% relative to hybrid inference without skipping and achieving task success rate up to 97.03% of that of target model only inference.
<div id='section'>PaperID: <span id='pid'>579, <a href='https://arxiv.org/pdf/2508.15201.pdf' target='_blank'>https://arxiv.org/pdf/2508.15201.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15201">Survey of Vision-Language-Action Models for Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.
<div id='section'>PaperID: <span id='pid'>580, <a href='https://arxiv.org/pdf/2507.15729.pdf' target='_blank'>https://arxiv.org/pdf/2507.15729.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jens V. RÃ¼ppel, Andrey Rudenko, Tim Schreiter, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15729">Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of Large Language Models (LLMs) creates an exciting potential for flexible, general knowledge-driven Human-Robot Interaction (HRI) systems for assistive robots. Existing HRI systems demonstrate great progress in interpreting and following user instructions, action generation, and robot task solving. On the other hand, bi-directional, multi-modal, and context-aware support of the user in collaborative tasks still remains an open challenge. In this paper, we present a gaze- and speech-informed interface to the assistive robot, which is able to perceive the working environment from multiple vision inputs and support the dynamic user in their tasks. Our system is designed to be modular and transferable to adapt to diverse tasks and robots, and it is capable of real-time use of language-based interaction state representation and fast on board perception modules. Its development was supported by multiple public dissemination events, contributing important considerations for improved robustness and user experience. Furthermore, in two lab studies, we compare the performance and user ratings of our system with those of a traditional scripted HRI pipeline. Our findings indicate that an LLM-based approach enhances adaptability and marginally improves user engagement and task execution metrics but may produce redundant output, while a scripted pipeline is well suited for more straightforward tasks.
<div id='section'>PaperID: <span id='pid'>581, <a href='https://arxiv.org/pdf/2503.23765.pdf' target='_blank'>https://arxiv.org/pdf/2503.23765.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yun Li, Yiming Zhang, Tao Lin, Xiangrui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23765">STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we introduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis.
<div id='section'>PaperID: <span id='pid'>582, <a href='https://arxiv.org/pdf/2503.22869.pdf' target='_blank'>https://arxiv.org/pdf/2503.22869.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alexey Gavryushin, Alexandros Delitzas, Luc Van Gool, Marc Pollefeys, Kaichun Mo, Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22869">SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D Hand-Object Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When humans grasp an object, they naturally form trajectories in their minds to manipulate it for specific tasks. Modeling hand-object interaction priors holds significant potential to advance robotic and embodied AI systems in learning to operate effectively within the physical world. We introduce SIGHT, a novel task focused on generating realistic and physically plausible 3D hand-object interaction trajectories from a single image and a brief language-based task description. Prior work on hand-object trajectory generation typically relies on textual input that lacks explicit grounding to the target object, or assumes access to 3D object meshes, which are often considerably more difficult to obtain than 2D images. We propose SIGHT-Fusion, a novel diffusion-based image-text conditioned generative model that tackles this task by retrieving the most similar 3D object mesh from a database and enforcing geometric hand-object interaction constraints via a novel inference-time diffusion guidance. We benchmark our model on the HOI4D and H2O datasets, adapting relevant baselines for this novel task. Experiments demonstrate our superior performance in the diversity and quality of generated trajectories, as well as in hand-object interaction geometry metrics.
<div id='section'>PaperID: <span id='pid'>583, <a href='https://arxiv.org/pdf/2502.18012.pdf' target='_blank'>https://arxiv.org/pdf/2502.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shunkun Liang, Dongcai Tan, Banglei Guan, Zhang Li, Guangcheng Dai, Nianpeng Pan, Liang Shen, Yang Shang, Qifeng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18012">High-precision visual navigation device calibration method based on collimator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation devices require precise calibration to achieve high-precision localization and navigation, which includes camera and attitude calibration. To address the limitations of time-consuming camera calibration and complex attitude adjustment processes, this study presents a collimator-based calibration method and system. Based on the optical characteristics of the collimator, a single-image camera calibration algorithm is introduced. In addition, integrated with the precision adjustment mechanism of the calibration frame, a rotation transfer model between coordinate systems enables efficient attitude calibration. Experimental results demonstrate that the proposed method achieves accuracy and stability comparable to traditional multi-image calibration techniques. Specifically, the re-projection errors are less than 0.1463 pixels, and average attitude angle errors are less than 0.0586 degrees with a standard deviation less than 0.0257 degrees, demonstrating high precision and robustness.
<div id='section'>PaperID: <span id='pid'>584, <a href='https://arxiv.org/pdf/2502.17971.pdf' target='_blank'>https://arxiv.org/pdf/2502.17971.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tim Schreiter, Andrey Rudenko, Jens V. RÃ¼ppel, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17971">Multimodal Interaction and Intention Communication for Industrial Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful adoption of industrial robots will strongly depend on their ability to safely and efficiently operate in human environments, engage in natural communication, understand their users, and express intentions intuitively while avoiding unnecessary distractions. To achieve this advanced level of Human-Robot Interaction (HRI), robots need to acquire and incorporate knowledge of their users' tasks and environment and adopt multimodal communication approaches with expressive cues that combine speech, movement, gazes, and other modalities. This paper presents several methods to design, enhance, and evaluate expressive HRI systems for non-humanoid industrial robots. We present the concept of a small anthropomorphic robot communicating as a proxy for its non-humanoid host, such as a forklift. We developed a multimodal and LLM-enhanced communication framework for this robot and evaluated it in several lab experiments, using gaze tracking and motion capture to quantify how users perceive the robot and measure the task progress.
<div id='section'>PaperID: <span id='pid'>585, <a href='https://arxiv.org/pdf/2501.18867.pdf' target='_blank'>https://arxiv.org/pdf/2501.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18867">UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.
<div id='section'>PaperID: <span id='pid'>586, <a href='https://arxiv.org/pdf/2501.16411.pdf' target='_blank'>https://arxiv.org/pdf/2501.16411.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16411">PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world -- likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding.
<div id='section'>PaperID: <span id='pid'>587, <a href='https://arxiv.org/pdf/2412.08591.pdf' target='_blank'>https://arxiv.org/pdf/2412.08591.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingfei Han, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08591">RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators. To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes $\sim$100K open-ended description-enriched trajectories with $\sim$200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.
<div id='section'>PaperID: <span id='pid'>588, <a href='https://arxiv.org/pdf/2601.08876.pdf' target='_blank'>https://arxiv.org/pdf/2601.08876.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuai Chen, Hao Chen, Yuanchen Bei, Tianyang Zhao, Zhibo Zhou, Feiran Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08876">The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.
<div id='section'>PaperID: <span id='pid'>589, <a href='https://arxiv.org/pdf/2512.22615.pdf' target='_blank'>https://arxiv.org/pdf/2512.22615.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22615">Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.
<div id='section'>PaperID: <span id='pid'>590, <a href='https://arxiv.org/pdf/2511.15379.pdf' target='_blank'>https://arxiv.org/pdf/2511.15379.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunjiao Zhou, Xinyan Chen, Junlang Qian, Lihua Xie, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15379">Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.
<div id='section'>PaperID: <span id='pid'>591, <a href='https://arxiv.org/pdf/2509.25951.pdf' target='_blank'>https://arxiv.org/pdf/2509.25951.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>ChunPing Lam, Xiangjia Chen, Chenming Wu, Hao Chen, Binzhi Sun, Guoxin Fang, Charlie C. L. Wang, Chengkai Dai, Yeung Yam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25951">Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel human-robot interaction (HRI) framework that enables intuitive gesture-driven control through a capacitance-based woven tactile skin. Unlike conventional interfaces that rely on panels or handheld devices, the woven tactile skin integrates seamlessly with curved robot surfaces, enabling embodied interaction and narrowing the gap between human intent and robot response. Its woven design combines fabric-like flexibility with structural stability and dense multi-channel sensing through the interlaced conductive threads. Building on this capability, we define a gesture-action mapping of 14 single- and multi-touch gestures that cover representative robot commands, including task-space motion and auxiliary functions. A lightweight convolution-transformer model designed for gesture recognition in real time achieves an accuracy of near-100%, outperforming prior baseline approaches. Experiments on robot arm tasks, including pick-and-place and pouring, demonstrate that our system reduces task completion time by up to 57% compared with keyboard panels and teach pendants. Overall, our proposed framework demonstrates a practical pathway toward more natural and efficient embodied HRI.
<div id='section'>PaperID: <span id='pid'>592, <a href='https://arxiv.org/pdf/2508.20840.pdf' target='_blank'>https://arxiv.org/pdf/2508.20840.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20840">Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
<div id='section'>PaperID: <span id='pid'>593, <a href='https://arxiv.org/pdf/2508.12948.pdf' target='_blank'>https://arxiv.org/pdf/2508.12948.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei Wei, Shaojie Zhang, Yonghao Dang, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12948">MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action recognition is a crucial task for intelligent robotics, particularly within the context of human-robot collaboration research. In self-supervised skeleton-based action recognition, the mask-based reconstruction paradigm learns the spatial structure and motion patterns of the skeleton by masking joints and reconstructing the target from unlabeled data. However, existing methods focus on a limited set of joints and low-order motion patterns, limiting the model's ability to understand complex motion patterns. To address this issue, we introduce MaskSem, a novel semantic-guided masking method for learning 3D hybrid high-order motion representations. This novel framework leverages Grad-CAM based on relative motion to guide the masking of joints, which can be represented as the most semantically rich temporal orgions. The semantic-guided masking process can encourage the model to explore more discriminative features. Furthermore, we propose using hybrid high-order motion as the reconstruction target, enabling the model to learn multi-order motion patterns. Specifically, low-order motion velocity and high-order motion acceleration are used together as the reconstruction target. This approach offers a more comprehensive description of the dynamic motion process, enhancing the model's understanding of motion patterns. Experiments on the NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla transformer, improves skeleton-based action recognition, making it more suitable for applications in human-robot interaction.
<div id='section'>PaperID: <span id='pid'>594, <a href='https://arxiv.org/pdf/2508.01184.pdf' target='_blank'>https://arxiv.org/pdf/2508.01184.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinhang Wan, Dongqiang Gou, Xinwang Liu, En Zhu, Xuming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01184">Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A core problem of Embodied AI is to learn object manipulation from observation, as humans do. To achieve this, it is important to localize 3D object affordance areas through observation such as images (3D affordance grounding) and understand their functionalities (affordance classification). Previous attempts usually tackle these two tasks separately, leading to inconsistent predictions due to lacking proper modeling of their dependency. In addition, these methods typically only ground the incomplete affordance areas depicted in images, failing to predict the full potential affordance areas, and operate at a fixed scale, resulting in difficulty in coping with affordances significantly varying in scale with respect to the whole object. To address these issues, we propose a novel approach that learns an affordance-aware 3D representation and employs a stage-wise inference strategy leveraging the dependency between grounding and classification tasks. Specifically, we first develop a cross-modal 3D representation through efficient fusion and multi-scale geometric feature propagation, enabling inference of full potential affordance areas at a suitable regional scale. Moreover, we adopt a simple two-stage prediction mechanism, effectively coupling grounding and classification for better affordance understanding. Experiments demonstrate the effectiveness of our method, showing improved performance in both affordance grounding and classification.
<div id='section'>PaperID: <span id='pid'>595, <a href='https://arxiv.org/pdf/2507.10894.pdf' target='_blank'>https://arxiv.org/pdf/2507.10894.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zongtao He, Liuyi Wang, Lu Chen, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10894">NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.
<div id='section'>PaperID: <span id='pid'>596, <a href='https://arxiv.org/pdf/2503.19317.pdf' target='_blank'>https://arxiv.org/pdf/2503.19317.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoting Peng, Haonan Chen, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19317">Towards Uncertainty Unification: A Case Study for Preference Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning human preferences is essential for human-robot interaction, as it enables robots to adapt their behaviors to align with human expectations and goals. However, the inherent uncertainties in both human behavior and robotic systems make preference learning a challenging task. While probabilistic robotics algorithms offer uncertainty quantification, the integration of human preference uncertainty remains underexplored. To bridge this gap, we introduce uncertainty unification and propose a novel framework, uncertainty-unified preference learning (UUPL), which enhances Gaussian Process (GP)-based preference learning by unifying human and robot uncertainties. Specifically, UUPL includes a human preference uncertainty model that improves GP posterior mean estimation, and an uncertainty-weighted Gaussian Mixture Model (GMM) that enhances GP predictive variance accuracy. Additionally, we design a user-specific calibration process to align uncertainty representations across users, ensuring consistency and reliability in the model performance. Comprehensive experiments and user studies demonstrate that UUPL achieves state-of-the-art performance in both prediction accuracy and user rating. An ablation study further validates the effectiveness of human uncertainty model and uncertainty-weighted GMM of UUPL.
<div id='section'>PaperID: <span id='pid'>597, <a href='https://arxiv.org/pdf/2502.11518.pdf' target='_blank'>https://arxiv.org/pdf/2502.11518.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Di Wu, Xian Wei, Guang Chen, Hao Shen, Xiangfeng Wang, Wenhao Li, Bo Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11518">Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied multi-agent systems (EMAS) have attracted growing attention for their potential to address complex, real-world challenges in areas such as logistics and robotics. Recent advances in foundation models pave the way for generative agents capable of richer communication and adaptive problem-solving. This survey provides a systematic examination of how EMAS can benefit from these generative capabilities. We propose a taxonomy that categorizes EMAS by system architectures and embodiment modalities, emphasizing how collaboration spans both physical and virtual contexts. Central building blocks, perception, planning, communication, and feedback, are then analyzed to illustrate how generative techniques bolster system robustness and flexibility. Through concrete examples, we demonstrate the transformative effects of integrating foundation models into embodied, multi-agent frameworks. Finally, we discuss challenges and future directions, underlining the significant promise of EMAS to reshape the landscape of AI-driven collaboration.
<div id='section'>PaperID: <span id='pid'>598, <a href='https://arxiv.org/pdf/2410.05191.pdf' target='_blank'>https://arxiv.org/pdf/2410.05191.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05191">LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building on the advancements of Large Language Models (LLMs) and Vision Language Models (VLMs), recent research has introduced Vision-Language-Action (VLA) models as an integrated solution for robotic manipulation tasks. These models take camera images and natural language task instructions as input and directly generate control actions for robots to perform specified tasks, greatly improving both decision-making capabilities and interaction with human users. However, the data-driven nature of VLA models, combined with their lack of interpretability, makes the assurance of their effectiveness and robustness a challenging task. This highlights the need for a reliable testing and evaluation platform. For this purpose, in this work, we propose LADEV, a comprehensive and efficient platform specifically designed for evaluating VLA models. We first present a language-driven approach that automatically generates simulation environments from natural language inputs, mitigating the need for manual adjustments and significantly improving testing efficiency. Then, to further assess the influence of language input on the VLA models, we implement a paraphrase mechanism that produces diverse natural language task instructions for testing. Finally, to expedite the evaluation process, we introduce a batch-style method for conducting large-scale testing of VLA models. Using LADEV, we conducted experiments on several state-of-the-art VLA models, demonstrating its effectiveness as a tool for evaluating these models. Our results showed that LADEV not only enhances testing efficiency but also establishes a solid baseline for evaluating VLA models, paving the way for the development of more intelligent and advanced robotic systems.
<div id='section'>PaperID: <span id='pid'>599, <a href='https://arxiv.org/pdf/2409.12894.pdf' target='_blank'>https://arxiv.org/pdf/2409.12894.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12894">VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative AI and multi-modal foundation models has shown significant potential in advancing robotic manipulation. Vision-language-action (VLA) models, in particular, have emerged as a promising approach for visuomotor control by leveraging large-scale vision-language data and robot demonstrations. However, current VLA models are typically evaluated using a limited set of hand-crafted scenes, leaving their general performance and robustness in diverse scenarios largely unexplored. To address this gap, we present VLATest, a fuzzing framework designed to generate robotic manipulation scenes for testing VLA models. Based on VLATest, we conducted an empirical study to assess the performance of seven representative VLA models. Our study results revealed that current VLA models lack the robustness necessary for practical deployment. Additionally, we investigated the impact of various factors, including the number of confounding objects, lighting conditions, camera poses, unseen objects, and task instruction mutations, on the VLA model's performance. Our findings highlight the limitations of existing VLA models, emphasizing the need for further research to develop reliable and trustworthy VLA applications.
<div id='section'>PaperID: <span id='pid'>600, <a href='https://arxiv.org/pdf/2602.13193.pdf' target='_blank'>https://arxiv.org/pdf/2602.13193.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>William Chen, Jagdeep Singh Bhatia, Catherine Glossop, Nikhil Mathihalli, Ria Doshi, Andy Tang, Danny Driess, Karl Pertsch, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13193">Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks. Website: steerable-policies.github.io
<div id='section'>PaperID: <span id='pid'>601, <a href='https://arxiv.org/pdf/2602.07007.pdf' target='_blank'>https://arxiv.org/pdf/2602.07007.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dongsheng Chen, Yuxuan Li, Yi Lin, Guanhua Chen, Jiaxin Zhang, Xiangyu Zhao, Lei Ma, Xin Yao, Xuetao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07007">ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring functional safety is essential for the deployment of Embodied AI in complex open-world environments. However, traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale in this domain. While HARA relies on enumerating risks for finite and pre-defined function lists, Embodied AI operates on open-ended natural language instructions, creating a challenge of combinatorial interaction risks. Whereas Large Language Models (LLMs) have emerged as a promising solution to this scalability challenge, they often lack physical grounding, yielding semantically superficial and incoherent hazard descriptions. To overcome these limitations, we propose a new framework ARGOS (AttRibute-Guided cOmbinatorial reaSoning), which bridges the gap between open-ended user instructions and concrete physical attributes. By dynamically decomposing entities from instructions into these fine-grained properties, ARGOS grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios. It then instantiates abstract safety standards, such as ISO 13482, into context-specific Functional Safety Requirements (FSRs) by integrating these scenarios with robot capabilities. Extensive experiments validate that ARGOS produces high-quality FSRs and outperforms baselines in identifying long-tail risks. Overall, this work paves the way for systematic and grounded functional safety requirement generation, a critical step toward the safe industrial deployment of Embodied AI.
<div id='section'>PaperID: <span id='pid'>602, <a href='https://arxiv.org/pdf/2601.17815.pdf' target='_blank'>https://arxiv.org/pdf/2601.17815.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yves Inglin, Jonas Frey, Changan Chen, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17815">Less Is More: Scalable Visual Navigation from Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.
<div id='section'>PaperID: <span id='pid'>603, <a href='https://arxiv.org/pdf/2512.01952.pdf' target='_blank'>https://arxiv.org/pdf/2512.01952.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyang He, Jay Patrikar, Dong-Ki Kim, Max Smith, Daniel McGann, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei, Sebastian Scherer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01952">GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.
<div id='section'>PaperID: <span id='pid'>604, <a href='https://arxiv.org/pdf/2508.08896.pdf' target='_blank'>https://arxiv.org/pdf/2508.08896.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08896">Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories.
<div id='section'>PaperID: <span id='pid'>605, <a href='https://arxiv.org/pdf/2505.23705.pdf' target='_blank'>https://arxiv.org/pdf/2505.23705.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23705">Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation.
<div id='section'>PaperID: <span id='pid'>606, <a href='https://arxiv.org/pdf/2505.08243.pdf' target='_blank'>https://arxiv.org/pdf/2505.08243.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08243">Training Strategies for Efficient Embodied Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.
<div id='section'>PaperID: <span id='pid'>607, <a href='https://arxiv.org/pdf/2504.09927.pdf' target='_blank'>https://arxiv.org/pdf/2504.09927.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haiyong Yu, Yanqiong Jin, Yonghao He, Wei Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09927">Efficient Task-specific Conditional Diffusion Policies: Shortcut Model Acceleration and SO(3) Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning, particularly Diffusion Policies based methods, has recently gained significant traction in embodied AI as a powerful approach to action policy generation. These models efficiently generate action policies by learning to predict noise. However, conventional Diffusion Policy methods rely on iterative denoising, leading to inefficient inference and slow response times, which hinder real-time robot control. To address these limitations, we propose a Classifier-Free Shortcut Diffusion Policy (CF-SDP) that integrates classifier-free guidance with shortcut-based acceleration, enabling efficient task-specific action generation while significantly improving inference speed. Furthermore, we extend diffusion modeling to the SO(3) manifold in shortcut model, defining the forward and reverse processes in its tangent space with an isotropic Gaussian distribution. This ensures stable and accurate rotational estimation, enhancing the effectiveness of diffusion-based control. Our approach achieves nearly 5x acceleration in diffusion inference compared to DDIM-based Diffusion Policy while maintaining task performance. Evaluations both on the RoboTwin simulation platform and real-world scenarios across various tasks demonstrate the superiority of our method.
<div id='section'>PaperID: <span id='pid'>608, <a href='https://arxiv.org/pdf/2504.00697.pdf' target='_blank'>https://arxiv.org/pdf/2504.00697.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marlene Wessels, Jorge de Heuvel, Leon MÃ¼ller, Anna Luisa Maier, Maren Bennewitz, Johannes Kraus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00697">Auditory Localization and Assessment of Consequential Robot Sounds: A Multi-Method Study in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile robots increasingly operate alongside humans but are often out of sight, so that humans need to rely on the sounds of the robots to recognize their presence. For successful human-robot interaction (HRI), it is therefore crucial to understand how humans perceive robots by their consequential sounds, i.e., operating noise. Prior research suggests that the sound of a quadruped Go1 is more detectable than that of a wheeled Turtlebot. This study builds on this and examines the human ability to localize consequential sounds of three robots (quadruped Go1, wheeled Turtlebot 2i, wheeled HSR) in Virtual Reality. In a within-subjects design, we assessed participants' localization performance for the robots with and without an acoustic vehicle alerting system (AVAS) for two velocities (0.3, 0.8 m/s) and two trajectories (head-on, radial). In each trial, participants were presented with the sound of a moving robot for 3~s and were tasked to point at its final position (localization task). Localization errors were measured as the absolute angular difference between the participants' estimated and the actual robot position. Results showed that the robot type significantly influenced the localization accuracy and precision, with the sound of the wheeled HSR (especially without AVAS) performing worst under all experimental conditions. Surprisingly, participants rated the HSR sound as more positive, less annoying, and more trustworthy than the Turtlebot and Go1 sound. This reveals a tension between subjective evaluation and objective auditory localization performance. Our findings highlight consequential robot sounds as a critical factor for designing intuitive and effective HRI, with implications for human-centered robot design and social navigation.
<div id='section'>PaperID: <span id='pid'>609, <a href='https://arxiv.org/pdf/2504.00682.pdf' target='_blank'>https://arxiv.org/pdf/2504.00682.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jorge de Heuvel, Sebastian MÃ¼ller, Marlene Wessels, Aftab Akhtar, Christian Bauckhage, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00682">Immersive Explainability: Visualizing Robot Navigation Decisions through XAI Semantic Scene Projections in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end robot policies achieve high performance through neural networks trained via reinforcement learning (RL). Yet, their black box nature and abstract reasoning pose challenges for human-robot interaction (HRI), because humans may experience difficulty in understanding and predicting the robot's navigation decisions, hindering trust development. We present a virtual reality (VR) interface that visualizes explainable AI (XAI) outputs and the robot's lidar perception to support intuitive interpretation of RL-based navigation behavior. By visually highlighting objects based on their attribution scores, the interface grounds abstract policy explanations in the scene context. This XAI visualization bridges the gap between obscure numerical XAI attribution scores and a human-centric semantic level of explanation. A within-subjects study with 24 participants evaluated the effectiveness of our interface for four visualization conditions combining XAI and lidar. Participants ranked scene objects across navigation scenarios based on their importance to the robot, followed by a questionnaire assessing subjective understanding and predictability. Results show that semantic projection of attributions significantly enhances non-expert users' objective understanding and subjective awareness of robot behavior. In addition, lidar visualization further improves perceived predictability, underscoring the value of integrating XAI and sensor for transparent, trustworthy HRI.
<div id='section'>PaperID: <span id='pid'>610, <a href='https://arxiv.org/pdf/2502.11918.pdf' target='_blank'>https://arxiv.org/pdf/2502.11918.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Runze Liu, Chenjia Bai, Jiafei Lyu, Shengjie Sun, Yali Du, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11918">VLP: Vision-Language Preference Learning for Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward engineering is one of the key challenges in Reinforcement Learning (RL). Preference-based RL effectively addresses this issue by learning from human feedback. However, it is both time-consuming and expensive to collect human preference labels. In this paper, we propose a novel \textbf{V}ision-\textbf{L}anguage \textbf{P}reference learning framework, named \textbf{VLP}, which learns a vision-language preference model to provide preference feedback for embodied manipulation tasks. To achieve this, we define three types of language-conditioned preferences and construct a vision-language preference dataset, which contains versatile implicit preference orders without human annotations. The preference model learns to extract language-related features, and then serves as a preference annotator in various downstream tasks. The policy can be learned according to the annotated preferences via reward learning or direct policy optimization. Extensive empirical results on simulated embodied manipulation tasks demonstrate that our method provides accurate preferences and generalizes to unseen tasks and unseen language instructions, outperforming the baselines by a large margin.
<div id='section'>PaperID: <span id='pid'>611, <a href='https://arxiv.org/pdf/2410.19374.pdf' target='_blank'>https://arxiv.org/pdf/2410.19374.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria Lombardi, Elisa Maiettini, Agnieszka Wykowska, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19374">Gaze estimation learning architecture as support to affective, social and cognitive studies in natural human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze is a crucial social cue in any interacting scenario and drives many mechanisms of social cognition (joint and shared attention, predicting human intention, coordination tasks). Gaze direction is an indication of social and emotional functions affecting the way the emotions are perceived. Evidence shows that embodied humanoid robots endowing social abilities can be seen as sophisticated stimuli to unravel many mechanisms of human social cognition while increasing engagement and ecological validity. In this context, building a robotic perception system to automatically estimate the human gaze only relying on robot's sensors is still demanding. Main goal of the paper is to propose a learning robotic architecture estimating the human gaze direction in table-top scenarios without any external hardware. Table-top tasks are largely used in many studies in experimental psychology because they are suitable to implement numerous scenarios allowing agents to collaborate while maintaining a face-to-face interaction. Such an architecture can provide a valuable support in studies where external hardware might represent an obstacle to spontaneous human behaviour, especially in environments less controlled than the laboratory (e.g., in clinical settings). A novel dataset was also collected with the humanoid robot iCub, including images annotated from 24 participants in different gaze conditions.
<div id='section'>PaperID: <span id='pid'>612, <a href='https://arxiv.org/pdf/2601.16449.pdf' target='_blank'>https://arxiv.org/pdf/2601.16449.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu, Yifei Dong, Shuyuan Tu, Qiyu Hu, Huiting Huang, Yuxiang Lin, Jun-Yan He, Kai Wang, Zheng Lian, Zhi-Qi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16449">Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.
<div id='section'>PaperID: <span id='pid'>613, <a href='https://arxiv.org/pdf/2511.00940.pdf' target='_blank'>https://arxiv.org/pdf/2511.00940.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Li, Xiang Bai, Jieyu Zhang, Zhuangzhe Wu, Che Xu, Ying Li, Chengkai Hou, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00940">URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized $[SEG]$ token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\% improvement), kinematic parameter prediction (average error reduction of 29\%), and physical executability (surpassing baselines by 50\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.
<div id='section'>PaperID: <span id='pid'>614, <a href='https://arxiv.org/pdf/2511.00091.pdf' target='_blank'>https://arxiv.org/pdf/2511.00091.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi "Jim" Fan, Guanya Shi, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00091">Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.
<div id='section'>PaperID: <span id='pid'>615, <a href='https://arxiv.org/pdf/2510.05681.pdf' target='_blank'>https://arxiv.org/pdf/2510.05681.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Suhyeok Jang, Dongyoung Kim, Changyeon Kim, Youngsuk Kim, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05681">Verifier-free Test-Time Sampling for Vision Language Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.
<div id='section'>PaperID: <span id='pid'>616, <a href='https://arxiv.org/pdf/2509.08126.pdf' target='_blank'>https://arxiv.org/pdf/2509.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08126">Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: https://z.umn.edu/ogrg
<div id='section'>PaperID: <span id='pid'>617, <a href='https://arxiv.org/pdf/2508.17832.pdf' target='_blank'>https://arxiv.org/pdf/2508.17832.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiping Wang, Yuxi Wang, Mengqi Zhou, Junsong Fan, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17832">HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic 3D indoor scene generation is crucial for virtual reality, interior design, embodied intelligence, and scene understanding. While existing methods have made progress in coarse-scale furniture arrangement, they struggle to capture fine-grained object placements, limiting the realism and utility of generated environments. This gap hinders immersive virtual experiences and detailed scene comprehension for embodied AI applications. To address these issues, we propose Hierarchical Layout Generation (HLG), a novel method for fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine hierarchical approach, refining scene layouts from large-scale furniture placement to intricate object arrangements. Specifically, our fine-grained layout alignment module constructs a hierarchical layout through vertical and horizontal decoupling, effectively decomposing complex 3D indoor scenes into multiple levels of granularity. Additionally, our trainable layout optimization network addresses placement issues, such as incorrect positioning, orientation errors, and object intersections, ensuring structurally coherent and physically plausible scene generation. We demonstrate the effectiveness of our approach through extensive experiments, showing superior performance in generating realistic indoor scenes compared to existing methods. This work advances the field of scene generation and opens new possibilities for applications requiring detailed 3D environments. We will release our code upon publication to encourage future research.
<div id='section'>PaperID: <span id='pid'>618, <a href='https://arxiv.org/pdf/2506.01174.pdf' target='_blank'>https://arxiv.org/pdf/2506.01174.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhammad Qasim Ali, Saeejith Nair, Alexander Wong, Yuchen Cui, Yuhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01174">GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structured scene representations are a core component of embodied agents, helping to consolidate raw sensory streams into readable, modular, and searchable formats. Due to their high computational overhead, many approaches build such representations in advance of the task. However, when the task specifications change, such static approaches become inadequate as they may miss key objects, spatial relations, and details. We introduce GraphPad, a modifiable structured memory that an agent can tailor to the needs of the task through API calls. It comprises a mutable scene graph representing the environment, a navigation log indexing frame-by-frame content, and a scratchpad for task-specific notes. Together, GraphPad serves as a dynamic workspace that remains complete, current, and aligned with the agent's immediate understanding of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a +3.0% increase over an image-only baseline using the same vision-language model, while operating with five times fewer input frames. These results show that allowing online, language-driven refinement of 3-D memory yields more informative representations without extra training or data collection.
<div id='section'>PaperID: <span id='pid'>619, <a href='https://arxiv.org/pdf/2503.12974.pdf' target='_blank'>https://arxiv.org/pdf/2503.12974.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xueying Jiang, Wenhao Li, Xiaoqin Zhang, Ling Shao, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12974">Exploring 3D Reasoning-Driven Planning: From Implicit Human Intentions to Route-Aware Activity Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D task planning has attracted increasing attention in human-robot interaction and embodied AI thanks to the recent advances in multimodal learning. However, most existing studies are facing two common challenges: 1) heavy reliance on explicit instructions with little reasoning on implicit user intention; 2) negligence of inter-step route planning on robot moves. We address the above challenges by proposing 3D Reasoning-Driven Planning, a novel 3D task that reasons the intended activities from implicit instructions and decomposes them into steps with inter-step routes and planning under the guidance of fine-grained 3D object shapes and locations from scene segmentation. We tackle the new 3D task from two perspectives. First, we construct ReasonPlan3D, a large-scale benchmark that covers diverse 3D scenes with rich implicit instructions and detailed annotations for multi-step task planning, inter-step route planning, and fine-grained segmentation. Second, we design a novel framework that introduces progressive plan generation with contextual consistency across multiple steps, as well as a scene graph that is updated dynamically for capturing critical objects and their spatial relations. Extensive experiments demonstrate the effectiveness of our benchmark and framework in reasoning activities from implicit human instructions, producing accurate stepwise task plans and seamlessly integrating route planning for multi-step moves. The dataset and code will be released.
<div id='section'>PaperID: <span id='pid'>620, <a href='https://arxiv.org/pdf/2410.11110.pdf' target='_blank'>https://arxiv.org/pdf/2410.11110.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pablo Soler Garcia, Petar Lukovic, Lucie Reynaud, Andrea Sgobbi, Federica Bruni, Martin Brun, Marc ZÃ¼nd, Riccardo Bollati, Marc Pollefeys, Hermann Blum, Zuria Bauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11110">HoloSpot: Intuitive Object Manipulation via Mixed Reality Drag-and-Drop</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction through mixed reality (MR) technologies enables novel, intuitive interfaces to control robots in remote operations. Such interfaces facilitate operations in hazardous environments, where human presence is risky, yet human oversight remains crucial. Potential environments include disaster response scenarios and areas with high radiation or toxic chemicals. In this paper we present an interface system projecting a 3D representation of a scanned room as a scaled-down 'dollhouse' hologram, allowing users to select and manipulate objects using a straightforward drag-and-drop interface. We then translate these drag-and-drop user commands into real-time robot actions based on the recent Spot-Compose framework. The Unity-based application provides an interactive tutorial and a user-friendly experience, ensuring ease of use. Through comprehensive end-to-end testing, we validate the system's capability in executing pick-and-place tasks and a complementary user study affirms the interface's intuitive controls. Our findings highlight the advantages of this interface in improving user experience and operational efficiency. This work lays the groundwork for a robust framework that advances the potential for seamless human-robot collaboration in diverse applications. Paper website: https://holospot.github.io/
<div id='section'>PaperID: <span id='pid'>621, <a href='https://arxiv.org/pdf/2512.10071.pdf' target='_blank'>https://arxiv.org/pdf/2512.10071.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junjie Bai, Yu-Wei Chao, Qizhi Chen, Jinwei Gu, Moo Jin Kim, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Ming-Yu Liu, Nic Ma, Kaichun Mo, Delin Qu, Shangkun Sun, Hongchi Xia, Fangyin Wei, Xiaohui Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10071">Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on $π_{0.5}$, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.
<div id='section'>PaperID: <span id='pid'>622, <a href='https://arxiv.org/pdf/2511.22609.pdf' target='_blank'>https://arxiv.org/pdf/2511.22609.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bo Wang, Jiehong Lin, Chenzhi Liu, Xinting Hu, Yifei Yu, Tianjia Liu, Zhongrui Wang, Xiaojuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22609">MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.
<div id='section'>PaperID: <span id='pid'>623, <a href='https://arxiv.org/pdf/2511.07412.pdf' target='_blank'>https://arxiv.org/pdf/2511.07412.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Han Zhang, Yiqing Shen, Roger D. Soberanis-Mukul, Ankita Ghosh, Hao Ding, Lalithkumar Seenivasan, Jose L. Porras, Zhekai Mao, Chenjia Li, Wenjie Xiao, Lonny Yarmus, Angela Christine Argento, Masaru Ishii, Mathias Unberath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07412">TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.
<div id='section'>PaperID: <span id='pid'>624, <a href='https://arxiv.org/pdf/2511.00108.pdf' target='_blank'>https://arxiv.org/pdf/2511.00108.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Zhang, Che Liu, Xiancong Ren, Hanchu Ni, Shuai Zhang, Zeyuan Ding, Jiayu Hu, Hanzhe Shan, Zhenwei Niu, Zhaoyang Liu, Shuang Liu, Yue Zhao, Junbo Qi, Qinfan Zhang, Dengjie Li, Yidong Wang, Jiachen Luo, Yong Dai, Zenglin Xu, Bin Shen, Qifan Wang, Jian Tang, Xiaozhu Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00108">Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.
<div id='section'>PaperID: <span id='pid'>625, <a href='https://arxiv.org/pdf/2509.20739.pdf' target='_blank'>https://arxiv.org/pdf/2509.20739.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guoyang Zhao, Yudong Li, Weiqing Qi, Kai Zhang, Bonan Liu, Kai Chen, Haoang Li, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20739">SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration. To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations. A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference. And a semantic-probabilistic topological map supports coarse-to-fine planning: LLM-based global reasoning for subgoal selection and vision-based local planning for obstacle avoidance. Integrated with reinforcement-learning locomotion controllers, the framework is deployable across diverse legged robot platforms. Experiments in simulation and real-world settings demonstrate consistent improvements in semantic accuracy, planning quality, and navigation success, while ablation studies further showcase the necessity of both hierarchical perception and fine local planning. This work introduces a new paradigm for SLAM-free, vision-language-driven navigation, shifting robotic exploration from geometry-centric mapping to semantics-driven decision making.
<div id='section'>PaperID: <span id='pid'>626, <a href='https://arxiv.org/pdf/2509.18953.pdf' target='_blank'>https://arxiv.org/pdf/2509.18953.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang, Tingsong Jiang, Weien Zhou, Wen Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18953">Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.
<div id='section'>PaperID: <span id='pid'>627, <a href='https://arxiv.org/pdf/2508.13446.pdf' target='_blank'>https://arxiv.org/pdf/2508.13446.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13446">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.
<div id='section'>PaperID: <span id='pid'>628, <a href='https://arxiv.org/pdf/2508.07611.pdf' target='_blank'>https://arxiv.org/pdf/2508.07611.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zifan Wang, Xun Yang, Jianzhuang Zhao, Jiaming Zhou, Teli Ma, Ziyao Gao, Arash Ajoudani, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07611">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of humanoid robots in unstructured, human-centric environments requires navigation capabilities that extend beyond simple locomotion to include robust perception, provable safety, and socially aware behavior. Current reinforcement learning approaches are often limited by blind controllers that lack environmental awareness or by vision-based systems that fail to perceive complex 3D obstacles. In this work, we present an end-to-end locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to motor commands, enabling robust navigation in cluttered dynamic scenes. We formulate the control problem as a Constrained Markov Decision Process (CMDP) to formally separate safety from task objectives. Our key contribution is a novel methodology that translates the principles of Control Barrier Functions (CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal Policy Optimization (P3O) to enforce safety constraints during training. Furthermore, we introduce a set of comfort-oriented rewards, grounded in human-robot interaction research, to promote motions that are smooth, predictable, and less intrusive. We demonstrate the efficacy of our framework through a successful sim-to-real transfer to a physical humanoid robot, which exhibits agile and safe navigation around both static and dynamic 3D obstacles.
<div id='section'>PaperID: <span id='pid'>629, <a href='https://arxiv.org/pdf/2507.13468.pdf' target='_blank'>https://arxiv.org/pdf/2507.13468.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiye Cao, Maia Stiber, Amama Mahmood, Maria Teresa Parreira, Wendy Ju, Micol Spitale, Hatice Gunes, Chien-Ming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13468">ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.
<div id='section'>PaperID: <span id='pid'>630, <a href='https://arxiv.org/pdf/2505.22050.pdf' target='_blank'>https://arxiv.org/pdf/2505.22050.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, Bo Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22050">Reinforced Reasoning for Embodied Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.
<div id='section'>PaperID: <span id='pid'>631, <a href='https://arxiv.org/pdf/2505.11528.pdf' target='_blank'>https://arxiv.org/pdf/2505.11528.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhang Huang, Jiazhao Zhang, Shilong Zou, Xinwang Liu, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11528">LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.
<div id='section'>PaperID: <span id='pid'>632, <a href='https://arxiv.org/pdf/2505.05592.pdf' target='_blank'>https://arxiv.org/pdf/2505.05592.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Noriaki Hirose, Lydia Ignatova, Kyle Stachowicz, Catherine Glossop, Sergey Levine, Dhruv Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05592">Learning to Drive Anywhere with Model-Based Reannotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
<div id='section'>PaperID: <span id='pid'>633, <a href='https://arxiv.org/pdf/2505.02836.pdf' target='_blank'>https://arxiv.org/pdf/2505.02836.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, Zhaoshuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02836">Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.
<div id='section'>PaperID: <span id='pid'>634, <a href='https://arxiv.org/pdf/2503.21056.pdf' target='_blank'>https://arxiv.org/pdf/2503.21056.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiqing Shen, Bohan Liu, Chenjia Li, Lalithkumar Seenivasan, Mathias Unberath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21056">Online Reasoning Video Segmentation with Just-in-Time Digital Twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- a LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as "just-in-time" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity.
<div id='section'>PaperID: <span id='pid'>635, <a href='https://arxiv.org/pdf/2503.11684.pdf' target='_blank'>https://arxiv.org/pdf/2503.11684.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Micol Spitale, Srikar Babu, Serhan Cakmak, Jiaee Cheong, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11684">Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the primary goals of Human-Robot Interaction (HRI) research is to develop robots that can interpret human behavior and adapt their responses accordingly. Adaptive learning models, such as continual and reinforcement learning, play a crucial role in improving robots' ability to interact effectively in real-world settings. However, these models face significant challenges due to the limited availability of real-world data, particularly in sensitive domains like healthcare and well-being. This data scarcity can hinder a robot's ability to adapt to new situations. To address these challenges, causality provides a structured framework for understanding and modeling the underlying relationships between actions, events, and outcomes. By moving beyond mere pattern recognition, causality enables robots to make more explainable and generalizable decisions. This paper presents an exploratory causality-based analysis through a case study of an adaptive robotic coach delivering positive psychology exercises over four weeks in a workplace setting. The robotic coach autonomously adapts to multimodal human behaviors, such as facial valence and speech duration. By conducting both macro- and micro-level causal analyses, this study aims to gain deeper insights into how adaptability can enhance well-being during interactions. Ultimately, this research seeks to advance our understanding of how causality can help overcome challenges in HRI, particularly in real-world applications.
<div id='section'>PaperID: <span id='pid'>636, <a href='https://arxiv.org/pdf/2503.07771.pdf' target='_blank'>https://arxiv.org/pdf/2503.07771.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Philipp Wu, Yide Shentu, Qiayuan Liao, Ding Jin, Menglong Guo, Koushil Sreenath, Xingyu Lin, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07771">RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from human demonstration is an effective approach for learning complex manipulation skills. However, existing approaches heavily focus on learning from passive human demonstration data for its simplicity in data collection. Interactive human teaching has appealing theoretical and practical properties, but they are not well supported by existing human-robot interfaces. This paper proposes a novel system that enables seamless control switching between human and an autonomous policy for bi-manual manipulation tasks, enabling more efficient learning of new tasks. This is achieved through a compliant, bilateral teleoperation system. Through simulation and hardware experiments, we demonstrate the value of our system in an interactive human teaching for learning complex bi-manual manipulation skills.
<div id='section'>PaperID: <span id='pid'>637, <a href='https://arxiv.org/pdf/2411.12286.pdf' target='_blank'>https://arxiv.org/pdf/2411.12286.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Teli Ma, Zifan Wang, Jiaming Zhou, Mengmeng Wang, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12286">GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict the visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 table-top real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. We also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.
<div id='section'>PaperID: <span id='pid'>638, <a href='https://arxiv.org/pdf/2410.03907.pdf' target='_blank'>https://arxiv.org/pdf/2410.03907.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03907">ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models~(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models~(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model's reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.
<div id='section'>PaperID: <span id='pid'>639, <a href='https://arxiv.org/pdf/2409.17114.pdf' target='_blank'>https://arxiv.org/pdf/2409.17114.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Adam Wolniakowski, Kanstantsin Miatliuk, Jose J. Quintana, Miguel A. Ferrer, Moises Diaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17114">Towards human-like kinematics in industrial robotic arms: a case study on a UR3 robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety in industrial robotic environments is a hot research topic in the area of human-robot interaction (HRI). Up to now, a robotic arm on an assembly line interacts with other machines away from human workers. Nowadays, robotic arm manufactures are aimed to their robots could increasingly perform tasks collaborating with humans. One of the ways to improve this collaboration is by making the movement of robots more humanlike. This way, it would be easier for a human to foresee the movement of the robot and approach it without fear of contact. The main difference between the movement of a human and of a robotic arm is that the former has a bell-shaped speed profile while the latter has a uniform speed one. To generate this speed profile, the kinematic theory of rapid human movements and its Sigma-Lognormal model has been used. This model is widely used to explain most of the basic phenomena related to the control of human movements. Both human-like and robotic-like movements are transferred to the UR3 robot. In this paper we detail the how the UR3 robot was programmed to produce both kinds of movement. The dissimilarities result between the input motion and output motion to the robot confirm the possibility to develop human-like velocities in the UR3 robot.
<div id='section'>PaperID: <span id='pid'>640, <a href='https://arxiv.org/pdf/2602.17573.pdf' target='_blank'>https://arxiv.org/pdf/2602.17573.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Konstantinos Foteinos, Georgios Angelidis, Aggelos Psiris, Vasileios Argyriou, Panagiotis Sarigiannidis, Georgios Th. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.17573">FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.
<div id='section'>PaperID: <span id='pid'>641, <a href='https://arxiv.org/pdf/2601.16163.pdf' target='_blank'>https://arxiv.org/pdf/2601.16163.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16163">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/
<div id='section'>PaperID: <span id='pid'>642, <a href='https://arxiv.org/pdf/2601.04061.pdf' target='_blank'>https://arxiv.org/pdf/2601.04061.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, Cai Zhou, Jiwen Lu, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04061">CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.
<div id='section'>PaperID: <span id='pid'>643, <a href='https://arxiv.org/pdf/2512.04537.pdf' target='_blank'>https://arxiv.org/pdf/2512.04537.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pei Yang, Hai Ci, Yiren Song, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04537">X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.
<div id='section'>PaperID: <span id='pid'>644, <a href='https://arxiv.org/pdf/2511.22098.pdf' target='_blank'>https://arxiv.org/pdf/2511.22098.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Quanjian Song, Yiren Song, Kelly Peng, Yuan Gao, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22098">WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.
<div id='section'>PaperID: <span id='pid'>645, <a href='https://arxiv.org/pdf/2510.19944.pdf' target='_blank'>https://arxiv.org/pdf/2510.19944.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, Zhongcong Xu, Xuanyu Yi, Zihao Yu, Jianfeng Zhang, Yifan Zhu, Rui Chen, Jinxin Chi, Zixian Du, Li Han, Lixin Huang, Kaihua Jiang, Yuhan Li, Guan Luo, Shuguang Wang, Qianyi Wu, Fan Yang, Junyang Zhang, Xuanmeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19944">Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing embodied AI agents requires scalable training environments that balance content diversity with physics accuracy. World simulators provide such environments but face distinct limitations: video-based methods generate diverse content but lack real-time physics feedback for interactive learning, while physics-based engines provide accurate dynamics but face scalability limitations from costly manual asset creation. We present Seed3D 1.0, a foundation model that generates simulation-ready 3D assets from single images, addressing the scalability challenge while maintaining physics rigor. Unlike existing 3D generation models, our system produces assets with accurate geometry, well-aligned textures, and realistic physically-based materials. These assets can be directly integrated into physics engines with minimal configuration, enabling deployment in robotic manipulation and simulation training. Beyond individual objects, the system scales to complete scene generation through assembling objects into coherent environments. By enabling scalable simulation-ready content creation, Seed3D 1.0 provides a foundation for advancing physics-based world simulators. Seed3D 1.0 is now available on https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D
<div id='section'>PaperID: <span id='pid'>646, <a href='https://arxiv.org/pdf/2509.14967.pdf' target='_blank'>https://arxiv.org/pdf/2509.14967.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ana Davila, Jacinto Colan, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14967">Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot collaboration in surgery is affected by the inherent ambiguity of verbal communication. This paper presents a framework for a robotic surgical assistant that interprets and disambiguates verbal instructions from a surgeon by grounding them in the visual context of the operating field. The system employs a two-level affordance-based reasoning process that first analyzes the surgical scene using a multimodal vision-language model and then reasons about the instruction using a knowledge base of tool capabilities. To ensure patient safety, a dual-set conformal prediction method is used to provide a statistically rigorous confidence measure for robot decisions, allowing it to identify and flag ambiguous commands. We evaluated our framework on a curated dataset of ambiguous surgical requests from cholecystectomy videos, demonstrating a general disambiguation rate of 60% and presenting a method for safer human-robot interaction in the operating room.
<div id='section'>PaperID: <span id='pid'>647, <a href='https://arxiv.org/pdf/2509.08222.pdf' target='_blank'>https://arxiv.org/pdf/2509.08222.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minjong Yoo, Jinwoo Jang, Wei-jin Park, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08222">Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents an Exploratory Retrieval-Augmented Planning (ExRAP) framework, designed to tackle continual instruction following tasks of embodied agents in dynamic, non-stationary environments. The framework enhances Large Language Models' (LLMs) embodied reasoning capabilities by efficiently exploring the physical environment and establishing the environmental context memory, thereby effectively grounding the task planning process in time-varying environment contexts. In ExRAP, given multiple continual instruction following tasks, each instruction is decomposed into queries on the environmental context memory and task executions conditioned on the query results. To efficiently handle these multiple tasks that are performed continuously and simultaneously, we implement an exploration-integrated task planning scheme by incorporating the {information-based exploration} into the LLM-based planning process. Combined with memory-augmented query evaluation, this integrated scheme not only allows for a better balance between the validity of the environmental context memory and the load of environment exploration, but also improves overall task performance. Furthermore, we devise a {temporal consistency refinement} scheme for query evaluation to address the inherent decay of knowledge in the memory. Through experiments with VirtualHome, ALFRED, and CARLA, our approach demonstrates robustness against a variety of embodied instruction following scenarios involving different instruction scales and types, and non-stationarity degrees, and it consistently outperforms other state-of-the-art LLM-based task planning approaches in terms of both goal success rate and execution efficiency.
<div id='section'>PaperID: <span id='pid'>648, <a href='https://arxiv.org/pdf/2509.03956.pdf' target='_blank'>https://arxiv.org/pdf/2509.03956.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minjong Yoo, Jinwoo Jang, Sihyung Yoon, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03956">World Model Implanting for Test-time Adaptation of Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
<div id='section'>PaperID: <span id='pid'>649, <a href='https://arxiv.org/pdf/2507.23042.pdf' target='_blank'>https://arxiv.org/pdf/2507.23042.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Santosh Patapati, Trisanth Srinivasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23042">Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.
<div id='section'>PaperID: <span id='pid'>650, <a href='https://arxiv.org/pdf/2507.11525.pdf' target='_blank'>https://arxiv.org/pdf/2507.11525.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ana Davila, Jacinto Colan, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11525">LLM-based ambiguity detection in natural language instructions for collaborative surgical robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues. Individual evaluator assessments are synthesized through conformal prediction, which yields non-conformity scores based on comparison to a labeled calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed classification accuracy exceeding 60% in differentiating ambiguous from unambiguous surgical instructions. Our approach improves the safety and reliability of human-robot collaboration in surgery by offering a mechanism to identify potentially ambiguous instructions before robot action.
<div id='section'>PaperID: <span id='pid'>651, <a href='https://arxiv.org/pdf/2410.00425.pdf' target='_blank'>https://arxiv.org/pdf/2410.00425.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Viswesh Nagaswamy Rajesh, Yong Woo Choi, Yen-Ru Chen, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00425">ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.
<div id='section'>PaperID: <span id='pid'>652, <a href='https://arxiv.org/pdf/2602.13710.pdf' target='_blank'>https://arxiv.org/pdf/2602.13710.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Yan, Zhenglin Wan, Feiyang Ye, Xingrui Yu, Hangyu Du, Yang You, Ivor Tsang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13710">HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.
<div id='section'>PaperID: <span id='pid'>653, <a href='https://arxiv.org/pdf/2601.00609.pdf' target='_blank'>https://arxiv.org/pdf/2601.00609.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.00609">NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.
<div id='section'>PaperID: <span id='pid'>654, <a href='https://arxiv.org/pdf/2512.03743.pdf' target='_blank'>https://arxiv.org/pdf/2512.03743.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kehlani Fay, Darin Anthony Djapri, Anya Zorin, James Clinton, Ali El Lahib, Hao Su, Michael T. Tolley, Sha Yi, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03743">Cross-embodied Co-design for Dexterous Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.
<div id='section'>PaperID: <span id='pid'>655, <a href='https://arxiv.org/pdf/2511.15605.pdf' target='_blank'>https://arxiv.org/pdf/2511.15605.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Senyu Fei, Siyin Wang, Li Ji, Ao Li, Shiduo Zhang, Liming Liu, Jinlong Hou, Jingjing Gong, Xianzhong Zhao, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15605">SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.
<div id='section'>PaperID: <span id='pid'>656, <a href='https://arxiv.org/pdf/2511.01331.pdf' target='_blank'>https://arxiv.org/pdf/2511.01331.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongyin Zhang, Shuo Zhang, Junxi Jin, Qixin Zeng, Runze Li, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01331">RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.
<div id='section'>PaperID: <span id='pid'>657, <a href='https://arxiv.org/pdf/2508.07501.pdf' target='_blank'>https://arxiv.org/pdf/2508.07501.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoye Zuo, Nikos Athanasiou, Ginger Delmas, Yiming Huang, Xingyu Fu, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07501">FormCoach: Lift Smarter, Not Harder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Good form is the difference between strength and strain, yet for the fast-growing community of at-home fitness enthusiasts, expert feedback is often out of reach. FormCoach transforms a simple camera into an always-on, interactive AI training partner, capable of spotting subtle form errors and delivering tailored corrections in real time, leveraging vision-language models (VLMs). We showcase this capability through a web interface and benchmark state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference video pairs spanning 22 strength and mobility exercises. To accelerate research in AI-driven coaching, we release both the dataset and an automated, rubric-based evaluation pipeline, enabling standardized comparison across models. Our benchmarks reveal substantial gaps compared to human-level coaching, underscoring both the challenges and opportunities in integrating nuanced, context-aware movement analysis into interactive AI systems. By framing form correction as a collaborative and creative process between humans and machines, FormCoach opens a new frontier in embodied AI.
<div id='section'>PaperID: <span id='pid'>658, <a href='https://arxiv.org/pdf/2507.04452.pdf' target='_blank'>https://arxiv.org/pdf/2507.04452.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingdong Wu, Lehong Wu, Yizhuo Wu, Weiyao Huang, Hongwei Fan, Zheyuan Hu, Haoran Geng, Jinzhou Li, Jiahe Ying, Long Yang, Yuanpei Chen, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04452">SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous learning of dexterous, long-horizon robotic skills has been a longstanding pursuit of embodied AI. Recent advances in robotic reinforcement learning (RL) have demonstrated remarkable performance and robustness in real-world visuomotor control tasks. However, applying RL in the real world faces challenges such as low sample efficiency, slow exploration, and significant reliance on human intervention. In contrast, simulators offer a safe and efficient environment for extensive exploration and data collection, while the visual sim-to-real gap, often a limiting factor, can be mitigated using real-to-sim techniques. Building on these, we propose SimLauncher, a novel framework that combines the strengths of real-world RL and real-to-sim-to-real approaches to overcome these challenges. Specifically, we first pre-train a visuomotor policy in the digital twin simulation environment, which then benefits real-world RL in two ways: (1) bootstrapping target values using extensive simulated demonstrations and real-world demonstrations derived from pre-trained policy rollouts, and (2) Incorporating action proposals from the pre-trained policy for better exploration. We conduct comprehensive experiments across multi-stage, contact-rich, and dexterous hand manipulation tasks. Compared to prior real-world RL approaches, SimLauncher significantly improves sample efficiency and achieves near-perfect success rates. We hope this work serves as a proof of concept and inspires further research on leveraging large-scale simulation pre-training to benefit real-world robotic RL.
<div id='section'>PaperID: <span id='pid'>659, <a href='https://arxiv.org/pdf/2505.10239.pdf' target='_blank'>https://arxiv.org/pdf/2505.10239.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10239">Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In physical human-robot interaction, force feedback has been the most common sensing modality to convey the human intention to the robot. It is widely used in admittance control to allow the human to direct the robot. However, it cannot be used in scenarios where direct force feedback is not available since manipulated objects are not always equipped with a force sensor. In this work, we study one such scenario: the collaborative pushing and pulling of heavy objects on frictional surfaces, a prevalent task in industrial settings. When humans do it, they communicate through verbal and non-verbal cues, where body poses, and movements often convey more than words. We propose a novel context-aware approach using Directed Graph Neural Networks to analyze spatio-temporal human posture data to predict human motion intention for non-verbal collaborative physical manipulation. Our experiments demonstrate that robot assistance significantly reduces human effort and improves task efficiency. The results indicate that incorporating posture-based context recognition, either together with or as an alternative to force sensing, enhances robot decision-making and control efficiency.
<div id='section'>PaperID: <span id='pid'>660, <a href='https://arxiv.org/pdf/2505.10105.pdf' target='_blank'>https://arxiv.org/pdf/2505.10105.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10105">EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EmbodiedMAE, a unified 3D multi-modal representation for robot manipulation. Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information. To overcome these limitations, we enhance the DROID dataset with high-quality depth maps and point clouds, constructing DROID-3D as a valuable supplement for 3D embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion. Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms. The model exhibits strong scaling behavior with size and promotes effective policy learning from 3D inputs. Experimental results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly in precise tabletop manipulation settings where spatial perception is critical.
<div id='section'>PaperID: <span id='pid'>661, <a href='https://arxiv.org/pdf/2505.05495.pdf' target='_blank'>https://arxiv.org/pdf/2505.05495.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyuan Zhou, Yilun Du, Yuncong Yang, Lei Han, Peihao Chen, Dit-Yan Yeung, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05495">Learning 3D Persistent Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.
<div id='section'>PaperID: <span id='pid'>662, <a href='https://arxiv.org/pdf/2504.20995.pdf' target='_blank'>https://arxiv.org/pdf/2504.20995.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20995">TesserAct: Learning 4D Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.
<div id='section'>PaperID: <span id='pid'>663, <a href='https://arxiv.org/pdf/2501.07468.pdf' target='_blank'>https://arxiv.org/pdf/2501.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yihao Liu, Xu Cao, Tingting Chen, Yankai Jiang, Junjie You, Minghua Wu, Xiaosong Wang, Mengling Feng, Yaochu Jin, Jintai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07468">From Screens to Scenes: A Survey of Embodied AI in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Healthcare systems worldwide face persistent challenges in efficiency, accessibility, and personalization. Powered by modern AI technologies such as multimodal large language models and world models, Embodied AI (EmAI) represents a transformative frontier, offering enhanced autonomy and the ability to interact with the physical world to address these challenges. As an interdisciplinary and rapidly evolving research domain, "EmAI in healthcare" spans diverse fields such as algorithms, robotics, and biomedicine. This complexity underscores the importance of timely reviews and analyses to track advancements, address challenges, and foster cross-disciplinary collaboration. In this paper, we provide a comprehensive overview of the "brain" of EmAI for healthcare, wherein we introduce foundational AI algorithms for perception, actuation, planning, and memory, and focus on presenting the healthcare applications spanning clinical interventions, daily care & companionship, infrastructure support, and biomedical research. Despite its promise, the development of EmAI for healthcare is hindered by critical challenges such as safety concerns, gaps between simulation platforms and real-world applications, the absence of standardized benchmarks, and uneven progress across interdisciplinary domains. We discuss the technical barriers and explore ethical considerations, offering a forward-looking perspective on the future of EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI systems is also introduced to guide further development. By providing systematic insights, this work aims to inspire innovation and practical applications, paving the way for a new era of intelligent, patient-centered healthcare.
<div id='section'>PaperID: <span id='pid'>664, <a href='https://arxiv.org/pdf/2412.20977.pdf' target='_blank'>https://arxiv.org/pdf/2412.20977.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fangwei Zhong, Kui Wu, Churan Wang, Hao Chen, Hai Ci, Zhoujun Li, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20977">UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects. UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.
<div id='section'>PaperID: <span id='pid'>665, <a href='https://arxiv.org/pdf/2412.08442.pdf' target='_blank'>https://arxiv.org/pdf/2412.08442.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, Alexander Toshev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08442">From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We examine the capability of Multimodal Large Language Models (MLLMs) to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. Specifically, our focus lies in areas such as Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. GEA is trained with supervised learning on a large dataset of embodied experiences and with online RL in interactive simulators. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents. The final GEA model achieves strong generalization performance to unseen tasks across diverse benchmarks compared to other generalist models and benchmark-specific approaches.
<div id='section'>PaperID: <span id='pid'>666, <a href='https://arxiv.org/pdf/2411.17735.pdf' target='_blank'>https://arxiv.org/pdf/2411.17735.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17735">3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over extended periods. Existing representations, such as object-centric 3D scene graphs, oversimplify spatial relationships by modeling scenes as isolated objects with restrictive textual relationships, making it difficult to address queries requiring nuanced spatial understanding. Moreover, these representations lack natural mechanisms for active exploration and memory management, hindering their application to lifelong autonomy. In this work, we propose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem employs informative multi-view images, termed Memory Snapshots, to represent the scene and capture rich visual information of explored regions. It further integrates frontier-based exploration by introducing Frontier Snapshots-glimpses of unexplored areas-enabling agents to make informed decisions by considering both known and potential new information. To support lifelong memory in active exploration settings, we present an incremental construction pipeline for 3D-Mem, as well as a memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that 3D-Mem significantly enhances agents' exploration and reasoning capabilities in 3D environments, highlighting its potential for advancing applications in embodied AI.
<div id='section'>PaperID: <span id='pid'>667, <a href='https://arxiv.org/pdf/2411.15590.pdf' target='_blank'>https://arxiv.org/pdf/2411.15590.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lixiang Yan, Dragan GaÅ¡eviÄ, Linxuan Zhao, Vanessa Echeverria, Yueqiao Jin, Roberto Martinez-Maldonado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15590">From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Learning Analytics (MMLA) leverages advanced sensing technologies and artificial intelligence to capture complex learning processes, but integrating diverse data sources into cohesive insights remains challenging. This study introduces a novel methodology for integrating latent class analysis (LCA) within MMLA to map monomodal behavioural indicators into parsimonious multimodal ones. Using a high-fidelity healthcare simulation context, we collected positional, audio, and physiological data, deriving 17 monomodal indicators. LCA identified four distinct latent classes: Collaborative Communication, Embodied Collaboration, Distant Interaction, and Solitary Engagement, each capturing unique monomodal patterns. Epistemic network analysis compared these multimodal indicators with the original monomodal indicators and found that the multimodal approach was more parsimonious while offering higher explanatory power regarding students' task and collaboration performances. The findings highlight the potential of LCA in simplifying the analysis of complex multimodal data while capturing nuanced, cross-modality behaviours, offering actionable insights for educators and enhancing the design of collaborative learning interventions. This study proposes a pathway for advancing MMLA, making it more parsimonious and manageable, and aligning with the principles of learner-centred education.
<div id='section'>PaperID: <span id='pid'>668, <a href='https://arxiv.org/pdf/2411.13927.pdf' target='_blank'>https://arxiv.org/pdf/2411.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xueying Jiang, Lewei Lu, Ling Shao, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13927">Multimodal 3D Reasoning Segmentation with Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent development in multimodal learning has greatly advanced the research in 3D scene understanding in various real-world tasks such as embodied AI. However, most existing studies are facing two common challenges: 1) they are short of reasoning ability for interaction and interpretation of human intentions and 2) they focus on scenarios with single-category objects and over-simplified textual descriptions and neglect multi-object scenarios with complicated spatial relations among objects. We address the above challenges by proposing a 3D reasoning segmentation task for reasoning segmentation with multiple objects in scenes. The task allows producing 3D segmentation masks and detailed textual explanations as enriched by 3D spatial relations among objects. To this end, we create ReasonSeg3D, a large-scale and high-quality benchmark that integrates 3D segmentation masks and 3D spatial relations with generated question-answer pairs. In addition, we design MORE3D, a novel 3D reasoning network that works with queries of multiple objects and is tailored for 3D scene understanding. MORE3D learns detailed explanations on 3D relations and employs them to capture spatial information of objects and reason textual outputs. Extensive experiments show that MORE3D excels in reasoning and segmenting complex multi-object 3D scenes. In addition, the created ReasonSeg3D offers a valuable platform for future exploration of 3D reasoning segmentation. The data and code will be released.
<div id='section'>PaperID: <span id='pid'>669, <a href='https://arxiv.org/pdf/2410.03488.pdf' target='_blank'>https://arxiv.org/pdf/2410.03488.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongcheng Wang, Peiqi Liu, Wenzhe Cai, Mingdong Wu, Zhengyu Qian, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03488">MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as ``I am thirsty.'' The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.
<div id='section'>PaperID: <span id='pid'>670, <a href='https://arxiv.org/pdf/2602.03983.pdf' target='_blank'>https://arxiv.org/pdf/2602.03983.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weikang Qiu, Tinglin Huang, Aosong Feng, Rex Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03983">Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.
<div id='section'>PaperID: <span id='pid'>671, <a href='https://arxiv.org/pdf/2601.16065.pdf' target='_blank'>https://arxiv.org/pdf/2601.16065.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan, Yangfan He, Yuchen Li, Jingqun Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16065">DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.
<div id='section'>PaperID: <span id='pid'>672, <a href='https://arxiv.org/pdf/2512.18386.pdf' target='_blank'>https://arxiv.org/pdf/2512.18386.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenhao Hu, Haonan Zhou, Zesheng Li, Liu Liu, Jiacheng Dong, Zhizhong Su, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18386">RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D scene representations have enabled high-fidelity novel view synthesis, yet adapting to discrete scene changes and constructing interactive 3D environments remain open challenges in vision and robotics. Existing approaches focus solely on updating a single scene without supporting novel-state synthesis. Others rely on diffusion-based object-background decoupling that works on one state at a time and cannot fuse information across multiple observations. To address these limitations, we introduce RecurGS, a recurrent fusion framework that incrementally integrates discrete Gaussian scene states into a single evolving representation capable of interaction. RecurGS detects object-level changes across consecutive states, aligns their geometric motion using semantic correspondence and Lie-algebra based SE(3) refinement, and performs recurrent updates that preserve historical structures through replay supervision. A voxelized, visibility-aware fusion module selectively incorporates newly observed regions while keeping stable areas fixed, mitigating catastrophic forgetting and enabling efficient long-horizon updates. RecurGS supports object-level manipulation, synthesizes novel scene states without requiring additional scans, and maintains photorealistic fidelity across evolving environments. Extensive experiments across synthetic and real-world datasets demonstrate that our framework delivers high-quality reconstructions with substantially improved update efficiency, providing a scalable step toward continuously interactive Gaussian worlds.
<div id='section'>PaperID: <span id='pid'>673, <a href='https://arxiv.org/pdf/2512.15047.pdf' target='_blank'>https://arxiv.org/pdf/2512.15047.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15047">HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.
<div id='section'>PaperID: <span id='pid'>674, <a href='https://arxiv.org/pdf/2512.05270.pdf' target='_blank'>https://arxiv.org/pdf/2512.05270.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianyi Wang, Jiseop Byeon, Ahmad Yehia, Huihai Wang, Yiming Xu, Tianyi Zeng, Ziran Wang, Junfeng Jiao, Christian Claudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05270">XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots' inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework's effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.
<div id='section'>PaperID: <span id='pid'>675, <a href='https://arxiv.org/pdf/2512.04308.pdf' target='_blank'>https://arxiv.org/pdf/2512.04308.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lei Zhang, Ju Dong, Kaixin Bai, Minheng Ni, Zoltan-Csaba Marton, Zhaopeng Chen, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04308">ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large multimodal models have enabled new opportunities in embodied AI, particularly in robotic manipulation. These models have shown strong potential in generalization and reasoning, but achieving reliable and responsible robotic behavior in real-world settings remains an open challenge. In high-stakes environments, robotic agents must go beyond basic task execution to perform risk-aware reasoning, moral decision-making, and physically grounded planning. We introduce ResponsibleRobotBench, a systematic benchmark designed to evaluate and accelerate progress in responsible robotic manipulation from simulation to real world. This benchmark consists of 23 multi-stage tasks spanning diverse risk types, including electrical, chemical, and human-related hazards, and varying levels of physical and planning complexity. These tasks require agents to detect and mitigate risks, reason about safety, plan sequences of actions, and engage human assistance when necessary. Our benchmark includes a general-purpose evaluation framework that supports multimodal model-based agents with various action representation modalities. The framework integrates visual perception, context learning, prompt construction, hazard detection, reasoning and planning, and physical execution. It also provides a rich multimodal dataset, supports reproducible experiments, and includes standardized metrics such as success rate, safety rate, and safe success rate. Through extensive experimental setups, ResponsibleRobotBench enables analysis across risk categories, task types, and agent configurations. By emphasizing physical reliability, generalization, and safety in decision-making, this benchmark provides a foundation for advancing the development of trustworthy, real-world responsible dexterous robotic systems. https://sites.google.com/view/responsible-robotbench
<div id='section'>PaperID: <span id='pid'>676, <a href='https://arxiv.org/pdf/2511.21192.pdf' target='_blank'>https://arxiv.org/pdf/2511.21192.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hui Lu, Yi Yu, Yiming Yang, Chenyu Yi, Qixin Zhang, Bingquan Shen, Alex C. Kot, Xudong Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21192">When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.
<div id='section'>PaperID: <span id='pid'>677, <a href='https://arxiv.org/pdf/2511.17925.pdf' target='_blank'>https://arxiv.org/pdf/2511.17925.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jeonghwan Kim, Wontaek Kim, Yidan Lu, Jin Cheng, Fatemeh Zargarbashi, Zicheng Zeng, Zekun Qi, Zhiyang Dou, Nitish Sontakke, Donghoon Baek, Sehoon Ha, Tianyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17925">Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.
<div id='section'>PaperID: <span id='pid'>678, <a href='https://arxiv.org/pdf/2511.02832.pdf' target='_blank'>https://arxiv.org/pdf/2511.02832.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02832">TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .
<div id='section'>PaperID: <span id='pid'>679, <a href='https://arxiv.org/pdf/2510.12370.pdf' target='_blank'>https://arxiv.org/pdf/2510.12370.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenli Shi, Clemence Grislain, Olivier Sigaud, Mohamed Chetouani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12370">Controlling Intent Expressiveness in Robot Motion with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Legibility of robot motion is critical in human-robot interaction, as it allows humans to quickly infer a robot's intended goal. Although traditional trajectory generation methods typically prioritize efficiency, they often fail to make the robot's intentions clear to humans. Meanwhile, existing approaches to legible motion usually produce only a single "most legible" trajectory, overlooking the need to modulate intent expressiveness in different contexts. In this work, we propose a novel motion generation framework that enables controllable legibility across the full spectrum, from highly legible to highly ambiguous motions. We introduce a modeling approach based on an Information Potential Field to assign continuous legibility scores to trajectories, and build upon it with a two-stage diffusion framework that first generates paths at specified legibility levels and then translates them into executable robot actions. Experiments in both 2D and 3D reaching tasks demonstrate that our approach produces diverse and controllable motions with varying degrees of legibility, while achieving performance comparable to SOTA. Code and project page: https://legibility-modulator.github.io.
<div id='section'>PaperID: <span id='pid'>680, <a href='https://arxiv.org/pdf/2510.05865.pdf' target='_blank'>https://arxiv.org/pdf/2510.05865.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lorenzo Baraldi, Zifan Zeng, Chongzhe Zhang, Aradhana Nayak, Hongbo Zhu, Feng Liu, Qunli Zhang, Peng Wang, Shiming Liu, Zheng Hu, Angelo Cangelosi, Lorenzo Baraldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05865">The Safety Challenge of World Models for Embodied AI Agents: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.
<div id='section'>PaperID: <span id='pid'>681, <a href='https://arxiv.org/pdf/2509.06266.pdf' target='_blank'>https://arxiv.org/pdf/2509.06266.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Sitong Mao, Shunbo Zhou, Yong Zhang, Mohammad Akbari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06266">Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.
<div id='section'>PaperID: <span id='pid'>682, <a href='https://arxiv.org/pdf/2508.13444.pdf' target='_blank'>https://arxiv.org/pdf/2508.13444.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianyu Li, Jeonghwan Kim, Wontaek Kim, Donghoon Baek, Seungeun Rho, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13444">Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in whole-body robot control have enabled humanoid and legged robots to execute increasingly agile and coordinated movements. However, standardized benchmarks for evaluating robotic athletic performance in real-world settings and in direct comparison to humans remain scarce. We present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable pipeline that leverages motion-sensing console games to evaluate whole-body robot control policies. Using Just Dance on the Nintendo Switch as a representative example, our system captures, reconstructs, and retargets in-game choreography for robotic execution. We validate the system on a Unitree G1 humanoid with an open-source whole-body controller, establishing a quantitative baseline for the robot's performance against a human player. In the paper, we discuss these results, which demonstrate the feasibility of using commercial games platform as physically grounded benchmarks and motivate future work to for benchmarking embodied AI.
<div id='section'>PaperID: <span id='pid'>683, <a href='https://arxiv.org/pdf/2507.21496.pdf' target='_blank'>https://arxiv.org/pdf/2507.21496.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ryo Terajima, Katsuma Inoue, Kohei Nakajima, Yasuo Kuniyoshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21496">Multifunctional physical reservoir computing in soft tensegrity robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have demonstrated that the dynamics of physical systems can be utilized for the desired information processing under the framework of physical reservoir computing (PRC). Robots with soft bodies are examples of such physical systems, and their nonlinear body-environment dynamics can be used to compute and generate the motor signals necessary for the control of their own behavior. In this simulation study, we extend this approach to control and embed not only one but also multiple behaviors into a type of soft robot called a tensegrity robot. The resulting system, consisting of the robot and the environment, is a multistable dynamical system that converges to different attractors from varying initial conditions. Furthermore, attractor analysis reveals that there exist "untrained attractors" in the state space of the system outside the training data. These untrained attractors reflect the intrinsic properties and structures of the tensegrity robot and its interactions with the environment. The impacts of these recent findings in PRC remain unexplored in embodied AI research. We here illustrate their potential to understand various features of embodied cognition that have not been fully addressed to date.
<div id='section'>PaperID: <span id='pid'>684, <a href='https://arxiv.org/pdf/2504.04573.pdf' target='_blank'>https://arxiv.org/pdf/2504.04573.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jieyi Zhang, Wenqiang Xu, Zhenjun Yu, Pengfei Xie, Tutian Tang, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04573">DexTOG: Learning Task-Oriented Dexterous Grasp with Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a novel language-guided diffusion-based learning framework, DexTOG, aimed at advancing the field of task-oriented grasping (TOG) with dexterous hands. Unlike existing methods that mainly focus on 2-finger grippers, this research addresses the complexities of dexterous manipulation, where the system must identify non-unique optimal grasp poses under specific task constraints, cater to multiple valid grasps, and search in a high degree-of-freedom configuration space in grasp planning. The proposed DexTOG includes a diffusion-based grasp pose generation model, DexDiffu, and a data engine to support the DexDiffu. By leveraging DexTOG, we also proposed a new dataset, DexTOG-80K, which was developed using a shadow robot hand to perform various tasks on 80 objects from 5 categories, showcasing the dexterity and multi-tasking capabilities of the robotic hand. This research not only presents a significant leap in dexterous TOG but also provides a comprehensive dataset and simulation validation, setting a new benchmark in robotic manipulation research.
<div id='section'>PaperID: <span id='pid'>685, <a href='https://arxiv.org/pdf/2504.01252.pdf' target='_blank'>https://arxiv.org/pdf/2504.01252.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kazuhiro Sasabuchi, Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Katsushi Ikeuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01252">Plan-and-Act using Large Language Models for Interactive Agreement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large language models (LLMs) are capable of planning robot actions. In this paper, we explore how LLMs can be used for planning actions with tasks involving situational human-robot interaction (HRI). A key problem of applying LLMs in situational HRI is balancing between "respecting the current human's activity" and "prioritizing the robot's task," as well as understanding the timing of when to use the LLM to generate an action plan. In this paper, we propose a necessary plan-and-act skill design to solve the above problems. We show that a critical factor for enabling a robot to switch between passive / active interaction behavior is to provide the LLM with an action text about the current robot's action. We also show that a second-stage question to the LLM (about the next timing to call the LLM) is necessary for planning actions at an appropriate timing. The skill design is applied to an Engage skill and is tested on four distinct interaction scenarios. We show that by using the skill design, LLMs can be leveraged to easily scale to different HRI scenarios with a reasonable success rate reaching 90% on the test scenarios.
<div id='section'>PaperID: <span id='pid'>686, <a href='https://arxiv.org/pdf/2503.15491.pdf' target='_blank'>https://arxiv.org/pdf/2503.15491.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kazuhiro Sasabuchi, Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Katsushi Ikeuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15491">Agreeing to Interact in Human-Robot Interaction using Large Language Models and Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction (HRI), the beginning of an interaction is often complex. Whether the robot should communicate with the human is dependent on several situational factors (e.g., the current human's activity, urgency of the interaction, etc.). We test whether large language models (LLM) and vision language models (VLM) can provide solutions to this problem. We compare four different system-design patterns using LLMs and VLMs, and test on a test set containing 84 human-robot situations. The test set mixes several publicly available datasets and also includes situations where the appropriate action to take is open-ended. Our results using the GPT-4o and Phi-3 Vision model indicate that LLMs and VLMs are capable of handling interaction beginnings when the desired actions are clear, however, challenge remains in the open-ended situations where the model must balance between the human and robot situation.
<div id='section'>PaperID: <span id='pid'>687, <a href='https://arxiv.org/pdf/2502.16718.pdf' target='_blank'>https://arxiv.org/pdf/2502.16718.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia FermÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16718">NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans' multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at https://www.snehesh.com/natsgld/ to support future HRI research.
<div id='section'>PaperID: <span id='pid'>688, <a href='https://arxiv.org/pdf/2502.07645.pdf' target='_blank'>https://arxiv.org/pdf/2502.07645.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhaoting Li, Rodrigo PÃ©rez-Dattari, Robert Babuska, Cosimo Della Santina, Jens Kober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07645">Beyond Behavior Cloning: Robustness through Interactive Imitation and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Behavior cloning (BC) traditionally relies on demonstration data, assuming the demonstrated actions are optimal. This can lead to overfitting under noisy data, particularly when expressive models are used (e.g., the energy-based model in Implicit BC). To address this, we extend behavior cloning into an iterative process of optimal action estimation within the Interactive Imitation Learning framework. Specifically, we introduce Contrastive policy Learning from Interactive Corrections (CLIC). CLIC leverages human corrections to estimate a set of desired actions and optimizes the policy to select actions from this set. We provide theoretical guarantees for the convergence of the desired action set to optimal actions in both single and multiple optimal action cases. Extensive simulation and real-robot experiments validate CLIC's advantages over existing state-of-the-art methods, including stable training of energy-based models, robustness to feedback noise, and adaptability to diverse feedback types beyond demonstrations. Our code will be publicly available soon.
<div id='section'>PaperID: <span id='pid'>689, <a href='https://arxiv.org/pdf/2412.02075.pdf' target='_blank'>https://arxiv.org/pdf/2412.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Liu Liu, Xinjie Wang, Jiaxiong Qiu, Tianwei Lin, Xiaolin Zhou, Zhizhong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02075">Gaussian Object Carver: Object-Compositional Gaussian Splatting with surfaces completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene reconstruction is a foundational problem in computer vision. Despite recent advancements in Neural Implicit Representations (NIR), existing methods often lack editability and compositional flexibility, limiting their use in scenarios requiring high interactivity and object-level manipulation. In this paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and scalable framework for object-compositional 3D scene reconstruction. GOC leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors and multi-view geometry regularization, to achieve high-quality and flexible reconstruction. Furthermore, we propose a zero-shot Object Surface Completion (OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved surfaces, ensuring object completeness even in occluded areas. Experimental results demonstrate that GOC improves reconstruction efficiency and geometric fidelity. It holds promise for advancing the practical application of digital twins in embodied AI, AR/VR, and interactive simulation environments.
<div id='section'>PaperID: <span id='pid'>690, <a href='https://arxiv.org/pdf/2409.18800.pdf' target='_blank'>https://arxiv.org/pdf/2409.18800.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junyou Zhu, Yanyuan Qiao, Siqi Zhang, Xingjian He, Qi Wu, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18800">MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced rapidly, yet the increasing size of models conflicts with the limited computational capabilities of Embodied AI platforms. To address this challenge, we aim to achieve both high model performance and practical deployability. Specifically, we focus on Vision-and-Language Navigation (VLN), a core task in Embodied AI. This paper introduces a two-stage knowledge distillation framework, producing a student model, MiniVLN, and showcasing the significant potential of distillation techniques in developing lightweight models. The proposed method aims to capture fine-grained knowledge during the pretraining phase and navigation-specific knowledge during the fine-tuning phase. Our findings indicate that the two-stage distillation approach is more effective in narrowing the performance gap between the teacher model and the student model compared to single-stage distillation. On the public R2R and REVERIE benchmarks, MiniVLN achieves performance on par with the teacher model while having only about 12% of the teacher model's parameter count.
<div id='section'>PaperID: <span id='pid'>691, <a href='https://arxiv.org/pdf/2409.13822.pdf' target='_blank'>https://arxiv.org/pdf/2409.13822.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruiqi Wang, Dezhong Zhao, Dayoon Suh, Ziqin Yuan, Guohua Chen, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13822">Personalization in Human-Robot Interaction through Preference-based Action Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preference-based reinforcement learning (PbRL) has shown significant promise for personalization in human-robot interaction (HRI) by explicitly integrating human preferences into the robot learning process. However, existing practices often require training a personalized robot policy from scratch, resulting in inefficient use of human feedback. In this paper, we propose preference-based action representation learning (PbARL), an efficient fine-tuning method that decouples common task structure from preference by leveraging pre-trained robot policies. Instead of directly fine-tuning the pre-trained policy with human preference, PbARL uses it as a reference for an action representation learning task that maximizes the mutual information between the pre-trained source domain and the target user preference-aligned domain. This approach allows the robot to personalize its behaviors while preserving original task performance and eliminates the need for extensive prior information from the source domain, thereby enhancing efficiency and practicality in real-world HRI scenarios. Empirical results on the Assistive Gym benchmark and a real-world user study (N=8) demonstrate the benefits of our method compared to state-of-the-art approaches.
<div id='section'>PaperID: <span id='pid'>692, <a href='https://arxiv.org/pdf/2602.17659.pdf' target='_blank'>https://arxiv.org/pdf/2602.17659.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.17659">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.
<div id='section'>PaperID: <span id='pid'>693, <a href='https://arxiv.org/pdf/2601.15197.pdf' target='_blank'>https://arxiv.org/pdf/2601.15197.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15197">LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose LangForce, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, LangForce significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.
<div id='section'>PaperID: <span id='pid'>694, <a href='https://arxiv.org/pdf/2512.14691.pdf' target='_blank'>https://arxiv.org/pdf/2512.14691.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Wen Xiao, Jiuxiang Gu, Nanyun Peng, Junjie Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14691">MMGR: Multi-Modal Generative Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.
<div id='section'>PaperID: <span id='pid'>695, <a href='https://arxiv.org/pdf/2511.12368.pdf' target='_blank'>https://arxiv.org/pdf/2511.12368.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiqing Shen, Mathias Unberath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12368">Fast Reasoning Segmentation for Images and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.
<div id='section'>PaperID: <span id='pid'>696, <a href='https://arxiv.org/pdf/2511.01571.pdf' target='_blank'>https://arxiv.org/pdf/2511.01571.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai, Ivan Laptev, Salman Khan, Yang Cong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01571">PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.
<div id='section'>PaperID: <span id='pid'>697, <a href='https://arxiv.org/pdf/2509.22353.pdf' target='_blank'>https://arxiv.org/pdf/2509.22353.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fan Wang, Zhiyuan Chen, Yuxuan Zhong, Sunjian Zheng, Pengtao Shao, Bo Yu, Shaoshan Liu, Jianan Wang, Ning Ding, Yang Cao, Yu Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22353">Context and Diversity Matter: The Emergence of In-Context Learning in World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capability of predicting environmental dynamics underpins both biological neural systems and general embodied AI in adapting to their surroundings. Yet prevailing approaches rest on static world models that falter when confronted with novel or rare configurations. We investigate in-context environment learning (ICEL), shifting attention from zero-shot performance to the growth and asymptotic limits of the world model. Our contributions are three-fold: (1) we formalize in-context learning of a world model and identify two core mechanisms: environment recognition and environment learning; (2) we derive error upper-bounds for both mechanisms that expose how the mechanisms emerge; and (3) we empirically confirm that distinct ICL mechanisms exist in the world model, and we further investigate how data distribution and model architecture affect ICL in a manner consistent with theory. These findings demonstrate the potential of self-adapting world models and highlight the key factors behind the emergence of ICEL, most notably the necessity of long context and diverse environments.
<div id='section'>PaperID: <span id='pid'>698, <a href='https://arxiv.org/pdf/2508.03053.pdf' target='_blank'>https://arxiv.org/pdf/2508.03053.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haojun Xu, Jiaqi Xiang, Wu Wei, Jinyu Chen, Linqing Zhong, Linjiang Huang, Hongyu Yang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03053">SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A typical human strategy for giving navigation guidance is to sketch route maps based on the environmental layout. Inspired by this, we introduce Sketch map-based visual Navigation (SkeNa), an embodied navigation task in which an agent must reach a goal in an unseen environment using only a hand-drawn sketch map as guidance. To support research for SkeNa, we present a large-scale dataset named SoR, comprising 54k trajectory and sketch map pairs across 71 indoor scenes. In SoR, we introduce two navigation validation sets with varying levels of abstraction in hand-drawn sketches, categorized based on their preservation of spatial scales in the environment, to facilitate future research. To construct SoR, we develop an automated sketch-generation pipeline that efficiently converts floor plans into hand-drawn representations. To solve SkeNa, we propose SkeNavigator, a navigation framework that aligns visual observations with hand-drawn maps to estimate navigation targets. It employs a Ray-based Map Descriptor (RMD) to enhance sketch map valid feature representation using equidistant sampling points and boundary distances. To improve alignment with visual observations, a Dual-Map Aligned Goal Predictor (DAGP) leverages the correspondence between sketch map features and on-site constructed exploration map features to predict goal position and guide navigation. SkeNavigator outperforms prior floor plan navigation methods by a large margin, improving SPL on the high-abstract validation set by 105% relatively. Our code and dataset will be released.
<div id='section'>PaperID: <span id='pid'>699, <a href='https://arxiv.org/pdf/2507.17376.pdf' target='_blank'>https://arxiv.org/pdf/2507.17376.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianshu Ruan, Aniketh Ramesh, Rustam Stolkin, Manolis Chiou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17376">An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the impact of high-level semantics (evaluation of the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction (HRI) in the context of mobile robot deployments. Although semantics has been widely researched in AI, how high-level semantics can benefit the HRT paradigm is underexplored, often fuzzy, and intractable. We applied a semantics-based framework that could reveal different indicators of the environment (i.e. how much semantic information exists) in a mock-up disaster response mission. In such missions, semantics are crucial as the HRT should handle complex situations and respond quickly with correct decisions, where humans might have a high workload and stress. Especially when human operators need to shift their attention between robots and other tasks, they will struggle to build Situational Awareness (SA) quickly. The experiment suggests that the presented semantics: 1) alleviate the perceived workload of human operators; 2) increase the operator's trust in the SA; and 3) help to reduce the reaction time in switching the level of autonomy when needed. Additionally, we find that participants with higher trust in the system are encouraged by high-level semantics to use teleoperation mode more.
<div id='section'>PaperID: <span id='pid'>700, <a href='https://arxiv.org/pdf/2507.10672.pdf' target='_blank'>https://arxiv.org/pdf/2507.10672.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhayy Ud Din, Waseem Akram, Lyes Saad Saoud, Jan Rosell, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10672">Vision Language Action Models in Robotic Manipulation: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework. This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models. These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems. Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape. Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks. Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies. This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.
<div id='section'>PaperID: <span id='pid'>701, <a href='https://arxiv.org/pdf/2507.06484.pdf' target='_blank'>https://arxiv.org/pdf/2507.06484.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fan-Yun Sun, Shengguang Wu, Christian Jacobsen, Thomas Yim, Haoming Zou, Alex Zook, Shangru Li, Yu-Hsin Chou, Ethem Can, Xunlei Wu, Clemens Eppner, Valts Blukis, Jonathan Tremblay, Jiajun Wu, Stan Birchfield, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06484">3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite large-scale pretraining endowing models with language and vision reasoning capabilities, improving their spatial reasoning capability remains challenging due to the lack of data grounded in the 3D world. While it is possible for humans to manually create immersive and interactive worlds through 3D graphics, as seen in applications such as VR, gaming, and robotics, this process remains highly labor-intensive. In this paper, we propose a scalable method for generating high-quality 3D environments that can serve as training data for foundation models. We recast 3D environment building as a sequential decision-making problem, employing Vision-Language-Models (VLMs) as policies that output actions to jointly craft a 3D environment's layout, materials, lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to generate more prompt-aligned 3D environments via self-improvement fine-tuning. We demonstrate the effectiveness of 3D-Generalist and the proposed training strategy in generating simulation-ready 3D environments. Furthermore, we demonstrate its quality and scalability in synthetic data generation by pretraining a vision foundation model on the generated data. After fine-tuning the pre-trained model on downstream tasks, we show that it surpasses models pre-trained on meticulously human-crafted synthetic data and approaches results achieved with real data orders of magnitude larger.
<div id='section'>PaperID: <span id='pid'>702, <a href='https://arxiv.org/pdf/2506.17811.pdf' target='_blank'>https://arxiv.org/pdf/2506.17811.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jacky Kwok, Christopher Agia, Rohan Sinha, Matt Foutter, Shulu Li, Ion Stoica, Azalia Mirhoseini, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17811">RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in visuomotor control, yet ensuring their robustness in unstructured real-world environments remains a persistent challenge. In this paper, we investigate test-time scaling through the lens of sampling and verification as means to enhance the robustness and generalization of VLAs. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on these insights, we introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbation and majority voting to construct an action proposal distribution, and then uses a Vision Language Model (VLM)-based verifier to select the optimal action. We propose a synthetic data generation pipeline for training such VLM-based action verifiers, and demonstrate that scaling the synthetic dataset consistently improves verification and downstream accuracy. Through extensive simulated and hardware experiments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25% absolute improvement on out-of-distribution tasks and 9% on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7% performance increase compared to fine-tuning VLAs alone.
<div id='section'>PaperID: <span id='pid'>703, <a href='https://arxiv.org/pdf/2506.07570.pdf' target='_blank'>https://arxiv.org/pdf/2506.07570.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yixuan Yang, Zhen Luo, Tongsheng Ding, Junru Lu, Mingqi Gao, Jinyu Yang, Victor Sanchez, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07570">OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.
<div id='section'>PaperID: <span id='pid'>704, <a href='https://arxiv.org/pdf/2505.17610.pdf' target='_blank'>https://arxiv.org/pdf/2505.17610.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Till Freihaut, Luca Viano, Volkan Cevher, Matthieu Geist, Giorgia Ramponi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17610">Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides the first expert sample complexity characterization for learning a Nash equilibrium from expert data in Markov Games. We show that a new quantity named the single policy deviation concentrability coefficient is unavoidable in the non-interactive imitation learning setting, and we provide an upper bound for behavioral cloning (BC) featuring such coefficient. BC exhibits substantial regret in games with high concentrability coefficient, leading us to utilize expert queries to develop and introduce two novel solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response oracle and learns an $\varepsilon$-Nash equilibrium with $\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses completely the best response oracle at the cost of a worse expert query complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide numerical evidence, confirming our theoretical findings.
<div id='section'>PaperID: <span id='pid'>705, <a href='https://arxiv.org/pdf/2505.01458.pdf' target='_blank'>https://arxiv.org/pdf/2505.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01458">A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.
<div id='section'>PaperID: <span id='pid'>706, <a href='https://arxiv.org/pdf/2502.10177.pdf' target='_blank'>https://arxiv.org/pdf/2502.10177.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingcong Lei, Yiming Zhao, Ge Wang, Zhixin Mai, Shuguang Cui, Yatong Han, Jinke Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10177">STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.
<div id='section'>PaperID: <span id='pid'>707, <a href='https://arxiv.org/pdf/2412.19562.pdf' target='_blank'>https://arxiv.org/pdf/2412.19562.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxiao Yang, Shenao Zhang, Zhihan Liu, Huaxiu Yao, Zhaoran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19562">Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on building a task planner for Embodied Instruction Following (EIF) using Large Language Models (LLMs). Previous works typically train a planner to imitate expert trajectories, treating this as a supervised task. While these methods achieve competitive performance, they often lack sufficient robustness. When a suboptimal action is taken, the planner may encounter an out-of-distribution state, which can lead to task failure. In contrast, we frame the task as a Partially Observable Markov Decision Process (POMDP) and aim to develop a robust planner under a few-shot assumption. Thus, we propose a closed-loop planner with an adaptation module and a novel hindsight method, aiming to use as much information as possible to assist the planner. Our experiments on the ALFRED dataset indicate that our planner achieves competitive performance under a few-shot assumption. For the first time, our few-shot agent's performance approaches and even surpasses that of the full-shot supervised agent.
<div id='section'>PaperID: <span id='pid'>708, <a href='https://arxiv.org/pdf/2602.13052.pdf' target='_blank'>https://arxiv.org/pdf/2602.13052.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhonghao Lyu, Ming Xiao, Mikael Skoglund, Merouane Debbah, H. Vincent Poor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13052">Quantization-Aware Collaborative Inference for Large Embodied AI Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.
<div id='section'>PaperID: <span id='pid'>709, <a href='https://arxiv.org/pdf/2512.23482.pdf' target='_blank'>https://arxiv.org/pdf/2512.23482.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marie S. Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23482">Theory of Mind for Explainable Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.
<div id='section'>PaperID: <span id='pid'>710, <a href='https://arxiv.org/pdf/2511.12882.pdf' target='_blank'>https://arxiv.org/pdf/2511.12882.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Taiyi Su, Jian Zhu, Yaxuan Li, Chong Ma, Zitai Huang, Yichen Zhu, Hanli Wang, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12882">Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.
<div id='section'>PaperID: <span id='pid'>711, <a href='https://arxiv.org/pdf/2510.22235.pdf' target='_blank'>https://arxiv.org/pdf/2510.22235.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yixiao Nie, Yang Zhang, Yingjie Jin, Zhepeng Wang, Xiu Li, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22235">CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using Composable Graphs of Thoughts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of self-driving cars and service robots is becoming increasingly prevalent across a wide array of fields, playing a crucial and expanding role in both industrial applications and everyday life. In parallel, the rapid advancements in Large Language Models (LLMs) have garnered substantial attention and interest within the research community. This paper introduces a novel vehicle-robot system that leverages the strengths of both autonomous vehicles and service robots. In our proposed system, two autonomous ego-vehicles transports service robots to locations within an office park, where they perform a series of tasks. The study explores the feasibility and potential benefits of incorporating LLMs into this system, with the aim of enhancing operational efficiency and maximizing the potential of the cooperative mechanisms between the vehicles and the robots. This paper proposes a novel inference mechanism which is called CGOT toward this type of system where an agent can carry another agent. Experimental results are presented to validate the performance of the proposed method.
<div id='section'>PaperID: <span id='pid'>712, <a href='https://arxiv.org/pdf/2510.07975.pdf' target='_blank'>https://arxiv.org/pdf/2510.07975.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingyang Sun, Jiude Wei, Qichen He, Donglin Wang, Cewu Lu, Jianhua Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07975">Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this "semantic-to-physical" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.
<div id='section'>PaperID: <span id='pid'>713, <a href='https://arxiv.org/pdf/2506.01563.pdf' target='_blank'>https://arxiv.org/pdf/2506.01563.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingfan Bao, Yan Pan, Tianhu Peng, Dimitrios Kanoulas, Chengxu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01563">Hierarchical Intention-Aware Expressive Motion Generation for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot interaction requires robots to identify human intentions and generate expressive, socially appropriate motions in real-time. Existing approaches often rely on fixed motion libraries or computationally expensive generative models. We propose a hierarchical framework that combines intention-aware reasoning via in-context learning (ICL) with real-time motion generation using diffusion models. Our system introduces structured prompting with confidence scoring, fallback behaviors, and social context awareness to enable intention refinement and adaptive response. Leveraging large-scale motion datasets and efficient latent-space denoising, the framework generates diverse, physically plausible gestures suitable for dynamic humanoid interactions. Experimental validation on a physical platform demonstrates the robustness and social alignment of our method in realistic scenarios.
<div id='section'>PaperID: <span id='pid'>714, <a href='https://arxiv.org/pdf/2505.23153.pdf' target='_blank'>https://arxiv.org/pdf/2505.23153.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fan Wang, Shaoshan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23153">Conceptual Framework Toward Embodied Collective Adaptive Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.
<div id='section'>PaperID: <span id='pid'>715, <a href='https://arxiv.org/pdf/2505.08213.pdf' target='_blank'>https://arxiv.org/pdf/2505.08213.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junda Huang, Jianshu Zhou, Honghao Guo, Yunhui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08213">HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robotics progresses toward general manipulation, dexterous hands are becoming increasingly critical. However, proprioception in dexterous hands remains a bottleneck due to limitations in volume and generality. In this work, we present HandCept, a novel visual-inertial proprioception framework designed to overcome the challenges of traditional joint angle estimation methods. HandCept addresses the difficulty of achieving accurate and robust joint angle estimation in dynamic environments where both visual and inertial measurements are prone to noise and drift. It leverages a zero-shot learning approach using a wrist-mounted RGB-D camera and 9-axis IMUs, fused in real time via a latency-free Extended Kalman Filter (EKF). Our results show that HandCept achieves joint angle estimation errors between $2^{\circ}$ and $4^{\circ}$ without observable drift, outperforming visual-only and inertial-only methods. Furthermore, we validate the stability and uniformity of the IMU system, demonstrating that a common base frame across IMUs simplifies system calibration. To support sim-to-real transfer, we also open-sourced our high-fidelity rendering pipeline, which is essential for training without real-world ground truth. This work offers a robust, generalizable solution for proprioception in dexterous hands, with significant implications for robotic manipulation and human-robot interaction.
<div id='section'>PaperID: <span id='pid'>716, <a href='https://arxiv.org/pdf/2505.07634.pdf' target='_blank'>https://arxiv.org/pdf/2505.07634.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jian Liu, Xiongtao Shi, Thai Duy Nguyen, Haitian Zhang, Tianxiang Zhang, Wei Sun, Yanjie Li, Athanasios V. Vasilakos, Giovanni Iacca, Arshad Ali Khan, Arvind Kumar, Jae Won Cho, Ajmal Mian, Lihua Xie, Erik Cambria, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07634">Neural Brain: A Neuroscience-inspired Framework for Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.
<div id='section'>PaperID: <span id='pid'>717, <a href='https://arxiv.org/pdf/2503.15764.pdf' target='_blank'>https://arxiv.org/pdf/2503.15764.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yong Xiao, Guangming Shi, Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15764">Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promising potential of AI and network convergence in improving networking performance and enabling new service capabilities has recently attracted significant interest. Existing network AI solutions, while powerful, are mainly built based on the close-loop and passive learning framework, resulting in major limitations in autonomous solution finding and dynamic environmental adaptation. Agentic AI has recently been introduced as a promising solution to address the above limitations and pave the way for true generally intelligent and beneficial AI systems. The key idea is to create a networking ecosystem to support a diverse range of autonomous and embodied AI agents in fulfilling their goals. In this paper, we focus on the novel challenges and requirements of agentic AI networking. We propose AgentNet, a novel framework for supporting interaction, collaborative learning, and knowledge transfer among AI agents. We introduce a general architectural framework of AgentNet and then propose a generative foundation model (GFM)-based implementation in which multiple GFM-as-agents have been created as an interactive knowledge-base to bootstrap the development of embodied AI agents according to different task requirements and environmental features. We consider two application scenarios, digital-twin-based industrial automation and metaverse-based infotainment system, to describe how to apply AgentNet for supporting efficient task-driven collaboration and interaction among AI agents.
<div id='section'>PaperID: <span id='pid'>718, <a href='https://arxiv.org/pdf/2503.01301.pdf' target='_blank'>https://arxiv.org/pdf/2503.01301.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanwen Zou, Junda Huang, Boyuan Liang, Honghao Guo, Zhengyang Liu, Xin Ma, Jianshu Zhou, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01301">Few-shot Sim2Real Based on High Fidelity Rendering with Force Feedback Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperation offers a promising approach to robotic data collection and human-robot interaction. However, existing teleoperation methods for data collection are still limited by efficiency constraints in time and space, and the pipeline for simulation-based data collection remains unclear. The problem is how to enhance task performance while minimizing reliance on real-world data. To address this challenge, we propose a teleoperation pipeline for collecting robotic manipulation data in simulation and training a few-shot sim-to-real visual-motor policy. Force feedback devices are integrated into the teleoperation system to provide precise end-effector gripping force feedback. Experiments across various manipulation tasks demonstrate that force feedback significantly improves both success rates and execution efficiency, particularly in simulation. Furthermore, experiments with different levels of visual rendering quality reveal that enhanced visual realism in simulation substantially boosts task performance while reducing the need for real-world data.
<div id='section'>PaperID: <span id='pid'>719, <a href='https://arxiv.org/pdf/2503.01238.pdf' target='_blank'>https://arxiv.org/pdf/2503.01238.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jensen Gao, Suneel Belkhale, Sudeep Dasari, Ashwin Balakrishna, Dhruv Shah, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01238">A Taxonomy for Evaluating Generalist Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning for robotics promises to unlock generalization to novel tasks and environments. Guided by this promise, many recent works have focused on scaling up robot data collection and developing larger, more expressive policies to achieve this. But how do we measure progress towards this goal of policy generalization in practice? Evaluating and quantifying generalization is the Wild West of modern robotics, with each work proposing and measuring different types of generalization in their own, often difficult to reproduce, settings. In this work, our goal is (1) to outline the forms of generalization we believe are important in robot manipulation in a comprehensive and fine-grained manner, and (2) to provide reproducible guidelines for measuring these notions of generalization. We first propose STAR-Gen, a taxonomy of generalization for robot manipulation structured around visual, semantic, and behavioral generalization. We discuss how our taxonomy encompasses most prior notions of generalization in robotics. Next, we instantiate STAR-Gen with a concrete real-world benchmark based on the widely-used Bridge V2 dataset. We evaluate a variety of state-of-the-art models on this benchmark to demonstrate the utility of our taxonomy in practice. Our taxonomy of generalization can yield many interesting insights into existing models: for example, we observe that current vision-language-action models struggle with various types of semantic generalization, despite the promise of pre-training on internet-scale language datasets. We believe STAR-Gen and our guidelines can improve the dissemination and evaluation of progress towards generalization in robotics, which we hope will guide model design and future data collection efforts. We provide videos and demos at our website stargen-taxonomy.github.io.
<div id='section'>PaperID: <span id='pid'>720, <a href='https://arxiv.org/pdf/2502.16976.pdf' target='_blank'>https://arxiv.org/pdf/2502.16976.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>An-Lan Wang, Nuo Chen, Kun-Yu Lin, Li Yuan-Ming, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16976">Task-Oriented 6-DoF Grasp Pose Detection in Clutters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In general, humans would grasp an object differently for different tasks, e.g., "grasping the handle of a knife to cut" vs. "grasping the blade to hand over". In the field of robotic grasp pose detection research, some existing works consider this task-oriented grasping and made some progress, but they are generally constrained by low-DoF gripper type or non-cluttered setting, which is not applicable for human assistance in real life. With an aim to get more general and practical grasp models, in this paper, we investigate the problem named Task-Oriented 6-DoF Grasp Pose Detection in Clutters (TO6DGC), which extends the task-oriented problem to a more general 6-DOF Grasp Pose Detection in Cluttered (multi-object) scenario. To this end, we construct a large-scale 6-DoF task-oriented grasping dataset, 6-DoF Task Grasp (6DTG), which features 4391 cluttered scenes with over 2 million 6-DoF grasp poses. Each grasp is annotated with a specific task, involving 6 tasks and 198 objects in total. Moreover, we propose One-Stage TaskGrasp (OSTG), a strong baseline to address the TO6DGC problem. Our OSTG adopts a task-oriented point selection strategy to detect where to grasp, and a task-oriented grasp generation module to decide how to grasp given a specific task. To evaluate the effectiveness of OSTG, extensive experiments are conducted on 6DTG. The results show that our method outperforms various baselines on multiple metrics. Real robot experiments also verify that our OSTG has a better perception of the task-oriented grasp points and 6-DoF grasp poses.
<div id='section'>PaperID: <span id='pid'>721, <a href='https://arxiv.org/pdf/2502.09142.pdf' target='_blank'>https://arxiv.org/pdf/2502.09142.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuchong Zhang, Bastian Orthmann, Michael C. Welle, Jonne Van Haastregt, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09142">LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded Robot Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of robotics and augmented reality (AR) presents transformative opportunities for advancing human-robot interaction (HRI) by improving usability, intuitiveness, and accessibility. This work introduces a controller-free, LLM-driven voice-commanded AR puppeteering system, enabling users to teleoperate a robot by manipulating its virtual counterpart in real time. By leveraging natural language processing (NLP) and AR technologies, our system -- prototyped using Meta Quest 3 -- eliminates the need for physical controllers, enhancing ease of use while minimizing potential safety risks associated with direct robot operation. A preliminary user demonstration successfully validated the system's functionality, demonstrating its potential for safer, more intuitive, and immersive robotic control.
<div id='section'>PaperID: <span id='pid'>722, <a href='https://arxiv.org/pdf/2502.04873.pdf' target='_blank'>https://arxiv.org/pdf/2502.04873.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaming Wang, Diwen Liu, Jizhuo Chen, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04873">Training-free Task-oriented Grasp Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a training-free pipeline for task-oriented grasp generation that combines pre-trained grasp generation models with vision-language models (VLMs). Unlike traditional approaches that focus solely on stable grasps, our method incorporates task-specific requirements by leveraging the semantic reasoning capabilities of VLMs. We evaluate five querying strategies, each utilizing different visual representations of candidate grasps, and demonstrate significant improvements over a baseline method in both grasp success and task compliance rates, with absolute gains of up to 36.9\% in overall success rate. Our results underline the potential of VLMs to enhance task-oriented manipulation, providing insights for future research in robotic grasping and human-robot interaction.
<div id='section'>PaperID: <span id='pid'>723, <a href='https://arxiv.org/pdf/2501.13518.pdf' target='_blank'>https://arxiv.org/pdf/2501.13518.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Manuel Benavent-Lledo, David Mulero-PÃ©rez, David Ortiz-Perez, Jose Garcia-Rodriguez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13518">Text-driven Online Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting actions as they occur is essential for applications like video surveillance, autonomous driving, and human-robot interaction. Known as online action detection, this task requires classifying actions in streaming videos, handling background noise, and coping with incomplete actions. Transformer architectures are the current state-of-the-art, yet the potential of recent advancements in computer vision, particularly vision-language models (VLMs), remains largely untapped for this problem, partly due to high computational costs. In this paper, we introduce TOAD: a Text-driven Online Action Detection architecture that supports zero-shot and few-shot learning. TOAD leverages CLIP (Contrastive Language-Image Pretraining) textual embeddings, enabling efficient use of VLMs without significant computational overhead. Our model achieves 82.46% mAP on the THUMOS14 dataset, outperforming existing methods, and sets new baselines for zero-shot and few-shot performance on the THUMOS14 and TVSeries datasets.
<div id='section'>PaperID: <span id='pid'>724, <a href='https://arxiv.org/pdf/2411.05342.pdf' target='_blank'>https://arxiv.org/pdf/2411.05342.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thanh Nguyen Canh, Ba Phuong Nguyen, Hong Quan Tran, Xiem HoangVan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05342">Development of a Human-Robot Interaction Platform for Dual-Arm Robots Based on ROS and Multimodal Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose the development of an interactive platform between humans and a dual-arm robotic system based on the Robot Operating System (ROS) and a multimodal artificial intelligence model. Our proposed platform consists of two main components: a dual-arm robotic hardware system and software that includes image processing tasks and natural language processing using a 3D camera and embedded computing. First, we designed and developed a dual-arm robotic system with a positional accuracy of less than 2 cm, capable of operating independently, performing industrial and service tasks while simultaneously simulating and modeling the robot in the ROS environment. Second, artificial intelligence models for image processing are integrated to execute object picking and classification tasks with an accuracy of over 90%. Finally, we developed remote control software using voice commands through a natural language processing model. Experimental results demonstrate the accuracy of the multimodal artificial intelligence model and the flexibility of the dual-arm robotic system in interactive human environments.
<div id='section'>PaperID: <span id='pid'>725, <a href='https://arxiv.org/pdf/2410.13002.pdf' target='_blank'>https://arxiv.org/pdf/2410.13002.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Makram Chahine, Alex Quach, Alaa Maalouf, Tsun-Hsuan Wang, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13002">Flex: End-to-End Text-Instructed Visual Navigation from Foundation Model Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end learning directly maps sensory inputs to actions, creating highly integrated and efficient policies for complex robotics tasks. However, such models often struggle to generalize beyond their training scenarios, limiting adaptability to new environments, tasks, and concepts. In this work, we investigate the minimal data requirements and architectural adaptations necessary to achieve robust closed-loop performance with vision-based control policies under unseen text instructions and visual distribution shifts. Our findings are synthesized in Flex (Fly lexically), a framework that uses pre-trained Vision Language Models (VLMs) as frozen patch-wise feature extractors, generating spatially aware embeddings that integrate semantic and visual information. We demonstrate the effectiveness of this approach on a quadrotor fly-to-target task, where agents trained via behavior cloning on a small simulated dataset successfully generalize to real-world scenes with diverse novel goals and command formulations.
<div id='section'>PaperID: <span id='pid'>726, <a href='https://arxiv.org/pdf/2409.03966.pdf' target='_blank'>https://arxiv.org/pdf/2409.03966.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongyi Chen, Yunchao Yao, Ruixuan Liu, Changliu Liu, Jeffrey Ichnowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03966">Automating Robot Failure Recovery Using Vision-Language Models With Optimized Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current robot autonomy struggles to operate beyond the assumed Operational Design Domain (ODD), the specific set of conditions and environments in which the system is designed to function, while the real-world is rife with uncertainties that may lead to failures. Automating recovery remains a significant challenge. Traditional methods often rely on human intervention to manually address failures or require exhaustive enumeration of failure cases and the design of specific recovery policies for each scenario, both of which are labor-intensive. Foundational Vision-Language Models (VLMs), which demonstrate remarkable common-sense generalization and reasoning capabilities, have broader, potentially unbounded ODDs. However, limitations in spatial reasoning continue to be a common challenge for many VLMs when applied to robot control and motion-level error recovery. In this paper, we investigate how optimizing visual and text prompts can enhance the spatial reasoning of VLMs, enabling them to function effectively as black-box controllers for both motion-level position correction and task-level recovery from unknown failures. Specifically, the optimizations include identifying key visual elements in visual prompts, highlighting these elements in text prompts for querying, and decomposing the reasoning process for failure detection and control generation. In experiments, prompt optimizations significantly outperform pre-trained Vision-Language-Action Models in correcting motion-level position errors and improve accuracy by 65.78% compared to VLMs with unoptimized prompts. Additionally, for task-level failures, optimized prompts enhanced the success rate by 5.8%, 5.8%, and 7.5% in VLMs' abilities to detect failures, analyze issues, and generate recovery plans, respectively, across a wide range of unknown errors in Lego assembly.
<div id='section'>PaperID: <span id='pid'>727, <a href='https://arxiv.org/pdf/2602.00686.pdf' target='_blank'>https://arxiv.org/pdf/2602.00686.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yujie Wei, Jiahan Fan, Jiyu Guo, Ruichen Zhen, Rui Shao, Xiu Su, Zeke Xie, Shuo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00686">Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.
<div id='section'>PaperID: <span id='pid'>728, <a href='https://arxiv.org/pdf/2602.00500.pdf' target='_blank'>https://arxiv.org/pdf/2602.00500.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia, Rui Shao, Xiu Su, Shuo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00500">Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.
<div id='section'>PaperID: <span id='pid'>729, <a href='https://arxiv.org/pdf/2601.21570.pdf' target='_blank'>https://arxiv.org/pdf/2601.21570.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixing Lei, Genjia Liu, Yuanshuo Zhang, Qipeng Liu, Chuan Wen, Shanghang Zhang, Wenzhao Lian, Siheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21570">EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.
<div id='section'>PaperID: <span id='pid'>730, <a href='https://arxiv.org/pdf/2601.00754.pdf' target='_blank'>https://arxiv.org/pdf/2601.00754.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria Teresa Parreira, Isabel Neto, Filipa Rocha, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.00754">Calling for Backup: How Children Navigate Successive Robot Communication Failures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.
<div id='section'>PaperID: <span id='pid'>731, <a href='https://arxiv.org/pdf/2512.19347.pdf' target='_blank'>https://arxiv.org/pdf/2512.19347.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Han Fang, Yize Huang, Yuheng Zhao, Paul Weng, Xiao Li, Yutong Ban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19347">OMP: One-step Meanflow Policy with Directional Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.
<div id='section'>PaperID: <span id='pid'>732, <a href='https://arxiv.org/pdf/2511.18561.pdf' target='_blank'>https://arxiv.org/pdf/2511.18561.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zitian Peng, Shiyao Zhang, Shanliang Yao, Xiaohui Zhu, Mengjie Huang, Prudence Wong, Yong Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18561">The Evaluation for Usability Methods of Unmanned Surface Vehicles: Are Current Usability Methods Viable for Unmanned Surface Vehicles? Insights from a Multiple Case Study Approach to Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Surface Vehicles (USVs) are increasingly utilised for diverse applications, ranging from environmental monitoring to security patrols. While USV technology is progressing, it remains clear that full autonomy is not achievable in all scenarios, and remote human intervention is still crucial, particularly in dynamic or complex environments. This continued reliance on human intervention highlights a range of Human-Robot Interaction (HRI) challenges that remain unresolved. Compared to the extensive body of HRI research in domains such as unmanned aerial vehicles and autonomous vehicles, HRI considerations specific to USVs remain significantly underexplored. Addressing this gap, our study investigates real-world usability challenges in USV operation through in-depth interviews with 9 engineers and users, supported by field observations. We focus especially on the difficulties beginner operators encounter and their coping strategies. Our findings reveal existing usability issues, mental models, and adaptation strategies of beginners that inform future user-centered design of USV systems, contributing new insights to the emerging field of maritime HRI. Based on these findings, we argue that current USV systems are poorly suited for beginner operation in dynamic inland and offshore environments, where operators must make timely decisions under uncertainty, manage complex spatial awareness, and adapt to changing environmental conditions. Furthermore, we identify key operational patterns in three representative use cases-harmful algal bloom detection, underwater concealed pipe inspection and post-construction hydrographic survey, and summarise key interaction constraints that should inform future maritime HRI design efforts.
<div id='section'>PaperID: <span id='pid'>733, <a href='https://arxiv.org/pdf/2510.09080.pdf' target='_blank'>https://arxiv.org/pdf/2510.09080.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shannon Liu, Maria Teresa Parreira, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09080">Training Models to Detect Successive Robot Errors from Human Reactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become more integrated into society, detecting robot errors is essential for effective human-robot interaction (HRI). When a robot fails repeatedly, how can it know when to change its behavior? Humans naturally respond to robot errors through verbal and nonverbal cues that intensify over successive failures-from confusion and subtle speech changes to visible frustration and impatience. While prior work shows that human reactions can indicate robot failures, few studies examine how these evolving responses reveal successive failures. This research uses machine learning to recognize stages of robot failure from human reactions. In a study with 26 participants interacting with a robot that made repeated conversational errors, behavioral features were extracted from video data to train models for individual users. The best model achieved 93.5% accuracy for detecting errors and 84.1% for classifying successive failures. Modeling the progression of human reactions enhances error detection and understanding of repeated interaction breakdowns in HRI.
<div id='section'>PaperID: <span id='pid'>734, <a href='https://arxiv.org/pdf/2509.05614.pdf' target='_blank'>https://arxiv.org/pdf/2509.05614.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanzhen Wang, Jiaming Xu, Jiayi Pan, Yongkang Zhou, Guohao Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05614">SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.
<div id='section'>PaperID: <span id='pid'>735, <a href='https://arxiv.org/pdf/2508.17753.pdf' target='_blank'>https://arxiv.org/pdf/2508.17753.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Theresa Pekarek Rosin, Julia Gachot, Henri-Leon Kordt, Matthias Kerzel, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17753">Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Speech Recognition (ASR) systems in real-world settings need to handle imperfect audio, often degraded by hardware limitations or environmental noise, while accommodating diverse user groups. In human-robot interaction (HRI), these challenges intersect to create a uniquely challenging recognition environment. We evaluate four state-of-the-art ASR systems on eight publicly available datasets that capture six dimensions of difficulty: domain-specific, accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis demonstrates significant variations in performance, hallucination tendencies, and inherent biases, despite similar scores on standard benchmarks. These limitations have serious implications for HRI, where recognition errors can interfere with task performance, user trust, and safety.
<div id='section'>PaperID: <span id='pid'>736, <a href='https://arxiv.org/pdf/2508.05543.pdf' target='_blank'>https://arxiv.org/pdf/2508.05543.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05543">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings.
<div id='section'>PaperID: <span id='pid'>737, <a href='https://arxiv.org/pdf/2507.12846.pdf' target='_blank'>https://arxiv.org/pdf/2507.12846.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Bandi Jai Krishna, Navid Kayhani, Oriana Peltzer, David D. Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel J. Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12846">Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.
<div id='section'>PaperID: <span id='pid'>738, <a href='https://arxiv.org/pdf/2507.07299.pdf' target='_blank'>https://arxiv.org/pdf/2507.07299.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sonia Raychaudhuri, Enrico Cancelli, Tommaso Campari, Lamberto Ballan, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07299">LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Despite these advances, we still lack a clear, language-focused benchmark for testing how well such agents ground the words in their instructions. We address this gap with LangNav, an open-set dataset specifically created to test an agent's ability to locate objects described at different levels of detail, from broad category names to fine attributes and object-object relations. Every description in LangNav was manually checked, yielding a lower error rate than existing lifelong- and semantic-navigation datasets. On top of LangNav we build LangNavBench, a benchmark that measures how well current semantic-navigation methods understand and act on these descriptions while moving toward their targets. LangNavBench allows us to systematically compare models on their handling of attributes, spatial and relational cues, and category hierarchies, offering the first thorough, language-centric evaluation of embodied navigation systems. We also present Multi-Layered Feature Map (MLFM), a method that builds a queryable multi-layered semantic map, particularly effective when dealing with small objects or instructions involving spatial relations. MLFM outperforms state-of-the-art mapping-based navigation baselines on the LangNav dataset.
<div id='section'>PaperID: <span id='pid'>739, <a href='https://arxiv.org/pdf/2506.17221.pdf' target='_blank'>https://arxiv.org/pdf/2506.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17221">VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.
<div id='section'>PaperID: <span id='pid'>740, <a href='https://arxiv.org/pdf/2506.05095.pdf' target='_blank'>https://arxiv.org/pdf/2506.05095.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaee Cheong, Yang Liu, Harold Soh, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05095">FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing prevalence and deployment of Emotion AI-powered facial affect analysis (FAA) tools, concerns about the trustworthiness of these systems have become more prominent. This first workshop on "Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)" aims to bring together researchers who are investigating different challenges in relation to trustworthiness-such as interpretability, uncertainty, biases, and privacy-across various facial affect analysis tasks, including macro/ micro-expression recognition, facial action unit detection, other corresponding applications such as pain and depression detection, as well as human-robot interaction and collaboration. In alignment with FG2025's emphasis on ethics, as demonstrated by the inclusion of an Ethical Impact Statement requirement for this year's submissions, this workshop supports FG2025's efforts by encouraging research, discussion and dialogue on trustworthy FAA.
<div id='section'>PaperID: <span id='pid'>741, <a href='https://arxiv.org/pdf/2505.09601.pdf' target='_blank'>https://arxiv.org/pdf/2505.09601.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09601">Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com
<div id='section'>PaperID: <span id='pid'>742, <a href='https://arxiv.org/pdf/2503.16492.pdf' target='_blank'>https://arxiv.org/pdf/2503.16492.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuzhi Lai, Shenghai Yuan, Boya Zhang, Benjamin Kiefer, Peizheng Li, Tianchen Deng, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16492">FAM-HRI: Foundation-Model Assisted Multi-Modal Human-Robot Interaction Combining Gaze and Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective Human-Robot Interaction (HRI) is crucial for enhancing accessibility and usability in real-world robotics applications. However, existing solutions often rely on gestures or language commands, making interaction inefficient and ambiguous, particularly for users with physical impairments. In this paper, we introduce FAM-HRI, an efficient multi-modal framework for human-robot interaction that integrates language and gaze inputs via foundation models. By leveraging lightweight Meta ARIA glasses, our system captures real-time multi-modal signals and utilizes large language models (LLMs) to fuse user intention with scene context, enabling intuitive and precise robot manipulation. Our method accurately determines gaze fixation time interval, reducing noise caused by the gaze dynamic nature. Experimental evaluations demonstrate that FAM-HRI achieves a high success rate in task execution while maintaining a low interaction time, providing a practical solution for individuals with limited physical mobility or motor impairments.
<div id='section'>PaperID: <span id='pid'>743, <a href='https://arxiv.org/pdf/2503.14501.pdf' target='_blank'>https://arxiv.org/pdf/2503.14501.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiaowei Miao, Kehan Li, Jinsheng Quan, Zhiyuan Min, Shaojie Ma, Yichao Xu, Yi Yang, Ping Liu, Yawei Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14501">Advances in 4D Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative artificial intelligence has recently progressed from static image and video synthesis to 3D content generation, culminating in the emergence of 4D generation-the task of synthesizing temporally coherent dynamic 3D assets guided by user input. As a burgeoning research frontier, 4D generation enables richer interactive and immersive experiences, with applications ranging from digital humans to autonomous driving. Despite rapid progress, the field lacks a unified understanding of 4D representations, generative frameworks, basic paradigms, and the core technical challenges it faces. This survey provides a systematic and in-depth review of the 4D generation landscape. To comprehensively characterize 4D generation, we first categorize fundamental 4D representations and outline associated techniques for 4D generation. We then present an in-depth analysis of representative generative pipelines based on conditions and representation methods. Subsequently, we discuss how motion and geometry priors are integrated into 4D outputs to ensure spatio-temporal consistency under various control schemes. From an application perspective, this paper summarizes 4D generation tasks in areas such as dynamic object/scene generation, digital human synthesis, editable 4D content, and embodied AI. Furthermore, we summarize and multi-dimensionally compare four basic paradigms for 4D generation: End-to-End, Generated-Data-Based, Implicit-Distillation-Based, and Explicit-Supervision-Based. Concluding our analysis, we highlight five key challenges-consistency, controllability, diversity, efficiency, and fidelity-and contextualize these with current approaches.By distilling recent advances and outlining open problems, this work offers a comprehensive and forward-looking perspective to guide future research in 4D generation.
<div id='section'>PaperID: <span id='pid'>744, <a href='https://arxiv.org/pdf/2502.09379.pdf' target='_blank'>https://arxiv.org/pdf/2502.09379.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jorgen Cani, Panagiotis Koletsis, Konstantinos Foteinos, Ioannis Kefaloukos, Lampros Argyriou, Manolis Falelakis, IvÃ¡n Del Pino, Angel Santamaria-Navarro, Martin Äech, OndÅej Severa, Alessandro Umbrico, Francesca Fracasso, AndreA Orlandini, Dimitrios Drakoulis, Evangelos Markakis, Iraklis Varlamis, Georgios Th. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09379">TRIFFID: Autonomous Robotic Aid For Increasing First Responders Efficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing complexity of natural disaster incidents demands innovative technological solutions to support first responders in their efforts. This paper introduces the TRIFFID system, a comprehensive technical framework that integrates unmanned ground and aerial vehicles with advanced artificial intelligence functionalities to enhance disaster response capabilities across wildfires, urban floods, and post-earthquake search and rescue missions. By leveraging state-of-the-art autonomous navigation, semantic perception, and human-robot interaction technologies, TRIFFID provides a sophisticated system composed of the following key components: hybrid robotic platform, centralized ground station, custom communication infrastructure, and smartphone application. The defined research and development activities demonstrate how deep neural networks, knowledge graphs, and multimodal information fusion can enable robots to autonomously navigate and analyze disaster environments, reducing personnel risks and accelerating response times. The proposed system enhances emergency response teams by providing advanced mission planning, safety monitoring, and adaptive task execution capabilities. Moreover, it ensures real-time situational awareness and operational support in complex and risky situations, facilitating rapid and precise information collection and coordinated actions.
<div id='section'>PaperID: <span id='pid'>745, <a href='https://arxiv.org/pdf/2410.13691.pdf' target='_blank'>https://arxiv.org/pdf/2410.13691.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13691">Jailbreaking LLM-Controlled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles. When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails. To assess the risks of deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from LLM-controlled robots, a phenomenon we experimentally demonstrate in three scenarios: (i) a white-box setting, wherein the attacker has full access to the NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker has partial access to a Clearpath Robotics Jackal UGV robot equipped with a GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each scenario and across three new datasets of harmful robotic actions, we demonstrate that RoboPAIR, as well as several static baselines, finds jailbreaks quickly and effectively, often achieving 100% attack success rates. Our results reveal, for the first time, that the risks of jailbroken LLMs extend far beyond text generation, given the distinct possibility that jailbroken robots could cause physical damage in the real world. Indeed, our results on the Unitree Go2 represent the first successful jailbreak of a deployed commercial robotic system. Addressing this emerging vulnerability is critical for ensuring the safe deployment of LLMs in robotics. Additional media is available at: https://robopair.org
<div id='section'>PaperID: <span id='pid'>746, <a href='https://arxiv.org/pdf/2409.18896.pdf' target='_blank'>https://arxiv.org/pdf/2409.18896.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Denys Iliash, Hanxiao Jiang, Yiming Zhang, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18896">S2O: Static to Openable Enhancement for Articulated 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress in large 3D datasets there are currently few interactive 3D object datasets, and their scale is limited due to the manual effort required in their construction. We introduce the static to openable (S2O) task which creates interactive articulated 3D objects from static counterparts through openable part detection, motion prediction, and interior geometry completion. We formulate a unified framework to tackle this task, and curate a challenging dataset of openable 3D objects that serves as a test bed for systematic evaluation. Our experiments benchmark methods from prior work, extended and improved methods, and simple yet effective heuristics for the S2O task. We find that turning static 3D objects into interactively openable counterparts is possible but that all methods struggle to generalize to realistic settings of the task, and we highlight promising future work directions. Our work enables efficient creation of interactive 3D objects for robotic manipulation and embodied AI tasks.
<div id='section'>PaperID: <span id='pid'>747, <a href='https://arxiv.org/pdf/2409.17621.pdf' target='_blank'>https://arxiv.org/pdf/2409.17621.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiangshan Liu, Wenlong Dong, Jiankun Wang, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17621">Leveraging Semantic and Geometric Information for Zero-Shot Robot-to-Human Handover</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) encompasses a wide range of collaborative tasks, with handover being one of the most fundamental. As robots become more integrated into human environments, the potential for service robots to assist in handing objects to humans is increasingly promising. In robot-to-human (R2H) handover, selecting the optimal grasp is crucial for success, as it requires avoiding interference with the humans preferred grasp region and minimizing intrusion into their workspace. Existing methods either inadequately consider geometric information or rely on data-driven approaches, which often struggle to generalize across diverse objects. To address these limitations, we propose a novel zero-shot system that combines semantic and geometric information to generate optimal handover grasps. Our method first identifies grasp regions using semantic knowledge from vision-language models (VLMs) and, by incorporating customized visual prompts, achieves finer granularity in region grounding. A grasp is then selected based on grasp distance and approach angle to maximize human ease and avoid interference. We validate our approach through ablation studies and real-world comparison experiments. Results demonstrate that our system improves handover success rates and provides a more user-preferred interaction experience. Videos, appendixes and more are available at https://sites.google.com/view/vlm-handover/.
<div id='section'>PaperID: <span id='pid'>748, <a href='https://arxiv.org/pdf/2602.14401.pdf' target='_blank'>https://arxiv.org/pdf/2602.14401.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingqian Yang, Hao Wang, Sai Qian Zhang, Jian Li, Yang Hua, Miao Pan, Tao Song, Zhengwei Qi, Haibing Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.14401">pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.
<div id='section'>PaperID: <span id='pid'>749, <a href='https://arxiv.org/pdf/2602.10377.pdf' target='_blank'>https://arxiv.org/pdf/2602.10377.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luoyang Sun, Jiwen Jiang, Yifeng Ding, Fengfa Li, Yan Song, Haifeng Zhang, Jian Ying, Lei Ren, Kun Zhan, Wei Chen, Yan Xie, Cheng Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.10377">Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.
<div id='section'>PaperID: <span id='pid'>750, <a href='https://arxiv.org/pdf/2601.20835.pdf' target='_blank'>https://arxiv.org/pdf/2601.20835.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20835">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.
<div id='section'>PaperID: <span id='pid'>751, <a href='https://arxiv.org/pdf/2601.07823.pdf' target='_blank'>https://arxiv.org/pdf/2601.07823.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiting Mei, Tenny Yin, Ola Shorinwa, Apurva Badithela, Zhonghe Zheng, Joseph Bruno, Madison Bland, Lihan Zha, Asher Hancock, Jaime Fernández Fisac, Philip Dames, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07823">Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.
<div id='section'>PaperID: <span id='pid'>752, <a href='https://arxiv.org/pdf/2512.24851.pdf' target='_blank'>https://arxiv.org/pdf/2512.24851.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xunyi Zhao, Gengze Zhou, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24851">VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.
<div id='section'>PaperID: <span id='pid'>753, <a href='https://arxiv.org/pdf/2512.21714.pdf' target='_blank'>https://arxiv.org/pdf/2512.21714.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21714">AstraNav-World: World Model for Foresight Control and Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled "envision-then-plan" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.
<div id='section'>PaperID: <span id='pid'>754, <a href='https://arxiv.org/pdf/2512.14666.pdf' target='_blank'>https://arxiv.org/pdf/2512.14666.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zechen Bai, Chen Gao, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14666">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.
<div id='section'>PaperID: <span id='pid'>755, <a href='https://arxiv.org/pdf/2512.14217.pdf' target='_blank'>https://arxiv.org/pdf/2512.14217.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14217">DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.
<div id='section'>PaperID: <span id='pid'>756, <a href='https://arxiv.org/pdf/2512.05964.pdf' target='_blank'>https://arxiv.org/pdf/2512.05964.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kevin Black, Allen Z. Ren, Michael Equi, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05964">Training-Time Action Conditioning for Efficient Real-Time Chunking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.
<div id='section'>PaperID: <span id='pid'>757, <a href='https://arxiv.org/pdf/2512.02569.pdf' target='_blank'>https://arxiv.org/pdf/2512.02569.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuchong Zhang, Yong Ma, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02569">Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.
<div id='section'>PaperID: <span id='pid'>758, <a href='https://arxiv.org/pdf/2512.01550.pdf' target='_blank'>https://arxiv.org/pdf/2512.01550.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fei Liu, Shichao Xie, Minghua Luo, Zedong Chu, Junjun Hu, Xiaolong Wu, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01550">NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.
<div id='section'>PaperID: <span id='pid'>759, <a href='https://arxiv.org/pdf/2511.05936.pdf' target='_blank'>https://arxiv.org/pdf/2511.05936.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Soujanya Poria, Navonil Majumder, Chia-Yu Hung, Amir Ali Bagherzadeh, Chuan Li, Kenneth Kwok, Ziwei Wang, Cheston Tan, Jiajun Wu, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05936">10 Open Challenges Steering the Future of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.
<div id='section'>PaperID: <span id='pid'>760, <a href='https://arxiv.org/pdf/2510.07730.pdf' target='_blank'>https://arxiv.org/pdf/2510.07730.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Changyeon Kim, Haeone Lee, Younggyo Seo, Kimin Lee, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07730">DEAS: DEtached value learning with Action Sequence for Scalable Offline RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.
<div id='section'>PaperID: <span id='pid'>761, <a href='https://arxiv.org/pdf/2510.03153.pdf' target='_blank'>https://arxiv.org/pdf/2510.03153.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hima Jacob Leven Suprabha, Laxmi Nag Laxminarayan Nagesh, Ajith Nair, Alvin Reuben Amal Selvaster, Ayan Khan, Raghuram Damarla, Sanju Hannah Samuel, Sreenithi Saravana Perumal, Titouan Puech, Venkataramireddy Marella, Vishal Sonar, Alessandro Suglia, Oliver Lemon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03153">Improving Cooperation in Collaborative Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.
<div id='section'>PaperID: <span id='pid'>762, <a href='https://arxiv.org/pdf/2509.25852.pdf' target='_blank'>https://arxiv.org/pdf/2509.25852.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zitong Bo, Yue Hu, Jinming Ma, Mingliang Zhou, Junhui Yin, Yachen Kang, Yuqi Liu, Tong Wu, Diyun Xiang, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25852">Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to execute long-horizon manipulation tasks from free-form language instructions remains a fundamental challenge in embodied AI. While vision-language models (VLMs) have shown promise as high-level planners, their deployment in the real world is hindered by two gaps: (i) the scarcity of large-scale, sequential manipulation data that couples natural language with multi-step action plans, and (ii) the absence of dense, interpretable rewards for fine-tuning VLMs on planning objectives. To address these issues, we propose REVER, a framework that empowers VLMs to generate and validate long-horizon manipulation plans from natural language instructions in real-world scenarios. Under REVER we train and release RoboFarseer, a VLM incentivized to emit chain-of-thought that perform temporal and spatial reasoning, ensuring physically plausible and logically coherent plans. To obtain training data, we leverage the Universal Manipulation Interface framework to capture hardware-agnostic demonstrations of atomic skills. An automated annotation engine converts each demonstration into vision-instruction-plan triplet. We introduce a verifiable reward that scores the generated plan by its ordered bipartite matching overlap with the ground-truth skill sequence. At run time, the fine-tuned VLM functions both as a planner and as a monitor, verifying step-wise completion. RoboFarseer matches or exceeds the performance of proprietary models that are orders of magnitude larger, while on open-ended planning it surpasses the best baseline by more than 40%. In real-world, long-horizon tasks, the complete system boosts overall success by roughly 60% compared with the same low-level controller without the planner. We will open-source both the dataset and the trained model upon publication.
<div id='section'>PaperID: <span id='pid'>763, <a href='https://arxiv.org/pdf/2509.25687.pdf' target='_blank'>https://arxiv.org/pdf/2509.25687.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinda Xue, Junjun Hu, Minghua Luo, Xie Shichao, Jintao Chen, Zixun Xie, Quan Kuichen, Guo Wei, Mu Xu, Zedong Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25687">OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.
<div id='section'>PaperID: <span id='pid'>764, <a href='https://arxiv.org/pdf/2509.02761.pdf' target='_blank'>https://arxiv.org/pdf/2509.02761.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-TÃ¼r, Gokhan Tur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02761">Plan Verification for LLM-Based Embodied Task Completion Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.
<div id='section'>PaperID: <span id='pid'>765, <a href='https://arxiv.org/pdf/2506.13189.pdf' target='_blank'>https://arxiv.org/pdf/2506.13189.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuchong Zhang, Bastian Orthmann, Shichen Ji, Michael Welle, Jonne Van Haastregt, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13189">Multimodal "Puppeteer": An Exploration of Robot Teleoperation Via Virtual Counterpart with LLM-Driven Voice and Gesture Interaction in Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of robotics and augmented reality (AR) holds transformative potential for advancing human-robot interaction (HRI), offering enhancements in usability, intuitiveness, accessibility, and collaborative task performance. This paper introduces and evaluates a novel multimodal AR-based robot puppeteer framework that enables intuitive teleoperation via virtual counterpart through large language model (LLM)-driven voice commands and hand gesture interactions. Utilizing the Meta Quest 3, users interact with a virtual counterpart robot in real-time, effectively "puppeteering" its physical counterpart within an AR environment. We conducted a within-subject user study with 42 participants performing robotic cube pick-and-place with pattern matching tasks under two conditions: gesture-only interaction and combined voice-and-gesture interaction. Both objective performance metrics and subjective user experience (UX) measures were assessed, including an extended comparative analysis between roboticists and non-roboticists. The results provide key insights into how multimodal input influences contextual task efficiency, usability, and user satisfaction in AR-based HRI. Our findings offer practical design implications for designing effective AR-enhanced HRI systems.
<div id='section'>PaperID: <span id='pid'>766, <a href='https://arxiv.org/pdf/2506.07339.pdf' target='_blank'>https://arxiv.org/pdf/2506.07339.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kevin Black, Manuel Y. Galliker, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07339">Real-Time Execution of Action Chunking Flow Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, "freezing" actions guaranteed to execute and "inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.
<div id='section'>PaperID: <span id='pid'>767, <a href='https://arxiv.org/pdf/2504.18945.pdf' target='_blank'>https://arxiv.org/pdf/2504.18945.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zishen Wan, Jiayi Qian, Yuhang Du, Jason Jabbour, Yilun Du, Yang Katie Zhao, Arijit Raychowdhury, Tushar Krishna, Vijay Janapa Reddi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18945">Generative AI in Embodied Systems: System-Level Analysis of Performance, Efficiency and Scalability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied systems, where generative autonomous agents engage with the physical world through integrated perception, cognition, action, and advanced reasoning powered by large language models (LLMs), hold immense potential for addressing complex, long-horizon, multi-objective tasks in real-world environments. However, deploying these systems remains challenging due to prolonged runtime latency, limited scalability, and heightened sensitivity, leading to significant system inefficiencies.
  In this paper, we aim to understand the workload characteristics of embodied agent systems and explore optimization solutions. We systematically categorize these systems into four paradigms and conduct benchmarking studies to evaluate their task performance and system efficiency across various modules, agent scales, and embodied tasks. Our benchmarking studies uncover critical challenges, such as prolonged planning and communication latency, redundant agent interactions, complex low-level control mechanisms, memory inconsistencies, exploding prompt lengths, sensitivity to self-correction and execution, sharp declines in success rates, and reduced collaboration efficiency as agent numbers increase. Leveraging these profiling insights, we suggest system optimization strategies to improve the performance, efficiency, and scalability of embodied agents across different paradigms. This paper presents the first system-level analysis of embodied AI agents, and explores opportunities for advancing future embodied system design.
<div id='section'>PaperID: <span id='pid'>768, <a href='https://arxiv.org/pdf/2502.03814.pdf' target='_blank'>https://arxiv.org/pdf/2502.03814.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Peihan Li, Zijian An, Shams Abrar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03814">Large Language Models for Multi-Robot Systems: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source GitHub repository.
<div id='section'>PaperID: <span id='pid'>769, <a href='https://arxiv.org/pdf/2412.05552.pdf' target='_blank'>https://arxiv.org/pdf/2412.05552.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gengze Zhou, Yicong Hong, Zun Wang, Chongyang Zhao, Mohit Bansal, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05552">SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into a unified and generic framework -- we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose a novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present a versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents.
<div id='section'>PaperID: <span id='pid'>770, <a href='https://arxiv.org/pdf/2410.11623.pdf' target='_blank'>https://arxiv.org/pdf/2410.11623.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sijie Cheng, Kechen Fang, Yangyang Yu, Sicheng Zhou, Bohao Li, Ye Tian, Tingguang Li, Lei Han, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11623">VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.
<div id='section'>PaperID: <span id='pid'>771, <a href='https://arxiv.org/pdf/2602.15918.pdf' target='_blank'>https://arxiv.org/pdf/2602.15918.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zelin Xu, Yupu Zhang, Saugat Adhikari, Saiful Islam, Tingsong Xiao, Zibo Liu, Shigang Chen, Da Yan, Zhe Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15918">EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.
<div id='section'>PaperID: <span id='pid'>772, <a href='https://arxiv.org/pdf/2601.21841.pdf' target='_blank'>https://arxiv.org/pdf/2601.21841.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiang Li, Ning Yan, Masood Mortazavi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21841">Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.
<div id='section'>PaperID: <span id='pid'>773, <a href='https://arxiv.org/pdf/2512.10394.pdf' target='_blank'>https://arxiv.org/pdf/2512.10394.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weifan Guan, Huasen Xi, Chenxiao Zhang, Aosheng Li, Qinghao Hu, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10394">RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.
<div id='section'>PaperID: <span id='pid'>774, <a href='https://arxiv.org/pdf/2512.05107.pdf' target='_blank'>https://arxiv.org/pdf/2512.05107.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Feng Xu, Guangyao Zhai, Xin Kong, Tingzhong Fu, Daniel F. N. Gordon, Xueli An, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05107">STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.
<div id='section'>PaperID: <span id='pid'>775, <a href='https://arxiv.org/pdf/2512.02458.pdf' target='_blank'>https://arxiv.org/pdf/2512.02458.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhongyi Cai, Yi Du, Chen Wang, Yu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02458">Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.
<div id='section'>PaperID: <span id='pid'>776, <a href='https://arxiv.org/pdf/2511.20714.pdf' target='_blank'>https://arxiv.org/pdf/2511.20714.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Inferix Team, Tianyu Feng, Yizeng Han, Jiahao He, Yuanyu He, Xi Lin, Teng Liu, Hanfeng Lu, Jiasheng Tang, Wei Wang, Zhiyuan Wang, Jichao Wu, Mingyang Yang, Yinghao Yu, Zeyu Zhang, Bohan Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20714">Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation. Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.
<div id='section'>PaperID: <span id='pid'>777, <a href='https://arxiv.org/pdf/2511.19119.pdf' target='_blank'>https://arxiv.org/pdf/2511.19119.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qirui Wang, Jingyi He, Yining Pan, Si Yong Yeo, Xulei Yang, Shijie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19119">MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.
<div id='section'>PaperID: <span id='pid'>778, <a href='https://arxiv.org/pdf/2510.24680.pdf' target='_blank'>https://arxiv.org/pdf/2510.24680.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zishuo Wang, Joel Loo, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24680">Fare: Failure Resilience in Learned Visual Navigation Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While imitation learning (IL) enables effective visual navigation, IL policies are prone to unpredictable failures in out-of-distribution (OOD) scenarios. We advance the notion of failure-resilient policies, which not only detect failures but also recover from them automatically. Failure recognition that identifies the factors causing failure is key to informing recovery: e.g. pinpointing image regions triggering failure detections can provide cues to guide recovery. We present Fare, a framework to construct failure-resilient IL policies, embedding OOD-detection and recognition in them without using explicit failure data, and pairing them with recovery heuristics. Real-world experiments show that Fare enables failure recovery across two different policy architectures, enabling robust long-range navigation in complex environments.
<div id='section'>PaperID: <span id='pid'>779, <a href='https://arxiv.org/pdf/2510.17111.pdf' target='_blank'>https://arxiv.org/pdf/2510.17111.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weifan Guan, Qinghao Hu, Aosheng Li, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17111">Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.
<div id='section'>PaperID: <span id='pid'>780, <a href='https://arxiv.org/pdf/2510.13375.pdf' target='_blank'>https://arxiv.org/pdf/2510.13375.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13375">DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator. Our code will be made publicly available.
<div id='section'>PaperID: <span id='pid'>781, <a href='https://arxiv.org/pdf/2510.04365.pdf' target='_blank'>https://arxiv.org/pdf/2510.04365.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhao Luo, Yuang Zhang, Kehua Chen, Xinyu Zheng, Shucheng Zhang, Sikai Chen, Yinhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04365">Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate pedestrian trajectory prediction is crucial for ensuring safety and efficiency in autonomous driving and human-robot interaction scenarios. Earlier studies primarily utilized sufficient observational data to predict future trajectories. However, in real-world scenarios, such as pedestrians suddenly emerging from blind spots, sufficient observational data is often unavailable (i.e. momentary trajectory), making accurate prediction challenging and increasing the risk of traffic accidents. Therefore, advancing research on pedestrian trajectory prediction under extreme scenarios is critical for enhancing traffic safety. In this work, we propose a novel framework termed Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists of two sequentially connected diffusion models: one for backward prediction, which generates unobserved historical trajectories, and the other for forward prediction, which forecasts future trajectories. Given that the generated unobserved historical trajectories may introduce additional noise, we propose a dual-head parameterization mechanism to estimate their aleatoric uncertainty and design a temporally adaptive noise module that dynamically modulates the noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.
<div id='section'>PaperID: <span id='pid'>782, <a href='https://arxiv.org/pdf/2509.11417.pdf' target='_blank'>https://arxiv.org/pdf/2509.11417.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shresth Grover, Akshay Gopalkrishnan, Bo Ai, Henrik I. Christensen, Hao Su, Xuanlin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11417">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.
<div id='section'>PaperID: <span id='pid'>783, <a href='https://arxiv.org/pdf/2509.01996.pdf' target='_blank'>https://arxiv.org/pdf/2509.01996.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chi Sun, Xian Wang, Abhishek Kumar, Chengbin Cui, Lik-Hang Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01996">MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot interaction (HRI) in multi-object teleoperation tasks faces significant challenges due to perceptual ambiguities in virtual reality (VR) environments and the limitations of single-modality intention recognition. This paper proposes a shared control framework that combines a virtual admittance (VA) model with a Multimodal-CNN-based Human Intention Perception Network (MMIPN) to enhance teleoperation performance and user experience. The VA model employs artificial potential fields to guide operators toward target objects by adjusting admittance force and optimizing motion trajectories. MMIPN processes multimodal inputs, including gaze movement, robot motions, and environmental context, to estimate human grasping intentions, helping to overcome depth perception challenges in VR. Our user study evaluated four conditions across two factors, and the results showed that MMIPN significantly improved grasp success rates, while the VA model enhanced movement efficiency by reducing path lengths. Gaze data emerged as the most crucial input modality. These findings demonstrate the effectiveness of combining multimodal cues with implicit guidance in VR-based teleoperation, providing a robust solution for multi-object grasping tasks and enabling more natural interactions across various applications in the future.
<div id='section'>PaperID: <span id='pid'>784, <a href='https://arxiv.org/pdf/2506.01196.pdf' target='_blank'>https://arxiv.org/pdf/2506.01196.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ishika Singh, Ankit Goyal, Stan Birchfield, Dieter Fox, Animesh Garg, Valts Blukis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01196">OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and multi-view RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA projects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/
<div id='section'>PaperID: <span id='pid'>785, <a href='https://arxiv.org/pdf/2505.08664.pdf' target='_blank'>https://arxiv.org/pdf/2505.08664.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Valerio Belcamino, Alessandro CarfÃ¬, Valeria Seidita, Fulvio Mastrogiovanni, Antonio Chella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08664">A Social Robot with Inner Speech for Dietary Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the use of inner speech as a mechanism to enhance transparency and trust in social robots for dietary advice. In humans, inner speech structures thought processes and decision-making; in robotics, it improves explainability by making reasoning explicit. This is crucial in healthcare scenarios, where trust in robotic assistants depends on both accurate recommendations and human-like dialogue, which make interactions more natural and engaging. Building on this, we developed a social robot that provides dietary advice, and we provided the architecture with inner speech capabilities to validate user input, refine reasoning, and generate clear justifications. The system integrates large language models for natural language understanding and a knowledge graph for structured dietary information. By making decisions more transparent, our approach strengthens trust and improves human-robot interaction in healthcare. We validated this by measuring the computational efficiency of our architecture and conducting a small user study, which assessed the reliability of inner speech in explaining the robot's behavior.
<div id='section'>PaperID: <span id='pid'>786, <a href='https://arxiv.org/pdf/2504.09778.pdf' target='_blank'>https://arxiv.org/pdf/2504.09778.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kevin Farias, Pablo Moraes, Igor Nunes, Juan Deniz, Sebastian Barcelona, Hiago Sodre, William Moraes, Monica Rodriguez, Ahilen Mazondo, Vincent Sandin, Gabriel da Silva, Victoria Saravia, Vinicio Melgar, Santiago Fernandez, Ricardo Grando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09778">RoboCup Rescue 2025 Team Description Paper UruBots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes the approach used by Team UruBots for participation in the 2025 RoboCup Rescue Robot League competition. Our team aims to participate for the first time in this competition at RoboCup, using experience learned from previous competitions and research. We present our vehicle and our approach to tackle the task of detecting and finding victims in search and rescue environments. Our approach contains known topics in robotics, such as ROS, SLAM, Human Robot Interaction and segmentation and perception. Our proposed approach is open source, available to the RoboCup Rescue community, where we aim to learn and contribute to the league.
<div id='section'>PaperID: <span id='pid'>787, <a href='https://arxiv.org/pdf/2504.06167.pdf' target='_blank'>https://arxiv.org/pdf/2504.06167.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanfang Lyu, Xiaoyu Wang, Nandi Zhang, Shuai Ma, Qian Zhu, Yuhan Luo, Fugee Tsung, Xiaojuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06167">Signaling Human Intentions to Service Robots: Understanding the Use of Social Cues during In-Person Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As social service robots become commonplace, it is essential for them to effectively interpret human signals, such as verbal, gesture, and eye gaze, when people need to focus on their primary tasks to minimize interruptions and distractions. Toward such a socially acceptable Human-Robot Interaction, we conducted a study ($N=24$) in an AR-simulated context of a coffee chat. Participants elicited social cues to signal intentions to an anthropomorphic, zoomorphic, grounded technical, or aerial technical robot waiter when they were speakers or listeners. Our findings reveal common patterns of social cues over intentions, the effects of robot morphology on social cue position and conversational role on social cue complexity, and users' rationale in choosing social cues. We offer insights into understanding social cues concerning perceptions of robots, cognitive load, and social context. Additionally, we discuss design considerations on approaching, social cue recognition, and response strategies for future service robots.
<div id='section'>PaperID: <span id='pid'>788, <a href='https://arxiv.org/pdf/2504.01588.pdf' target='_blank'>https://arxiv.org/pdf/2504.01588.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luca Garello, Giulia Belgiovine, Gabriele Russo, Francesco Rea, Alessandra Sciutti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01588">Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating robotics into everyday scenarios like tutoring or physical training requires robots capable of adaptive, socially engaging, and goal-oriented interactions. While Large Language Models show promise in human-like communication, their standalone use is hindered by memory constraints and contextual incoherence. This work presents a multimodal, cognitively inspired framework that enhances LLM-based autonomous decision-making in social and task-oriented Human-Robot Interaction. Specifically, we develop an LLM-based agent for a robot trainer, balancing social conversation with task guidance and goal-driven motivation. To further enhance autonomy and personalization, we introduce a memory system for selecting, storing and retrieving experiences, facilitating generalized reasoning based on knowledge built across different interactions. A preliminary HRI user study and offline experiments with a synthetic dataset validate our approach, demonstrating the system's ability to manage complex interactions, autonomously drive training tasks, and build and retrieve contextual memories, advancing socially intelligent robotics.
<div id='section'>PaperID: <span id='pid'>789, <a href='https://arxiv.org/pdf/2412.04728.pdf' target='_blank'>https://arxiv.org/pdf/2412.04728.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinyan Yu, Yiyuan Wang, Tram Thi Minh Tran, Yi Zhao, Julie Stephany Berrio Perez, Marius Hoggenmuller, Justine Humphry, Lian Loke, Lynn Masuda, Callum Parker, Martin Tomitsch, Stewart Worrall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04728">Robots in the Wild: Contextually-Adaptive Human-Robot Interactions in Urban Public Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing transition of human-robot interaction (HRI) context from controlled settings to dynamic, real-world public environments calls for enhanced adaptability in robotic systems. This can go beyond algorithmic navigation or traditional HRI strategies in structured settings, requiring the ability to navigate complex public urban systems containing multifaceted dynamics and various socio-technical needs. Therefore, our proposed workshop seeks to extend the boundaries of adaptive HRI research beyond predictable, semi-structured contexts and highlight opportunities for adaptable robot interactions in urban public environments. This half-day workshop aims to explore design opportunities and challenges in creating contextually-adaptive HRI within these spaces and establish a network of interested parties within the OzCHI research community. By fostering ongoing discussions, sharing of insights, and collaborations, we aim to catalyse future research that empowers robots to navigate the inherent uncertainties and complexities of real-world public interactions.
<div id='section'>PaperID: <span id='pid'>790, <a href='https://arxiv.org/pdf/2410.11402.pdf' target='_blank'>https://arxiv.org/pdf/2410.11402.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sixu Yan, Zeyu Zhang, Muzhi Han, Zaijin Wang, Qi Xie, Zhitian Li, Zhehan Li, Hangxin Liu, Xinggang Wang, Song-Chun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11402">M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.
<div id='section'>PaperID: <span id='pid'>791, <a href='https://arxiv.org/pdf/2602.09765.pdf' target='_blank'>https://arxiv.org/pdf/2602.09765.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xijie Huang, Weiqi Gai, Tianyue Wu, Congyu Wang, Zhiyang Liu, Xin Zhou, Yuze Wu, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.09765">NavDreamer: Video Models as Zero-Shot 3D Navigators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.
<div id='section'>PaperID: <span id='pid'>792, <a href='https://arxiv.org/pdf/2602.09600.pdf' target='_blank'>https://arxiv.org/pdf/2602.09600.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxi Wang, Wenqi Ouyang, Tianyi Wei, Yi Dong, Zhiqi Shen, Xingang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.09600">Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.
<div id='section'>PaperID: <span id='pid'>793, <a href='https://arxiv.org/pdf/2602.05233.pdf' target='_blank'>https://arxiv.org/pdf/2602.05233.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang, Chang Xu, Jiaolong Yang, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.05233">MobileManiBench: Simplifying Model Verification for Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.
<div id='section'>PaperID: <span id='pid'>794, <a href='https://arxiv.org/pdf/2601.04596.pdf' target='_blank'>https://arxiv.org/pdf/2601.04596.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinyan Yu, Marius Hoggenmüller, Tram Thi Minh Tran, Martin Tomitsch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04596">Feel the Presence: The Effects of Haptic Sensation on VR-Based Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual reality (VR) has been increasingly utilised as a simulation tool for human-robot interaction (HRI) studies due to its ability to facilitate fast and flexible prototyping. Despite efforts to achieve high validity in VR studies, haptic sensation, an essential sensory modality for perception and a critical factor in enhancing VR realism, is often absent from these experiments. Studying an interactive robot help-seeking scenario, we used a VR simulation with haptic gloves that provide highly realistic tactile and force feedback to examine the effects of haptic sensation on VR-based HRI. We compared participants' sense of presence and their assessments of the robot to a traditional setup using hand controllers. Our results indicate that haptic sensation enhanced participants' social and self-presence in VR and fostered more diverse and natural bodily engagement. Additionally, haptic sensations significantly influenced participants' affective-related perceptions of the robot. Our study provides insights to guide HRI researchers in building VR-based simulations that better align with their study contexts and objectives.
<div id='section'>PaperID: <span id='pid'>795, <a href='https://arxiv.org/pdf/2601.02167.pdf' target='_blank'>https://arxiv.org/pdf/2601.02167.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei He, Xiang Li, Per Ola Kristensson, Ge Lin Kan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02167">LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual locomotion remains a challenge in VR, especially in space-limited environments where room-scale walking is impractical. We present LocoScooter, a low-cost, deployable locomotion interface combining foot-sliding on a compact treadmill with handlebar steering inspired by scooter riding. Built from commodity hardware, it supports embodied navigation through familiar, physically engaging movement. In a within-subject study (N = 14), LocoScooter significantly improved immersion, enjoyment, and bodily involvement over joystick navigation, while maintaining comparable efficiency and usability. Despite higher physical demand, users did not report increased fatigue, suggesting familiar movements can enrich VR navigation.
<div id='section'>PaperID: <span id='pid'>796, <a href='https://arxiv.org/pdf/2512.08518.pdf' target='_blank'>https://arxiv.org/pdf/2512.08518.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nadezhda Kushina, Ko Watanabe, Aarthi Kannan, Ashita Ashok, Andreas Dengel, Karsten Berns
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08518">SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.
<div id='section'>PaperID: <span id='pid'>797, <a href='https://arxiv.org/pdf/2510.21302.pdf' target='_blank'>https://arxiv.org/pdf/2510.21302.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sanghyun Ahn, Wonje Choi, Junyong Lee, Jinwoo Park, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21302">Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2% over Code-as-Policies baselines and attains over 86.8% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments.
<div id='section'>PaperID: <span id='pid'>798, <a href='https://arxiv.org/pdf/2510.19429.pdf' target='_blank'>https://arxiv.org/pdf/2510.19429.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wonje Choi, Jooyoung Kim, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19429">NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of adopting language models (LMs) for embodied tasks in dynamic environments, where online access to large-scale inference engines or symbolic planners is constrained due to latency, connectivity, and resource limitations. To this end, we present NeSyPr, a novel embodied reasoning framework that compiles knowledge via neurosymbolic proceduralization, thereby equipping LM-based agents with structured, adaptive, and timely reasoning capabilities. In NeSyPr, task-specific plans are first explicitly generated by a symbolic tool leveraging its declarative knowledge. These plans are then transformed into composable procedural representations that encode the plans' implicit production rules, enabling the resulting composed procedures to be seamlessly integrated into the LM's inference process. This neurosymbolic proceduralization abstracts and generalizes multi-step symbolic structured path-finding and reasoning into single-step LM inference, akin to human knowledge compilation. It supports efficient test-time inference without relying on external symbolic guidance, making it well suited for deployment in latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating its efficient reasoning capabilities over large-scale reasoning models and a symbolic planner, while using more compact LMs.
<div id='section'>PaperID: <span id='pid'>799, <a href='https://arxiv.org/pdf/2510.11878.pdf' target='_blank'>https://arxiv.org/pdf/2510.11878.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anastasiya Pechko, Piotr Borycki, Joanna Waczyńska, Daniel Barczyk, Agata Szymańska, Sławomir Tadeja, Przemysław Spurek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11878">GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce \our{} (\textbf{G}aussian \textbf{S}platting for \textbf{V}irtual \textbf{E}nvironment \textbf{R}endering and \textbf{S}cene \textbf{E}diting), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, \our{} facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.
<div id='section'>PaperID: <span id='pid'>800, <a href='https://arxiv.org/pdf/2510.07077.pdf' target='_blank'>https://arxiv.org/pdf/2510.07077.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07077">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .
<div id='section'>PaperID: <span id='pid'>801, <a href='https://arxiv.org/pdf/2509.25139.pdf' target='_blank'>https://arxiv.org/pdf/2509.25139.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yue Zhang, Tianyi Ma, Zun Wang, Yanyuan Qiao, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25139">Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.
<div id='section'>PaperID: <span id='pid'>802, <a href='https://arxiv.org/pdf/2509.18447.pdf' target='_blank'>https://arxiv.org/pdf/2509.18447.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rishabh Madan, Jiawei Lin, Mahika Goel, Angchen Xie, Xiaoyu Liang, Marcus Lee, Justin Guo, Pranav N. Thakkar, Rohan Banerjee, Jose Barreiros, Kate Tsui, Tom Silver, Tapomayukh Bhattacharjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18447">PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives--trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.
<div id='section'>PaperID: <span id='pid'>803, <a href='https://arxiv.org/pdf/2507.16124.pdf' target='_blank'>https://arxiv.org/pdf/2507.16124.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16124">Benchmarking LLM Privacy Recognition for Social Robot Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.
<div id='section'>PaperID: <span id='pid'>804, <a href='https://arxiv.org/pdf/2506.22355.pdf' target='_blank'>https://arxiv.org/pdf/2506.22355.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, ThÃ©o Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22355">Embodied AI Agents: Modeling the World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
<div id='section'>PaperID: <span id='pid'>805, <a href='https://arxiv.org/pdf/2506.01031.pdf' target='_blank'>https://arxiv.org/pdf/2506.01031.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanyuan Qiao, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie, Xinyu Wang, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01031">NavBench: Probing Multimodal Large Language Models for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.
<div id='section'>PaperID: <span id='pid'>806, <a href='https://arxiv.org/pdf/2504.16373.pdf' target='_blank'>https://arxiv.org/pdf/2504.16373.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yasra Chandio, Diana Romero, Salma Elmalaki, Fatima Anwar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16373">What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain if sensor and system-logged behavioral signals capture how users experience that collaboration. This disconnect stems from a fundamental gap: behavioral signals are observable and continuous, while collaboration is interpreted subjectively, shaped by internal states like presence, cognitive availability, and social awareness. Our core insight is that sensor signals serve as observable manifestations of subjective experiences in MR collaboration, and they can be captured through sensor data such as shared gaze, speech, spatial movement, and other system-logged performance metrics. We propose the Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links observable interaction patterns to users' subjective perceptions of collaboration and internal cognitive states through sensor-based indicators and task performance metrics. To validate this model, we conducted a study with 48 participants across 12 MR groups engaged in a collaborative image-sorting task. Our findings show a correlation between sensed behavior and perceived collaboration, particularly through shared attention and proximity.
<div id='section'>PaperID: <span id='pid'>807, <a href='https://arxiv.org/pdf/2504.13944.pdf' target='_blank'>https://arxiv.org/pdf/2504.13944.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tace McNamara, Jon McCormack, Maria Teresa Llano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13944">Mixer Metaphors: audio interfaces for non-musical applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The NIME conference traditionally focuses on interfaces for music and musical expression. In this paper we reverse this tradition to ask, can interfaces developed for music be successfully appropriated to non-musical applications? To help answer this question we designed and developed a new device, which uses interface metaphors borrowed from analogue synthesisers and audio mixing to physically control the intangible aspects of a Large Language Model. We compared two versions of the device, with and without the audio-inspired augmentations, with a group of artists who used each version over a one week period. Our results show that the use of audio-like controls afforded more immediate, direct and embodied control over the LLM, allowing users to creatively experiment and play with the device over its non-mixer counterpart. Our project demonstrates how cross-sensory metaphors can support creative thinking and embodied practice when designing new technological interfaces.
<div id='section'>PaperID: <span id='pid'>808, <a href='https://arxiv.org/pdf/2503.21767.pdf' target='_blank'>https://arxiv.org/pdf/2503.21767.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21767">Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary 3D scene understanding is crucial for robotics applications, such as natural language-driven manipulation, human-robot interaction, and autonomous navigation. Existing methods for querying 3D Gaussian Splatting often struggle with inconsistent 2D mask supervision and lack a robust 3D point-level retrieval mechanism. In this work, (i) we present a novel point-level querying framework that performs tracking on segmentation masks to establish a semantically consistent ground-truth for distilling the language Gaussians; (ii) we introduce a GT-anchored querying approach that first retrieves the distilled ground-truth and subsequently uses the ground-truth to query the individual Gaussians. Extensive experiments on three benchmark datasets demonstrate that the proposed method outperforms state-of-the-art performance. Our method achieves an mIoU improvement of +4.14, +20.42, and +1.7 on the LERF, 3D-OVS, and Replica datasets. These results validate our framework as a promising step toward open-vocabulary understanding in real-world robotic systems.
<div id='section'>PaperID: <span id='pid'>809, <a href='https://arxiv.org/pdf/2503.09758.pdf' target='_blank'>https://arxiv.org/pdf/2503.09758.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weizheng Wang, Ike Obi, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09758">Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in robotics and large language models (LLMs) have sparked growing interest in human-robot collaboration and embodied intelligence. To enable the broader deployment of robots in human-populated environments, socially-aware robot navigation (SAN) has become a key research area. While deep reinforcement learning approaches that integrate human-robot interaction (HRI) with path planning have demonstrated strong benchmark performance, they often struggle to adapt to new scenarios and environments. LLMs offer a promising avenue for zero-shot navigation through commonsense inference. However, most existing LLM-based frameworks rely on centralized decision-making, lack robust verification mechanisms, and face inconsistencies in translating macro-actions into precise low-level control signals. To address these challenges, we propose SAMALM, a decentralized multi-agent LLM actor-critic framework for multi-robot social navigation. In this framework, a set of parallel LLM actors, each reflecting distinct robot personalities or configurations, directly generate control signals. These actions undergo a two-tier verification process via a global critic that evaluates group-level behaviors and individual critics that assess each robot's context. An entropy-based score fusion mechanism further enhances self-verification and re-query, improving both robustness and coordination. Experimental results confirm that SAMALM effectively balances local autonomy with global oversight, yielding socially compliant behaviors and strong adaptability across diverse multi-robot scenarios. More details and videos about this work are available at: https://sites.google.com/view/SAMALM.
<div id='section'>PaperID: <span id='pid'>810, <a href='https://arxiv.org/pdf/2503.02280.pdf' target='_blank'>https://arxiv.org/pdf/2503.02280.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Carolina Silva-Plata, Carlos Rosel, Barnabas Gavin Cangan, Hosam Alagi, BjÃ¶rn Hein, Robert K. Katzschmann, RubÃ©n FernÃ¡ndez, Yosra Mojtahedi, Stefan Escaida Navarro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02280">Model-Based Capacitive Touch Sensing in Soft Robotics: Achieving Robust Tactile Interactions for Artistic Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a touch technology to achieve tactile interactivity for human-robot interaction (HRI) in soft robotics. By combining a capacitive touch sensor with an online solid mechanics simulation provided by the SOFA framework, contact detection is achieved for arbitrary shapes. Furthermore, the implementation of the capacitive touch technology presented here is selectively sensitive to human touch (conductive objects), while it is largely unaffected by the deformations created by the pneumatic actuation of our soft robot. Multi-touch interactions are also possible. We evaluated our approach with an organic soft robotics sculpture that was created by a visual artist. In particular, we evaluate that the touch localization capabilities are robust under the deformation of the device. We discuss the potential this approach has for the arts and entertainment as well as other domains.
<div id='section'>PaperID: <span id='pid'>811, <a href='https://arxiv.org/pdf/2503.00508.pdf' target='_blank'>https://arxiv.org/pdf/2503.00508.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dehao Huang, Wenlong Dong, Chao Tang, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00508">HGDiffuser: Efficient Task-Oriented Grasp Generation via Human-Guided Grasp Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) is essential for robots to perform manipulation tasks, requiring grasps that are both stable and compliant with task-specific constraints. Humans naturally grasp objects in a task-oriented manner to facilitate subsequent manipulation tasks. By leveraging human grasp demonstrations, current methods can generate high-quality robotic parallel-jaw task-oriented grasps for diverse objects and tasks. However, they still encounter challenges in maintaining grasp stability and sampling efficiency. These methods typically rely on a two-stage process: first performing exhaustive task-agnostic grasp sampling in the 6-DoF space, then applying demonstration-induced constraints (e.g., contact regions and wrist orientations) to filter candidates. This leads to inefficiency and potential failure due to the vast sampling space. To address this, we propose the Human-guided Grasp Diffuser (HGDiffuser), a diffusion-based framework that integrates these constraints into a guided sampling process. Through this approach, HGDiffuser directly generates 6-DoF task-oriented grasps in a single stage, eliminating exhaustive task-agnostic sampling. Furthermore, by incorporating Diffusion Transformer (DiT) blocks as the feature backbone, HGDiffuser improves grasp generation quality compared to MLP-based methods. Experimental results demonstrate that our approach significantly improves the efficiency of task-oriented grasp generation, enabling more effective transfer of human grasping strategies to robotic systems. To access the source code and supplementary videos, visit https://sites.google.com/view/hgdiffuser.
<div id='section'>PaperID: <span id='pid'>812, <a href='https://arxiv.org/pdf/2502.07358.pdf' target='_blank'>https://arxiv.org/pdf/2502.07358.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoran Chen, Yiteng Xu, Yiming Ren, Yaoqin Ye, Xinran Li, Ning Ding, Yuxuan Wu, Yaoze Liu, Peishan Cong, Ziyi Wang, Bushi Liu, Yuhan Chen, Zhiyang Dou, Xiaokun Leng, Manyi Li, Yuexin Ma, Changhe Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07358">SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for Adaptive Human-Robot Symbiosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of intelligent robots seeks to seamlessly integrate them into the human world, providing assistance and companionship in daily life and work, with the ultimate goal of achieving human-robot symbiosis. This requires robots with intelligent interaction abilities to work naturally and effectively with humans. However, current robotic simulators fail to support real human participation, limiting their ability to provide authentic interaction experiences and gather valuable human feedback essential for enhancing robotic capabilities. In this paper, we introduce SymBridge, the first human-in-the-loop cyber-physical interactive system designed to enable the safe and efficient development, evaluation, and optimization of human-robot interaction methods. Specifically, we employ augmented reality technology to enable real humans to interact with virtual robots in physical environments, creating an authentic interactive experience. Building on this, we propose a novel robotic interaction model that generates responsive, precise robot actions in real time through continuous human behavior observation. The model incorporates multi-resolution human motion features and environmental affordances, ensuring contextually adaptive robotic responses. Additionally, SymBridge enables continuous robot learning by collecting human feedback and dynamically adapting the robotic interaction model. By leveraging a carefully designed system architecture and modules, SymBridge builds a bridge between humans and robots, as well as between cyber and physical spaces, providing a natural and realistic online interaction experience while facilitating the continuous evolution of robotic intelligence. Extensive experiments, user studies, and real robot testing demonstrate the promising performance of the system and highlight its potential to significantly advance research on human-robot symbiosis.
<div id='section'>PaperID: <span id='pid'>813, <a href='https://arxiv.org/pdf/2412.10439.pdf' target='_blank'>https://arxiv.org/pdf/2412.10439.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou, Bo Du, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10439">CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts.
<div id='section'>PaperID: <span id='pid'>814, <a href='https://arxiv.org/pdf/2411.09823.pdf' target='_blank'>https://arxiv.org/pdf/2411.09823.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yian Wang, Xiaowen Qiu, Jiageng Liu, Zhehuan Chen, Jiting Cai, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09823">Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating large-scale interactive 3D environments is essential for the development of Robotics and Embodied AI research. Current methods, including manual design, procedural generation, diffusion-based scene generation, and large language model (LLM) guided scene design, are hindered by limitations such as excessive human effort, reliance on predefined rules or training datasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image generative models better capture scene and object configuration than LLMs, we address these challenges by introducing Architect, a generative framework that creates complex and realistic 3D embodied environments leveraging diffusion-based 2D image inpainting. In detail, we utilize foundation visual perception models to obtain each generated object from the image and leverage pre-trained depth estimation models to lift the generated 2D image to 3D space. Our pipeline is further extended to a hierarchical and iterative inpainting process to continuously generate placement of large furniture and small objects to enrich the scene. This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments.
<div id='section'>PaperID: <span id='pid'>815, <a href='https://arxiv.org/pdf/2411.08579.pdf' target='_blank'>https://arxiv.org/pdf/2411.08579.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Youzhi Liu, Fanglong Yao, Yuanchang Yue, Guangluan Xu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08579">NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.
<div id='section'>PaperID: <span id='pid'>816, <a href='https://arxiv.org/pdf/2409.16033.pdf' target='_blank'>https://arxiv.org/pdf/2409.16033.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenlong Dong, Dehao Huang, Jiangshan Liu, Chao Tang, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16033">RTAGrasp: Learning Task-Oriented Grasping from Human Videos via Retrieval, Transfer, and Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) is crucial for robots to accomplish manipulation tasks, requiring the determination of TOG positions and directions. Existing methods either rely on costly manual TOG annotations or only extract coarse grasping positions or regions from human demonstrations, limiting their practicality in real-world applications. To address these limitations, we introduce RTAGrasp, a Retrieval, Transfer, and Alignment framework inspired by human grasping strategies. Specifically, our approach first effortlessly constructs a robot memory from human grasping demonstration videos, extracting both TOG position and direction constraints. Then, given a task instruction and a visual observation of the target object, RTAGrasp retrieves the most similar human grasping experience from its memory and leverages semantic matching capabilities of vision foundation models to transfer the TOG constraints to the target object in a training-free manner. Finally, RTAGrasp aligns the transferred TOG constraints with the robot's action for execution. Evaluations on the public TOG benchmark, TaskGrasp dataset, show the competitive performance of RTAGrasp on both seen and unseen object categories compared to existing baseline methods. Real-world experiments further validate its effectiveness on a robotic arm. Our code, appendix, and video are available at \url{https://sites.google.com/view/rtagrasp/home}.
<div id='section'>PaperID: <span id='pid'>817, <a href='https://arxiv.org/pdf/2409.13837.pdf' target='_blank'>https://arxiv.org/pdf/2409.13837.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mani Amani, Reza Akhavian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13837">Adaptive Robot Perception in Construction Environments using 4D BIM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) is a pivotal component of robot perception for physical Human Robot Interaction (pHRI) tasks. In construction robotics, it is vital that robots have an accurate and robust perception of worker activities. This enhanced perception is the foundation of trustworthy and safe Human-Robot Collaboration (HRC) in an industrial setting. Many developed HAR algorithms lack the robustness and adaptability to ensure seamless HRC. Recent works have employed multi-modal approaches to increase feature considerations. This paper further expands previous research to include 4D building information modeling (BIM) schedule data. We created a pipeline that transforms high-level BIM schedule activities into a set of low-level tasks in real-time. The framework then utilizes this subset as a tool to restrict the solution space that the HAR algorithm can predict activities from. By limiting this subspace through 4D BIM schedule data, the algorithm has a higher chance of predicting the true possible activities from a smaller pool of possibilities in a localized setting as compared to calculating all global possibilities at every point. Results indicate that the proposed approach achieves higher confidence predictions over the base model without leveraging the BIM data.
<div id='section'>PaperID: <span id='pid'>818, <a href='https://arxiv.org/pdf/2409.01581.pdf' target='_blank'>https://arxiv.org/pdf/2409.01581.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixuan Guo, Yifan Xie, Weijing Xie, Peng Huang, Fei Ma, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01581">GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.
<div id='section'>PaperID: <span id='pid'>819, <a href='https://arxiv.org/pdf/2602.10556.pdf' target='_blank'>https://arxiv.org/pdf/2602.10556.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lihan Zha, Asher J. Hancock, Mingtong Zhang, Tenny Yin, Yixuan Huang, Dhruv Shah, Allen Z. Ren, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.10556">LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.
<div id='section'>PaperID: <span id='pid'>820, <a href='https://arxiv.org/pdf/2512.06558.pdf' target='_blank'>https://arxiv.org/pdf/2512.06558.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Md Mofijul Islam, Alexi Gladstone, Sujan Sarker, Ganesh Nanduru, Md Fahim, Keyan Du, Aman Chadha, Tariq Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06558">Embodied Referring Expression Comprehension in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.
<div id='section'>PaperID: <span id='pid'>821, <a href='https://arxiv.org/pdf/2512.04597.pdf' target='_blank'>https://arxiv.org/pdf/2512.04597.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tao Wu, Chuhao Zhou, Guangyu Zhao, Haozhi Cao, Yewen Pu, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04597">When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.
<div id='section'>PaperID: <span id='pid'>822, <a href='https://arxiv.org/pdf/2511.17384.pdf' target='_blank'>https://arxiv.org/pdf/2511.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Li, Lichi Li, Anh Dao, Xinyu Zhou, Yicheng Qiao, Zheda Mai, Daeun Lee, Zichen Chen, Zhen Tan, Mohit Bansal, Yu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17384">IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the "collision rate" and "warning rate" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.
<div id='section'>PaperID: <span id='pid'>823, <a href='https://arxiv.org/pdf/2511.10376.pdf' target='_blank'>https://arxiv.org/pdf/2511.10376.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xun Huang, Shijia Zhao, Yunxiang Wang, Xin Lu, Wanfa Zhang, Rongsheng Qu, Weixin Li, Yunhong Wang, Chenglu Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10376">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relation
<div id='section'>PaperID: <span id='pid'>824, <a href='https://arxiv.org/pdf/2511.00933.pdf' target='_blank'>https://arxiv.org/pdf/2511.00933.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiangyu Shi, Zerui Li, Yanyuan Qiao, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00933">Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.
<div id='section'>PaperID: <span id='pid'>825, <a href='https://arxiv.org/pdf/2510.00906.pdf' target='_blank'>https://arxiv.org/pdf/2510.00906.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Julian Lemmel, Manuel Kranzl, Adam Lamine, Philipp Neubauer, Radu Grosu, Sophie A. Neubauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00906">TubeDAgger: Reducing the Number of Expert Interventions with Stochastic Reach-Tubes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive Imitation Learning deals with training a novice policy from expert demonstrations in an online fashion. The established DAgger algorithm trains a robust novice policy by alternating between interacting with the environment and retraining of the network. Many variants thereof exist, that differ in the method of discerning whether to allow the novice to act or return control to the expert. We propose the use of stochastic reachtubes - common in verification of dynamical systems - as a novel method for estimating the necessity of expert intervention. Our approach does not require fine-tuning of decision thresholds per environment and effectively reduces the number of expert interventions, especially when compared with related approaches that make use of a doubt classification model.
<div id='section'>PaperID: <span id='pid'>826, <a href='https://arxiv.org/pdf/2510.00600.pdf' target='_blank'>https://arxiv.org/pdf/2510.00600.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pietro Mazzaglia, Cansu Sancaktar, Markus Peschl, Daniel Dijkman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00600">Hybrid Training for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.
<div id='section'>PaperID: <span id='pid'>827, <a href='https://arxiv.org/pdf/2509.24768.pdf' target='_blank'>https://arxiv.org/pdf/2509.24768.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Eric Hannus, Miika Malin, Tran Nguyen Le, Ville Kyrki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24768">IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.
<div id='section'>PaperID: <span id='pid'>828, <a href='https://arxiv.org/pdf/2509.23655.pdf' target='_blank'>https://arxiv.org/pdf/2509.23655.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rokas Bendikas, Daniel Dijkman, Markus Peschl, Sanjay Haresh, Pietro Mazzaglia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23655">Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.
<div id='section'>PaperID: <span id='pid'>829, <a href='https://arxiv.org/pdf/2509.09594.pdf' target='_blank'>https://arxiv.org/pdf/2509.09594.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09594">ObjectReact: Learning Object-Relative Control for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/
<div id='section'>PaperID: <span id='pid'>830, <a href='https://arxiv.org/pdf/2508.20664.pdf' target='_blank'>https://arxiv.org/pdf/2508.20664.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kan Chen, Zhen Meng, Xiangmin Xu, Jiaming Yang, Emma Li, Philip G. Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20664">Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time human-device interaction in industrial Metaverse faces challenges such as high computational load, limited bandwidth, and strict latency. This paper proposes a task-oriented edge-assisted cross-system framework using digital twins (DTs) to enable responsive interactions. By predicting operator motions, the system supports: 1) proactive Metaverse rendering for visual feedback, and 2) preemptive control of remote devices. The DTs are decoupled into two virtual functions-visual display and robotic control-optimizing both performance and adaptability. To enhance generalizability, we introduce the Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates the framework's effectiveness: in a Trajectory-Based Drawing Control task, it reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene representation task for nuclear decommissioning, it achieves a PSNR of 22.11, SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's capability to ensure spatial precision and visual fidelity in real-time, high-risk industrial environments.
<div id='section'>PaperID: <span id='pid'>831, <a href='https://arxiv.org/pdf/2508.07839.pdf' target='_blank'>https://arxiv.org/pdf/2508.07839.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07839">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affective tactile interaction constitutes a fundamental component of human communication. In natural human-human encounters, touch is seldom experienced in isolation; rather, it is inherently multisensory. Individuals not only perceive the physical sensation of touch but also register the accompanying auditory cues generated through contact. The integration of haptic and auditory information forms a rich and nuanced channel for emotional expression. While extensive research has examined how robots convey emotions through facial expressions and speech, their capacity to communicate social gestures and emotions via touch remains largely underexplored. To address this gap, we developed a multimodal interaction system incorporating a 5*5 grid of 25 vibration motors synchronized with audio playback, enabling robots to deliver combined haptic-audio stimuli. In an experiment involving 32 Chinese participants, ten emotions and six social gestures were presented through vibration, sound, or their combination. Participants rated each stimulus on arousal and valence scales. The results revealed that (1) the combined haptic-audio modality significantly enhanced decoding accuracy compared to single modalities; (2) each individual channel-vibration or sound-effectively supported certain emotions recognition, with distinct advantages depending on the emotional expression; and (3) gestures alone were generally insufficient for conveying clearly distinguishable emotions. These findings underscore the importance of multisensory integration in affective human-robot interaction and highlight the complementary roles of haptic and auditory cues in enhancing emotional communication.
<div id='section'>PaperID: <span id='pid'>832, <a href='https://arxiv.org/pdf/2508.07650.pdf' target='_blank'>https://arxiv.org/pdf/2508.07650.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Helong Huang, Min Cen, Kai Tan, Xingyue Quan, Guowei Huang, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07650">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.
<div id='section'>PaperID: <span id='pid'>833, <a href='https://arxiv.org/pdf/2507.22905.pdf' target='_blank'>https://arxiv.org/pdf/2507.22905.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22905">Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) become increasingly integrated into robotic systems, their potential to generate socially and culturally appropriate affective touch remains largely unexplored. This study investigates whether LLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive tactile behaviours to convey emotions in human-robot interaction. We produced text based touch descriptions for 12 distinct emotions across three cultural contexts (Chinese, Belgian, and unspecified), and examined their interpretability in both robot-to-human and human-to-robot scenarios. A total of 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified) evaluated these LLM-generated tactile behaviours for emotional decoding and perceived appropriateness. Results reveal that: (1) under matched cultural conditions, participants successfully decoded six out of twelve emotions-mainly socially oriented emotions such as love and Ekman emotions such as anger, however, self-focused emotions like pride and embarrassment were more difficult to interpret; (2) tactile behaviours were perceived as more appropriate when directed from human to robot than from robot to human, revealing an asymmetry in social expectations based on interaction roles; (3) behaviours interpreted as aggressive (e.g., anger), overly intimate (e.g., love), or emotionally ambiguous (i.e., not clearly decodable) were significantly more likely to be rated as inappropriate; and (4) cultural mismatches reduced decoding accuracy and increased the likelihood of behaviours being judged as inappropriate.
<div id='section'>PaperID: <span id='pid'>834, <a href='https://arxiv.org/pdf/2506.19179.pdf' target='_blank'>https://arxiv.org/pdf/2506.19179.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19179">Situated Haptic Interaction: Exploring the Role of Context in Affective Perception of Robotic Touch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affective interaction is not merely about recognizing emotions; it is an embodied, situated process shaped by context and co-created through interaction. In affective computing, the role of haptic feedback within dynamic emotional exchanges remains underexplored. This study investigates how situational emotional cues influence the perception and interpretation of haptic signals given by a robot. In a controlled experiment, 32 participants watched video scenarios in which a robot experienced either positive actions (such as being kissed), negative actions (such as being slapped) or neutral actions. After each video, the robot conveyed its emotional response through haptic communication, delivered via a wearable vibration sleeve worn by the participant. Participants rated the robot's emotional state-its valence (positive or negative) and arousal (intensity)-based on the video, the haptic feedback, and the combination of the two. The study reveals a dynamic interplay between visual context and touch. Participants' interpretation of haptic feedback was strongly shaped by the emotional context of the video, with visual context often overriding the perceived valence of the haptic signal. Negative haptic cues amplified the perceived valence of the interaction, while positive cues softened it. Furthermore, haptics override the participants' perception of arousal of the video. Together, these results offer insights into how situated haptic feedback can enrich affective human-robot interaction, pointing toward more nuanced and embodied approaches to emotional communication with machines.
<div id='section'>PaperID: <span id='pid'>835, <a href='https://arxiv.org/pdf/2506.15377.pdf' target='_blank'>https://arxiv.org/pdf/2506.15377.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruoyu Wang, Xinshu Li, Chen Wang, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15377">Efficient and Generalizable Environmental Understanding for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.
<div id='section'>PaperID: <span id='pid'>836, <a href='https://arxiv.org/pdf/2505.24266.pdf' target='_blank'>https://arxiv.org/pdf/2505.24266.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guanren Qiao, Sixu Lin, Ronglai Zuo, Zhizheng Wu, Kui Jia, Guiliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24266">SignBot: Learning Human-to-Humanoid Sign Language Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language is a natural and visual form of language that uses movements and expressions to convey meaning, serving as a crucial means of communication for individuals who are deaf or hard-of-hearing (DHH). However, the number of people proficient in sign language remains limited, highlighting the need for technological advancements to bridge communication gaps and foster interactions with minorities. Based on recent advancements in embodied humanoid robots, we propose SignBot, a novel framework for human-robot sign language interaction. SignBot integrates a cerebellum-inspired motion control component and a cerebral-oriented module for comprehension and interaction. Specifically, SignBot consists of: 1) Motion Retargeting, which converts human sign language datasets into robot-compatible kinematics; 2) Motion Control, which leverages a learning-based paradigm to develop a robust humanoid control policy for tracking sign language gestures; and 3) Generative Interaction, which incorporates translator, responser, and generator of sign language, thereby enabling natural and effective communication between robots and humans. Simulation and real-world experimental results demonstrate that SignBot can effectively facilitate human-robot interaction and perform sign language motions with diverse robots and datasets. SignBot represents a significant advancement in automatic sign language interaction on embodied humanoid robot platforms, providing a promising solution to improve communication accessibility for the DHH community.
<div id='section'>PaperID: <span id='pid'>837, <a href='https://arxiv.org/pdf/2505.22088.pdf' target='_blank'>https://arxiv.org/pdf/2505.22088.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sam O'Connor Russell, Naomi Harte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22088">Visual Cues Support Robust Turn-taking Prediction in Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate predictive turn-taking models (PTTMs) are essential for naturalistic human-robot interaction. However, little is known about their performance in noise. This study therefore explores PTTM performance in types of noise likely to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10 dB music noise. Training with noisy data enables a multimodal PTTM, which includes visual features to better exploit visual cues, with 72% accuracy in 10 dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all noise types and SNRs, highlighting its ability to exploit visual cues; however, this does not always generalise to new types of noise. Analysis also reveals that successful training relies on accurate transcription, limiting the use of ASR-derived transcriptions to clean conditions. We make code publicly available for future research.
<div id='section'>PaperID: <span id='pid'>838, <a href='https://arxiv.org/pdf/2505.21043.pdf' target='_blank'>https://arxiv.org/pdf/2505.21043.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sam O'Connor Russell, Naomi Harte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21043">Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.
<div id='section'>PaperID: <span id='pid'>839, <a href='https://arxiv.org/pdf/2503.16965.pdf' target='_blank'>https://arxiv.org/pdf/2503.16965.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Hu, Jing Li, Zhongzhu Pu, Hou Pong Chan, Yu Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16965">Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models exhibited immense potential for embodied AI, yet they often lack the sophisticated situational reasoning required for complex decision-making. This paper shows that VLMs can achieve surprisingly strong decision-making performance when visual scenes are represented merely as text-only descriptions, suggesting foundational reasoning can be effectively learned from language. Motivated by this insight, we propose Praxis-VLM, a reasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO algorithm on textual scenarios to instill robust reasoning capabilities, where models learn to evaluate actions and their consequences. These reasoning skills, acquired purely from text, successfully transfer to multimodal inference with visual inputs, significantly reducing reliance on scarce paired image-text training data. Experiments across diverse decision-making benchmarks demonstrate that Praxis-VLM substantially outperforms standard supervised fine-tuning, exhibiting superior performance and generalizability. Further analysis confirms that our models engage in explicit and effective reasoning, underpinning their enhanced performance and adaptability.
<div id='section'>PaperID: <span id='pid'>840, <a href='https://arxiv.org/pdf/2502.06838.pdf' target='_blank'>https://arxiv.org/pdf/2502.06838.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixiao Wang, Jieya Zhou, Su Zheng, Shuo Yin, Kaichao Liang, Shoubo Hu, Xiao Chen, Bei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06838">TorchResist: Open-Source Differentiable Resist Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent decades have witnessed remarkable advancements in artificial intelligence (AI), including large language models (LLMs), image and video generative models, and embodied AI systems. These advancements have led to an explosive increase in the demand for computational power, challenging the limits of Moore's Law. Optical lithography, a critical technology in semiconductor manufacturing, faces significant challenges due to its high costs. To address this, various lithography simulators have been developed. However, many of these simulators are limited by their inadequate photoresist modeling capabilities. This paper presents TorchResist, an open-source, differentiable photoresist simulator.TorchResist employs an analytical approach to model the photoresist process, functioning as a white-box system with at most twenty interpretable parameters. Leveraging modern differentiable programming techniques and parallel computing on GPUs, TorchResist enables seamless co-optimization with other tools across multiple related tasks. Our experimental results demonstrate that TorchResist achieves superior accuracy and efficiency compared to existing solutions. The source code is publicly available.
<div id='section'>PaperID: <span id='pid'>841, <a href='https://arxiv.org/pdf/2501.07224.pdf' target='_blank'>https://arxiv.org/pdf/2501.07224.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07224">Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Touch is a fundamental aspect of emotion-rich communication, playing a vital role in human interaction and offering significant potential in human-robot interaction. Previous research has demonstrated that a sparse representation of human touch can effectively convey social tactile signals. However, advances in human-robot tactile interaction remain limited, as many humanoid robots possess simplistic capabilities, such as only opening and closing their hands, restricting nuanced tactile expressions. In this study, we explore how a robot can use sparse representations of tactile vibrations to convey emotions to a person. To achieve this, we developed a wearable sleeve integrated with a 5x5 grid of vibration motors, enabling the robot to communicate diverse tactile emotions and gestures. Using chain prompts within a Large Language Model (LLM), we generated distinct 10-second vibration patterns corresponding to 10 emotions (e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap). Participants (N = 32) then rated each vibration stimulus based on perceived valence and arousal. People are accurate at recognising intended emotions, a result which aligns with earlier findings. These results highlight the LLM's ability to generate emotional haptic data and effectively convey emotions through tactile signals. By translating complex emotional and tactile expressions into vibratory patterns, this research demonstrates how LLMs can enhance physical interaction between humans and robots.
<div id='section'>PaperID: <span id='pid'>842, <a href='https://arxiv.org/pdf/2412.14058.pdf' target='_blank'>https://arxiv.org/pdf/2412.14058.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14058">Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.
<div id='section'>PaperID: <span id='pid'>843, <a href='https://arxiv.org/pdf/2412.06877.pdf' target='_blank'>https://arxiv.org/pdf/2412.06877.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thomas Pouplin, Katarzyna Kobalczyk, Hao Sun, Mihaela van der Schaar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06877">The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce TEDUO, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, TEDUO operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that TEDUO achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone.
<div id='section'>PaperID: <span id='pid'>844, <a href='https://arxiv.org/pdf/2412.01292.pdf' target='_blank'>https://arxiv.org/pdf/2412.01292.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongyan Zhi, Peihao Chen, Junyan Li, Shuailei Ma, Xinyu Sun, Tianhang Xiang, Yinjie Lei, Mingkui Tan, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01292">LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.
<div id='section'>PaperID: <span id='pid'>845, <a href='https://arxiv.org/pdf/2409.12192.pdf' target='_blank'>https://arxiv.org/pdf/2409.12192.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zichen Jeff Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Haldar, Lerrel Pinto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12192">DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io
<div id='section'>PaperID: <span id='pid'>846, <a href='https://arxiv.org/pdf/2602.12032.pdf' target='_blank'>https://arxiv.org/pdf/2602.12032.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jingxian Lu, Wenke Xia, Yuxuan Wu, Zhiwu Lu, Di Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.12032">When would Vision-Proprioception Policies Fail in Robotic Manipulation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.
<div id='section'>PaperID: <span id='pid'>847, <a href='https://arxiv.org/pdf/2602.05863.pdf' target='_blank'>https://arxiv.org/pdf/2602.05863.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Roger Girgis, Rodrigue de Schaetzen, Luke Rowe, Azalée Robitaille, Christopher Pal, Liam Paull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.05863">Constrained Group Relative Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.
<div id='section'>PaperID: <span id='pid'>848, <a href='https://arxiv.org/pdf/2602.00675.pdf' target='_blank'>https://arxiv.org/pdf/2602.00675.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Valerio Belcamino, Mariya Kilina, Alessandro Carfì, Valeria Seidita, Fulvio Mastrogiovanni, Antonio Chella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00675">Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.
<div id='section'>PaperID: <span id='pid'>849, <a href='https://arxiv.org/pdf/2601.14323.pdf' target='_blank'>https://arxiv.org/pdf/2601.14323.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bingxin Xu, Yuzhang Shang, Binghui Wang, Emilio Ferrara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14323">SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.
<div id='section'>PaperID: <span id='pid'>850, <a href='https://arxiv.org/pdf/2512.20902.pdf' target='_blank'>https://arxiv.org/pdf/2512.20902.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siqi Mu, Shuo Wen, Yang Lu, Ruihong Jiang, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20902">Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.
<div id='section'>PaperID: <span id='pid'>851, <a href='https://arxiv.org/pdf/2512.10668.pdf' target='_blank'>https://arxiv.org/pdf/2512.10668.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jingxuan Zhang, Tianqi Yu, Yatu Zhang, Jinze Wu, Kaixin Yao, Jingyang Liu, Yuyao Zhang, Jiayuan Gu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10668">XDen-1K: A Density Field Dataset of Real-World Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.
<div id='section'>PaperID: <span id='pid'>852, <a href='https://arxiv.org/pdf/2512.01223.pdf' target='_blank'>https://arxiv.org/pdf/2512.01223.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01223">S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.
<div id='section'>PaperID: <span id='pid'>853, <a href='https://arxiv.org/pdf/2511.23143.pdf' target='_blank'>https://arxiv.org/pdf/2511.23143.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Enrico Saccon, Davide De Martini, Matteo Saveriano, Edoardo Lamon, Luigi Palopoli, Marco Roveri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23143">Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.
<div id='section'>PaperID: <span id='pid'>854, <a href='https://arxiv.org/pdf/2511.09515.pdf' target='_blank'>https://arxiv.org/pdf/2511.09515.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09515">WMPO: World Model-based Policy Optimization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.
<div id='section'>PaperID: <span id='pid'>855, <a href='https://arxiv.org/pdf/2511.03997.pdf' target='_blank'>https://arxiv.org/pdf/2511.03997.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Peiyao Wang, Weining Wang, Qi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03997">PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.
<div id='section'>PaperID: <span id='pid'>856, <a href='https://arxiv.org/pdf/2510.15018.pdf' target='_blank'>https://arxiv.org/pdf/2510.15018.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingxuan Liu, Honglin He, Elisa Ricci, Wayne Wu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15018">UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.
<div id='section'>PaperID: <span id='pid'>857, <a href='https://arxiv.org/pdf/2510.14836.pdf' target='_blank'>https://arxiv.org/pdf/2510.14836.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14836">QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.
<div id='section'>PaperID: <span id='pid'>858, <a href='https://arxiv.org/pdf/2509.22271.pdf' target='_blank'>https://arxiv.org/pdf/2509.22271.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Felix Glawe, Tim Schmeckel, Philipp Brauner, Martina Ziefle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22271">Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human autonomy and sense of agency are increasingly recognised as critical for user well-being, motivation, and the ethical deployment of robots in human-robot interaction (HRI). Given the rapid development of artificial intelligence, robot capabilities and their potential to function as colleagues and companions are growing. This systematic literature review synthesises 22 empirical studies selected from an initial pool of 728 articles published between 2011 and 2024. Articles were retrieved from major scientific databases and identified based on empirical focus and conceptual relevance, namely, how to preserve and promote human autonomy and sense of agency in HRI. Derived through thematic synthesis, five clusters of potentially influential factors are revealed: robot adaptiveness, communication style, anthropomorphism, presence of a robot and individual differences. Measured through psychometric scales or the intentional binding paradigm, perceptions of autonomy and agency varied across industrial, educational, healthcare, care, and hospitality settings. The review underscores the theoretical differences between both concepts, but their yet entangled use in HRI. Despite increasing interest, the current body of empirical evidence remains limited and fragmented, underscoring the necessity for standardised definitions, more robust operationalisations, and further exploratory and qualitative research. By identifying existing gaps and highlighting emerging trends, this review contributes to the development of human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles, ultimately supporting well-being in human-robot interaction.
<div id='section'>PaperID: <span id='pid'>859, <a href='https://arxiv.org/pdf/2509.21981.pdf' target='_blank'>https://arxiv.org/pdf/2509.21981.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21981">CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.
<div id='section'>PaperID: <span id='pid'>860, <a href='https://arxiv.org/pdf/2509.18311.pdf' target='_blank'>https://arxiv.org/pdf/2509.18311.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Benjamin A. Christie, Sagar Parekh, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18311">Fine-Tuning Robot Policies While Maintaining User Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works introduce general-purpose robot policies. These policies provide a strong prior over how robots should behave -- e.g., how a robot arm should manipulate food items. But in order for robots to match an individual person's needs, users typically fine-tune these generalized policies -- e.g., showing the robot arm how to make their own preferred dinners. Importantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat). Other agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors. This leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents? We here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies. Our core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot's network. With the correct key, the robot's policy switches to match that user's preferences -- but with incorrect keys, the robot reverts to its baseline behaviors. We show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks. PRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches. See videos and code here: https://prop-icra26.github.io.
<div id='section'>PaperID: <span id='pid'>861, <a href='https://arxiv.org/pdf/2509.16122.pdf' target='_blank'>https://arxiv.org/pdf/2509.16122.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Carter Sifferman, Mohit Gupta, Michael Gleicher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16122">Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We provide a method for detecting and localizing objects near a robot arm using arm-mounted miniature time-of-flight sensors. A key challenge when using arm-mounted sensors is differentiating between the robot itself and external objects in sensor measurements. To address this challenge, we propose a computationally lightweight method which utilizes the raw time-of-flight information captured by many off-the-shelf, low-resolution time-of-flight sensor. We build an empirical model of expected sensor measurements in the presence of the robot alone, and use this model at runtime to detect objects in proximity to the robot. In addition to avoiding robot self-detections in common sensor configurations, the proposed method enables extra flexibility in sensor placement, unlocking configurations which achieve more efficient coverage of a radius around the robot arm. Our method can detect small objects near the arm and localize the position of objects along the length of a robot link to reasonable precision. We evaluate the performance of the method with respect to object type, location, and ambient light level, and identify limiting factors on performance inherent in the measurement principle. The proposed method has potential applications in collision avoidance and in facilitating safe human-robot interaction.
<div id='section'>PaperID: <span id='pid'>862, <a href='https://arxiv.org/pdf/2508.06095.pdf' target='_blank'>https://arxiv.org/pdf/2508.06095.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mitchell Abrams, Thies Oelerich, Christian Hartl-Nesic, Andreas Kugi, Matthias Scheutz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06095">Incremental Language Understanding for Online Motion Planning of Robot Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction requires robots to process language incrementally, adapting their actions in real-time based on evolving speech input. Existing approaches to language-guided robot motion planning typically assume fully specified instructions, resulting in inefficient stop-and-replan behavior when corrections or clarifications occur. In this paper, we introduce a novel reasoning-based incremental parser which integrates an online motion planning algorithm within the cognitive architecture. Our approach enables continuous adaptation to dynamic linguistic input, allowing robots to update motion plans without restarting execution. The incremental parser maintains multiple candidate parses, leveraging reasoning mechanisms to resolve ambiguities and revise interpretations when needed. By combining symbolic reasoning with online motion planning, our system achieves greater flexibility in handling speech corrections and dynamically changing constraints. We evaluate our framework in real-world human-robot interaction scenarios, demonstrating online adaptions of goal poses, constraints, or task objectives. Our results highlight the advantages of integrating incremental language understanding with real-time motion planning for natural and fluid human-robot collaboration. The experiments are demonstrated in the accompanying video at www.acin.tuwien.ac.at/42d5.
<div id='section'>PaperID: <span id='pid'>863, <a href='https://arxiv.org/pdf/2508.02505.pdf' target='_blank'>https://arxiv.org/pdf/2508.02505.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria Lombardi, Carmela Calabrese, Davide Ghiglino, Caterina Foglino, Davide De Tommaso, Giulia Da Lisca, Lorenzo Natale, Agnieszka Wykowska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02505">Would you let a humanoid play storytelling with your child? A usability study on LLM-powered narrative Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in human-robot interaction research lies in developing robotic systems that can effectively perceive and interpret social cues, facilitating natural and adaptive interactions. In this work, we present a novel framework for enhancing the attention of the iCub humanoid robot by integrating advanced perceptual abilities to recognise social cues, understand surroundings through generative models, such as ChatGPT, and respond with contextually appropriate social behaviour. Specifically, we propose an interaction task implementing a narrative protocol (storytelling task) in which the human and the robot create a short imaginary story together, exchanging in turn cubes with creative images placed on them. To validate the protocol and the framework, experiments were performed to quantify the degree of usability and the quality of experience perceived by participants interacting with the system. Such a system can be beneficial in promoting effective human robot collaborations, especially in assistance, education and rehabilitation scenarios where the social awareness and the robot responsiveness play a pivotal role.
<div id='section'>PaperID: <span id='pid'>864, <a href='https://arxiv.org/pdf/2507.03049.pdf' target='_blank'>https://arxiv.org/pdf/2507.03049.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ferran GebellÃ­, AnaÃ­s Garrell, Jan-Gerrit Habekost, SÃ©verin Lemaignan, Stefan Wermter, Raquel Ros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03049">Personalised Explanations in Long-term Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of Human-Robot Interaction (HRI), a fundamental challenge is to facilitate human understanding of robots. The emerging domain of eXplainable HRI (XHRI) investigates methods to generate explanations and evaluate their impact on human-robot interactions. Previous works have highlighted the need to personalise the level of detail of these explanations to enhance usability and comprehension. Our paper presents a framework designed to update and retrieve user knowledge-memory models, allowing for adapting the explanations' level of detail while referencing previously acquired concepts. Three architectures based on our proposed framework that use Large Language Models (LLMs) are evaluated in two distinct scenarios: a hospital patrolling robot and a kitchen assistant robot. Experimental results demonstrate that a two-stage architecture, which first generates an explanation and then personalises it, is the framework architecture that effectively reduces the level of detail only when there is related user knowledge.
<div id='section'>PaperID: <span id='pid'>865, <a href='https://arxiv.org/pdf/2506.11773.pdf' target='_blank'>https://arxiv.org/pdf/2506.11773.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zikang Leng, Megha Thukral, Yaqi Liu, Hrudhai Rajasekhar, Shruthi K. Hiremath, Jiaman He, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11773">AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge in developing robust and generalizable Human Activity Recognition (HAR) systems for smart homes is the lack of large and diverse labeled datasets. Variations in home layouts, sensor configurations, and individual behaviors further exacerbate this issue. To address this, we leverage the idea of embodied AI agents-virtual agents that perceive and act within simulated environments guided by internal world models. We introduce AgentSense, a virtual data generation pipeline in which agents live out daily routines in simulated smart homes, with behavior guided by Large Language Models (LLMs). The LLM generates diverse synthetic personas and realistic routines grounded in the environment, which are then decomposed into fine-grained actions. These actions are executed in an extended version of the VirtualHome simulator, which we augment with virtual ambient sensors that record the agents' activities. Our approach produces rich, privacy-preserving sensor data that reflects real-world diversity. We evaluate AgentSense on five real HAR datasets. Models pretrained on the generated data consistently outperform baselines, especially in low-resource settings. Furthermore, combining the generated virtual sensor data with a small amount of real data achieves performance comparable to training on full real-world datasets. These results highlight the potential of using LLM-guided embodied agents for scalable and cost-effective sensor data generation in HAR.
<div id='section'>PaperID: <span id='pid'>866, <a href='https://arxiv.org/pdf/2505.21567.pdf' target='_blank'>https://arxiv.org/pdf/2505.21567.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Feng Jiang, Zihao Zheng, Xiuping Cui, Maoliang Li, JIayu Chen, Xiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21567">EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.
<div id='section'>PaperID: <span id='pid'>867, <a href='https://arxiv.org/pdf/2504.06124.pdf' target='_blank'>https://arxiv.org/pdf/2504.06124.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Benjamin A. Christie, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06124">Safe Interaction via Monte Carlo Linear-Quadratic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: https://youtu.be/KJuHeiWVuWY.
<div id='section'>PaperID: <span id='pid'>868, <a href='https://arxiv.org/pdf/2504.02679.pdf' target='_blank'>https://arxiv.org/pdf/2504.02679.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Francesco Bianchin, Robert Lefringhausen, Elisa Gaetan, Samuel Tesfazgi, Sandra Hirche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02679">A Set-Theoretic Robust Control Approach for Linear Quadratic Games with Unknown Counterparts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring robust decision-making in multi-agent systems is challenging when agents have distinct, possibly conflicting objectives and lack full knowledge of each other s strategies. This is apparent in safety-critical applications such as human-robot interaction and assisted driving, where uncertainty arises not only from unknown adversary strategies but also from external disturbances. To address this, the paper proposes a robust adaptive control approach based on linear quadratic differential games. Our method allows a controlled agent to iteratively refine its belief about the adversary strategy and disturbances using a set-membership approach, while simultaneously adapting its policy to guarantee robustness against the uncertain adversary policy and improve performance over time. We formally derive theoretical guarantees on the robustness of the proposed control scheme and its convergence to epsilon-Nash strategies. The effectiveness of our approach is demonstrated in a numerical simulation.
<div id='section'>PaperID: <span id='pid'>869, <a href='https://arxiv.org/pdf/2412.17282.pdf' target='_blank'>https://arxiv.org/pdf/2412.17282.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Riku Uemura, Kanji Tanaka, Kenta Tsukahara, Daiki Iwata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17282">LMD-PGN: Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point goal navigation (PGN) is a mapless navigation approach that trains robots to visually navigate to goal points without relying on pre-built maps. Despite significant progress in handling complex environments using deep reinforcement learning, current PGN methods are designed for single-robot systems, limiting their generalizability to multi-robot scenarios with diverse platforms. This paper addresses this limitation by proposing a knowledge transfer framework for PGN, allowing a teacher robot to transfer its learned navigation model to student robots, including those with unknown or black-box platforms. We introduce a novel knowledge distillation (KD) framework that transfers first-person-view (FPV) representations (view images, turning/forward actions) to universally applicable third-person-view (TPV) representations (local maps, subgoals). The state is redefined as reconstructed local maps using SLAM, while actions are mapped to subgoals on a predefined grid. To enhance training efficiency, we propose a sampling-efficient KD approach that aligns training episodes via a noise-robust local map descriptor (LMD). Although validated on 2D wheeled robots, this method can be extended to 3D action spaces, such as drones. Experiments conducted in Habitat-Sim demonstrate the feasibility of the proposed framework, requiring minimal implementation effort. This study highlights the potential for scalable and cross-platform PGN solutions, expanding the applicability of embodied AI systems in multi-robot scenarios.
<div id='section'>PaperID: <span id='pid'>870, <a href='https://arxiv.org/pdf/2412.13211.pdf' target='_blank'>https://arxiv.org/pdf/2412.13211.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arth Shukla, Stone Tao, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13211">ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.
<div id='section'>PaperID: <span id='pid'>871, <a href='https://arxiv.org/pdf/2412.11523.pdf' target='_blank'>https://arxiv.org/pdf/2412.11523.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daiki Iwata, Kanji Tanaka, Shoya Miyazaki, Kouki Terashima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11523">ON as ALC: Active Loop Closing Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In simultaneous localization and mapping, active loop closing (ALC) is an active vision problem that aims to visually guide a robot to maximize the chances of revisiting previously visited points, thereby resetting the drift errors accumulated in the incrementally built map during travel. However, current mainstream navigation strategies that leverage such incomplete maps as workspace prior knowledge often fail in modern long-term autonomy long-distance travel scenarios where map accumulation errors become significant. To address these limitations of map-based navigation, this paper is the first to explore mapless navigation in the embodied AI field, in particular, to utilize object-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques that efficiently explore target objects without using such a prior map. Specifically, in this work, we start from an off-the-shelf mapless ON planner, extend it to utilize a prior map, and further show that the performance in long-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss" and ``ON loss". This study highlights a simple and effective approach, called ALC-ON (ALCON), to accelerate the progress of challenging long-distance ALC technology by leveraging the growing frontier-guided, data-driven, and LLM-guided ON technologies.
<div id='section'>PaperID: <span id='pid'>872, <a href='https://arxiv.org/pdf/2410.14868.pdf' target='_blank'>https://arxiv.org/pdf/2410.14868.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sung-Wook Lee, Xuhui Kang, Yen-Ling Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14868">Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, diffusion policy has shown impressive results in handling multi-modal tasks in robotic manipulation. However, it has fundamental limitations in out-of-distribution failures that persist due to compounding errors and its limited capability to extrapolate. One way to address these limitations is robot-gated DAgger, an interactive imitation learning with a robot query system to actively seek expert help during policy rollout. While robot-gated DAgger has high potential for learning at scale, existing methods like Ensemble-DAgger struggle with highly expressive policies: They often misinterpret policy disagreements as uncertainty at multi-modal decision points. To address this problem, we introduce Diff-DAgger, an efficient robot-gated DAgger algorithm that leverages the training objective of diffusion policy. We evaluate Diff-DAgger across different robot tasks including stacking, pushing, and plugging, and show that Diff-DAgger improves the task failure prediction by 39.0%, the task completion rate by 20.6%, and reduces the wall-clock time by a factor of 7.8. We hope that this work opens up a path for efficiently incorporating expressive yet data-hungry policies into interactive robot learning settings. The project website is available at: https://diffdagger.github.io.
<div id='section'>PaperID: <span id='pid'>873, <a href='https://arxiv.org/pdf/2410.05554.pdf' target='_blank'>https://arxiv.org/pdf/2410.05554.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maulik Bhatt, Iman Askari, Yue Yu, Ufuk Topcu, Huazhen Fang, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05554">MultiNash-PF: A Particle Filtering Approach for Computing Multiple Local Generalized Nash Equilibria in Trajectory Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern robotic systems frequently engage in complex multi-agent interactions, many of which are inherently multi-modal, i.e., they can lead to multiple distinct outcomes. To interact effectively, robots must recognize the possible interaction modes and adapt to the one preferred by other agents. In this work, we propose MultiNash-PF, an efficient algorithm for capturing the multimodality in multi-agent interactions. We model interaction outcomes as equilibria of a game-theoretic planner, where each equilibrium corresponds to a distinct interaction mode. Our framework formulates interactive planning as Constrained Potential Trajectory Games (CPTGs), in which local Generalized Nash Equilibria (GNEs) represent plausible interaction outcomes. We propose to integrate the potential game approach with implicit particle filtering, a sample-efficient method for non-convex trajectory optimization. We utilize implicit particle filtering to identify the coarse estimates of multiple local minimizers of the game's potential function. MultiNash-PF then refines these estimates with optimization solvers, obtaining different local GNEs. We show through numerical simulations that MultiNash-PF reduces computation time by up to 50\% compared to a baseline. We further demonstrate the effectiveness of our algorithm in real-world human-robot interaction scenarios, where it successfully accounts for the multi-modal nature of interactions and resolves potential conflicts in real-time.
<div id='section'>PaperID: <span id='pid'>874, <a href='https://arxiv.org/pdf/2409.20548.pdf' target='_blank'>https://arxiv.org/pdf/2409.20548.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20548">Robi Butler: Multimodal Remote Interaction with a Household Robot Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imagine a future when we can Zoom-call a robot to manage household chores remotely. This work takes one step in this direction. Robi Butler is a new household robot assistant that enables seamless multimodal remote interaction. It allows the human user to monitor its environment from a first-person view, issue voice or text commands, and specify target objects through hand-pointing gestures. At its core, a high-level behavior module, powered by Large Language Models (LLMs), interprets multimodal instructions to generate multistep action plans. Each plan consists of open-vocabulary primitives supported by vision-language models, enabling the robot to process both textual and gestural inputs. Zoom provides a convenient interface to implement remote interactions between the human and the robot. The integration of these components allows Robi Butler to ground remote multimodal instructions in real-world home environments in a zero-shot manner. We evaluated the system on various household tasks, demonstrating its ability to execute complex user commands with multimodal inputs. We also conducted a user study to examine how multimodal interaction influences user experiences in remote human-robot interaction. These results suggest that with the advances in robot foundation models, we are moving closer to the reality of remote household robot assistants.
<div id='section'>PaperID: <span id='pid'>875, <a href='https://arxiv.org/pdf/2409.02669.pdf' target='_blank'>https://arxiv.org/pdf/2409.02669.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruoyu Wang, Yao Liu, Yuanjiang Cao, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02669">Causality-Aware Transformer Networks for Robotic Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current research in Visual Navigation reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Navigation tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Navigation. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.
<div id='section'>PaperID: <span id='pid'>876, <a href='https://arxiv.org/pdf/2409.00015.pdf' target='_blank'>https://arxiv.org/pdf/2409.00015.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Georgios Bakirtzis, Andrea Aler Tubella, Andreas Theodorou, David Danks, Ufuk Topcu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00015">Navigating the sociotechnical labyrinth: Dynamic certification for responsible embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sociotechnical requirements shape the governance of artificially intelligent (AI) systems. In an era where embodied AI technologies are rapidly reshaping various facets of contemporary society, their inherent dynamic adaptability presents a unique blend of opportunities and challenges. Traditional regulatory mechanisms, often designed for static -- or slower-paced -- technologies, find themselves at a crossroads when faced with the fluid and evolving nature of AI systems. Moreover, typical problems in AI, for example, the frequent opacity and unpredictability of the behaviour of the systems, add additional sociotechnical challenges.
  To address these interconnected issues, we introduce the concept of dynamic certification, an adaptive regulatory framework specifically crafted to keep pace with the continuous evolution of AI systems. The complexity of these challenges requires common progress in multiple domains: technical, socio-governmental, and regulatory. Our proposed transdisciplinary approach is designed to ensure the safe, ethical, and practical deployment of AI systems, aligning them bidirectionally with the real-world contexts in which they operate. By doing so, we aim to bridge the gap between rapid technological advancement and effective regulatory oversight, ensuring that AI systems not only achieve their intended goals but also adhere to ethical standards and societal values.
<div id='section'>PaperID: <span id='pid'>877, <a href='https://arxiv.org/pdf/2602.13197.pdf' target='_blank'>https://arxiv.org/pdf/2602.13197.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Albert J. Zhai, Kuo-Hao Zeng, Jiasen Lu, Ali Farhadi, Shenlong Wang, Wei-Chiu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13197">Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.
<div id='section'>PaperID: <span id='pid'>878, <a href='https://arxiv.org/pdf/2602.08392.pdf' target='_blank'>https://arxiv.org/pdf/2602.08392.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.08392">BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.
<div id='section'>PaperID: <span id='pid'>879, <a href='https://arxiv.org/pdf/2601.12395.pdf' target='_blank'>https://arxiv.org/pdf/2601.12395.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chao Wang, Anna Belardinelli, Michael Gienger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12395">XR$^3$: An Extended Reality Platform for Social-Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social-physical human-robot interaction (spHRI) is difficult to study: building and programming robots that integrate multiple interaction modalities is costly and slow, while VR-based prototypes often lack physical contact, breaking users' visuo-tactile expectations. We present XR$^3$, a co-located dual-VR-headset platform for HRI research in which an attendee and a hidden operator share the same physical space while experiencing different virtual embodiments. The attendee sees an expressive virtual robot that interacts face-to-face in a shared virtual environment. In real time, the robot's upper-body motion, head and gaze behavior, and facial expressions are mapped from the operator's tracked limbs and face signals. Because the operator is co-present and calibrated in the same coordinate frame, the operator can also touch the attendee, enabling perceived robot touch synchronized with the robot's visible hands. Finger and hand motion is mapped to the robot avatar using inverse kinematics to support precise contact. Beyond motion retargeting, XR$^3$ supports social retargeting of multiple nonverbal cues that can be experimentally varied while keeping physical interaction constant. We detail the system design and calibration, and demonstrate the platform in a touch-based Wizard-of-Oz study, lowering the barrier to prototyping and evaluating embodied, contact-based robot behaviors.
<div id='section'>PaperID: <span id='pid'>880, <a href='https://arxiv.org/pdf/2601.09698.pdf' target='_blank'>https://arxiv.org/pdf/2601.09698.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tony Danjun Wang, Tolga Birdal, Nassir Navab, Lennart Bastian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09698">COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.
<div id='section'>PaperID: <span id='pid'>881, <a href='https://arxiv.org/pdf/2512.14442.pdf' target='_blank'>https://arxiv.org/pdf/2512.14442.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14442">A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.
<div id='section'>PaperID: <span id='pid'>882, <a href='https://arxiv.org/pdf/2509.19843.pdf' target='_blank'>https://arxiv.org/pdf/2509.19843.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Filippo Ziliotto, Jelin Raphael Akkara, Alessandro Daniele, Lamberto Ballan, Luciano Serafini, Tommaso Campari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19843">PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors. In this work, we introduce PersONAL (PERSonalized Object Navigation And Localization, a comprehensive benchmark designed to study personalization in Embodied AI. Agents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics. The benchmark supports two evaluation modes: (1) active navigation in unseen environments, and (2) object grounding in previously mapped scenes. Experiments with state-of-the-art baselines reveal a substantial gap to human performance, highlighting the need for embodied agents capable of perceiving, reasoning, and memorizing over personalized information; paving the way towards real-world assistive robot.
<div id='section'>PaperID: <span id='pid'>883, <a href='https://arxiv.org/pdf/2509.14932.pdf' target='_blank'>https://arxiv.org/pdf/2509.14932.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tobias JÃ¼lg, Pierre Krack, Seongjin Bien, Yannik Blei, Khaled Gamal, Ken Nakahara, Johannes Hechtl, Roberto Calandra, Wolfram Burgard, Florian Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14932">Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) mark a major shift in robot learning. They replace specialized architectures and task-tailored components of expert policies with large-scale data collection and setup-specific fine-tuning. In this machine learning-focused workflow that is centered around models and scalable training, traditional robotics software frameworks become a bottleneck, while robot simulations offer only limited support for transitioning from and to real-world experiments. In this work, we close this gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from the ground up to support research in robot learning with large-scale generalist policies. At its core, RCS features a modular and easily extensible layered architecture with a unified interface for simulated and physical robots, facilitating sim-to-real transfer. Despite its minimal footprint and dependencies, it offers a complete feature set, enabling both real-world experiments and large-scale training in simulation. Our contribution is twofold: First, we introduce the architecture of RCS and explain its design principles. Second, we evaluate its usability and performance along the development cycle of VLA and RL policies. Our experiments also provide an extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed light on how simulation data can improve real-world policy performance. Our code, datasets, weights, and videos are available at: https://robotcontrolstack.github.io/
<div id='section'>PaperID: <span id='pid'>884, <a href='https://arxiv.org/pdf/2509.14748.pdf' target='_blank'>https://arxiv.org/pdf/2509.14748.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria Ibrahim, Alap Kshirsagar, Dorothea Koert, Jan Peters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14748">Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication is essential for safety and efficiency in human-robot collaboration, particularly in shared workspaces. This paper investigates the impact of nonverbal communication on human-robot interaction (HRI) by integrating reactive light signals and emotional displays into a robotic system. We equipped a Franka Emika Panda robot with an LED strip on its end effector and an animated facial display on a tablet to convey movement intent through colour-coded signals and facial expressions. We conducted a human-robot collaboration experiment with 18 participants, evaluating three conditions: LED signals alone, LED signals with reactive emotional displays, and LED signals with pre-emptive emotional displays. We collected data through questionnaires and position tracking to assess anticipation of potential collisions, perceived clarity of communication, and task performance. The results indicate that while emotional displays increased the perceived interactivity of the robot, they did not significantly improve collision anticipation, communication clarity, or task efficiency compared to LED signals alone. These findings suggest that while emotional cues can enhance user engagement, their impact on task performance in shared workspaces is limited.
<div id='section'>PaperID: <span id='pid'>885, <a href='https://arxiv.org/pdf/2509.04356.pdf' target='_blank'>https://arxiv.org/pdf/2509.04356.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04356">SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.
<div id='section'>PaperID: <span id='pid'>886, <a href='https://arxiv.org/pdf/2508.10287.pdf' target='_blank'>https://arxiv.org/pdf/2508.10287.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simindokht Jahangard, Mehrzad Mohammadi, Yi Shen, Zhixi Cai, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10287">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Vision-Language Models (VLMs) and large language models (LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI agents like robots. However, existing visual reasoning benchmarks often suffer from several limitations: they lack a clear definition of reasoning complexity, offer have no control to generate questions over varying difficulty and task customization, and fail to provide structured, step-by-step reasoning annotations (workflows). To bridge these gaps, we formalize reasoning complexity, introduce an adaptive query engine that generates customizable questions of varying complexity with detailed intermediate annotations, and extend the JRDB dataset with human-object interaction and geometric relationship annotations to create JRDB-Reasoning, a benchmark tailored for visual reasoning in human-crowded environments. Our engine and benchmark enable fine-grained evaluation of visual reasoning frameworks and dynamic assessment of visual-language models across reasoning levels.
<div id='section'>PaperID: <span id='pid'>887, <a href='https://arxiv.org/pdf/2508.04338.pdf' target='_blank'>https://arxiv.org/pdf/2508.04338.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaohong Zhong, Alessandro Albini, Giammarco Caroleo, Giorgio Cannata, Perla Maiolino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04338">Improving Tactile Gesture Recognition with Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile gesture recognition systems play a crucial role in Human-Robot Interaction (HRI) by enabling intuitive communication between humans and robots. The literature mainly addresses this problem by applying machine learning techniques to classify sequences of tactile images encoding the pressure distribution generated when executing the gestures. However, some gestures can be hard to differentiate based on the information provided by tactile images alone. In this paper, we present a simple yet effective way to improve the accuracy of a gesture recognition classifier. Our approach focuses solely on processing the tactile images used as input by the classifier. In particular, we propose to explicitly highlight the dynamics of the contact in the tactile image by computing the dense optical flow. This additional information makes it easier to distinguish between gestures that produce similar tactile images but exhibit different contact dynamics. We validate the proposed approach in a tactile gesture recognition task, showing that a classifier trained on tactile images augmented with optical flow information achieved a 9% improvement in gesture classification accuracy compared to one trained on standard tactile images.
<div id='section'>PaperID: <span id='pid'>888, <a href='https://arxiv.org/pdf/2507.05419.pdf' target='_blank'>https://arxiv.org/pdf/2507.05419.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aliasghar Khani, Arianna Rampini, Bruno Roy, Larasika Nadela, Noa Kaplan, Evan Atherton, Derek Cheung, Jacky Bibliowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05419">Motion Generation: A Survey of Generative Approaches and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion generation, the task of synthesizing realistic motion sequences from various conditioning inputs, has become a central problem in computer vision, computer graphics, and robotics, with applications ranging from animation and virtual agents to human-robot interaction. As the field has rapidly progressed with the introduction of diverse modeling paradigms including GANs, autoencoders, autoregressive models, and diffusion-based techniques, each approach brings its own advantages and limitations. This growing diversity has created a need for a comprehensive and structured review that specifically examines recent developments from the perspective of the generative approach employed.
  In this survey, we provide an in-depth categorization of motion generation methods based on their underlying generative strategies. Our main focus is on papers published in top-tier venues since 2023, reflecting the most recent advancements in the field. In addition, we analyze architectural principles, conditioning mechanisms, and generation settings, and compile a detailed overview of the evaluation metrics and datasets used across the literature. Our objective is to enable clearer comparisons and identify open challenges, thereby offering a timely and foundational reference for researchers and practitioners navigating the rapidly evolving landscape of motion generation.
<div id='section'>PaperID: <span id='pid'>889, <a href='https://arxiv.org/pdf/2506.18466.pdf' target='_blank'>https://arxiv.org/pdf/2506.18466.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matti KrÃ¼ger, Daniel Tanneberg, Chao Wang, Stephan Hasler, Michael Gienger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18466">Mirror Eyes: Explainable Human-Robot Interaction at a Glance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gaze of a person tends to reflect their interest. This work explores what happens when this statement is taken literally and applied to robots. Here we present a robot system that employs a moving robot head with a screen-based eye model that can direct the robot's gaze to points in physical space and present a reflection-like mirror image of the attended region on top of each eye. We conducted a user study with 33 participants, who were asked to instruct the robot to perform pick-and-place tasks, monitor the robot's task execution, and interrupt it in case of erroneous actions. Despite a deliberate lack of instructions about the role of the eyes and a very brief system exposure, participants felt more aware about the robot's information processing, detected erroneous actions earlier, and rated the user experience higher when eye-based mirroring was enabled compared to non-reflective eyes. These results suggest a beneficial and intuitive utilization of the introduced method in cooperative human-robot interaction.
<div id='section'>PaperID: <span id='pid'>890, <a href='https://arxiv.org/pdf/2506.14233.pdf' target='_blank'>https://arxiv.org/pdf/2506.14233.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amirreza Payandeh, Anuj Pokhrel, Daeun Song, Marcos Zampieri, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14233">Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.
<div id='section'>PaperID: <span id='pid'>891, <a href='https://arxiv.org/pdf/2505.24786.pdf' target='_blank'>https://arxiv.org/pdf/2505.24786.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Eran Bamani Beeri, Eden Nissinman, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24786">DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic hand gestures play a pivotal role in assistive human-robot interaction (HRI), facilitating intuitive, non-verbal communication, particularly for individuals with mobility constraints or those operating robots remotely. Current gesture recognition methods are mostly limited to short-range interactions, reducing their utility in scenarios demanding robust assistive communication from afar. In this paper, we introduce a novel approach designed specifically for assistive robotics, enabling dynamic gesture recognition at extended distances of up to 30 meters, thereby significantly improving accessibility and quality of life. Our proposed Distance-aware Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust processing and classification of gesture sequences captured under challenging conditions, including significant physical attenuation, reduced resolution, and dynamic gesture variations commonly experienced in real-world assistive environments. We further introduce the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model robustness across varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures. By effectively interpreting gestures from considerable distances, DiG-Net significantly enhances the usability of assistive robots in home healthcare, industrial safety, and remote assistance scenarios, enabling seamless and intuitive interactions for users regardless of physical limitations
<div id='section'>PaperID: <span id='pid'>892, <a href='https://arxiv.org/pdf/2505.13186.pdf' target='_blank'>https://arxiv.org/pdf/2505.13186.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Philipp Scholl, Alexander Dietrich, Sebastian Wolf, Jinoh Lee, Alin-Albu SchÃ¤ffer, Gitta Kutyniok, Maged Iskandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13186">Interpretable Robotic Friction Learning via Symbolic Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately modeling the friction torque in robotic joints has long been challenging due to the request for a robust mathematical description. Traditional model-based approaches are often labor-intensive, requiring extensive experiments and expert knowledge, and they are difficult to adapt to new scenarios and dependencies. On the other hand, data-driven methods based on neural networks are easier to implement but often lack robustness, interpretability, and trustworthiness--key considerations for robotic hardware and safety-critical applications such as human-robot interaction. To address the limitations of both approaches, we propose the use of symbolic regression (SR) to estimate the friction torque. SR generates interpretable symbolic formulas similar to those produced by model-based methods while being flexible to accommodate various dynamic effects and dependencies. In this work, we apply SR algorithms to approximate the friction torque using collected data from a KUKA LWR-IV+ robot. Our results show that SR not only yields formulas with comparable complexity to model-based approaches but also achieves higher accuracy. Moreover, SR-derived formulas can be seamlessly extended to include load dependencies and other dynamic factors.
<div id='section'>PaperID: <span id='pid'>893, <a href='https://arxiv.org/pdf/2505.10872.pdf' target='_blank'>https://arxiv.org/pdf/2505.10872.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenxi Jiang, Chuhao Zhou, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10872">REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.
<div id='section'>PaperID: <span id='pid'>894, <a href='https://arxiv.org/pdf/2504.14822.pdf' target='_blank'>https://arxiv.org/pdf/2504.14822.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rui Qiu, Shijie Chen, Yu Su, Po-Yin Yen, Han-Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14822">Completing A Systematic Review in Hours instead of Months with Interactive AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.
<div id='section'>PaperID: <span id='pid'>895, <a href='https://arxiv.org/pdf/2503.23760.pdf' target='_blank'>https://arxiv.org/pdf/2503.23760.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Manuel Scheibl, Birte Richter, Alissa MÃ¼ller, Michael Beetz, Britta Wrede
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23760">Towards a cognitive architecture to enable natural language interaction in co-constructive task learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research addresses the question, which characteristics a cognitive architecture must have to leverage the benefits of natural language in Co-Constructive Task Learning (CCTL). To provide context, we first discuss Interactive Task Learning (ITL), the mechanisms of the human memory system, and the significance of natural language and multi-modality. Next, we examine the current state of cognitive architectures, analyzing their capabilities to inform a concept of CCTL grounded in multiple sources. We then integrate insights from various research domains to develop a unified framework. Finally, we conclude by identifying the remaining challenges and requirements necessary to achieve CCTL in Human-Robot Interaction (HRI).
<div id='section'>PaperID: <span id='pid'>896, <a href='https://arxiv.org/pdf/2503.19692.pdf' target='_blank'>https://arxiv.org/pdf/2503.19692.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>AndrÃ© GroÃ, Birte Richter, Bjarne Thomzik, Britta Wrede
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19692">Leveraging Cognitive States for Adaptive Scaffolding of Understanding in Explanatory Tasks in HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how scaffolding strategies influence human understanding in human-robot interaction is important for developing effective assistive systems. This empirical study investigates linguistic scaffolding strategies based on negation as an important means that de-biases the user from potential errors but increases processing costs and hesitations as a means to ameliorate processing costs. In an adaptive strategy, the user state with respect to the current state of understanding and processing capacity was estimated via a scoring scheme based on task performance, prior scaffolding strategy, and current eye gaze behavior. In the study, the adaptive strategy of providing negations and hesitations was compared with a non-adaptive strategy of providing only affirmations. The adaptive scaffolding strategy was generated using the computational model SHIFT. Our findings indicate that using adaptive scaffolding strategies with SHIFT tends to (1) increased processing costs, as reflected in longer reaction times, but (2) improved task understanding, evidenced by a lower error rate of almost 23%. We assessed the efficiency of SHIFT's selected scaffolding strategies across different cognitive states, finding that in three out of five states, the error rate was lower compared to the baseline condition. We discuss how these results align with the assumptions of the SHIFT model and highlight areas for refinement. Moreover, we demonstrate how scaffolding strategies, such as negation and hesitation, contribute to more effective human-robot explanatory dialogues.
<div id='section'>PaperID: <span id='pid'>897, <a href='https://arxiv.org/pdf/2503.16447.pdf' target='_blank'>https://arxiv.org/pdf/2503.16447.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>AndrÃ© GroÃ, Birte Richter, Britta Wrede
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16447">SHIFT: An Interdisciplinary Framework for Scaffolding Human Attention and Understanding in Explanatory Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a domain-independent approach for adaptive scaffolding in robotic explanation generation to guide tasks in human-robot interaction. We present a method for incorporating interdisciplinary research results into a computational model as a pre-configured scoring system implemented in a framework called SHIFT. This involves outlining a procedure for integrating concepts from disciplines outside traditional computer science into a robotics computational framework. Our approach allows us to model the human cognitive state into six observable states within the human partner model. To study the pre-configuration of the system, we implement a reinforcement learning approach on top of our model. This approach allows adaptation to individuals who deviate from the configuration of the scoring system. Therefore, in our proof-of-concept evaluation, the model's adaptability on four different user types shows that the models' adaptation performs better, i.e., recouped faster after exploration and has a higher accumulated reward with our pre-configured scoring system than without it. We discuss further strategies of speeding up the learning phase to enable a realistic adaptation behavior to real users. The system is accessible through docker and supports querying via ROS.
<div id='section'>PaperID: <span id='pid'>898, <a href='https://arxiv.org/pdf/2503.15496.pdf' target='_blank'>https://arxiv.org/pdf/2503.15496.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Giulio Antonio Abbo, Maria Jose Pinto-Bernal, Martijn Catrycke, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15496">Fast Multi-Party Open-Ended Conversation with a Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the implementation and evaluation of a conversational agent designed for multi-party open-ended interactions. Leveraging state-of-the-art technologies such as voice direction of arrival, voice recognition, face tracking, and large language models, the system aims to facilitate natural and intuitive human-robot conversations. Deployed on the Furhat robot, the system was tested with 30 participants engaging in open-ended group conversations and then in two overlapping discussions. Quantitative metrics, such as latencies and recognition accuracy, along with qualitative measures from user questionnaires, were collected to assess performance. The results highlight the system's effectiveness in managing multi-party interactions, though improvements are needed in response relevance and latency. This study contributes valuable insights for advancing human-robot interaction, particularly in enhancing the naturalness and engagement in group conversations.
<div id='section'>PaperID: <span id='pid'>899, <a href='https://arxiv.org/pdf/2503.08306.pdf' target='_blank'>https://arxiv.org/pdf/2503.08306.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Steeven Janny, HervÃ© Poirier, Leonid Antsfeld, Guillaume Bono, Gianluca Monaci, Boris Chidlovskii, Francesco Giuliari, Alessio Del Bue, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08306">Reasoning in visual navigation of end-to-end trained agents: a dynamical systems approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at europe.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.
<div id='section'>PaperID: <span id='pid'>900, <a href='https://arxiv.org/pdf/2503.05833.pdf' target='_blank'>https://arxiv.org/pdf/2503.05833.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tobias JÃ¼lg, Wolfram Burgard, Florian Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05833">Refined Policy Distillation: From VLA Generalists to RL Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action Models (VLAs) have demonstrated remarkable generalization capabilities in real-world experiments. However, their success rates are often not on par with expert policies, and they require fine-tuning when the setup changes. In this work, we introduce Refined Policy Distillation (RPD), a novel Reinforcement Learning (RL)-based policy refinement method that bridges this performance gap through a combination of on-policy RL with behavioral cloning. The core idea of RPD is to distill and refine VLAs into compact, high-performing expert policies by guiding the student policy during RL exploration using the actions of a teacher VLA, resulting in increased sample efficiency and faster convergence. We complement our method by fine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in simulation. While this is a key requirement for applying RL, it also yields new insights beyond existing studies on VLA performance in real-world settings. Our experimental results across various manipulation tasks show that RPD enables the RL student to learn expert policies that outperform the VLA teacher in both dense and sparse reward settings, while also achieving faster convergence than the RL baseline. Our approach is even robust to changes in camera perspective and can generalize to task variations that the underlying VLA cannot solve. Our code, dataset, VLA checkpoints, and videos are available at https://refined-policy-distillation.github.io
<div id='section'>PaperID: <span id='pid'>901, <a href='https://arxiv.org/pdf/2502.01536.pdf' target='_blank'>https://arxiv.org/pdf/2502.01536.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01536">VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators. However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a Real-to-Sim-to-Real framework that generates photorealistic and physically interactive "digital twin" simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories.
<div id='section'>PaperID: <span id='pid'>902, <a href='https://arxiv.org/pdf/2501.05628.pdf' target='_blank'>https://arxiv.org/pdf/2501.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Giulio Antonio Abbo, Tony Belpaeme, Micol Spitale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05628">Concerns and Values in Human-Robot Interactions: A Focus on Social Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots, as AI with physical instantiation, inhabit our social and physical world, where their actions have both social and physical consequences, posing challenges for researchers when designing social robots. This study starts with a scoping review to identify discussions and potential concerns arising from interactions with robotic systems. Two focus groups of technology ethics experts then validated a comprehensive list of key topics and values in human-robot interaction (HRI) literature. These insights were integrated into the HRI Value Compass web tool, to help HRI researchers identify ethical values in robot design. The tool was evaluated in a pilot study. This work benefits the HRI community by highlighting key concerns in human-robot interactions and providing an instrument to help researchers design robots that align with human values, ensuring future robotic systems adhere to these values in social applications.
<div id='section'>PaperID: <span id='pid'>903, <a href='https://arxiv.org/pdf/2412.10726.pdf' target='_blank'>https://arxiv.org/pdf/2412.10726.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tao Wu, Chuhao Zhou, Yen Heng Wong, Lin Gu, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10726">NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Vision-Language Models (VLMs) has significantly advanced the development of Embodied Question Answering (EQA), enhancing agents' abilities in language understanding and reasoning within complex and realistic scenarios. However, EQA in real-world scenarios remains challenging, as human-posed questions often contain noise that can interfere with an agent's exploration and response, bringing challenges especially for language beginners and non-expert users. To address this, we introduce a NoisyEQA benchmark designed to evaluate an agent's ability to recognize and correct noisy questions. This benchmark introduces four common types of noise found in real-world applications: Latent Hallucination Noise, Memory Noise, Perception Noise, and Semantic Noise generated through an automated dataset creation framework. Additionally, we also propose a 'Self-Correction' prompting mechanism and a new evaluation metric to enhance and measure both noise detection capability and answer quality. Our comprehensive evaluation reveals that current EQA agents often struggle to detect noise in questions, leading to responses that frequently contain erroneous information. Through our Self-Correct Prompting mechanism, we can effectively improve the accuracy of agent answers.
<div id='section'>PaperID: <span id='pid'>904, <a href='https://arxiv.org/pdf/2412.10402.pdf' target='_blank'>https://arxiv.org/pdf/2412.10402.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Filippo Ziliotto, Tommaso Campari, Luciano Serafini, Lamberto Ballan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10402">TANGO: Training-free Embodied AI Agents for Open-world Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.
<div id='section'>PaperID: <span id='pid'>905, <a href='https://arxiv.org/pdf/2411.18413.pdf' target='_blank'>https://arxiv.org/pdf/2411.18413.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Eran Bamani Beeri, Eden Nissinman, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18413">Robust Dynamic Gesture Recognition at Ultra-Long Distances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic hand gestures play a crucial role in conveying nonverbal information for Human-Robot Interaction (HRI), eliminating the need for complex interfaces. Current models for dynamic gesture recognition suffer from limitations in effective recognition range, restricting their application to close proximity scenarios. In this letter, we present a novel approach to recognizing dynamic gestures in an ultra-range distance of up to 28 meters, enabling natural, directive communication for guiding robots in both indoor and outdoor environments. Our proposed SlowFast-Transformer (SFT) model effectively integrates the SlowFast architecture with Transformer layers to efficiently process and classify gesture sequences captured at ultra-range distances, overcoming challenges of low resolution and environmental noise. We further introduce a distance-weighted loss function shown to enhance learning and improve model robustness at varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 95.1% on a diverse dataset with challenging ultra-range gestures. This enables robots to react appropriately to human commands from a far distance, providing an essential enhancement in HRI, especially in scenarios requiring seamless and natural interaction.
<div id='section'>PaperID: <span id='pid'>906, <a href='https://arxiv.org/pdf/2409.06369.pdf' target='_blank'>https://arxiv.org/pdf/2409.06369.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lukas Rustler, Matej Misar, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06369">Adaptive Electronic Skin Sensitivity for Safe Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial electronic skins covering complete robot bodies can make physical human-robot collaboration safe and hence possible. Standards for collaborative robots (e.g., ISO/TS 15066) prescribe permissible forces and pressures during contacts with the human body. These characteristics of the collision depend on the speed of the colliding robot link but also on its effective mass. Thus, to warrant contacts complying with the Power and Force Limiting (PFL) collaborative regime but at the same time maximizing productivity, protective skin thresholds should be set individually for different parts of the robot bodies and dynamically on the run. Here we present and empirically evaluate four scenarios: (a) static and uniform - fixed thresholds for the whole skin, (b) static but different settings for robot body parts, (c) dynamically set based on every link velocity, (d) dynamically set based on effective mass of every robot link. We perform experiments in simulation and on a real 6-axis collaborative robot arm (UR10e) completely covered with sensitive skin (AIRSKIN) comprising eleven individual pads. On a mock pick-and-place scenario with transient collisions with the robot body parts and two collision reactions (stop and avoid), we demonstrate the boost in productivity in going from the most conservative setting of the skin thresholds (a) to the most adaptive setting (d). The threshold settings for every skin pad are adapted with a frequency of 25 Hz. This work can be easily extended for platforms with more degrees of freedom and larger skin coverage (humanoids) and to social human-robot interaction scenarios where contacts with the robot will be used for communication.
<div id='section'>PaperID: <span id='pid'>907, <a href='https://arxiv.org/pdf/2602.09203.pdf' target='_blank'>https://arxiv.org/pdf/2602.09203.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amy Koike, Serena Ge Guo, Xinning He, Callie Y. Kim, Dakota Sullivan, Bilge Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.09203">Elements of Robot Morphology: Supporting Designers in Robot Form Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.
<div id='section'>PaperID: <span id='pid'>908, <a href='https://arxiv.org/pdf/2602.06974.pdf' target='_blank'>https://arxiv.org/pdf/2602.06974.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Faith Johnson, Bryan Bo Cao, Shubham Jain, Ashwin Ashok, Kristin Dana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.06974">FeudalNav: A Simple Framework for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.
<div id='section'>PaperID: <span id='pid'>909, <a href='https://arxiv.org/pdf/2602.06038.pdf' target='_blank'>https://arxiv.org/pdf/2602.06038.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.06038">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.
<div id='section'>PaperID: <span id='pid'>910, <a href='https://arxiv.org/pdf/2602.03200.pdf' target='_blank'>https://arxiv.org/pdf/2602.03200.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wendi Hu, Haonan Zhou, Wenhao Hu, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03200">Hand3R: Online 4D Hand-Scene Reconstruction in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.
<div id='section'>PaperID: <span id='pid'>911, <a href='https://arxiv.org/pdf/2601.23285.pdf' target='_blank'>https://arxiv.org/pdf/2601.23285.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>MH Farhadi, Ali Rabiee, Sima Ghafoori, Anna Cetera, Andrew Fisher, Reza Abiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.23285">End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.
<div id='section'>PaperID: <span id='pid'>912, <a href='https://arxiv.org/pdf/2601.22387.pdf' target='_blank'>https://arxiv.org/pdf/2601.22387.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Victor Nikhil Antony, Adithya R N, Sarah Derrick, Zhili Gong, Peter M. Donley, Chien-Ming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.22387">Plant-Inspired Robot Design Metaphors for Ambient HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Plants offer a paradoxical model for interaction: they are ambient, low-demand presences that nonetheless shape atmosphere, routines, and relationships through temporal rhythms and subtle expressions. In contrast, most human-robot interaction (HRI) has been grounded in anthropomorphic and zoomorphic paradigms, producing overt, high-demand forms of engagement. Using a Research through Design (RtD) methodology, we explore plants as metaphoric inspiration for HRI; we conducted iterative cycles of ideation, prototyping, and reflection to investigate what design primitives emerge from plant metaphors and morphologies, and how these primitives can be combined into expressive robotic forms. We present a suite of speculative, open-source prototypes that help probe plant-inspired presence, temporality, form, and gestures. We deepened our learnings from design and prototyping through prototype-centered workshops that explored people's perceptions and imaginaries of plant-inspired robots. This work contributes: (1) Set of plant-inspired robotic artifacts; (2) Designerly insights on how people perceive plant-inspired robots; and (3) Design consideration to inform how to use plant metaphors to reshape HRI.
<div id='section'>PaperID: <span id='pid'>913, <a href='https://arxiv.org/pdf/2512.21627.pdf' target='_blank'>https://arxiv.org/pdf/2512.21627.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Botao Ren, Junjun Hu, Xinda Xue, Minghua Luo, Jintao Chen, Haochen Bai, Liangliang You, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21627">AstraNav-Memory: Contexts Compression for Long Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lifelong embodied navigation requires agents to accumulate, retain, and exploit spatial-semantic experience across tasks, enabling efficient exploration in novel environments and rapid goal reaching in familiar ones. While object-centric memory is interpretable, it depends on detection and reconstruction pipelines that limit robustness and scalability. We propose an image-centric memory framework that achieves long-term implicit memory via an efficient visual context compression module end-to-end coupled with a Qwen2.5-VL-based navigation policy. Built atop a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, our visual tokenizer supports configurable compression rates; for example, under a representative 16$\times$ compression setting, each image is encoded with about 30 tokens, expanding the effective context capacity from tens to hundreds of images. Experimental results on GOAT-Bench and HM3D-OVON show that our method achieves state-of-the-art navigation performance, improving exploration in unfamiliar environments and shortening paths in familiar ones. Ablation studies further reveal that moderate compression provides the best balance between efficiency and accuracy. These findings position compressed image-centric memory as a practical and scalable interface for lifelong embodied agents, enabling them to reason over long visual histories and navigate with human-like efficiency.
<div id='section'>PaperID: <span id='pid'>914, <a href='https://arxiv.org/pdf/2512.21430.pdf' target='_blank'>https://arxiv.org/pdf/2512.21430.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yusuf Ali, Gryphon Patlin, Karthik Kothuri, Muhammad Zubair Irshad, Wuwei Liang, Zsolt Kira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21430">EVE: A Generator-Verifier System for Generative Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visuomotor policies based on generative architectures such as diffusion and flow-based matching have shown strong performance but degrade under distribution shifts, demonstrating limited recovery capabilities without costly finetuning. In the language modeling domain, test-time compute scaling has revolutionized reasoning capabilities of modern LLMs by leveraging additional inference-time compute for candidate solution refinement. These methods typically leverage foundation models as verification modules in a zero-shot manner to synthesize improved candidate solutions. In this work, we hypothesize that generative policies can similarly benefit from additional inference-time compute that employs zero-shot VLM-based verifiers. A systematic analysis of improving policy performance through the generation-verification framework remains relatively underexplored in the current literature. To this end, we introduce EVE - a modular, generator-verifier interaction framework - that boosts the performance of pretrained generative policies at test time, with no additional training. EVE wraps a frozen base policy with multiple zero-shot, VLM-based verifier agents. Each verifier proposes action refinements to the base policy candidate actions, while an action incorporator fuses the aggregated verifier output into the base policy action prediction to produce the final executed action. We study design choices for generator-verifier information interfacing across a system of verifiers with distinct capabilities. Across a diverse suite of manipulation tasks, EVE consistently improves task success rates without any additional policy training. Through extensive ablations, we isolate the contribution of verifier capabilities and action incorporator strategies, offering practical guidelines to build scalable, modular generator-verifier systems for embodied control.
<div id='section'>PaperID: <span id='pid'>915, <a href='https://arxiv.org/pdf/2512.09903.pdf' target='_blank'>https://arxiv.org/pdf/2512.09903.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ryan Meegan, Adam D'Souza, Bryan Bo Cao, Shubham Jain, Kristin Dana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09903">YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.
<div id='section'>PaperID: <span id='pid'>916, <a href='https://arxiv.org/pdf/2511.17335.pdf' target='_blank'>https://arxiv.org/pdf/2511.17335.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chiori Hori, Yoshiki Masuyama, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17335">Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.
<div id='section'>PaperID: <span id='pid'>917, <a href='https://arxiv.org/pdf/2511.07750.pdf' target='_blank'>https://arxiv.org/pdf/2511.07750.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Durgakant Pushp, Weizhe Chen, Zheng Chen, Chaomin Luo, Jason M. Gregory, Lantao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07750">Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.
<div id='section'>PaperID: <span id='pid'>918, <a href='https://arxiv.org/pdf/2510.20818.pdf' target='_blank'>https://arxiv.org/pdf/2510.20818.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20818">VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/
<div id='section'>PaperID: <span id='pid'>919, <a href='https://arxiv.org/pdf/2510.08173.pdf' target='_blank'>https://arxiv.org/pdf/2510.08173.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haolin Yang, Yuxing Long, Zhuoyuan Yu, Zihan Yang, Minghan Wang, Jiapeng Xu, Yihan Wang, Ziyan Yu, Wenzhe Cai, Lei Kang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08173">NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.
<div id='section'>PaperID: <span id='pid'>920, <a href='https://arxiv.org/pdf/2510.07869.pdf' target='_blank'>https://arxiv.org/pdf/2510.07869.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07869">USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater environments present unique challenges for robotic operation, including complex hydrodynamics, limited visibility, and constrained communication. Although data-driven approaches have advanced embodied intelligence in terrestrial robots and enabled task-specific autonomous underwater robots, developing underwater intelligence capable of autonomously performing multiple tasks remains highly challenging, as large-scale, high-quality underwater datasets are still scarce. To address these limitations, we introduce USIM, a simulation-based multi-task Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over 561K frames from 1,852 trajectories, totaling approximately 15.6 hours of BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from visual navigation to mobile manipulation. Building upon this dataset, we propose U0, a VLA model for general underwater robots, which integrates binocular vision and other sensor modalities through multimodal fusion, and further incorporates a convolution-attention-based perception focus enhancement module (CAP) to improve spatial understanding and mobile manipulation. Across tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking, the framework achieves a success rate of 80%, while in challenging mobile manipulation tasks, it reduces the distance to the target by 21.2% compared with baseline methods, demonstrating its effectiveness. USIM and U0 show that VLA models can be effectively applied to underwater robotic applications, providing a foundation for scalable dataset construction, improved task autonomy, and the practical realization of intelligent general underwater robots.
<div id='section'>PaperID: <span id='pid'>921, <a href='https://arxiv.org/pdf/2509.25970.pdf' target='_blank'>https://arxiv.org/pdf/2509.25970.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bojun Zhang, Hangjian Ye, Hao Zheng, Jianzheng Huang, Zhengyu Lin, Zhenhong Guo, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25970">PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained 3D part segmentation is crucial for enabling embodied AI systems to perform complex manipulation tasks, such as interacting with specific functional components of an object. However, existing interactive segmentation methods are largely confined to coarse, instance-level targets, while non-interactive approaches struggle with sparse, real-world scans and suffer from a severe lack of annotated data. To address these limitations, we introduce PinPoint3D, a novel interactive framework for fine-grained, multi-granularity 3D segmentation, capable of generating precise part-level masks from only a few user point clicks. A key component of our work is a new 3D data synthesis pipeline that we developed to create a large-scale, scene-level dataset with dense part annotations, overcoming a critical bottleneck that has hindered progress in this field. Through comprehensive experiments and user studies, we demonstrate that our method significantly outperforms existing approaches, achieving an average IoU of around 55.8% on each object part under first-click settings and surpassing 71.3% IoU with only a few additional clicks. Compared to current state-of-the-art baselines, PinPoint3D yields up to a 16% improvement in IoU and precision, highlighting its effectiveness on challenging, sparse point clouds with high efficiency. Our work represents a significant step towards more nuanced and precise machine perception and interaction in complex 3D environments.
<div id='section'>PaperID: <span id='pid'>922, <a href='https://arxiv.org/pdf/2509.23155.pdf' target='_blank'>https://arxiv.org/pdf/2509.23155.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Abdul Monaf Chowdhury, Akm Moshiur Rahman Mazumder, Rabeya Akter, Safaeid Hossain Arib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23155">LAGEA: Language Guided Embodied Agents for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation benefits from foundation models that describe goals, but today's agents still lack a principled way to learn from their own mistakes. We ask whether natural language can serve as feedback, an error reasoning signal that helps embodied agents diagnose what went wrong and correct course. We introduce LAGEA (Language Guided Embodied Agents), a framework that turns episodic, schema-constrained reflections from a vision language model (VLM) into temporally grounded guidance for reinforcement learning. LAGEA summarizes each attempt in concise language, localizes the decisive moments in the trajectory, aligns feedback with visual state in a shared representation, and converts goal progress and feedback agreement into bounded, step-wise shaping rewardswhose influence is modulated by an adaptive, failure-aware coefficient. This design yields dense signals early when exploration needs direction and gracefully recedes as competence grows. On the Meta-World MT10 embodied manipulation benchmark, LAGEA improves average success over the state-of-the-art (SOTA) methods by 9.0% on random goals and 5.3% on fixed goals, while converging faster. These results support our hypothesis: language, when structured and grounded in time, is an effective mechanism for teaching robots to self-reflect on mistakes and make better choices. Code will be released soon.
<div id='section'>PaperID: <span id='pid'>923, <a href='https://arxiv.org/pdf/2509.19954.pdf' target='_blank'>https://arxiv.org/pdf/2509.19954.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pinhao Song, Yurui Du, Ophelie Saussus, Sofie De Schrijver, Irene Caprara, Peter Janssen, Renaud Detry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19954">Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human-robot interaction. RT-V2 jointly models a user's long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human-computer interaction studies with keyboard input, and brain-machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies.
<div id='section'>PaperID: <span id='pid'>924, <a href='https://arxiv.org/pdf/2509.19002.pdf' target='_blank'>https://arxiv.org/pdf/2509.19002.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19002">VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
<div id='section'>PaperID: <span id='pid'>925, <a href='https://arxiv.org/pdf/2509.15404.pdf' target='_blank'>https://arxiv.org/pdf/2509.15404.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoting Peng, Katherine Driggs-Campbell, Roy Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15404">Trust-Aware Embodied Bayesian Persuasion for Mixed-Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and efficient interaction between autonomous vehicles (AVs) and human-driven vehicles (HVs) is a critical challenge for future transportation systems. While game-theoretic models capture how AVs influence HVs, they often suffer from a long-term decay of influence and can be perceived as manipulative, eroding the human's trust. This can paradoxically lead to riskier human driving behavior over repeated interactions. In this paper, we address this challenge by proposing the Trust-Aware Embodied Bayesian Persuasion (TA-EBP) framework. Our work makes three key contributions: First, we apply Bayesian persuasion to model communication at traffic intersections, offering a transparent alternative to traditional game-theoretic models. Second, we introduce a trust parameter to the persuasion framework, deriving a theorem for the minimum trust level required for influence. Finally, we ground the abstract signals of Bayesian persuasion theory into a continuous, physically meaningful action space, deriving a second theorem for the optimal signal magnitude, realized as an AV's forward nudge. Additionally, we validate our framework in a mixed-autonomy traffic simulation, demonstrating that TA-EBP successfully persuades HVs to drive more cautiously, eliminating collisions and improving traffic flow compared to baselines that either ignore trust or lack communication. Our work provides a transparent and non-strategic framework for influence in human-robot interaction, enhancing both safety and efficiency.
<div id='section'>PaperID: <span id='pid'>926, <a href='https://arxiv.org/pdf/2509.09889.pdf' target='_blank'>https://arxiv.org/pdf/2509.09889.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Giulia Botta, Marco Botta, Cristina Gena, Alessandro Mazzei, Massimo Donini, Alberto Lillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09889">Using the Pepper Robot to Support Sign Language Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are increasingly experimented in public and assistive settings, but their accessibility for Deaf users remains quite underexplored. Italian Sign Language (LIS) is a fully-fledged natural language that relies on complex manual and non-manual components. Enabling robots to communicate using LIS could foster more inclusive human robot interaction, especially in social environments such as hospitals, airports, or educational settings. This study investigates whether a commercial social robot, Pepper, can produce intelligible LIS signs and short signed LIS sentences. With the help of a Deaf student and his interpreter, an expert in LIS, we co-designed and implemented 52 LIS signs on Pepper using either manual animation techniques or a MATLAB based inverse kinematics solver. We conducted a exploratory user study involving 12 participants proficient in LIS, both Deaf and hearing. Participants completed a questionnaire featuring 15 single-choice video-based sign recognition tasks and 2 open-ended questions on short signed sentences. Results shows that the majority of isolated signs were recognized correctly, although full sentence recognition was significantly lower due to Pepper's limited articulation and temporal constraints. Our findings demonstrate that even commercially available social robots like Pepper can perform a subset of LIS signs intelligibly, offering some opportunities for a more inclusive interaction design. Future developments should address multi-modal enhancements (e.g., screen-based support or expressive avatars) and involve Deaf users in participatory design to refine robot expressivity and usability.
<div id='section'>PaperID: <span id='pid'>927, <a href='https://arxiv.org/pdf/2509.02749.pdf' target='_blank'>https://arxiv.org/pdf/2509.02749.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Giorgia Buracchio, Ariele Callegari, Massimo Donini, Cristina Gena, Antonio Lieto, Alberto Lillo, Claudio Mattutino, Alessandro Mazzei, Linda Pigureddu, Manuel Striani, Fabiana Vernero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02749">The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper presents an experiment on the effects of adaptive emotional alignment between agents, considered a prerequisite for empathic communication, in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the robot's persuasive effectiveness, (ii) the user's communication style, and (iii) the attribution of mental states and empathy to the robot. In an experiment with 42 participants, two conditions were compared: one with neutral communication and another where the robot provided responses adapted to the emotions expressed by the users. The results show that emotional alignment does not influence users' communication styles or have a persuasive effect. However, it significantly influences attribution of mental states to the robot and its perceived empathy
<div id='section'>PaperID: <span id='pid'>928, <a href='https://arxiv.org/pdf/2508.20812.pdf' target='_blank'>https://arxiv.org/pdf/2508.20812.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20812">Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art.
<div id='section'>PaperID: <span id='pid'>929, <a href='https://arxiv.org/pdf/2507.01667.pdf' target='_blank'>https://arxiv.org/pdf/2507.01667.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gianluca Monaci, Philippe Weinzaepfel, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01667">What does really matter in image goal navigation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image goal navigation requires two different skills: firstly, core navigation skills, including the detection of free space and obstacles, and taking decisions based on an internal representation; and secondly, computing directional information by comparing visual observations to the goal image. Current state-of-the-art methods either rely on dedicated image-matching, or pre-training of computer vision modules on relative pose estimation. In this paper, we study whether this task can be efficiently solved with end-to-end training of full agents with RL, as has been claimed by recent work. A positive answer would have impact beyond Embodied AI and allow training of relative pose estimation from reward for navigation alone. In a large study we investigate the effect of architectural choices like late fusion, channel stacking, space-to-depth projections and cross-attention, and their role in the emergence of relative pose estimators from navigation training. We show that the success of recent methods is influenced up to a certain extent by simulator settings, leading to shortcuts in simulation. However, we also show that these capabilities can be transferred to more realistic setting, up to some extend. We also find evidence for correlations between navigation performance and probed (emerging) relative pose estimation performance, an important sub skill.
<div id='section'>PaperID: <span id='pid'>930, <a href='https://arxiv.org/pdf/2506.17561.pdf' target='_blank'>https://arxiv.org/pdf/2506.17561.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, Lin Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17561">VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.
<div id='section'>PaperID: <span id='pid'>931, <a href='https://arxiv.org/pdf/2503.16451.pdf' target='_blank'>https://arxiv.org/pdf/2503.16451.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenhui Tan, Boyuan Li, Chuhao Jin, Wenbing Huang, Xiting Wang, Ruihua Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16451">Think-Then-React: Towards Unconstrained Human Action-to-Reaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games. Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion. To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions. First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts. Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding. Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.
<div id='section'>PaperID: <span id='pid'>932, <a href='https://arxiv.org/pdf/2503.00283.pdf' target='_blank'>https://arxiv.org/pdf/2503.00283.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Victor Nikhil Antony, Maia Stiber, Chien-Ming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00283">Xpress: A System For Dynamic, Context-Aware Robot Facial Expressions using Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expressions are vital in human communication and significantly influence outcomes in human-robot interaction (HRI), such as likeability, trust, and companionship. However, current methods for generating robotic facial expressions are often labor-intensive, lack adaptability across contexts and platforms, and have limited expressive ranges--leading to repetitive behaviors that reduce interaction quality, particularly in long-term scenarios. We introduce Xpress, a system that leverages language models (LMs) to dynamically generate context-aware facial expressions for robots through a three-phase process: encoding temporal flow, conditioning expressions on context, and generating facial expression code. We demonstrated Xpress as a proof-of-concept through two user studies (n=15x2) and a case study with children and parents (n=13), in storytelling and conversational scenarios to assess the system's context-awareness, expressiveness, and dynamism. Results demonstrate Xpress's ability to dynamically produce expressive and contextually appropriate facial expressions, highlighting its versatility and potential in HRI applications.
<div id='section'>PaperID: <span id='pid'>933, <a href='https://arxiv.org/pdf/2412.10694.pdf' target='_blank'>https://arxiv.org/pdf/2412.10694.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junliang Li, Kai Ye, Haolan Kang, Mingxuan Liang, Yuhang Wu, Zhenhua Liu, Huiping Zhuang, Rui Huang, Yongquan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10694">Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, as robotics has advanced, human-robot collaboration has gained increasing importance. However, current robots struggle to fully and accurately interpret human intentions from voice commands alone. Traditional gripper and suction systems often fail to interact naturally with humans, lack advanced manipulation capabilities, and are not adaptable to diverse tasks, especially in unstructured environments. This paper introduces the Embodied Dexterous Grasping System (EDGS), designed to tackle object grasping in cluttered environments for human-robot interaction. We propose a novel approach to semantic-object alignment using a Vision-Language Model (VLM) that fuses voice commands and visual information, significantly enhancing the alignment of multi-dimensional attributes of target objects in complex scenarios. Inspired by human hand-object interactions, we develop a robust, precise, and efficient grasping strategy, incorporating principles like the thumb-object axis, multi-finger wrapping, and fingertip interaction with an object's contact mechanics. We also design experiments to assess Referring Expression Representation Enrichment (RERE) in referring expression segmentation, demonstrating that our system accurately detects and matches referring expressions. Extensive experiments confirm that EDGS can effectively handle complex grasping tasks, achieving stability and high success rates, highlighting its potential for further development in the field of Embodied AI.
<div id='section'>PaperID: <span id='pid'>934, <a href='https://arxiv.org/pdf/2411.09893.pdf' target='_blank'>https://arxiv.org/pdf/2411.09893.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Faith Johnson, Bryan Bo Cao, Ashwin Ashok, Shubham Jain, Kristin Dana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09893">Memory Proxy Maps for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation takes inspiration from humans, who navigate in previously unseen environments using vision without detailed environment maps. Inspired by this, we introduce a novel no-RL, no-graph, no-odometry approach to visual navigation using feudal learning to build a three tiered agent. Key to our approach is a memory proxy map (MPM), an intermediate representation of the environment learned in a self-supervised manner by the high-level manager agent that serves as a simplified memory, approximating what the agent has seen. We demonstrate that recording observations in this learned latent space is an effective and efficient memory proxy that can remove the need for graphs and odometry in visual navigation tasks. For the mid-level manager agent, we develop a waypoint network (WayNet) that outputs intermediate subgoals, or waypoints, imitating human waypoint selection during local navigation. For the low-level worker agent, we learn a classifier over a discrete action space that avoids local obstacles and moves the agent towards the WayNet waypoint. The resulting feudal navigation network offers a novel approach with no RL, no graph, no odometry, and no metric map; all while achieving SOTA results on the image goal navigation task.
<div id='section'>PaperID: <span id='pid'>935, <a href='https://arxiv.org/pdf/2410.05791.pdf' target='_blank'>https://arxiv.org/pdf/2410.05791.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruocheng Wang, Pei Xu, Haochen Shi, Elizabeth Schumann, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05791">FÃ¼rElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.
<div id='section'>PaperID: <span id='pid'>936, <a href='https://arxiv.org/pdf/2409.14296.pdf' target='_blank'>https://arxiv.org/pdf/2409.14296.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Naoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14296">HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-20 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.
<div id='section'>PaperID: <span id='pid'>937, <a href='https://arxiv.org/pdf/2602.14831.pdf' target='_blank'>https://arxiv.org/pdf/2602.14831.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dániel Szabó, Aku Visuri, Benjamin Tag, Simo Hosio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.14831">Robot-Wearable Conversation Hand-off for Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating large and complex indoor environments, such as universities, airports, and hospitals, can be cognitively demanding and requires attention and effort. While mobile applications provide convenient navigation support, they occupy the user's hands and visual attention, limiting natural interaction. In this paper, we explore conversation hand-off as a method for multi-device indoor navigation, where a Conversational Agent (CA) transitions seamlessly from a stationary social robot to a wearable device. We evaluated robot-only, wearable-only, and robot-to-wearable hand-off in a university campus setting using a within-subjects design with N=24 participants. We find that conversation hand-off is experienced as engaging, even though no performance benefits were observed, and most preferred using the wearable-only system. Our findings suggest that the design of such re-embodied assistants should maintain a shared voice and state across embodiments. We demonstrate how conversational hand-offs can bridge cognitive and physical transitions, enriching human interaction with embodied AI.
<div id='section'>PaperID: <span id='pid'>938, <a href='https://arxiv.org/pdf/2601.19839.pdf' target='_blank'>https://arxiv.org/pdf/2601.19839.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jeanne Malécot, Hamed Rahimi, Jeanne Cattoni, Marie Samson, Mouad Abrini, Mahdi Khoramshahi, Maribel Pino, Mohamed Chetouani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19839">HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.
<div id='section'>PaperID: <span id='pid'>939, <a href='https://arxiv.org/pdf/2601.19462.pdf' target='_blank'>https://arxiv.org/pdf/2601.19462.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Riccardo Zanella, Federico Califano, Stefano Stramigioli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19462">Physical Human-Robot Interaction: A Critical Review of Safety Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to provide a clear and rigorous understanding of commonly recognized safety constraints in physical human-robot interaction, i.e. ISO/TS 15066, by examining how they are obtained and which assumptions support them. We clarify the interpretation and practical impact of key simplifying assumptions, show how these modeling choices affect both safety and performance across the system, and indicate specific design parameters that can be adjusted in safety-critical control implementations. Numerical examples are provided to quantify performance degradation induced by common approximations and simplifying design choices. Furthermore, the fundamental role of energy in safety assessment is emphasized, and focused insights are offered on the existing body of work concerning energy-based safety methodologies.
<div id='section'>PaperID: <span id='pid'>940, <a href='https://arxiv.org/pdf/2512.20206.pdf' target='_blank'>https://arxiv.org/pdf/2512.20206.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20206">TongSIM: A General Platform for Simulating Intelligent Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.
<div id='section'>PaperID: <span id='pid'>941, <a href='https://arxiv.org/pdf/2512.17435.pdf' target='_blank'>https://arxiv.org/pdf/2512.17435.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Teng Wang, Xinxin Zhao, Wenzhe Cai, Changyin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17435">ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.
<div id='section'>PaperID: <span id='pid'>942, <a href='https://arxiv.org/pdf/2512.00493.pdf' target='_blank'>https://arxiv.org/pdf/2512.00493.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Boshi Tang, Henry Zheng, Rui Huang, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00493">CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.
<div id='section'>PaperID: <span id='pid'>943, <a href='https://arxiv.org/pdf/2511.09104.pdf' target='_blank'>https://arxiv.org/pdf/2511.09104.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amirhossein Kazemipour, Robert K. Katzschmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09104">Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.
<div id='section'>PaperID: <span id='pid'>944, <a href='https://arxiv.org/pdf/2510.07067.pdf' target='_blank'>https://arxiv.org/pdf/2510.07067.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daria Pugacheva, Andrey Moskalenko, Denis Shepelev, Andrey Kuznetsov, Vlad Shakhuro, Elena Tutubalina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07067">Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action (VLA) models are widely used in Embodied AI, enabling robots to interpret and execute language instructions. However, their robustness to natural language variability in real-world scenarios has not been thoroughly investigated. In this work, we present a novel systematic study of the robustness of state-of-the-art VLA models under linguistic perturbations. Specifically, we evaluate model performance under two types of instruction noise: (1) human-generated paraphrasing and (2) the addition of irrelevant context. We further categorize irrelevant contexts into two groups according to their length and their semantic and lexical proximity to robot commands. In this study, we observe consistent performance degradation as context size expands. We also demonstrate that the model can exhibit relative robustness to random context, with a performance drop within 10%, while semantically and lexically similar context of the same length can trigger a quality decline of around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To mitigate this, we propose an LLM-based filtering framework that extracts core commands from noisy inputs. Incorporating our filtering step allows models to recover up to 98.5% of their original performance under noisy conditions.
<div id='section'>PaperID: <span id='pid'>945, <a href='https://arxiv.org/pdf/2509.21986.pdf' target='_blank'>https://arxiv.org/pdf/2509.21986.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21986">Developing Vision-Language-Action Model from Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $Ï_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.
<div id='section'>PaperID: <span id='pid'>946, <a href='https://arxiv.org/pdf/2509.18576.pdf' target='_blank'>https://arxiv.org/pdf/2509.18576.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zeyi Kang, Liang He, Yanxin Zhang, Zuheng Ming, Kaixing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18576">LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.
<div id='section'>PaperID: <span id='pid'>947, <a href='https://arxiv.org/pdf/2509.00218.pdf' target='_blank'>https://arxiv.org/pdf/2509.00218.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aleksandra Landowska, Aislinn D Gomez Bergin, Ayodeji O. Abioye, Jayati Deshmukh, Andriana Bouadouki, Maria Wheadon, Athina Georgara, Dominic Price, Tuyen Nguyen, Shuang Ao, Lokesh Singh, Yi Long, Raffaele Miele, Joel E. Fischer, Sarvapali D. Ramchurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00218">Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting -- UKAIRS 2025 (Copy)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces and overviews a multidisciplinary project aimed at developing responsible and adaptive multi-human multi-robot (MHMR) systems for complex, dynamic settings. The project integrates co-design, ethical frameworks, and multimodal sensing to create AI-driven robots that are emotionally responsive, context-aware, and aligned with the needs of diverse users. We outline the project's vision, methodology, and early outcomes, demonstrating how embodied AI can support sustainable, ethical, and human-centred futures.
<div id='section'>PaperID: <span id='pid'>948, <a href='https://arxiv.org/pdf/2508.19257.pdf' target='_blank'>https://arxiv.org/pdf/2508.19257.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19257">TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.
<div id='section'>PaperID: <span id='pid'>949, <a href='https://arxiv.org/pdf/2508.02219.pdf' target='_blank'>https://arxiv.org/pdf/2508.02219.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02219">CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.
<div id='section'>PaperID: <span id='pid'>950, <a href='https://arxiv.org/pdf/2508.02062.pdf' target='_blank'>https://arxiv.org/pdf/2508.02062.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02062">RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $Ï_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$Ï_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.
<div id='section'>PaperID: <span id='pid'>951, <a href='https://arxiv.org/pdf/2507.02521.pdf' target='_blank'>https://arxiv.org/pdf/2507.02521.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ayodeji O. Abioye, Jayati Deshmukh, Athina Georgara, Dominic Price, Tuyen Nguyen, Aleksandra Landowska, Amel Bennaceur, Joel E. Fischer, Sarvapali D. Ramchurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02521">Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research investigates strategies for multi-robot coordination in multi-human environments. It proposes a multi-objective learning-based coordination approach to addressing the problem of path planning, navigation, task scheduling, task allocation, and human-robot interaction in multi-human multi-robot (MHMR) settings.
<div id='section'>PaperID: <span id='pid'>952, <a href='https://arxiv.org/pdf/2506.17516.pdf' target='_blank'>https://arxiv.org/pdf/2506.17516.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhou Chen, Sanjoy Kundu, Harsimran S. Baweja, Sathyanarayanan N. Aakur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17516">EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active event perception, the ability to dynamically detect, track, and summarize events in real time, is essential for embodied intelligence in tasks such as human-AI collaboration, assistive robotics, and autonomous navigation. However, existing approaches often depend on predefined action spaces, annotated datasets, and extrinsic rewards, limiting their adaptability and scalability in dynamic, real-world scenarios. Inspired by cognitive theories of event perception and predictive coding, we propose EASE, a self-supervised framework that unifies spatiotemporal representation learning and embodied control through free energy minimization. EASE leverages prediction errors and entropy as intrinsic signals to segment events, summarize observations, and actively track salient actors, operating without explicit annotations or external rewards. By coupling a generative perception model with an action-driven control policy, EASE dynamically aligns predictions with observations, enabling emergent behaviors such as implicit memory, target continuity, and adaptability to novel environments. Extensive evaluations in simulation and real-world settings demonstrate EASE's ability to achieve privacy-preserving and scalable event perception, providing a robust foundation for embodied systems in unscripted, dynamic tasks.
<div id='section'>PaperID: <span id='pid'>953, <a href='https://arxiv.org/pdf/2506.05651.pdf' target='_blank'>https://arxiv.org/pdf/2506.05651.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05651">Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding relationships between objects is central to visual intelligence, with applications in embodied AI, assistive systems, and scene understanding. Yet, most visual relationship detection (VRD) models rely on a fixed predicate set, limiting their generalization to novel interactions. A key challenge is the inability to visually ground semantically plausible, but unannotated, relationships hypothesized from external knowledge. This work introduces an iterative visual grounding framework that leverages large language models (LLMs) as structured relational priors. Inspired by expectation-maximization (EM), our method alternates between generating candidate scene graphs from detected objects using an LLM (expectation) and training a visual model to align these hypotheses with perceptual evidence (maximization). This process bootstraps relational understanding beyond annotated data and enables generalization to unseen predicates. Additionally, we introduce a new benchmark for open-world VRD on Visual Genome with 21 held-out predicates and evaluate under three settings: seen, unseen, and mixed. Our model outperforms LLM-only, few-shot, and debiased baselines, achieving mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on these three sets. These results highlight the promise of grounded LLM priors for scalable open-world visual understanding.
<div id='section'>PaperID: <span id='pid'>954, <a href='https://arxiv.org/pdf/2505.06378.pdf' target='_blank'>https://arxiv.org/pdf/2505.06378.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxiang Wei, Zhuoqi Zeng, Yue Zhong, Jiawen Kang, Ryan Wen Liu, M. Shamim Hossain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06378">Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of large language models and embodied Artificial Intelligence (AI) in the intelligent transportation scenarios, the combination of them in intelligent transportation spawns the Vehicular Embodied AI Network (VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local advanced AI applications are defined as vehicular embodied AI agents, enabling capabilities such as environment perception and multi-agent collaboration. Due to computation latency and resource constraints, the local AI applications and services running on vehicular embodied AI agents need to be migrated, and subsequently referred to as vehicular embodied AI agent twins, which drive the advancement of vehicular embodied AI networks to offload intensive tasks to Roadside Units (RSUs), mitigating latency problems while maintaining service quality. Recognizing workload imbalance among RSUs in traditional approaches, we model AV-RSU interactions as a Stackelberg game to optimize bandwidth resource allocation for efficient migration. A Tiny Multi-Agent Bidirectional LSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to approximate the Stackelberg equilibrium through decentralized coordination. Furthermore, a personalized neural network pruning algorithm based on Path eXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities by identifying task-critical parameters in trained models, reducing model complexity with less performance degradation. Experimental validation confirms the algorithm's effectiveness in balancing system load and minimizing delays, demonstrating significant improvements in vehicular embodied AI agent deployment.
<div id='section'>PaperID: <span id='pid'>955, <a href='https://arxiv.org/pdf/2505.05589.pdf' target='_blank'>https://arxiv.org/pdf/2505.05589.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jingzhong Lin, Xinru Li, Yuanyuan Qi, Bohao Zhang, Wenxiang Liu, Kecheng Tang, Wenxuan Huang, Xiangfeng Xu, Bangyan Li, Changbo Wang, Gaoqi He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05589">ReactDance: Hierarchical Representation for High-Fidelity and Coherent Long-Form Reactive Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reactive dance generation (RDG), the task of generating a dance conditioned on a lead dancer's motion, holds significant promise for enhancing human-robot interaction and immersive digital entertainment. Despite progress in duet synchronization and motion-music alignment, two key challenges remain: generating fine-grained spatial interactions and ensuring long-term temporal coherence. In this work, we introduce \textbf{ReactDance}, a diffusion framework that operates on a novel hierarchical latent space to address these spatiotemporal challenges in RDG. First, for high-fidelity spatial expression and fine-grained control, we propose Hierarchical Finite Scalar Quantization (\textbf{HFSQ}). This multi-scale motion representation effectively disentangles coarse body posture from subtle limb dynamics, enabling independent and detailed control over both aspects through a layered guidance mechanism. Second, to efficiently generate long sequences with high temporal coherence, we propose Blockwise Local Context (\textbf{BLC}), a non-autoregressive sampling strategy. Departing from slow, frame-by-frame generation, BLC partitions the sequence into blocks and synthesizes them in parallel via periodic causal masking and positional encodings. Coherence across these blocks is ensured by a dense sliding-window training approach that enriches the representation with local temporal context. Extensive experiments show that ReactDance substantially outperforms state-of-the-art methods in motion quality, long-term coherence, and sampling efficiency.
<div id='section'>PaperID: <span id='pid'>956, <a href='https://arxiv.org/pdf/2504.00848.pdf' target='_blank'>https://arxiv.org/pdf/2504.00848.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yushan Zhang, AljoÅ¡a OÅ¡ep, Laura Leal-TaixÃ©, Tim Meinhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00848">Zero-Shot 4D Lidar Panoptic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot 4D segmentation and recognition of arbitrary objects in Lidar is crucial for embodied navigation, with applications ranging from streaming perception to semantic mapping and localization. However, the primary challenge in advancing research and developing generalized, versatile methods for spatio-temporal scene understanding in Lidar lies in the scarcity of datasets that provide the necessary diversity and scale of annotations.To overcome these challenges, we propose SAL-4D (Segment Anything in Lidar--4D), a method that utilizes multi-modal robotic sensor setups as a bridge to distill recent developments in Video Object Segmentation (VOS) in conjunction with off-the-shelf Vision-Language foundation models to Lidar. We utilize VOS models to pseudo-label tracklets in short video sequences, annotate these tracklets with sequence-level CLIP tokens, and lift them to the 4D Lidar space using calibrated multi-modal sensory setups to distill them to our SAL-4D model. Due to temporal consistent predictions, we outperform prior art in 3D Zero-Shot Lidar Panoptic Segmentation (LPS) over $5$ PQ, and unlock Zero-Shot 4D-LPS.
<div id='section'>PaperID: <span id='pid'>957, <a href='https://arxiv.org/pdf/2503.06469.pdf' target='_blank'>https://arxiv.org/pdf/2503.06469.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>George Tang, Aditya Agarwal, Weiqiao Han, Trevor Darrell, Yutong Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06469">Vector Quantized Feature Fields for Fast 3D Semantic Lifting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We generalize lifting to semantic lifting by incorporating per-view masks that indicate relevant pixels for lifting tasks. These masks are determined by querying corresponding multiscale pixel-aligned feature maps, which are derived from scene representations such as distilled feature fields and feature point clouds. However, storing per-view feature maps rendered from distilled feature fields is impractical, and feature point clouds are expensive to store and query. To enable lightweight on-demand retrieval of pixel-aligned relevance masks, we introduce the Vector-Quantized Feature Field. We demonstrate the effectiveness of the Vector-Quantized Feature Field on complex indoor and outdoor scenes. Semantic lifting, when paired with a Vector-Quantized Feature Field, can unlock a myriad of applications in scene representation and embodied intelligence. Specifically, we showcase how our method enables text-driven localized scene editing and significantly improves the efficiency of embodied question answering.
<div id='section'>PaperID: <span id='pid'>958, <a href='https://arxiv.org/pdf/2503.03911.pdf' target='_blank'>https://arxiv.org/pdf/2503.03911.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ahmad Hafez, Alireza Naderi Akhormeh, Amr Hegazy, Amr Alanwar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03911">Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of Large Language Models (LLMs) in robotic systems presents unique safety challenges, particularly in unpredictable environments. Although LLMs, leveraging zero-shot learning, enhance human-robot interaction and decision-making capabilities, their inherent probabilistic nature and lack of formal guarantees raise significant concerns for safety-critical applications. Traditional model-based verification approaches often rely on precise system models, which are difficult to obtain for real-world robotic systems and may not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or environmental uncertainties. To address these challenges, this paper introduces a safety assurance framework for LLM-controlled robots based on data-driven reachability analysis, a formal verification technique that ensures all possible system trajectories remain within safe operational limits. Our framework specifically investigates the problem of instructing an LLM to navigate the robot to a specified goal and assesses its ability to generate low-level control actions that successfully guide the robot safely toward that goal. By leveraging historical data to construct reachable sets of states for the robot-LLM system, our approach provides rigorous safety guarantees against unsafe behaviors without relying on explicit analytical models. We validate the framework through experimental case studies in autonomous navigation and task planning, demonstrating its effectiveness in mitigating risks associated with LLM-generated commands. This work advances the integration of formal methods into LLM-based robotics, offering a principled and practical approach to ensuring safety in next-generation autonomous systems.
<div id='section'>PaperID: <span id='pid'>959, <a href='https://arxiv.org/pdf/2503.00480.pdf' target='_blank'>https://arxiv.org/pdf/2503.00480.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andreas Christou, Daniel F. N. Gordon, Theodoros Stouraitis, Juan C. Moreno, Sethu Vijayakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00480">Model-based optimisation for the personalisation of robot-assisted gait training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalised rehabilitation can be key to promoting gait independence and quality of life. Robots can enhance therapy by systematically delivering support in gait training, but often use one-size-fits-all control methods, which can be suboptimal. Here, we describe a model-based optimisation method for designing and fine-tuning personalised robotic controllers. As a case study, we formulate the objective of providing assistance as needed as an optimisation problem, and we demonstrate how musculoskeletal modelling can be used to develop personalised interventions. Eighteen healthy participants (age = 26 +/- 4) were recruited and the personalised control parameters for each were obtained to provide assistance as needed during a unilateral tracking task. A comparison was carried out between the personalised controller and the non-personalised controller. In simulation, a significant improvement was predicted when the personalised parameters were used. Experimentally, responses varied: six subjects showed significant improvements with the personalised parameters, eight subjects showed no obvious change, while four subjects performed worse. High interpersonal and intra-personal variability was observed with both controllers. This study highlights the importance of personalised control in robot-assisted gait training, and the need for a better estimation of human-robot interaction and human behaviour to realise the benefits of model-based optimisation.
<div id='section'>PaperID: <span id='pid'>960, <a href='https://arxiv.org/pdf/2502.14185.pdf' target='_blank'>https://arxiv.org/pdf/2502.14185.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Parag Khanna, Andreas Naoum, Elmira Yadollahi, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14185">REFLEX Dataset: A Multimodal Dataset of Human Reactions to Robot Failures and Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents REFLEX: Robotic Explanations to FaiLures and Human EXpressions, a comprehensive multimodal dataset capturing human reactions to robot failures and subsequent explanations in collaborative settings. It aims to facilitate research into human-robot interaction dynamics, addressing the need to study reactions to both initial failures and explanations, as well as the evolution of these reactions in long-term interactions. By providing rich, annotated data on human responses to different types of failures, explanation levels, and explanation varying strategies, the dataset contributes to the development of more robust, adaptive, and satisfying robotic systems capable of maintaining positive relationships with human collaborators, even during challenges like repeated failures.
<div id='section'>PaperID: <span id='pid'>961, <a href='https://arxiv.org/pdf/2502.01092.pdf' target='_blank'>https://arxiv.org/pdf/2502.01092.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dabin Kim, Inkyu Jang, Youngsoo Han, Sunwoo Hwang, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01092">Enhancing Feature Tracking Reliability for Visual Navigation using Real-Time Safety Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are extensively used for localizing a robot's pose, particularly in environments where global localization tools such as GPS or motion capture systems are unavailable. In many visual navigation systems, localization is achieved by detecting and tracking visual features or landmarks, which provide information about the sensor's relative pose. For reliable feature tracking and accurate pose estimation, it is crucial to maintain visibility of a sufficient number of features. This requirement can sometimes conflict with the robot's overall task objective. In this paper, we approach it as a constrained control problem. By leveraging the invariance properties of visibility constraints within the robot's kinematic model, we propose a real-time safety filter based on quadratic programming. This filter takes a reference velocity command as input and produces a modified velocity that minimally deviates from the reference while ensuring the information score from the currently visible features remains above a user-specified threshold. Numerical simulations demonstrate that the proposed safety filter preserves the invariance condition and ensures the visibility of more features than the required minimum. We also validated its real-world performance by integrating it into a visual simultaneous localization and mapping (SLAM) algorithm, where it maintained high estimation quality in challenging environments, outperforming a simple tracking controller.
<div id='section'>PaperID: <span id='pid'>962, <a href='https://arxiv.org/pdf/2501.02127.pdf' target='_blank'>https://arxiv.org/pdf/2501.02127.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Parag Khanna, Elmira Yadollahi, Iolanda Leite, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02127">How do Humans take an Object from a Robot: Behavior changes observed in a User Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate human-robot interaction and gain human trust, a robot should recognize and adapt to changes in human behavior. This work documents different human behaviors observed while taking objects from an interactive robot in an experimental study, categorized across two dimensions: pull force applied and handedness. We also present the changes observed in human behavior upon repeated interaction with the robot to take various objects.
<div id='section'>PaperID: <span id='pid'>963, <a href='https://arxiv.org/pdf/2501.01366.pdf' target='_blank'>https://arxiv.org/pdf/2501.01366.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Austin T. Wang, ZeMing Gong, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01366">ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.
<div id='section'>PaperID: <span id='pid'>964, <a href='https://arxiv.org/pdf/2410.16919.pdf' target='_blank'>https://arxiv.org/pdf/2410.16919.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tomoyuki Kagaya, Yuxuan Lou, Thong Jing Yuan, Subramanian Lakshmi, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Koki Oguri, Felix Wick, Yang You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16919">EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities. However, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation. To address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks. We validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.
<div id='section'>PaperID: <span id='pid'>965, <a href='https://arxiv.org/pdf/2410.13407.pdf' target='_blank'>https://arxiv.org/pdf/2410.13407.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kui Yang, Nieqing Cao, Yan Ding, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13407">BestMan: A Modular Mobile Manipulator Platform for Embodied AI with Unified Simulation-Hardware APIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) emphasizes agents' ability to perceive, understand, and act in physical environments. Simulation platforms play a crucial role in advancing this field by enabling the validation and optimization of algorithms. However, existing platforms face challenges such as multilevel technical integration complexity, insufficient modularity, interface heterogeneity, and adaptation to diverse hardware. We present BestMan, a simulation platform based on PyBullet, designed to address these issues. BestMan introduces an integrated multilevel skill chain for seamless coordination across perception, planning, and control; a highly modular architecture for flexible algorithm integration; unified interfaces for smooth simulation-to-reality transfer; and a hardware-agnostic approach for adapting to various mobile manipulator configurations. These features collectively simplify development and enhance platform expandability, making BestMan a valuable tool for Embodied AI research.
<div id='section'>PaperID: <span id='pid'>966, <a href='https://arxiv.org/pdf/2602.17128.pdf' target='_blank'>https://arxiv.org/pdf/2602.17128.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Huishi Huang, Jack Klusmann, Haozhe Wang, Shuchen Ji, Fengkang Ying, Yiyuan Zhang, John Nassour, Gordon Cheng, Daniela Rus, Jun Liu, Marcelo H Ang, Cecilia Laschi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.17128">Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.
<div id='section'>PaperID: <span id='pid'>967, <a href='https://arxiv.org/pdf/2602.14837.pdf' target='_blank'>https://arxiv.org/pdf/2602.14837.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lorenzo Mur Labadia, Ruben Martinez-Cantin, Jose J. Guerrero, Giovanni M. Farinella, Antonino Furnari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.14837">Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.
<div id='section'>PaperID: <span id='pid'>968, <a href='https://arxiv.org/pdf/2602.07434.pdf' target='_blank'>https://arxiv.org/pdf/2602.07434.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Songhua Yang, Xuetao Li, Xuanye Fei, Mengde Li, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07434">Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \underline{\textit{S}}peech, \underline{\textit{E}}motion, and \underline{\textit{M}}otion, we present \textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \underline{\textit{e}}dge-deployed versions (\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.
<div id='section'>PaperID: <span id='pid'>969, <a href='https://arxiv.org/pdf/2602.04208.pdf' target='_blank'>https://arxiv.org/pdf/2602.04208.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hyeonbeom Choi, Daechul Ahn, Youhan Lee, Taewook Kang, Seongwon Cho, Jonghyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04208">SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.
<div id='section'>PaperID: <span id='pid'>970, <a href='https://arxiv.org/pdf/2602.03782.pdf' target='_blank'>https://arxiv.org/pdf/2602.03782.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li, Bing Li, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03782">QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.
<div id='section'>PaperID: <span id='pid'>971, <a href='https://arxiv.org/pdf/2602.01693.pdf' target='_blank'>https://arxiv.org/pdf/2602.01693.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kewei Hu, Michael Zhang, Wei Ying, Tianhao Liu, Guoqiang Hao, Zimeng Li, Wanchan Yu, Jiajian Jing, Fangwen Chen, Hanwen Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.01693">GSR: Learning Structured Reasoning for Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.
<div id='section'>PaperID: <span id='pid'>972, <a href='https://arxiv.org/pdf/2601.16667.pdf' target='_blank'>https://arxiv.org/pdf/2601.16667.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuohao Li, Yinghao Li, Jian-Jian Jiang, Lang Zhou, Tianyu Zhang, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16667">ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations.
<div id='section'>PaperID: <span id='pid'>973, <a href='https://arxiv.org/pdf/2601.07553.pdf' target='_blank'>https://arxiv.org/pdf/2601.07553.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07553">VirtualEnv: A Platform for Embodied AI Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) continue to improve in reasoning and decision-making, there is a growing need for realistic and interactive environments where their abilities can be rigorously evaluated. We present VirtualEnv, a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. VirtualEnv supports rich agent-environment interactions, including object manipulation, navigation, and adaptive multi-agent collaboration, as well as game-inspired mechanics like escape rooms and procedurally generated environments. We provide a user-friendly API built on top of Unreal Engine, allowing researchers to deploy and control LLM-driven agents using natural language instructions. We integrate large-scale LLMs and vision-language models (VLMs), such as GPT-based models, to generate novel environments and structured tasks from multimodal inputs. Our experiments benchmark the performance of several popular LLMs across tasks of increasing complexity, analyzing differences in adaptability, planning, and multi-agent coordination. We also describe our methodology for procedural task generation, task validation, and real-time environment control. VirtualEnv is released as an open-source platform, we aim to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.
<div id='section'>PaperID: <span id='pid'>974, <a href='https://arxiv.org/pdf/2512.24985.pdf' target='_blank'>https://arxiv.org/pdf/2512.24985.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24985">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.
<div id='section'>PaperID: <span id='pid'>975, <a href='https://arxiv.org/pdf/2512.10046.pdf' target='_blank'>https://arxiv.org/pdf/2512.10046.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10046">SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.
<div id='section'>PaperID: <span id='pid'>976, <a href='https://arxiv.org/pdf/2511.15279.pdf' target='_blank'>https://arxiv.org/pdf/2511.15279.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiashu Yang, Yifan Han, Yucheng Xie, Ning Guo, Wenzhao Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15279">Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.
<div id='section'>PaperID: <span id='pid'>977, <a href='https://arxiv.org/pdf/2511.14291.pdf' target='_blank'>https://arxiv.org/pdf/2511.14291.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxin Zhang, Ziyu Lu, Hongbo Duan, Keyu Fan, Pengting Luo, Peiyu Zhuang, Mengyu Yang, Houde Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14291">GEN3D: Generating Domain-Free 3D Scenes from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.
<div id='section'>PaperID: <span id='pid'>978, <a href='https://arxiv.org/pdf/2511.13524.pdf' target='_blank'>https://arxiv.org/pdf/2511.13524.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuhang Peng, Yizhou Pan, Xinning He, Jihaoyu Yang, Xinyu Yin, Han Wang, Xiaoji Zheng, Chao Gao, Jiangtao Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13524">FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.
<div id='section'>PaperID: <span id='pid'>979, <a href='https://arxiv.org/pdf/2511.09958.pdf' target='_blank'>https://arxiv.org/pdf/2511.09958.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiangyi Wei, Haotian Zhang, Xinyi Cao, Siyu Xie, Weifeng Ge, Yang Li, Changbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09958">Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.
<div id='section'>PaperID: <span id='pid'>980, <a href='https://arxiv.org/pdf/2511.08294.pdf' target='_blank'>https://arxiv.org/pdf/2511.08294.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Laura Bragagnolo, Leonardo Barcellona, Stefano Ghidoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08294">SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.
<div id='section'>PaperID: <span id='pid'>981, <a href='https://arxiv.org/pdf/2511.01493.pdf' target='_blank'>https://arxiv.org/pdf/2511.01493.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei Huang, Jiaxin Li, Zang Wan, Huijun Di, Wei Liang, Zhu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01493">Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guiding an agent to a specific target in indoor environments based solely on RGB inputs and a floor plan is a promising yet challenging problem. Although existing methods have made significant progress, two challenges remain unresolved. First, the modality gap between egocentric RGB observations and the floor plan hinders the integration of visual and spatial information for both local obstacle avoidance and global planning. Second, accurate localization is critical for navigation performance, but remains challenging at deployment in unseen environments due to the lack of explicit geometric alignment between RGB inputs and floor plans. We propose a novel diffusion-based policy, denoted as GlocDiff, which integrates global path planning from the floor plan with local depth-aware features derived from RGB observations. The floor plan offers explicit global guidance, while the depth features provide implicit geometric cues, collectively enabling precise prediction of optimal navigation directions and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation during training to enhance robustness against pose estimation errors, and we find that combining this with a relatively stable VO module during inference results in significantly improved navigation performance. Extensive experiments on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in achieving superior navigation performance, and the success of real-world deployments also highlights its potential for widespread practical applications.
<div id='section'>PaperID: <span id='pid'>982, <a href='https://arxiv.org/pdf/2510.19752.pdf' target='_blank'>https://arxiv.org/pdf/2510.19752.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ameesh Shah, William Chen, Adwait Godbole, Federico Mora, Sanjit A. Seshia, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19752">Learning Affordances at Inference-Time for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.
<div id='section'>PaperID: <span id='pid'>983, <a href='https://arxiv.org/pdf/2510.09731.pdf' target='_blank'>https://arxiv.org/pdf/2510.09731.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhammad Munsif, Waqas Ahmad, Amjid Ali, Mohib Ullah, Adnan Hussain, Sung Wook Baik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09731">Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected Vision Systems (CVS) are transforming a variety of applications, including autonomous vehicles, smart cities, surveillance, and human-robot interaction. These systems harness multi-view multi-camera (MVMC) data to provide enhanced situational awareness through the integration of MVMC tracking, re-identification (Re-ID), and action understanding (AU). However, deploying CVS in real-world, dynamic environments presents a number of challenges, particularly in addressing occlusions, diverse viewpoints, and environmental variability. Existing surveys have focused primarily on isolated tasks such as tracking, Re-ID, and AU, often neglecting their integration into a cohesive system. These reviews typically emphasize single-view setups, overlooking the complexities and opportunities provided by multi-camera collaboration and multi-view data analysis. To the best of our knowledge, this survey is the first to offer a comprehensive and integrated review of MVMC that unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a unique taxonomy to better understand the critical components of CVS, dividing it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We systematically arrange and summarize the state-of-the-art datasets, methodologies, results, and evaluation metrics, providing a structured view of the field's progression. Furthermore, we identify and discuss the open research questions and challenges, along with emerging technologies such as lifelong learning, privacy, and federated learning, that need to be addressed for future advancements. The paper concludes by outlining key research directions for enhancing the robustness, efficiency, and adaptability of CVS in complex, real-world applications. We hope this survey will inspire innovative solutions and guide future research toward the next generation of intelligent and adaptive CVS.
<div id='section'>PaperID: <span id='pid'>984, <a href='https://arxiv.org/pdf/2509.25200.pdf' target='_blank'>https://arxiv.org/pdf/2509.25200.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Christian Arzate Cruz, Edwin C. Montiel-Vazquez, Chikara Maeda, Randy Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25200">When and How to Express Empathy in Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating empathetic behavior into robots can improve their social effectiveness and interaction quality. In this paper, we present whEE (when and how to express empathy), a framework that enables social robots to detect when empathy is needed and generate appropriate responses. Using large language models, whEE identifies key behavioral empathy cues in human interactions. We evaluate it in human-robot interaction scenarios with our social robot, Haru. Results show that whEE effectively identifies and responds to empathy cues, providing valuable insights for designing social robots capable of adaptively modulating their empathy levels across various interaction contexts.
<div id='section'>PaperID: <span id='pid'>985, <a href='https://arxiv.org/pdf/2509.08157.pdf' target='_blank'>https://arxiv.org/pdf/2509.08157.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Viraj Parimi, Brian C. Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08157">Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe navigation is essential for autonomous systems operating in hazardous environments, especially when multiple agents must coordinate using just visual inputs over extended time horizons. Traditional planning methods excel at solving long-horizon tasks but rely on predefined distance metrics, while safe Reinforcement Learning (RL) can learn complex behaviors using high-dimensional inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an intermediate graph from replay buffer states, pruning unsafe edges, and using Conflict-Based Search (CBS) for multi-agent path planning. Although effective, this graph-pruning approach can be overly conservative, limiting mission efficiency by precluding missions that must traverse high-risk regions. To address this limitation, we propose RB-CBS, a novel extension to CBS that dynamically allocates and adjusts user-specified risk bound ($Î$) across agents to flexibly trade off safety and speed. Our improved planner ensures that each agent receives a local risk budget ($Î´$) enabling more efficient navigation while still respecting overall safety constraints. Experimental results demonstrate that this iterative risk-allocation framework yields superior performance in complex environments, allowing multiple agents to find collision-free paths within the user-specified $Î$.
<div id='section'>PaperID: <span id='pid'>986, <a href='https://arxiv.org/pdf/2509.00210.pdf' target='_blank'>https://arxiv.org/pdf/2509.00210.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00210">Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.
<div id='section'>PaperID: <span id='pid'>987, <a href='https://arxiv.org/pdf/2508.19788.pdf' target='_blank'>https://arxiv.org/pdf/2508.19788.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19788">Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for estimating accident-prone regions in everyday indoor scenes, aimed at improving real-time risk awareness in service robots operating in human-centric environments. As robots become integrated into daily life, particularly in homes, the ability to anticipate and respond to environmental hazards is crucial for ensuring user safety, trust, and effective human-robot interaction. Our approach models object-level risk and context through a semantic graph-based propagation algorithm. Each object is represented as a node with an associated risk score, and risk propagates asymmetrically from high-risk to low-risk objects based on spatial proximity and accident relationship. This enables the robot to infer potential hazards even when they are not explicitly visible or labeled. Designed for interpretability and lightweight onboard deployment, our method is validated on a dataset with human-annotated risk regions, achieving a binary risk detection accuracy of 75%. The system demonstrates strong alignment with human perception, particularly in scenes involving sharp or unstable objects. These results underline the potential of context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces. This framework could serve as a foundation for future systems that make context-driven safety decisions, provide real-time alerts, or autonomously assist users in avoiding or mitigating hazards within home environments.
<div id='section'>PaperID: <span id='pid'>988, <a href='https://arxiv.org/pdf/2508.16465.pdf' target='_blank'>https://arxiv.org/pdf/2508.16465.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Jean-SÃ©bastien Franco, GrÃ©gory Rogez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16465">HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.
<div id='section'>PaperID: <span id='pid'>989, <a href='https://arxiv.org/pdf/2507.20370.pdf' target='_blank'>https://arxiv.org/pdf/2507.20370.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michele Grimaldi, Carlo Cernicchiaro, Sebastian Realpe Rua, Alaaeddine El-Masri-El-Chaarani, Markus Buchholz, Loizos Michael, Pere Ridao Rodriguez, Ignacio Carlucho, Yvan R. Petillot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20370">Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic platforms have become essential for marine operations by providing regular and continuous access to offshore assets, such as underwater infrastructure inspection, environmental monitoring, and resource exploration. However, the complex and dynamic nature of underwater environments, characterized by limited visibility, unpredictable currents, and communication constraints, presents significant challenges that demand advanced autonomy while ensuring operator trust and oversight. Central to addressing these challenges are knowledge representation and reasoning techniques, particularly knowledge graphs and retrieval-augmented generation (RAG) systems, that enable robots to efficiently structure, retrieve, and interpret complex environmental data. These capabilities empower robotic agents to reason, adapt, and respond effectively to changing conditions. The primary goal of this work is to demonstrate both multi-agent autonomy and shared autonomy, where multiple robotic agents operate independently while remaining connected to a human supervisor. We show how a RAG-powered large language model, augmented with knowledge graph data and domain taxonomy, enables autonomous multi-agent decision-making and facilitates seamless human-robot interaction, resulting in 100\% mission validation and behavior completeness. Finally, ablation studies reveal that without structured knowledge from the graph and/or taxonomy, the LLM is prone to hallucinations, which can compromise decision quality.
<div id='section'>PaperID: <span id='pid'>990, <a href='https://arxiv.org/pdf/2506.20212.pdf' target='_blank'>https://arxiv.org/pdf/2506.20212.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrea Bussolan, Oliver Avram, Andrea Pignata, Gianvito Urgese, Stefano Baraldo, Anna Valente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20212">Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of Industry 5.0, manufacturers are increasingly prioritizing worker well-being alongside mass customization. Stress-aware Human-Robot Collaboration (HRC) plays a crucial role in this paradigm, where robots must adapt their behavior to human mental states to improve collaboration fluency and safety. This paper presents a novel framework that integrates Federated Learning (FL) to enable personalized mental state evaluation while preserving user privacy. By leveraging physiological signals, including EEG, ECG, EDA, EMG, and respiration, a multimodal model predicts an operator's stress level, facilitating real-time robot adaptation. The FL-based approach allows distributed on-device training, ensuring data confidentiality while improving model generalization and individual customization. Results demonstrate that the deployment of an FL approach results in a global model with performance in stress prediction accuracy comparable to a centralized training approach. Moreover, FL allows for enhancing personalization, thereby optimizing human-robot interaction in industrial settings, while preserving data privacy. The proposed framework advances privacy-preserving, adaptive robotics to enhance workforce well-being in smart manufacturing.
<div id='section'>PaperID: <span id='pid'>991, <a href='https://arxiv.org/pdf/2506.15293.pdf' target='_blank'>https://arxiv.org/pdf/2506.15293.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Francesco Chiossi, Julian Rasch, Robin Welsch, Albrecht Schmidt, Florian Michahelles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15293">Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments.
<div id='section'>PaperID: <span id='pid'>992, <a href='https://arxiv.org/pdf/2506.09697.pdf' target='_blank'>https://arxiv.org/pdf/2506.09697.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Paolo Franceschi, Andrea Bussolan, Vincenzo Pomponi, Oliver Avram, Stefano Baraldo, Anna Valente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09697">Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, industries are showing a growing interest in human-robot collaboration, particularly for shared tasks. This requires intelligent strategies to plan a robot's motions, considering both task constraints and human-specific factors such as height and movement preferences. This work introduces a novel approach to generate personalized trajectories using Dynamic Movement Primitives (DMPs), enhanced with real-time velocity scaling based on human feedback. The method was rigorously tested in industrial-grade experiments, focusing on the collaborative transport of an engine cowl lip section. Comparative analysis between DMP-generated trajectories and a state-of-the-art motion planner (BiTRRT) highlights their adaptability combined with velocity scaling. Subjective user feedback further demonstrates a clear preference for DMP- based interactions. Objective evaluations, including physiological measurements from brain and skin activity, reinforce these findings, showcasing the advantages of DMPs in enhancing human-robot interaction and improving user experience.
<div id='section'>PaperID: <span id='pid'>993, <a href='https://arxiv.org/pdf/2504.01293.pdf' target='_blank'>https://arxiv.org/pdf/2504.01293.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingyang Xu, Jiayi Shao, Yulan Ju, Ximing Shen, Qingyuan Gao, Weijen Chen, Qing Zhang, Yun Suen Pai, Giulia Barbareschi, Matthias Hoppe, Kouta Minamizawa, Kai Kunze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01293">Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flying robots, such as quadrotor drones, offer new possibilities for human-robot interaction but often pose safety risks due to fast-spinning propellers, rigid structures, and noise. In contrast, lighter-than-air flapping-wing robots, inspired by animal movement, offer a soft, quiet, and touch-safe alternative. Building on these advantages, we present Cuddle-Fish, a soft flapping-wing floating robot designed for close-proximity interactions in indoor spaces. Through a user study with 24 participants, we explored their perceptions of the robot and experiences during a series of co-located demonstrations in which the robot moved near them. Results showed that participants felt safe, willingly engaged in touch-based interactions with the robot, and exhibited spontaneous affective behaviours, such as patting, stroking, hugging, and cheek-touching, without external prompting. They also reported positive emotional responses towards the robot. These findings suggest that the soft floating robot with flapping wings can serve as a novel and socially acceptable alternative to traditional rigid flying robots, opening new potential for applications in companionship, affective interaction, and play in everyday indoor environments.
<div id='section'>PaperID: <span id='pid'>994, <a href='https://arxiv.org/pdf/2503.20916.pdf' target='_blank'>https://arxiv.org/pdf/2503.20916.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cosima du Pasquier, Jennifer Grannen, Chuer Pan, Serin L. Huber, Aliyah Smith, Monroe Kennedy, Shuran Song, Dorsa Sadigh, Allison M. Okamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20916">A Study of Perceived Safety for Soft Robotics in Caregiving Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this project, we focus on human-robot interaction in caregiving scenarios like bathing, where physical contact is inevitable and necessary for proper task execution because force must be applied to the skin. Using finite element analysis, we designed a 3D-printed gripper combining positive and negative pressure for secure yet compliant handling. Preliminary tests showed it exerted a lower, more uniform pressure profile than a standard rigid gripper. In a user study, participants' trust in robots significantly increased after they experienced a brief bathing demonstration performed by a robotic arm equipped with the soft gripper. These results suggest that soft robotics can enhance perceived safety and acceptance in intimate caregiving scenarios.
<div id='section'>PaperID: <span id='pid'>995, <a href='https://arxiv.org/pdf/2503.05825.pdf' target='_blank'>https://arxiv.org/pdf/2503.05825.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Wang, Sherwin Stephen Chan, Mingyuan Lei, Lek Syn Lim, Henry Johan, Bingran Zuo, Wei Tech Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05825">A Human-In-The-Loop Simulation Framework for Evaluating Control Strategies in Gait Assistive Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the global population ages, effective rehabilitation and mobility aids will become increasingly critical. Gait assistive robots are promising solutions, but designing adaptable controllers for various impairments poses a significant challenge. This paper presented a Human-In-The-Loop (HITL) simulation framework tailored specifically for gait assistive robots, addressing unique challenges posed by passive support systems. We incorporated a realistic physical human-robot interaction (pHRI) model to enable a quantitative evaluation of robot control strategies, highlighting the performance of a speed-adaptive controller compared to a conventional PID controller in maintaining compliance and reducing gait distortion. We assessed the accuracy of the simulated interactions against that of the real-world data and revealed discrepancies in the adaptation strategies taken by the human and their effect on the human's gait. This work underscored the potential of HITL simulation as a versatile tool for developing and fine-tuning personalized control policies for various users.
<div id='section'>PaperID: <span id='pid'>996, <a href='https://arxiv.org/pdf/2502.18749.pdf' target='_blank'>https://arxiv.org/pdf/2502.18749.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Heng San, Vasanthamaran Ravichandram, J-Anne Yow, Sherwin Stephen Chan, Yifan Wang, Wei Tech Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18749">Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe and comfortable bite transfer during robot-assisted feeding is challenging due to the close physical human-robot interaction required. This paper presents a novel approach to modeling physical human-robot interaction in a physics-based simulator (MuJoCo) using soft-body dynamics. We integrate a flexible head model with a rigid skeleton while accounting for internal dynamics, enabling the flexible model to be actuated by the skeleton. Incorporating realistic soft-skin contact dynamics in simulation allows for systematically evaluating bite transfer parameters, such as insertion depth and entry angle, and their impact on user safety and comfort. Our findings suggest that a straight-in-straight-out strategy minimizes forces and enhances user comfort in robot-assisted feeding, assuming a static head. This simulation-based approach offers a safer and more controlled alternative to real-world experimentation. Supplementary videos can be found at: https://tinyurl.com/224yh2kx.
<div id='section'>PaperID: <span id='pid'>997, <a href='https://arxiv.org/pdf/2412.02863.pdf' target='_blank'>https://arxiv.org/pdf/2412.02863.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lucas Nogueira Nobrega, Ewerton de Oliveira, Martin Saska, Tiago Nascimento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02863">Proximal Control of UAVs with Federated Learning for Human-Robot Collaborative Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human-robot interaction (HRI) is a growing area of research. In HRI, complex command (action) classification is still an open problem that usually prevents the real applicability of such a technique. The literature presents some works that use neural networks to detect these actions. However, occlusion is still a major issue in HRI, especially when using uncrewed aerial vehicles (UAVs), since, during the robot's movement, the human operator is often out of the robot's field of view. Furthermore, in multi-robot scenarios, distributed training is also an open problem. In this sense, this work proposes an action recognition and control approach based on Long Short-Term Memory (LSTM) Deep Neural Networks with two layers in association with three densely connected layers and Federated Learning (FL) embedded in multiple drones. The FL enabled our approach to be trained in a distributed fashion, i.e., access to data without the need for cloud or other repositories, which facilitates the multi-robot system's learning. Furthermore, our multi-robot approach results also prevented occlusion situations, with experiments with real robots achieving an accuracy greater than 96%.
<div id='section'>PaperID: <span id='pid'>998, <a href='https://arxiv.org/pdf/2411.03873.pdf' target='_blank'>https://arxiv.org/pdf/2411.03873.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Italo Belli, Florian van Melis, J. Micah Prendergast, Ajay Seth, Luka Peternel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03873">Biomechanics-Aware Trajectory Optimization for Online Navigation during Robotic Physiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic devices provide a great opportunity to assist in delivering physical therapy and rehabilitation movements, yet current robot-assisted methods struggle to incorporate biomechanical metrics essential for safe and effective therapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization approach to online robotic Navigation of human musculoskeletal loads for rotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the human shoulder into an optimal control framework, generating strain-minimizing trajectories for real-time control of therapeutic movements. \addedText{Its core strength lies in the ability to adapt biomechanics-informed trajectories online to unpredictable volitional human actions or reflexive reactions during physical human-robot interaction based on robot-sensed motion and forces. BATON's adaptability is enabled by a real-time, model-based estimator that infers changes in muscle activity via a rapid redundancy solver driven by robot pose and force/torque sensor data. We validated BATON through physical human-robot interaction experiments, assessing response speed, motion smoothness, and interaction forces.
<div id='section'>PaperID: <span id='pid'>999, <a href='https://arxiv.org/pdf/2410.00349.pdf' target='_blank'>https://arxiv.org/pdf/2410.00349.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Christian Arzate Cruz, Yotam Sechayk, Takeo Igarashi, Randy Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00349">Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans use multiple communication channels to interact with each other. For instance, body gestures or facial expressions are commonly used to convey an intent. The use of such non-verbal cues has motivated the development of prediction models. One such approach is predicting arousal and valence (AV) from facial expressions. However, making these models accurate for human-robot interaction (HRI) settings is challenging as it requires handling multiple subjects, challenging conditions, and a wide range of facial expressions. In this paper, we propose a data augmentation (DA) technique to improve the performance of AV predictors using 3D morphable models (3DMM). We then utilize this approach in an HRI setting with a mediator robot and a group of three humans. Our augmentation method creates synthetic sequences for underrepresented values in the AV space of the SEWA dataset, which is the most comprehensive dataset with continuous AV labels. Results show that using our DA method improves the accuracy and robustness of AV prediction in real-time applications. The accuracy of our models on the SEWA dataset is 0.793 for arousal and valence.
<div id='section'>PaperID: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2409.01630.pdf' target='_blank'>https://arxiv.org/pdf/2409.01630.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenxiao Zhang, Xiangrui Kong, Thomas Braunl, Jin B. Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01630">SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose \textit{SafeEmbodAI}, a safety framework for integrating mobile robots into embodied AI systems. \textit{SafeEmbodAI} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.
<div id='section'>PaperID: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2602.16173.pdf' target='_blank'>https://arxiv.org/pdf/2602.16173.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kaiqu Liang, Julia Kruk, Shengyi Qian, Xianjun Yang, Shengjie Bi, Yuanshun Yao, Shaoliang Nie, Mingyang Zhang, Lijuan Liu, Jaime Fernández Fisac, Shuyan Zhou, Saghar Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.16173">Learning Personalized Agents from Human Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.
<div id='section'>PaperID: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2602.07082.pdf' target='_blank'>https://arxiv.org/pdf/2602.07082.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoming Wang, Qiyao Xue, Weichen Liu, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07082">MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.
<div id='section'>PaperID: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2601.19634.pdf' target='_blank'>https://arxiv.org/pdf/2601.19634.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19634">AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.
<div id='section'>PaperID: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2601.16409.pdf' target='_blank'>https://arxiv.org/pdf/2601.16409.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yeasir Rayhan, Walid G. Aref
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16409">Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Move\,37 marks one of the major breakthroughs in AI in terms of its ability to surpass human expertise and discover novel strategies beyond the traditional game play in the strategic two-player board game of Go. The domains of Natural Language Processing, Computer Vision, and Robotics have also undergone a similar phenomenon through the advent of large foundational models in the form of Large Language Models (LLMs), Vision Language Models (VLMs) and Vision Language Action models (VLAs), respectively. In this paper, we investigate the current state of Artificial Intelligence for Database Systems research (AI4DB), and assess how far AI4DB systems are from achieving their own Move\,37 moment. We envision a Generative Database Agent (Gen-DBA, for short) as the pathway to achieving Move\,37 for database systems that will bring generative reasoning and creativity into the realm of database learning tasks. This vision paper explores this direction by presenting the recipe for building Gen-DBA that encompasses but is not limited to a Transformer backbone, a hardware-grounded tokenization mechanism, a two-stage Goal-Directed Next Token Prediction training paradigm, and a generative inference process.
<div id='section'>PaperID: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2512.17958.pdf' target='_blank'>https://arxiv.org/pdf/2512.17958.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Farida Mohsen, Ali Safa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17958">Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.
<div id='section'>PaperID: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2512.03418.pdf' target='_blank'>https://arxiv.org/pdf/2512.03418.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqi Ji, Junjie Ke, Lihuo He, Jun Liu, Kaifan Zhang, Yu-Kun Lai, Guiguang Ding, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03418">YOLOA: Real-Time Affordance Detection via LLM Adapter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.
<div id='section'>PaperID: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2512.01052.pdf' target='_blank'>https://arxiv.org/pdf/2512.01052.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhtadin, Mochammad Hilmi Rusydiansyah, Mauridhi Hery Purnomo, I Ketut Eddy Purnama, Chastine Fatichah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01052">Autonomous Grasping On Quadruped Robot With Task Level Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quadruped robots are increasingly used in various applications due to their high mobility and ability to operate in diverse terrains. However, most available quadruped robots are primarily focused on mobility without object manipulation capabilities. Equipping a quadruped robot with a robotic arm and gripper introduces a challenge in manual control, especially in remote scenarios that require complex commands. This research aims to develop an autonomous grasping system on a quadruped robot using a task-level interaction approach. The system includes hardware integration of a robotic arm and gripper onto the quadruped robot's body, a layered control system designed using ROS, and a web-based interface for human-robot interaction. The robot is capable of autonomously performing tasks such as navigation, object detection, and grasping using GraspNet. Testing was conducted through real-world scenarios to evaluate navigation, object selection and grasping, and user experience. The results show that the robot can perform tasks accurately and consistently, achieving a grasping success rate of 75 % from 12 trials. Therefore, the system demonstrates significant potential in enhancing the capabilities of quadruped robots as service robots in real-world environments.
<div id='section'>PaperID: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2511.21557.pdf' target='_blank'>https://arxiv.org/pdf/2511.21557.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hui Zhou, Siyuan Huang, Minxing Li, Hao Zhang, Lue Fan, Shaoshuai Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21557">VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.
<div id='section'>PaperID: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2511.18082.pdf' target='_blank'>https://arxiv.org/pdf/2511.18082.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wencheng Ye, Tianshi Wang, Lei Zhu, Fengling Li, Guoli Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18082">ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.
<div id='section'>PaperID: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2511.17199.pdf' target='_blank'>https://arxiv.org/pdf/2511.17199.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanyu Zhou, Chuanhao Ma, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17199">VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.
<div id='section'>PaperID: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2510.25268.pdf' target='_blank'>https://arxiv.org/pdf/2510.25268.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25268">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating hand grasps with language instructions is a widely studied topic that benefits from embodied AI and VR/AR applications. While transferring into hand articulatied object interaction (HAOI), the hand grasps synthesis requires not only object functionality but also long-term manipulation sequence along the object deformation. This paper proposes a novel HAOI sequence generation framework SynHLMA, to synthesize hand language manipulation for articulated objects. Given a complete point cloud of an articulated object, we utilize a discrete HAOI representation to model each hand object interaction frame. Along with the natural language embeddings, the representations are trained by an HAOI manipulation language model to align the grasping process with its language description in a shared representation space. A joint-aware loss is employed to ensure hand grasps follow the dynamic variations of articulated object joints. In this way, our SynHLMA achieves three typical hand manipulation tasks for articulated objects of HAOI generation, HAOI prediction and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and experimental results demonstrate the superior hand grasp sequence generation performance comparing with state-of-the-art. We also show a robotics grasp application that enables dexterous grasps execution from imitation learning using the manipulation sequence provided by our SynHLMA. Our codes and datasets will be made publicly available.
<div id='section'>PaperID: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2510.24161.pdf' target='_blank'>https://arxiv.org/pdf/2510.24161.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24161">BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have advanced vision-language reasoning and are increasingly deployed in embodied agents. However, significant limitations remain: MLLMs generalize poorly across digital-physical spaces and embodiments; vision-language-action models (VLAs) produce low-level actions yet lack robust high-level embodied reasoning; and most embodied large language models (ELLMs) are constrained to digital-space with poor generalization to the physical world. Thus, unified models that operate seamlessly across digital and physical spaces while generalizing across embodiments and tasks remain absent. We introduce the \textbf{Boundless Large Model (BLM$_1$)}, a multimodal spatial foundation model that preserves instruction following and reasoning, incorporates embodied knowledge, and supports robust cross-embodiment control. BLM$_1$ integrates three key capabilities -- \textit{cross-space transfer, cross-task learning, and cross-embodiment generalization} -- via a two-stage training paradigm. Stage I injects embodied knowledge into the MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module through an intent-bridging interface that extracts high-level semantics from the MLLM to guide control, without fine-tuning the MLLM backbone. This process is supported by a self-collected cross-embodiment demonstration suite spanning four robot embodiments and six progressively challenging tasks. Evaluations across digital and physical benchmarks show that a single BLM$_1$ instance outperforms four model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving $\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical tasks.
<div id='section'>PaperID: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2510.21991.pdf' target='_blank'>https://arxiv.org/pdf/2510.21991.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mateo Clemente, Leo Brunswic, Rui Heng Yang, Xuan Zhao, Yasser Khalil, Haoyu Lei, Amir Rasouli, Yinchuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21991">Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models, such as diffusion policy, have achieved state-of-the-art results in robotic manipulation by imitating expert demonstrations. While diffusion models were originally developed for vision tasks like image and video generation, many of their inference strategies have been directly transferred to control domains without adaptation. In this work, we show that by tailoring the denoising process to the specific characteristics of embodied AI tasks -- particularly structured, low-dimensional nature of action distributions -- diffusion policies can operate effectively with as few as 5 neural function evaluations (NFE). Building on this insight, we propose a population-based sampling strategy, genetic denoising, which enhances both performance and stability by selecting denoising trajectories with low out-of-distribution risk. Our method solves challenging tasks with only 2 NFE while improving or matching performance. We evaluate our approach across 14 robotic manipulation tasks from D4RL and Robomimic, spanning multiple action horizons and inference budgets. In over 2 million evaluations, our method consistently outperforms standard diffusion-based policies, achieving up to 20\% performance gains with significantly fewer inference steps.
<div id='section'>PaperID: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2510.16240.pdf' target='_blank'>https://arxiv.org/pdf/2510.16240.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lukas Zbinden, Nigel Nelson, Juo-Tung Chen, Xinhao Chen, Ji Woong, Kim, Mahdi Azizian, Axel Krieger, Sean Huver
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16240">Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of surgical robots and vision-language-action models has accelerated the development of autonomous surgical policies and efficient assessment strategies. However, evaluating these policies directly on physical robotic platforms such as the da Vinci Research Kit (dVRK) remains hindered by high costs, time demands, reproducibility challenges, and variability in execution. World foundation models (WFM) for physical AI offer a transformative approach to simulate complex real-world surgical tasks, such as soft tissue deformation, with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune of the Cosmos WFM, which, together with a trained video classifier, enables fully automated online evaluation and benchmarking of surgical policies. We evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop suture pad tasks, the automated pipeline achieves strong correlation between online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si platform, as well as good agreement between human labelers and the V-JEPA 2-derived video classifier. Additionally, preliminary experiments with ex-vivo porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising alignment with real-world evaluations, highlighting the platform's potential for more complex surgical procedures.
<div id='section'>PaperID: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2510.11094.pdf' target='_blank'>https://arxiv.org/pdf/2510.11094.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junxiang Wang, Han Zhang, Zehao Wang, Huaiyuan Chen, Pu Wang, Weidong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11094">Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective rehabilitation methods are essential for the recovery of lower limb dysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great potentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are usually heavy and need a lot of work to help the patients to put them on. Moreover, it also requires extra compliance control to guarantee the safety. In contrast, soft exoskeletons are easy and comfortable to wear and have intrinsic compliance, but their complex nonlinear human-robot interaction dynamics would pose significant challenges for control. In this work, based on the pneumatic actuators inspired by origami, we design a rehabilitation exoskeleton for knee that is easy and comfortable to wear. To guarantee the control performance and enable a nice human-robot interaction, we first use Deep Koopman Network to model the human-robot interaction dynamics. In particular, by viewing the electromyography (EMG) signals and the duty cycle of the PWM wave that controls the pneumatic robot's valves and pump as the inputs, the linear Koopman model accurately captures the complex human-robot interaction dynamics. Next, based on the obtained Koopman model, we further use Model Predictive Control (MPC) to control the soft robot and help the user to do rehabilitation training in real-time. The goal of the rehabilitation training is to track a given reference signal shown on the screen. Experiments show that by integrating the EMG signals into the Koopman model, we have improved the model accuracy to great extent. In addition, a personalized Koopman model trained from the individual's own data performs better than the non-personalized model. Consequently, our control framework outperforms the traditional PID control in both passive and active training modes. Hence the proposed method provides a new control framework for soft rehabilitation robots.
<div id='section'>PaperID: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2509.23121.pdf' target='_blank'>https://arxiv.org/pdf/2509.23121.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuai Li, Chen Yizhe, Li Dong, Liu Sichao, Lan Dapeng, Liu Yu, Zhibo Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23121">Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of artificial intelligence (AI) in industry is accelerating the shift from traditional automation to intelligent systems with perception and cognition. Vision language-action (VLA) models have been a key paradigm in AI to unify perception, reasoning, and control. Has the performance of the VLA models met the industrial requirements? In this paper, from the perspective of industrial deployment, we compare the performance of existing state-of-the-art VLA models in industrial scenarios and analyze the limitations of VLA models for real-world industrial deployment from the perspectives of data collection and model architecture. The results show that the VLA models retain their ability to perform simple grasping tasks even in industrial settings after fine-tuning. However, there is much room for performance improvement in complex industrial environments, diverse object categories, and high precision placing tasks. Our findings provide practical insight into the adaptability of VLA models for industrial use and highlight the need for task-specific enhancements to improve their robustness, generalization, and precision.
<div id='section'>PaperID: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2509.22573.pdf' target='_blank'>https://arxiv.org/pdf/2509.22573.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Farida Mohsen, Ali Safa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22573">MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently detecting human intent to interact with ubiquitous robots is crucial for effective human-robot interaction (HRI) and collaboration. Over the past decade, deep learning has gained traction in this field, with most existing approaches relying on multimodal inputs, such as RGB combined with depth (RGB-D), to classify time-sequence windows of sensory data as interactive or non-interactive. In contrast, we propose a novel RGB-only pipeline for predicting human interaction intent with frame-level precision, enabling faster robot responses and improved service quality. A key challenge in intent prediction is the class imbalance inherent in real-world HRI datasets, which can hinder the model's training and generalization. To address this, we introduce MINT-RVAE, a synthetic sequence generation method, along with new loss functions and training strategies that enhance generalization on out-of-sample data. Our approach achieves state-of-the-art performance (AUROC: 0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB input and supporting precise frame onset prediction. Finally, to support future research, we openly release our new dataset with frame-level labeling of human interaction intent.
<div id='section'>PaperID: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2509.22120.pdf' target='_blank'>https://arxiv.org/pdf/2509.22120.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alireza Aliyari, Gholamreza Vossoughi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22120">Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of exoskeleton robots is increasing due to the rising number of musculoskeletal injuries. However, their effectiveness depends heavily on the design of control systems. Designing robust controllers is challenging because of uncertainties in human-robot systems. Among various control strategies, Model Predictive Control (MPC) is a powerful approach due to its ability to handle constraints and optimize performance. Previous studies have used linearization-based methods to implement robust MPC on exoskeletons, but these can degrade performance due to nonlinearities in the robot's dynamics. To address this gap, this paper proposes a Robust Nonlinear Model Predictive Control (RNMPC) method, called multi-stage NMPC, to control a two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem. This method uses multiple scenarios to represent system uncertainties. The study focuses on minimizing human-robot interaction forces during the swing phase, particularly when the robot carries unknown loads. Simulations and experimental tests show that the proposed method significantly improves robustness, outperforming non-robust NMPC. It achieves lower tracking errors and interaction forces under various uncertainties. For instance, when a 2 kg unknown payload is combined with external disturbances, the RMS values of thigh and shank interaction forces for multi-stage NMPC are reduced by 77 and 94 percent, respectively, compared to non-robust NMPC.
<div id='section'>PaperID: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2509.16670.pdf' target='_blank'>https://arxiv.org/pdf/2509.16670.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenhuan Lu, Xinyue Song, Wenjun Ke, Zhizhi Yu, Wenhao Yang, Jianguo Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16670">Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio grounding, or speech-driven open-set object detection, aims to localize and identify objects directly from speech, enabling generalization beyond predefined categories. This task is crucial for applications like human-robot interaction where textual input is impractical. However, progress in this domain faces a fundamental bottleneck from the scarcity of large-scale, paired audio-image data, and is further constrained by previous methods that rely on indirect, text-mediated pipelines. In this paper, we introduce Speech-to-See (Speech2See), an end-to-end approach built on a pre-training and fine-tuning paradigm. Specifically, in the pre-training stage, we design a Query-Guided Semantic Aggregation module that employs learnable queries to condense redundant speech embeddings into compact semantic representations. During fine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts (MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation. Extensive experiments show that Speech2See achieves robust and adaptable performance across multiple benchmarks, demonstrating its strong generalization ability and broad applicability.
<div id='section'>PaperID: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2509.13378.pdf' target='_blank'>https://arxiv.org/pdf/2509.13378.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mattias Wingren, SÃ¶ren Andersson, Sara Rosenberg, Malin Andtfolk, Susanne HÃ¤gglund, Prashani Jayasingha Arachchige, Linda Nyholm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13378">Using role-play and Hierarchical Task Analysis for designing human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the use of two methods we believe warrant more use than they currently have in the field of human-robot interaction: role-play and Hierarchical Task Analysis. Some of its potential is showcased through our use of them in an ongoing research project which entails developing a robot application meant to assist at a community pharmacy. The two methods have provided us with several advantages. The role-playing provided a controlled and adjustable environment for understanding the customers' needs where pharmacists could act as models for the robot's behavior; and the Hierarchical Task Analysis ensured the behavior displayed was modelled correctly and aided development through facilitating co-design. Future research could focus on developing task analysis methods especially suited for social robot interaction.
<div id='section'>PaperID: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2509.11402.pdf' target='_blank'>https://arxiv.org/pdf/2509.11402.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alessandra Rossi, Patrick Holthaus, Gabriella Lakatos, SÃ­lvia Moros, Ali Fallahi, Murat Kirtay, Marie Postma, Erhan Oztop
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11402">TRUST 2025: SCRITA and RTSS @ RO-MAN 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The TRUST workshop is the result of a collaboration between two established workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic Societies). This joint initiative brings together the complementary goals of these workshops to advance research on trust from both the human and robot perspectives. Website: https://scrita.herts.ac.uk/2025/
<div id='section'>PaperID: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2509.06031.pdf' target='_blank'>https://arxiv.org/pdf/2509.06031.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junhui Huang, Yuhe Gong, Changsheng Li, Xingguang Duan, Luis Figueredo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06031">ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ZLATTE, a geometry-aware, learning-free framework for language-driven trajectory reshaping in human-robot interaction. Unlike prior learning-based methods, ZLATTE leverages Vision-Language Models to register objects as geometric primitives and employs a Large Language Model to translate natural language instructions into explicit geometric and kinematic constraints. These constraints are integrated into a potential field optimization to adapt initial trajectories while preserving feasibility and safety. A multi-agent strategy further enhances robustness under complex or conflicting commands. Simulation and real-world experiments demonstrate that ZLATTE achieves smoother, safer, and more interpretable trajectory modifications compared to state-of-the-art baselines.
<div id='section'>PaperID: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2509.01113.pdf' target='_blank'>https://arxiv.org/pdf/2509.01113.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haiyun Zhang, Kelvin HoLam Heung, Gabrielle J. Naquila, Ashwin Hingwe, Ashish D. Deshpande
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01113">A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement in physical human-robot interaction (HRI) has accelerated the development of soft robot designs and controllers. Controlling soft robots, especially soft hand grasping, is challenging due to their continuous deformation, motivating the use of reduced model-based controllers for real-time dynamic performance. Most existing models, however, suffer from computational inefficiency and complex parameter identification, limiting their real-time applicability. To address this, we propose a paradigm coupling Pseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter estimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate PRBM plus LDM for predicting position and force output from pressure input and benchmark its performance. We then implement PRBM plus LDM as the basis for closed-loop position and force controllers. Compared to a simple PID controller, the PRBM plus LDM position controller achieves lower error (average maximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force control, PRBM plus LDM outperforms constant pressure grasping in pinching tasks on delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70, brass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a computationally efficient and accurate modeling technique for soft actuators, enabling stable and flexible grasping with precise force regulation.
<div id='section'>PaperID: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2509.00660.pdf' target='_blank'>https://arxiv.org/pdf/2509.00660.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Felipe Arias-Russi, Yuanchen Bai, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00660">CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz (WoZ) controlled robots to explore navigation, conversational dynamics, human-in-the-loop interactions, and more to explore appropriate robot behaviors in everyday settings. However, existing WoZ tools are often limited to one context, making them less adaptable across different settings, users, and robotic platforms. To mitigate these issues, we introduce a Context-Adaptable Robot Interface System (CARIS) that combines advanced robotic capabilities such teleoperation, human perception, human-robot dialogue, and multimodal data recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ control a robot in two contexts: 1) mental health companion and as a 2) tour guide. Furthermore, we identified areas of improvement for CARIS, including smoother integration between movement and communication, clearer functionality separation, recommended prompts, and one-click communication options to enhance the usability wizard control of CARIS. This project offers a publicly available, context-adaptable tool for the HRI community, enabling researchers to streamline data-driven approaches to intelligent robot behavior.
<div id='section'>PaperID: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2508.20959.pdf' target='_blank'>https://arxiv.org/pdf/2508.20959.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Curtis C. Johnson, Daniel Webb, David Hill, Marc D. Killpack
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20959">Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling tactile sensing for robust whole-body manipulation is a significant challenge, often limited by wiring complexity, data throughput, and system reliability. This paper presents a complete architecture designed to overcome these barriers. Our approach pairs open-source, fabric-based sensors with custom readout electronics that reduce signal crosstalk to less than 3.3% through hardware-based mitigation. Critically, we introduce a novel, daisy-chained SPI bus topology that avoids the practical limitations of common wireless protocols and the prohibitive wiring complexity of USB hub-based systems. This architecture streams synchronized data from over 8,000 taxels across 1 square meter of sensing area at update rates exceeding 50 FPS, confirming its suitability for real-time control. We validate the system's efficacy in a whole-body grasping task where, without feedback, the robot's open-loop trajectory results in an uncontrolled application of force that slowly crushes a deformable cardboard box. With real-time tactile feedback, the robot transforms this motion into a gentle, stable grasp, successfully manipulating the object without causing structural damage. This work provides a robust and well-characterized platform to enable future research in advanced whole-body control and physical human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2508.18539.pdf' target='_blank'>https://arxiv.org/pdf/2508.18539.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kaijie Xu, Clark Verbrugge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18539">Adaptive Visual Navigation Assistant in 3D RPGs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In complex 3D game environments, players rely on visual affordances to spot map transition points. Efficient identification of such points is important to client-side auto-mapping, and provides an objective basis for evaluating map cue presentation. In this work, we formalize the task of detecting traversable Spatial Transition Points (STPs)-connectors between two sub regions-and selecting the singular Main STP (MSTP), the unique STP that lies on the designer-intended critical path toward the player's current macro-objective, from a single game frame, proposing this as a new research focus. We introduce a two-stage deep-learning pipeline that first detects potential STPs using Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses local and global visual features. Both stages benefit from parameter-efficient adapters, and we further introduce an optional retrieval-augmented fusion step. Our primary goal is to establish the feasibility of this problem and set baseline performance metrics. We validate our approach on a custom-built, diverse dataset collected from five Action RPG titles. Our experiments reveal a key trade-off: while full-network fine-tuning produces superior STP detection with sufficient data, adapter-only transfer is significantly more robust and effective in low-data scenarios and for the MSTP selection task. By defining this novel problem, providing a baseline pipeline and dataset, and offering initial insights into efficient model adaptation, we aim to contribute to future AI-driven navigation aids and data-informed level-design tools.
<div id='section'>PaperID: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2508.06547.pdf' target='_blank'>https://arxiv.org/pdf/2508.06547.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Heran Wu, Zirun Zhou, Jingfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06547">A tutorial note on collecting simulated data for vision-language-action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.
<div id='section'>PaperID: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2508.01723.pdf' target='_blank'>https://arxiv.org/pdf/2508.01723.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Danyang Li, Zenghui Yang, Guangpeng Qi, Songtao Pang, Guangyong Shang, Qiang Ma, Zheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01723">OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grounding natural language instructions to visual observations is fundamental for embodied agents operating in open-world environments. Recent advances in visual-language mapping have enabled generalizable semantic representations by leveraging vision-language models (VLMs). However, these methods often fall short in aligning free-form language commands with specific scene instances, due to limitations in both instance-level semantic consistency and instruction interpretation. We present OpenMap, a zero-shot open-vocabulary visual-language map designed for accurate instruction grounding in navigation tasks. To address semantic inconsistencies across views, we introduce a Structural-Semantic Consensus constraint that jointly considers global geometric structure and vision-language similarity to guide robust 3D instance-level aggregation. To improve instruction interpretation, we propose an LLM-assisted Instruction-to-Instance Grounding module that enables fine-grained instance selection by incorporating spatial context and expressive target descriptions. We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and instruction-to-target retrieval tasks. Experimental results show that OpenMap outperforms state-of-the-art baselines in zero-shot settings, demonstrating the effectiveness of our method in bridging free-form language and 3D perception for embodied navigation.
<div id='section'>PaperID: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2507.23544.pdf' target='_blank'>https://arxiv.org/pdf/2507.23544.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ryo Miyoshi, Yuki Okafuji, Takuya Iwamoto, Junya Nakanishi, Jun Baba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23544">User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the demand for social robots has grown, requiring them to adapt their behaviors based on users' states. Accurately assessing user experience (UX) in human-robot interaction (HRI) is crucial for achieving this adaptability. UX is a multi-faceted measure encompassing aspects such as sentiment and engagement, yet existing methods often focus on these individually. This study proposes a UX estimation method for HRI by leveraging multimodal social signals. We construct a UX dataset and develop a Transformer-based model that utilizes facial expressions and voice for estimation. Unlike conventional models that rely on momentary observations, our approach captures both short- and long-term interaction patterns using a multi-instance learning framework. This enables the model to capture temporal dynamics in UX, providing a more holistic representation. Experimental results demonstrate that our method outperforms third-party human evaluators in UX estimation.
<div id='section'>PaperID: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2507.10055.pdf' target='_blank'>https://arxiv.org/pdf/2507.10055.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhtadin, I Wayan Agus Darmawan, Muhammad Hilmi Rusydiansyah, I Ketut Eddy Purnama, Chastine Fatichah, Mauridhi Hery Purnomo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10055">Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Direct and natural interaction is essential for intuitive human-robot collaboration, eliminating the need for additional devices such as joysticks, tablets, or wearable sensors. In this paper, we present a lightweight deep learning-based hand gesture recognition system that enables humans to control collaborative robots naturally and efficiently. This model recognizes eight distinct hand gestures with only 1,103 parameters and a compact size of 22 KB, achieving an accuracy of 93.5%. To further optimize the model for real-world deployment on edge devices, we applied quantization and pruning using TensorFlow Lite, reducing the final model size to just 7 KB. The system was successfully implemented and tested on a Universal Robot UR5 collaborative robot within a real-time robotic framework based on ROS2. The results demonstrate that even extremely lightweight models can deliver accurate and responsive hand gesture-based control for collaborative robots, opening new possibilities for natural human-robot interaction in constrained environments.
<div id='section'>PaperID: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2506.20795.pdf' target='_blank'>https://arxiv.org/pdf/2506.20795.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Stephanie KÃ¤s, Anton Burenko, Louis Markert, Onur Alp Culha, Dennis Mack, Timm Linder, Bastian Leibe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20795">How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures enable non-verbal human-robot communication, especially in noisy environments like agile production. Traditional deep learning-based gesture recognition relies on task-specific architectures using images, videos, or skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs) and Vision Language Models (VLMs) with their strong generalization abilities offer potential to reduce system complexity by replacing dedicated task-specific modules. This study investigates adapting such models for dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing skeleton-based approach). We introduce NUGGET, a dataset tailored for human-robot communication in intralogistics environments, to evaluate the different gesture recognition approaches. In our experiments, HD-GCN achieves best performance, but V-JEPA comes close with a simple, task-specific classification head - thus paving a possible way towards reducing system complexity, by using it as a shared multi-task model. In contrast, Gemini struggles to differentiate gestures based solely on textual descriptions in the zero-shot setting, highlighting the need of further research on suitable input representations for gestures.
<div id='section'>PaperID: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2506.19747.pdf' target='_blank'>https://arxiv.org/pdf/2506.19747.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Stephanie KÃ¤s, Sven Peter, Henrik Thillmann, Anton Burenko, David Benjamin Adrian, Dennis Mack, Timm Linder, Bastian Leibe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19747">Systematic Comparison of Projection Methods for Monocular 3D Human Pose Estimation on Fisheye Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fisheye cameras offer robots the ability to capture human movements across a wider field of view (FOV) than standard pinhole cameras, making them particularly useful for applications in human-robot interaction and automotive contexts. However, accurately detecting human poses in fisheye images is challenging due to the curved distortions inherent to fisheye optics. While various methods for undistorting fisheye images have been proposed, their effectiveness and limitations for poses that cover a wide FOV has not been systematically evaluated in the context of absolute human pose estimation from monocular fisheye images. To address this gap, we evaluate the impact of pinhole, equidistant and double sphere camera models, as well as cylindrical projection methods, on 3D human pose estimation accuracy. We find that in close-up scenarios, pinhole projection is inadequate, and the optimal projection method varies with the FOV covered by the human pose. The usage of advanced fisheye models like the double sphere model significantly enhances 3D human pose estimation accuracy. We propose a heuristic for selecting the appropriate projection model based on the detection bounding box to enhance prediction quality. Additionally, we introduce and evaluate on our novel dataset FISHnCHIPS, which features 3D human skeleton annotations in fisheye images, including images from unconventional angles, such as extreme close-ups, ground-mounted cameras, and wide-FOV poses, available at: https://www.vision.rwth-aachen.de/fishnchips
<div id='section'>PaperID: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2506.18256.pdf' target='_blank'>https://arxiv.org/pdf/2506.18256.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuo Jiang, Boce Hu, Linfeng Zhao, Lawson L. S. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18256">Robot Tactile Gesture Recognition Based on Full-body Modular E-skin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of robot electronic skin technology, various tactile sensors, enhanced by AI, are unlocking a new dimension of perception for robots. In this work, we explore how robots equipped with electronic skin can recognize tactile gestures and interpret them as human commands. We developed a modular robot E-skin, composed of multiple irregularly shaped skin patches, which can be assembled to cover the robot's body while capturing real-time pressure and pose data from thousands of sensing points. To process this information, we propose an equivariant graph neural network-based recognizer that efficiently and accurately classifies diverse tactile gestures, including poke, grab, stroke, and double-pat. By mapping the recognized gestures to predefined robot actions, we enable intuitive human-robot interaction purely through tactile input.
<div id='section'>PaperID: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2506.17967.pdf' target='_blank'>https://arxiv.org/pdf/2506.17967.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mariya Hendriksen, Tabish Rashid, David Bignell, Raluca Georgescu, Abdelhak Lemkhenter, Katja Hofmann, Sam Devlin, Sarah Parisot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17967">Adapting Vision-Language Models for Evaluating World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models -- generative models that simulate environment dynamics conditioned on past observations and actions -- are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency -- capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce a evaluation protocol targeting two recognition tasks -- action recognition and character recognition -- each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a method for adapting VLMs to rollout evaluation under data and compute constraints. We conduct a large-scale study comparing full, partial, and parameter-efficient finetuning across task formats, context lengths, sampling strategies, and data compositions. The resulting unified evaluator matches the performance of task-specific baselines using a single checkpoint. Human studies confirm strong alignment with human judgments, establishing UNIVERSE as a scalable, semantics-aware evaluator for world models.
<div id='section'>PaperID: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2506.17462.pdf' target='_blank'>https://arxiv.org/pdf/2506.17462.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bernard Lange, Anil Yildiz, Mansur Arief, Shehryar Khattak, Mykel Kochenderfer, Georgios Georgakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17462">General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed data flows, limiting generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot integrations typically depend on pre-mapped spaces, hard-coded representations, and myopic exploration. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose navigation framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools available within modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query the robotic modules, reason over multimodal inputs, and select appropriate navigation actions. This approach enables robust navigation and reasoning in previously unmapped environments, providing a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves state-of-the-art performance, demonstrating effective exploration, navigation, and embodied question answering without relying on handcrafted plans, fixed input representations, or pre-existing maps.
<div id='section'>PaperID: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2506.06630.pdf' target='_blank'>https://arxiv.org/pdf/2506.06630.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Heeju Ko, Sungjune Kim, Gyeongrok Oh, Jeongyoon Yoon, Honglak Lee, Sujin Jang, Seungryong Kim, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06630">Active Test-time Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.
<div id='section'>PaperID: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2506.03709.pdf' target='_blank'>https://arxiv.org/pdf/2506.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03709">AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.
<div id='section'>PaperID: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2506.00098.pdf' target='_blank'>https://arxiv.org/pdf/2506.00098.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Edgar Welte, Rania Rayyes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00098">Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous manipulation is a crucial yet highly complex challenge in humanoid robotics, demanding precise, adaptable, and sample-efficient learning methods. As humanoid robots are usually designed to operate in human-centric environments and interact with everyday objects, mastering dexterous manipulation is critical for real-world deployment. Traditional approaches, such as reinforcement learning and imitation learning, have made significant strides, but they often struggle due to the unique challenges of real-world dexterous manipulation, including high-dimensional control, limited training data, and covariate shift. This survey provides a comprehensive overview of these challenges and reviews existing learning-based methods for real-world dexterous manipulation, spanning imitation learning, reinforcement learning, and hybrid approaches. A promising yet underexplored direction is interactive imitation learning, where human feedback actively refines a robots behavior during training. While interactive imitation learning has shown success in various robotic tasks, its application to dexterous manipulation remains limited. To address this gap, we examine current interactive imitation learning techniques applied to other robotic tasks and discuss how these methods can be adapted to enhance dexterous manipulation. By synthesizing state-of-the-art research, this paper highlights key challenges, identifies gaps in current methodologies, and outlines potential directions for leveraging interactive imitation learning to improve dexterous robotic skills.
<div id='section'>PaperID: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2505.03929.pdf' target='_blank'>https://arxiv.org/pdf/2505.03929.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rafael R. Baptista, Nina R. Gerszberg, Ricardo V. Godoy, Gustavo J. G. Lahr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03929">MIHRaGe: A Mixed-Reality Interface for Human-Robot Interaction via Gaze-Oriented Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Individuals with upper limb mobility impairments often require assistive technologies to perform activities of daily living. While gaze-tracking has emerged as a promising method for robotic assistance, existing solutions lack sufficient feedback mechanisms, leading to uncertainty in user intent recognition and reduced adaptability. This paper presents the MIHRAGe interface, an integrated system that combines gaze-tracking, robotic assistance, and a mixed-reality to create an immersive environment for controlling the robot using only eye movements. The system was evaluated through an experimental protocol involving four participants, assessing gaze accuracy, robotic positioning precision, and the overall success of a pick and place task. Results showed an average gaze fixation error of 1.46 cm, with individual variations ranging from 1.28 cm to 2.14 cm. The robotic arm demonstrated an average positioning error of +-1.53 cm, with discrepancies attributed to interface resolution and calibration constraints. In a pick and place task, the system achieved a success rate of 80%, highlighting its potential for improving accessibility in human-robot interaction with visual feedback to the user.
<div id='section'>PaperID: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2504.14135.pdf' target='_blank'>https://arxiv.org/pdf/2504.14135.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonathan Embley-Riches, Jianwei Liu, Simon Julier, Dimitrios Kanoulas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14135">Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity simulation is essential for robotics research, enabling safe and efficient testing of perception, control, and navigation algorithms. However, achieving both photorealistic rendering and accurate physics modeling remains a challenge. This paper presents a novel simulation framework--the Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced rendering capabilities with MuJoCo's high-precision physics simulation. Our approach enables realistic robotic perception while maintaining accurate physical interactions, facilitating benchmarking and dataset generation for vision-based robotics applications. The system supports complex environmental effects, such as smoke, fire, and water dynamics, which are critical for evaluating robotic performance under adverse conditions. We benchmark visual navigation and SLAM methods within our framework, demonstrating its utility for testing real-world robustness in controlled yet diverse scenarios. By bridging the gap between physics accuracy and photorealistic rendering, our framework provides a powerful tool for advancing robotics research and sim-to-real transfer.
<div id='section'>PaperID: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2504.03629.pdf' target='_blank'>https://arxiv.org/pdf/2504.03629.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cody Simons, Aritra Samanta, Amit K. Roy-Chowdhury, Konstantinos Karydis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03629">SeGuE: Semantic Guided Exploration for Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of embodied AI applications has enabled robots to perform complex tasks which require a sophisticated understanding of their environment. To enable successful robot operation in such settings, maps must be constructed so that they include semantic information, in addition to geometric information. In this paper, we address the novel problem of semantic exploration, whereby a mobile robot must autonomously explore an environment to fully map both its structure and the semantic appearance of features. We develop a method based on next-best-view exploration, where potential poses are scored based on the semantic features visible from that pose. We explore two alternative methods for sampling potential views and demonstrate the effectiveness of our framework in both simulation and physical experiments. Automatic creation of high-quality semantic maps can enable robots to better understand and interact with their environments and enable future embodied AI applications to be more easily deployed.
<div id='section'>PaperID: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2504.00839.pdf' target='_blank'>https://arxiv.org/pdf/2504.00839.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00839">Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.
<div id='section'>PaperID: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2503.19240.pdf' target='_blank'>https://arxiv.org/pdf/2503.19240.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Guo, Jianfei Zhu, Wei Fan, Chunzhi Yi, Feng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19240">Beyond Object Categories: Multi-Attribute Reference Understanding for Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring expression comprehension (REC) aims at achieving object localization based on natural language descriptions. However, existing REC approaches are constrained by object category descriptions and single-attribute intention descriptions, hindering their application in real-world scenarios. In natural human-robot interactions, users often express their desires through individual states and intentions, accompanied by guiding gestures, rather than detailed object descriptions. To address this challenge, we propose Multi-ref EC, a novel task framework that integrates state descriptions, derived intentions, and embodied gestures to locate target objects. We introduce the State-Intention-Gesture Attributes Reference (SIGAR) dataset, which combines state and intention expressions with embodied references. Through extensive experiments with various baseline models on SIGAR, we demonstrate that properly ordered multi-attribute references contribute to improved localization performance, revealing that single-attribute reference is insufficient for natural human-robot interaction scenarios. Our findings underscore the importance of multi-attribute reference expressions in advancing visual-language understanding.
<div id='section'>PaperID: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2503.07547.pdf' target='_blank'>https://arxiv.org/pdf/2503.07547.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nina Moorman, Michelle Zhao, Matthew B. Luebbers, Sanne Van Waveren, Reid Simmons, Henny Admoni, Sonia Chernova, Matthew Gombolay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07547">Bi-Directional Mental Model Reconciliation for Human-Robot Interaction with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interactions, human and robot agents maintain internal mental models of their environment, their shared task, and each other. The accuracy of these representations depends on each agent's ability to perform theory of mind, i.e. to understand the knowledge, preferences, and intentions of their teammate. When mental models diverge to the extent that it affects task execution, reconciliation becomes necessary to prevent the degradation of interaction. We propose a framework for bi-directional mental model reconciliation, leveraging large language models to facilitate alignment through semi-structured natural language dialogue. Our framework relaxes the assumption of prior model reconciliation work that either the human or robot agent begins with a correct model for the other agent to align to. Through our framework, both humans and robots are able to identify and communicate missing task-relevant context during interaction, iteratively progressing toward a shared mental model.
<div id='section'>PaperID: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2501.19259.pdf' target='_blank'>https://arxiv.org/pdf/2501.19259.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amogh Joshi, Sourav Sanyal, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19259">Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.
<div id='section'>PaperID: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2501.05750.pdf' target='_blank'>https://arxiv.org/pdf/2501.05750.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sonia Raychaudhuri, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05750">Semantic Mapping in Indoor Embodied AI -- A Survey on Advances, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent embodied agents (e.g. robots) need to perform complex semantic tasks in unfamiliar environments. Among many skills that the agents need to possess, building and maintaining a semantic map of the environment is most crucial in long-horizon tasks. A semantic map captures information about the environment in a structured way, allowing the agent to reference it for advanced reasoning throughout the task. While existing surveys in embodied AI focus on general advancements or specific tasks like navigation and manipulation, this paper provides a comprehensive review of semantic map-building approaches in embodied AI, specifically for indoor navigation. We categorize these approaches based on their structural representation (spatial grids, topological graphs, dense point-clouds or hybrid maps) and the type of information they encode (implicit features or explicit environmental data). We also explore the strengths and limitations of the map building techniques, highlight current challenges, and propose future research directions. We identify that the field is moving towards developing open-vocabulary, queryable, task-agnostic map representations, while high memory demands and computational inefficiency still remaining to be open challenges. This survey aims to guide current and future researchers in advancing semantic mapping techniques for embodied AI systems.
<div id='section'>PaperID: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2501.04929.pdf' target='_blank'>https://arxiv.org/pdf/2501.04929.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amy Koike, Yuki Okafuji, Kenya Hoshimure, Jun Baba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04929">What Drives You to Interact?: The Role of User Motivation for a Robot in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we aim to understand how user motivation shapes human-robot interaction (HRI) in the wild. To explore this, we conducted a field study by deploying a fully autonomous conversational robot in a shopping mall over two days. Through sequential video analysis, we identified five patterns of interaction fluency (Smooth, Awkward, Active, Messy, and Quiet), four types of user motivation for interacting with the robot (Function, Experiment, Curiosity, and Education), and user positioning towards the robot. We further analyzed how these motivations and positioning influence interaction fluency. Our findings suggest that incorporating users' motivation types into the design of robot behavior can enhance interaction fluency, engagement, and user satisfaction in real-world HRI scenarios.
<div id='section'>PaperID: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2501.00038.pdf' target='_blank'>https://arxiv.org/pdf/2501.00038.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuanbo Hou, Qiaoqiao Ren, Wenwu Wang, Dick Botteldooren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00038">Sound-Based Recognition of Touch Gestures and Emotions for Enhanced Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion recognition and touch gesture decoding are crucial for advancing human-robot interaction (HRI), especially in social environments where emotional cues and tactile perception play important roles. However, many humanoid robots, such as Pepper, Nao, and Furhat, lack full-body tactile skin, limiting their ability to engage in touch-based emotional and gesture interactions. In addition, vision-based emotion recognition methods usually face strict GDPR compliance challenges due to the need to collect personal facial data. To address these limitations and avoid privacy issues, this paper studies the potential of using the sounds produced by touching during HRI to recognise tactile gestures and classify emotions along the arousal and valence dimensions. Using a dataset of tactile gestures and emotional interactions from 28 participants with the humanoid robot Pepper, we design an audio-only lightweight touch gesture and emotion recognition model with only 0.24M parameters, 0.94MB model size, and 0.7G FLOPs. Experimental results show that the proposed sound-based touch gesture and emotion recognition model effectively recognises the arousal and valence states of different emotions, as well as various tactile gestures, when the input audio length varies. The proposed model is low-latency and achieves similar results as well-known pretrained audio neural networks (PANNs), but with much smaller FLOPs, parameters, and model size.
<div id='section'>PaperID: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2412.06808.pdf' target='_blank'>https://arxiv.org/pdf/2412.06808.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shipeng Liu, FNU Shrutika, Boshen Zhang, Zhehui Huang, Gaurav Sukhatme, Feifei Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06808">Effect of Adaptive Communication Support on LLM-powered Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot collaboration requires robot to adopt their roles and levels of support based on human needs, task requirements, and complexity. Traditional human-robot teaming often relies on a pre-determined robot communication scheme, restricting teamwork adaptability in complex tasks. Leveraging strong communication capabilities of Large Language Models (LLMs), we propose a Human-Robot Teaming Framework with Multi-Modal Language feedback (HRT-ML), a framework designed to enhance human-robot interaction by adjusting the frequency and content of language-based feedback. HRT-ML framework includes two core modules: a Coordinator for high-level, low-frequency strategic guidance, and a Manager for subtask-specific, high-frequency instructions, enabling passive and active interactions with human teammates. To assess the impact of language feedback in collaborative scenarios, we conducted experiments in an enhanced Overcooked environment with varying levels of task complexity (easy, medium, hard) and feedback frequency (inactive, passive, active, superactive). Our results show that as task complexity increases relative to human capabilities, human teammates exhibited a stronger preference towards robotic agents that can offer frequent, proactive support. However, when task complexities exceed the LLM's capacity, noisy and inaccurate feedback from superactive robotic agents can instead hinder team performance, as it requires human teammates to increase their effort to interpret and respond to a large number of communications, with limited performance return. Our results offer a general principle for robotic agents to dynamically adjust their levels and frequencies of communications to work seamlessly with humans and achieve improved teaming performance.
<div id='section'>PaperID: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2412.00711.pdf' target='_blank'>https://arxiv.org/pdf/2412.00711.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Carson Kohlbrenner, Caleb Escobedo, S. Sandra Bae, Alexander Dickhans, Alessandro Roncone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00711">GenTact Toolbox: A Computational Design Pipeline to Procedurally Generate Context-Driven 3D Printed Whole-Body Artificial Skins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing whole-body tactile skins for robots remains a challenging task, as existing solutions often prioritize modular, one-size-fits-all designs, which, while versatile, fail to account for the robot's specific shape and the unique demands of its operational context. In this work, we introduce GenTact Toolbox, a computational pipeline for creating versatile whole-body tactile skins tailored to both robot shape and application domain. Our method includes procedural mesh generation for conforming to a robot's topology, task-driven simulation to refine sensor distribution, and multi-material 3D printing for shape-agnostic fabrication. We validate our approach by creating and deploying six capacitive sensing skins on a Franka Research 3 robot arm in a human-robot interaction scenario. This work represents a shift from "one-size-fits-all" tactile sensors toward context-driven, highly adaptable designs that can be customized for a wide range of robotic systems and applications. The project website is available at https://hiro-group.ronc.one/gentacttoolbox
<div id='section'>PaperID: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2411.07644.pdf' target='_blank'>https://arxiv.org/pdf/2411.07644.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rotem Atari, Eran Bamani, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07644">Human Arm Pose Estimation with a Shoulder-worn Force-Myography Device for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human pose estimation is essential for effective Human-Robot Interaction (HRI). By observing a user's arm movements, robots can respond appropriately, whether it's providing assistance or avoiding collisions. While visual perception offers potential for human pose estimation, it can be hindered by factors like poor lighting or occlusions. Additionally, wearable inertial sensors, though useful, require frequent calibration as they do not provide absolute position information. Force-myography (FMG) is an alternative approach where muscle perturbations are externally measured. It has been used to observe finger movements, but its application to full arm state estimation is unexplored. In this letter, we investigate the use of a wearable FMG device that can observe the state of the human arm for real-time applications of HRI. We propose a Transformer-based model to map FMG measurements from the shoulder of the user to the physical pose of the arm. The model is also shown to be transferable to other users with limited decline in accuracy. Through real-world experiments with a robotic arm, we demonstrate collision avoidance without relying on visual perception.
<div id='section'>PaperID: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2411.07223.pdf' target='_blank'>https://arxiv.org/pdf/2411.07223.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunhao Luo, Yilun Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07223">Grounding Video Models to Actions through Goal Conditioned Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large video models, pretrained on massive amounts of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data are available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.
<div id='section'>PaperID: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2410.17058.pdf' target='_blank'>https://arxiv.org/pdf/2410.17058.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yenan Shen, Naomi Ehrich Leonard, Bassam Bamieh, Juncal Arbelaiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17058">Optimal gait design for nonlinear soft robotic crawlers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robots offer a frontier in robotics with enormous potential for safe human-robot interaction and agility in uncertain environments. A stepping stone towards unlocking their potential is a control theory tailored to soft robotics, including a principled framework for gait design. We analyze the problem of optimal gait design for a soft crawling body - the crawler. The crawler is an elastic body with the control signal defined as actuation forces between segments of the body. We consider the simplest such crawler: a two-segmented body with a passive mechanical connection modeling the viscoelastic body dynamics and a symmetric control force modeling actuation between the two body segments. The model accounts for the nonlinear asymmetric friction with the ground, which together with the symmetric actuation forces enable the crawler's locomotion. Using a describing-function analysis, we show that when the body is forced sinusoidally, the optimal actuator contraction frequency corresponds to the body's natural frequency when operating with only passive dynamics. We then use the framework of Optimal Periodic Control (OPC) to design optimal force cycles of arbitrary waveform and the corresponding crawling gaits. We provide a hill-climbing algorithm to solve the OPC problem numerically. Our proposed methods and results inform the design of optimal forcing and gaits for more complex and multi-segmented crawling soft bodies.
<div id='section'>PaperID: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2410.08852.pdf' target='_blank'>https://arxiv.org/pdf/2410.08852.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michelle Zhao, Reid Simmons, Henny Admoni, Aaditya Ramdas, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08852">Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior.
<div id='section'>PaperID: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2409.19459.pdf' target='_blank'>https://arxiv.org/pdf/2409.19459.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cody Simons, Zhichao Liu, Brandon Marcus, Amit K. Roy-Chowdhury, Konstantinos Karydis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19459">Language-guided Robust Navigation for Mobile Robots in Dynamically-changing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we develop an embodied AI system for human-in-the-loop navigation with a wheeled mobile robot. We propose a direct yet effective method of monitoring the robot's current plan to detect changes in the environment that impact the intended trajectory of the robot significantly and then query a human for feedback. We also develop a means to parse human feedback expressed in natural language into local navigation waypoints and integrate it into a global planning system, by leveraging a map of semantic features and an aligned obstacle map. Extensive testing in simulation and physical hardware experiments with a resource-constrained wheeled robot tasked to navigate in a real-world environment validate the efficacy and robustness of our method. This work can support applications like precision agriculture and construction, where persistent monitoring of the environment provides a human with information about the environment state.
<div id='section'>PaperID: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2409.10078.pdf' target='_blank'>https://arxiv.org/pdf/2409.10078.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Meng Chu, Xuan Zhang, Zhedong Zheng, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10078">3D-TAFS: A Training-free Framework for 3D Affordance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating high-level linguistic instructions into precise robotic actions in the physical world remains challenging, particularly when considering the feasibility of interacting with 3D objects. In this paper, we introduce 3D-TAFS, a novel training-free multimodal framework for 3D affordance segmentation. To facilitate a comprehensive evaluation of such frameworks, we present IndoorAfford-Bench, a large-scale benchmark containing 9,248 images spanning 20 diverse indoor scenes across 6 areas, supporting standardized interaction queries. In particular, our framework integrates a large multimodal model with a specialized 3D vision network, enabling a seamless fusion of 2D and 3D visual understanding with language comprehension. Extensive experiments on IndoorAfford-Bench validate the proposed 3D-TAFS's capability in handling interactive 3D affordance segmentation tasks across diverse settings, showcasing competitive performance across various metrics. Our results highlight 3D-TAFS's potential for enhancing human-robot interaction based on affordance understanding in complex indoor environments, advancing the development of more intuitive and efficient robotic frameworks for real-world applications.
<div id='section'>PaperID: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2602.15063.pdf' target='_blank'>https://arxiv.org/pdf/2602.15063.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yufeng Wang, Yuan Xu, Anastasia Nikolova, Yuxuan Wang, Jianyu Wang, Chongyang Wang, Xin Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15063">How Do We Research Human-Robot Interaction in the Age of Large Language Models? A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in large language models (LLMs) are profoundly reshaping the field of human-robot interaction (HRI). While prior work has highlighted the technical potential of LLMs, few studies have systematically examined their human-centered impact (e.g., human-oriented understanding, user modeling, and levels of autonomy), making it difficult to consolidate emerging challenges in LLM-driven HRI systems. Therefore, we conducted a systematic literature search following the PRISMA guideline, identifying 86 articles that met our inclusion criteria. Our findings reveal that: (1) LLMs are transforming the fundamentals of HRI by reshaping how robots sense context, generate socially grounded interactions, and maintain continuous alignment with human needs in embodied settings; and (2) current research is largely exploratory, with different studies focusing on different facets of LLM-driven HRI, resulting in wide-ranging choices of experimental setups, study methods, and evaluation metrics. Finally, we identify key design considerations and challenges, offering a coherent overview and guidelines for future research at the intersection of LLMs and HRI.
<div id='section'>PaperID: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2602.06427.pdf' target='_blank'>https://arxiv.org/pdf/2602.06427.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxiang Zhao, Yirong Yang, Yanqing Zhu, Yanfen Shen, Chiyu Wang, Zhining Gu, Pei Shi, Wei Guo, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.06427">Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.
<div id='section'>PaperID: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2602.00494.pdf' target='_blank'>https://arxiv.org/pdf/2602.00494.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongyu Zhou, Chia-An fan, Yihao Dong, Shuto Takashita, Masahiko Inami, Zhanna Sarsenbayeva, Anusha Withana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00494">SRL Proxemics: Spatial Guidelines for Supernumerary Robotic Limbs in Near-Body Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wearable supernumerary robotic limbs (SRLs) sit at the intersection of human augmentation and embodied AI, transforming into extensions of the human body. However, their movements within the intimate near-body space raise unresolved challenges for perceived safety, user control, and trust. In this paper, we present results from a Wizard-of-Oz study (n=18), where participants completed near-body collaboration tasks with SRLs to explore these challenges. We collected qualitative data through think-aloud protocols and semi-structured interviews, complemented by physiological signals and post-task ratings. Findings indicate that greater autonomy did not inherently enhance perceived safety or trust. Instead, participants identified near-body zones and paired them with clear coordination rules. They also expressed expectations for how different arm components should behave, shaping preferences around autonomy, perceived safety, and trust. Building on these insights, we introduce SRL Proxemics, a zone- and segment-level design framework showing that autonomy is not monolithic: perceived safety hinges on spatially calibrated, legible behaviors, not higher autonomy.
<div id='section'>PaperID: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2601.03044.pdf' target='_blank'>https://arxiv.org/pdf/2601.03044.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mingjie Pan, Siyuan Feng, Qinglin Zhang, Xinchen Li, Jianheng Song, Chendi Qu, Yi Wang, Chuankang Li, Ziyu Xiong, Zhi Chen, Yi Liu, Jianlan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03044">SOP: A Scalable Online Post-Training System for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.
<div id='section'>PaperID: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2601.01872.pdf' target='_blank'>https://arxiv.org/pdf/2601.01872.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongbo Duan, Shangyi Luo, Zhiyuan Deng, Yanbo Chen, Yuanhao Chiang, Yi Liu, Fangming Liu, Xueqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01872">CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.
<div id='section'>PaperID: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2512.24129.pdf' target='_blank'>https://arxiv.org/pdf/2512.24129.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Manuel Bied, John Arockiasamy, Andy Comeca, Maximilian Schrapel, Victoria Yang, Alexey Rolich, Barbara Bruno, Maike Schwammberger, Dieter Fiems, Alexey Vinel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24129">ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike.
<div id='section'>PaperID: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2512.07177.pdf' target='_blank'>https://arxiv.org/pdf/2512.07177.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fanjun Bu, Melina Tsai, Audrey Tjokro, Tapomayukh Bhattacharjee, Jorge Ortiz, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07177">Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.
<div id='section'>PaperID: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2512.06002.pdf' target='_blank'>https://arxiv.org/pdf/2512.06002.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Evan Conway, David Porfirio, David Chan, Mark Roberts, Laura M. Hiatt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06002">POrTAL: Plan-Orchestrated Tree Assembly for Lookahead</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.
<div id='section'>PaperID: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2511.18173.pdf' target='_blank'>https://arxiv.org/pdf/2511.18173.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Enrico Pallotta, Sina Mokhtarzadeh Azar, Lars Doorenbos, Serdar Ozsoy, Umar Iqbal, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18173">EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.
<div id='section'>PaperID: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2511.08007.pdf' target='_blank'>https://arxiv.org/pdf/2511.08007.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifei Cao, Yu Liu, Guolong Wang, Zhu Liu, Kai Wang, Xianjie Zhang, Jizhe Yu, Xun Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08007">EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric visual query localization is vital for embodied AI and VR/AR, yet remains challenging due to camera motion, viewpoint changes, and appearance variations. We present EAGLE, a novel framework that leverages episodic appearance- and geometry-aware memory to achieve unified 2D-3D visual query localization in egocentric vision. Inspired by avian memory consolidation, EAGLE synergistically integrates segmentation guided by an appearance-aware meta-learning memory (AMM), with tracking driven by a geometry-aware localization memory (GLM). This memory consolidation mechanism, through structured appearance and geometry memory banks, stores high-confidence retrieval samples, effectively supporting both long- and short-term modeling of target appearance variations. This enables precise contour delineation with robust spatial discrimination, leading to significantly improved retrieval accuracy. Furthermore, by integrating the VQL-2D output with a visual geometry grounded Transformer (VGGT), we achieve a efficient unification of 2D and 3D tasks, enabling rapid and accurate back-projection into 3D space. Our method achieves state-ofthe-art performance on the Ego4D-VQ benchmark.
<div id='section'>PaperID: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2511.05491.pdf' target='_blank'>https://arxiv.org/pdf/2511.05491.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05491">Visual Spatial Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8\%$ on MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.
<div id='section'>PaperID: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2510.11307.pdf' target='_blank'>https://arxiv.org/pdf/2510.11307.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sabrina McCallum, Amit Parekh, Alessandro Suglia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11307">FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current approaches to embodied AI tend to learn policies from expert demonstrations. However, without a mechanism to evaluate the quality of demonstrated actions, they are limited to learning from optimal behaviour, or they risk replicating errors and inefficiencies. While reinforcement learning offers one alternative, the associated exploration typically results in sacrificing data efficiency. This work explores how agents trained with imitation learning can learn robust representations from both optimal and suboptimal demonstrations when given access to constructive language feedback as a means to contextualise different modes of behaviour. We directly provide language feedback embeddings as part of the input sequence into a Transformer-based policy, and optionally complement the traditional next action prediction objective with auxiliary self-supervised learning objectives for feedback prediction. We test our approach on a range of embodied Vision-and-Language tasks in our custom BabyAI-XGen environment and show significant improvements in agents' compositional generalisation abilities and robustness, suggesting that our data-efficient method allows models to successfully convert suboptimal behaviour into learning opportunities. Overall, our results suggest that language feedback is a competitive and intuitive alternative to intermediate scalar rewards for language-specified embodied tasks.
<div id='section'>PaperID: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2510.09951.pdf' target='_blank'>https://arxiv.org/pdf/2510.09951.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiao-Xiong Lin, Yuk Hoi Yiu, Christian Leibold
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09951">Egocentric Visual Navigation through Hippocampal Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential activation of place-tuned neurons in an animal during navigation is typically interpreted as reflecting the sequence of input from adjacent positions along the trajectory. More recent theories about such place cells suggest sequences arise from abstract cognitive objectives like planning. Here, we propose a mechanistic and parsimonious interpretation to complement these ideas: hippocampal sequences arise from intrinsic recurrent circuitry that propagates activity without readily available input, acting as a temporal memory buffer for extremely sparse inputs.We implement a minimal sequence generator inspired by neurobiology and pair it with an actor-critic learner for egocentric visual navigation. Our agent reliably solves a continuous maze without explicit geometric cues, with performance depending on the length of the recurrent sequence. Crucially, the model outperforms LSTM cores under sparse input conditions (16 channels, ~2.5% activity), but not under dense input, revealing a strong interaction between representational sparsity and memory architecture.In contrast to LSTM agents, hidden sequence units develop localized place fields, distance-dependent spatial kernels, and task-dependent remapping, while inputs orthogonalize and spatial information increases across layers. These phenomena align with neurobiological data and are causal to performance. Together, our results show that sparse input synergizes with sequence-generating dynamics, providing both a mechanistic account of place cell sequences in the mammalian hippocampus and a simple inductive bias for reinforcement learning based on sparse egocentric inputs in navigation tasks.
<div id='section'>PaperID: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2510.08227.pdf' target='_blank'>https://arxiv.org/pdf/2510.08227.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mariana Fernandez-Espinosa, Kai Zhang, Jad Bendarkawi, Ashley Ponce, Sean Chidozie Mata, Aminah Aliu, Lei Zhang, Francisco Fernandez Medina, Elena Mangione-Lora, Andres Monroy-Hernandez, Diego Gomez-Zara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08227">Practicing a Second Language Without Fear: Mixed Reality Agents for Interactive Group Conversation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing speaking proficiency in a second language can be cognitively demanding and emotionally taxing, often triggering fear of making mistakes or being excluded from larger groups. While current learning tools show promise for speaking practice, most focus on dyadic, scripted scenarios, limiting opportunities for dynamic group interactions. To address this gap, we present ConversAR, a Mixed Reality system that leverages Generative AI and XR to support situated and personalized group conversations. It integrates embodied AI agents, scene recognition, and generative 3D props anchored to real-world surroundings. Based on a formative study with experts in language acquisition, we developed and tested this system with a user study with 21 second-language learners. Results indicate that the system enhanced learner engagement, increased willingness to communicate, and offered a safe space for speaking. We discuss the implications for integrating Generative AI and XR into the design of future language learning applications.
<div id='section'>PaperID: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2510.00167.pdf' target='_blank'>https://arxiv.org/pdf/2510.00167.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Diego Ortiz Barbosa, Mohit Agrawal, Yash Malegaonkar, Luis Burbano, Axel Andersson, György Dán, Henrik Sandberg, Alvaro A. Cardenas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00167">Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous drones must often respond to sudden events, such as alarms, faults, or unexpected changes in their environment, that require immediate and adaptive decision-making. Traditional approaches rely on safety engineers hand-coding large sets of recovery rules, but this strategy cannot anticipate the vast range of real-world contingencies and quickly becomes incomplete. Recent advances in embodied AI, powered by large visual language models, provide commonsense reasoning to assess context and generate appropriate actions in real time. We demonstrate this capability in a simulated urban benchmark in the Unreal Engine, where drones dynamically interpret their surroundings and decide on sudden maneuvers for safe landings. Our results show that embodied AI makes possible a new class of adaptive recovery and decision-making pipelines that were previously infeasible to design by hand, advancing resilience and safety in autonomous aerial systems.
<div id='section'>PaperID: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2509.21243.pdf' target='_blank'>https://arxiv.org/pdf/2509.21243.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiyeon Koo, Taewan Cho, Hyunjoon Kang, Eunseom Pyo, Tae Gyun Oh, Taeryang Kim, Andrew Jaeyong Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21243">RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert. RetoVLA maintains a lightweight structure while leveraging this repurposed spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness through a series of comprehensive experiments. On our custom-built 7-DOF robot arm, the model achieves a 17.1%p absolute improvement in success rates for complex manipulation tasks. Our results confirm that reusing Register Tokens directly enhances spatial reasoning, demonstrating that what was previously discarded as an artifact is in fact a valuable, unexplored resource for robotic intelligence. A video demonstration is available at: https://youtu.be/2CseBR-snZg
<div id='section'>PaperID: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2509.12008.pdf' target='_blank'>https://arxiv.org/pdf/2509.12008.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqing Song, Cesare Tonola, Stefano Savazzi, Sanaz Kianoush, Nicola Pedrocchi, Stephan Sigg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12008">Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly prevalent in both homes and industrial settings, the demand for intuitive and efficient human-machine interaction continues to rise. Gesture recognition offers an intuitive control method that does not require physical contact with devices and can be implemented using various sensing technologies. Wireless solutions are particularly flexible and minimally invasive. While camera-based vision systems are commonly used, they often raise privacy concerns and can struggle in complex or poorly lit environments. In contrast, radar sensing preserves privacy, is robust to occlusions and lighting, and provides rich spatial data such as distance, relative velocity, and angle. We present a gesture-controlled robotic arm using mm-wave radar for reliable, contactless motion recognition. Nine gestures are recognized and mapped to real-time commands with precision. Case studies are conducted to demonstrate the system practicality, performance and reliability for gesture-based robotic manipulation. Unlike prior work that treats gesture recognition and robotic control separately, our system unifies both into a real-time pipeline for seamless, contactless human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2509.11622.pdf' target='_blank'>https://arxiv.org/pdf/2509.11622.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingyun Chen, Qing Xiao, Zitao Zhang, Eli Blevis, Selma Å abanoviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11622">Robots that Evolve with Us: Modular Co-Design for Personalization, Adaptability, and Sustainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many current robot designs prioritize efficiency and one-size-fits-all solutions, oftentimes overlooking personalization, adaptability, and sustainability. To explore alternatives, we conducted two co-design workshops with 23 participants, who engaged with a modular robot co-design framework. Using components we provided as building blocks, participants combined, removed, and invented modules to envision how modular robots could accompany them from childhood through adulthood and into older adulthood. The participants' designs illustrate how modularity (a) enables personalization through open-ended configuration, (b) adaptability across shifting life-stage needs, and (c) sustainability through repair, reuse, and continuity. We therefore derive design principles that establish modularity as a foundation for lifespan-oriented human-robot interaction. This work reframes modular robotics as a flexible and expressive co-design approach, supporting robots that evolve with people, rather than static products optimized for single moments or contexts of use.
<div id='section'>PaperID: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2508.05310.pdf' target='_blank'>https://arxiv.org/pdf/2508.05310.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jelle Luijkx, Zlatan AjanoviÄ, Laura Ferranti, Jens Kober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05310">ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io.
<div id='section'>PaperID: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
<div id='section'>PaperID: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2506.15607.pdf' target='_blank'>https://arxiv.org/pdf/2506.15607.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shailesh, Alok Raj, Nayan Kumar, Priya Shukla, Andrew Melnik, Micheal Beetz, Gora Chand Nandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15607">GRIM: Task-Oriented Grasping with Conditioning on Generative Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-Oriented Grasping (TOG) presents a significant challenge, requiring a nuanced understanding of task semantics, object affordances, and the functional constraints dictating how an object should be grasped for a specific task. To address these challenges, we introduce GRIM (Grasp Re-alignment via Iterative Matching), a novel training-free framework for task-oriented grasping. Initially, a coarse alignment strategy is developed using a combination of geometric cues and principal component analysis (PCA)-reduced DINO features for similarity scoring. Subsequently, the full grasp pose associated with the retrieved memory instance is transferred to the aligned scene object and further refined against a set of task-agnostic, geometrically stable grasps generated for the scene object, prioritizing task compatibility. In contrast to existing learning-based methods, GRIM demonstrates strong generalization capabilities, achieving robust performance with only a small number of conditioning examples.
<div id='section'>PaperID: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2505.19803.pdf' target='_blank'>https://arxiv.org/pdf/2505.19803.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fuze Sun, Lingyu Li, Shixiangyue Meng, Xiaoming Teng, Terry R. Payne, Paul Craig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19803">Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the integration of individual human traits into an empathetically adaptive educational robot tutor system designed to improve student engagement and learning outcomes with corresponding Engagement Vector measurement. While prior research in the field of Human-Robot Interaction (HRI) has examined the integration of the traits, such as emotional intelligence, memory-driven personalization, and non-verbal communication, by themselves, they have thus-far neglected to consider their synchronized integration into a cohesive, operational education framework. To address this gap, we customize a Multi-Modal Large Language Model (LLaMa 3.2 from Meta) deployed with modules for human-like traits (emotion, memory and gestures) into an AI-Agent framework. This constitutes to the robot's intelligent core mimicing the human emotional system, memory architecture and gesture control to allow the robot to behave more empathetically while recognizing and responding appropriately to the student's emotional state. It can also recall the student's past learning record and adapt its style of interaction accordingly. This allows the robot tutor to react to the student in a more sympathetic manner by delivering personalized verbal feedback synchronized with relevant gestures. Our study investigates the extent of this effect through the introduction of Engagement Vector Model which can be a surveyor's pole for judging the quality of HRI experience. Quantitative and qualitative results demonstrate that such an empathetic responsive approach significantly improves student engagement and learning outcomes compared with a baseline humanoid robot without these human-like traits. This indicates that robot tutors with empathetic capabilities can create a more supportive, interactive learning experience that ultimately leads to better outcomes for the student.
<div id='section'>PaperID: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2505.15954.pdf' target='_blank'>https://arxiv.org/pdf/2505.15954.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nasim Paykari, Ali Alfatemi, Damian M. Lyons, Mohamed Rahouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15954">Integrating Robotic Navigation with Blockchain: A Novel PoS-Based Approach for Heterogeneous Robotic Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work explores a novel integration of blockchain methodologies with Wide Area Visual Navigation (WAVN) to address challenges in visual navigation for a heterogeneous team of mobile robots deployed for unstructured applications in agriculture, forestry, etc. Focusing on overcoming challenges such as GPS independence, environmental changes, and computational limitations, the study introduces the Proof of Stake (PoS) mechanism, commonly used in blockchain systems, into the WAVN framework \cite{Lyons_2022}. This integration aims to enhance the cooperative navigation capabilities of robotic teams by prioritizing robot contributions based on their navigation reliability. The methodology involves a stake weight function, consensus score with PoS, and a navigability function, addressing the computational complexities of robotic cooperation and data validation. This innovative approach promises to optimize robotic teamwork by leveraging blockchain principles, offering insights into the scalability, efficiency, and overall system performance. The project anticipates significant advancements in autonomous navigation and the broader application of blockchain technology beyond its traditional financial context.
<div id='section'>PaperID: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2504.18355.pdf' target='_blank'>https://arxiv.org/pdf/2504.18355.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maximilian Xiling Li, Korbinian Rudolf, Nils Blank, Rudolf Lioutikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18355">Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a "this looks like that" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.
<div id='section'>PaperID: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2503.16548.pdf' target='_blank'>https://arxiv.org/pdf/2503.16548.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Elisabeth Menendez, Michael Gienger, Santiago MartÃ­nez, Carlos Balaguer, Anna Belardinelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16548">SemanticScanpath: Combining Gaze and Speech for Situated Human-Robot Interaction Using LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have substantially improved the conversational capabilities of social robots. Nevertheless, for an intuitive and fluent human-robot interaction, robots should be able to ground the conversation by relating ambiguous or underspecified spoken utterances to the current physical situation and to the intents expressed non verbally by the user, for example by using referential gaze. Here we propose a representation integrating speech and gaze to enable LLMs to obtain higher situated awareness and correctly resolve ambiguous requests. Our approach relies on a text-based semantic translation of the scanpath produced by the user along with the verbal requests and demonstrates LLM's capabilities to reason about gaze behavior, robustly ignoring spurious glances or irrelevant objects. We validate the system across multiple tasks and two scenarios, showing its generality and accuracy, and demonstrate its implementation on a robotic platform, closing the loop from request interpretation to execution.
<div id='section'>PaperID: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2503.13048.pdf' target='_blank'>https://arxiv.org/pdf/2503.13048.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haofeng Chen, Bedrich Himmel, Bin Li, Xiaojie Wang, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13048">Multi-Touch and Bending Perception Using Electrical Impedance Tomography for Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electrical Impedance Tomography (EIT) offers a promising solution for distributed tactile sensing with minimal wiring and full-surface coverage in robotic applications. However, EIT-based tactile sensors face significant challenges during surface bending. Deformation alters the baseline impedance distribution and couples with touch-induced conductivity variations, complicating signal interpretation. To address this challenge, we present a novel sensing framework that integrates a deep neural network for interaction state classification with a dynamic adaptive reference strategy to decouple touch and deformation signals, while a data-driven regression model translates EIT voltage changes into continuous bending angles. The framework is validated using a magnetic hydrogel composite sensor that conforms to bendable surfaces. Experimental evaluations demonstrate that the proposed framework achieves precise and robust bending angle estimation, high accuracy in distinguishing touch, bending, and idle states, and significantly improves touch localization quality under bending deformation compared to conventional fixed-reference methods. Real-time experiments confirm the system's capability to reliably detect multi-touch interactions and track bending angles across varying deformation conditions. This work paves the way for flexible EIT-based robotic skins capable of rich multimodal sensing in robotics and human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2502.11142.pdf' target='_blank'>https://arxiv.org/pdf/2502.11142.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11142">NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.
<div id='section'>PaperID: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2501.15068.pdf' target='_blank'>https://arxiv.org/pdf/2501.15068.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dongjiang Li, Bo Peng, Chang Li, Ning Qiao, Qi Zheng, Lei Sun, Yusen Qin, Bangguo Li, Yifeng Luan, Bo Wu, Yibing Zhan, Mingang Sun, Tong Xu, Lusong Li, Hui Shen, Xiaodong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15068">An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied manipulation is a fundamental ability in the realm of embodied artificial intelligence. Although current embodied manipulation models show certain generalizations in specific settings, they struggle in new environments and tasks due to the complexity and diversity of real-world scenarios. The traditional end-to-end data collection and training manner leads to significant data demands. Decomposing end-to-end tasks into atomic skills helps reduce data requirements and improves the task success rate. However, existing methods are limited by predefined skill sets that cannot be dynamically updated. To address the issue, we introduce a three-wheeled data-driven method to build an atomic skill library. We divide tasks into subtasks using the Vision-Language-Planning (VLP). Then, atomic skill definitions are formed by abstracting the subtasks. Finally, an atomic skill library is constructed via data collection and Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands dynamically with the three-wheel update strategy, the range of tasks it can cover grows naturally. In this way, our method shifts focus from end-to-end tasks to atomic skills, significantly reducing data costs while maintaining high performance and enabling efficient adaptation to new tasks. Extensive experiments in real-world settings demonstrate the effectiveness and efficiency of our approach.
<div id='section'>PaperID: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2412.20826.pdf' target='_blank'>https://arxiv.org/pdf/2412.20826.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fanjun Bu, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20826">ReStory: VLM-augmentation of Social Human-Robot Interaction Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internet-scaled datasets are a luxury for human-robot interaction (HRI) researchers, as collecting natural interaction data in the wild is time-consuming and logistically challenging. The problem is exacerbated by robots' different form factors and interaction modalities. Inspired by recent work on ethnomethodological and conversation analysis (EMCA) in the domain of HRI, we propose ReStory, a method that has the potential to augment existing in-the-wild human-robot interaction datasets leveraging Vision Language Models. While still requiring human supervision, ReStory is capable of synthesizing human-interpretable interaction scenarios in the form of storyboards. We hope our proposed approach provides HRI researchers and interaction designers with a new angle to utilizing their valuable and scarce data.
<div id='section'>PaperID: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2412.19139.pdf' target='_blank'>https://arxiv.org/pdf/2412.19139.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dejie Yang, Zijing Zhao, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19139">PlanLLM: Video Procedure Planning with Refinable Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video procedure planning, i.e., planning a sequence of action steps given the video frames of start and goal states, is an essential ability for embodied AI. Recent works utilize Large Language Models (LLMs) to generate enriched action step description texts to guide action step decoding. Although LLMs are introduced, these methods decode the action steps into a closed-set of one-hot vectors, limiting the model's capability of generalizing to new steps or tasks. Additionally, fixed action step descriptions based on world-level commonsense may contain noise in specific instances of visual states. In this paper, we propose PlanLLM, a cross-modal joint learning framework with LLMs for video procedure planning. We propose an LLM-Enhanced Planning module which fully uses the generalization ability of LLMs to produce free-form planning output and to enhance action step decoding. We also propose Mutual Information Maximization module to connect world-level commonsense of step descriptions and sample-specific information of visual states, enabling LLMs to employ the reasoning ability to generate step sequences. With the assistance of LLMs, our method can both closed-set and open vocabulary procedure planning tasks. Our PlanLLM achieves superior performance on three benchmarks, demonstrating the effectiveness of our designs.
<div id='section'>PaperID: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2412.18516.pdf' target='_blank'>https://arxiv.org/pdf/2412.18516.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>David SobrÃ­n-Hidalgo, Ãngel Manuel Guerrero-Higueras, Vicente MatellÃ¡n-Olivera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18516">Generating Explanations for Autonomous Robots: a Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building trust between humans and robots has long interested the robotics community. Various studies have aimed to clarify the factors that influence the development of user trust. In Human-Robot Interaction (HRI) environments, a critical aspect of trust development is the robot's ability to make its behavior understandable. The concept of an eXplainable Autonomous Robot (XAR) addresses this requirement. However, giving a robot self-explanatory abilities is a complex task. Robot behavior includes multiple skills and diverse subsystems. This complexity led to research into a wide range of methods for generating explanations about robot behavior. This paper presents a systematic literature review that analyzes existing strategies for generating explanations in robots and studies the current XAR trends. Results indicate promising advancements in explainability systems. However, these systems are still unable to fully cover the complex behavior of autonomous robots. Furthermore, we also identify a lack of consensus on the theoretical concept of explainability, and the need for a robust methodology to assess explainability methods and tools has been identified.
<div id='section'>PaperID: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2411.14322.pdf' target='_blank'>https://arxiv.org/pdf/2411.14322.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arjun P S, Andrew Melnik, Gora Chand Nandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14322">SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting and Dense Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Experience Goal Visual Rearrangement task stands as a foundational challenge within Embodied AI, requiring an agent to construct a robust world model that accurately captures the goal state. The agent uses this world model to restore a shuffled scene to its original configuration, making an accurate representation of the world essential for successfully completing the task. In this work, we present a novel framework that leverages on 3D Gaussian Splatting as a 3D scene representation for experience goal visual rearrangement task. Recent advances in volumetric scene representation like 3D Gaussian Splatting, offer fast rendering of high quality and photo-realistic novel views. Our approach enables the agent to have consistent views of the current and the goal setting of the rearrangement task, which enables the agent to directly compare the goal state and the shuffled state of the world in image space. To compare these views, we propose to use a dense feature matching method with visual features extracted from a foundation model, leveraging its advantages of a more universal feature representation, which facilitates robustness, and generalization. We validate our approach on the AI2-THOR rearrangement challenge benchmark and demonstrate improvements over the current state of the art methods
<div id='section'>PaperID: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2411.14092.pdf' target='_blank'>https://arxiv.org/pdf/2411.14092.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thomas Woehrle, Arun N. Sivakumar, Naveen Uppalapati, Girish Chowdhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14092">MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous under-canopy navigation faces additional challenges compared to over-canopy settings - for example the tight spacing between the crop rows, degraded GPS accuracy and excessive clutter. Keypoint-based visual navigation has been shown to perform well in these conditions, however the differences between agricultural environments in terms of lighting, season, soil and crop type mean that a domain shift will likely be encountered at some point of the robot deployment. In this paper, we explore the use of Meta-Learning to overcome this domain shift using a minimal amount of data. We train a base-learner that can quickly adapt to new conditions, enabling more robust navigation in low-data regimes.
<div id='section'>PaperID: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2411.09299.pdf' target='_blank'>https://arxiv.org/pdf/2411.09299.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simone Arreghini, Antonio Paolillo, Gabriele Abbate, Alessandro Giusti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09299">Hearing the Robot's Mind: Sonification for Explicit Feedback in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are required not only to understand human intentions but also to effectively communicate their intentions or own internal states to users. This study explores the use of sonification to provide explicit auditory feedback, enhancing mutual understanding in HRI. We introduce a novel sonification approach that conveys the robot's internal state, linked to its perception of nearby individuals and their interaction intentions. The approach is evaluated through a two-fold user study: an online video-based survey with $26$ participants and live experiments with $10$ participants. Results indicate that while sonification improves the robot's expressivity and communication effectiveness, the design of the auditory feedback needs refinement to enhance user experience. Participants found the auditory cues useful but described the sounds as uninteresting and unpleasant. These findings underscore the importance of carefully designed auditory feedback in developing more effective and engaging HRI systems.
<div id='section'>PaperID: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2411.03483.pdf' target='_blank'>https://arxiv.org/pdf/2411.03483.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Caio Mucchiani, Dimitrios Chatziparaschis, Konstantinos Karydis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03483">Augmented-Reality Enabled Crop Monitoring with Robot Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of augmented reality (AR), extended reality (XR), and virtual reality (VR) technologies in agriculture has shown significant promise in enhancing various agricultural practices. Mobile robots have also been adopted as assessment tools in precision agriculture, improving economic efficiency and productivity, and minimizing undesired effects such as weeds and pests. Despite considerable work on both fronts, the combination of a versatile User Interface (UI) provided by an AR headset with the integration and direct interaction and control of a mobile field robot has not yet been fully explored or standardized. This work aims to address this gap by providing real-time data input and control output of a mobile robot for precision agriculture through a virtual environment enabled by an AR headset interface. The system leverages open-source computational tools and off-the-shelf hardware for effective integration. Distinctive case studies are presented where growers or technicians can interact with a legged robot via an AR headset and a UI. Users can teleoperate the robot to gather information in an area of interest, request real-time graphed status of an area, or have the robot autonomously navigate to selected areas for measurement updates. The proposed system utilizes a custom local navigation method with a fixed holographic coordinate system in combination with QR codes. This step toward fusing AR and robotics in agriculture aims to provide practical solutions for real-time data management and control enabled by human-robot interaction. The implementation can be extended to various robot applications in agriculture and beyond, promoting a unified framework for on-demand and autonomous robot operation in the field.
<div id='section'>PaperID: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2410.18633.pdf' target='_blank'>https://arxiv.org/pdf/2410.18633.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kieran Gilday, Chapa Sirithunge, Fumiya Iida, Josie Hughes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18633">Embodied Manipulation with Past and Future Morphologies through an Open Parametric Hand Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A human-shaped robotic hand offers unparalleled versatility and fine motor skills, enabling it to perform a broad spectrum of tasks with precision, power and robustness. Across the paleontological record and animal kingdom we see a wide range of alternative hand and actuation designs. Understanding the morphological design space and the resulting emergent behaviors can not only aid our understanding of dexterous manipulation and its evolution, but also assist design optimization, achieving, and eventually surpassing human capabilities. Exploration of hand embodiment has to date been limited by inaccessibility of customizable hands in the real-world, and by the reality gap in simulation of complex interactions. We introduce an open parametric design which integrates techniques for simplified customization, fabrication, and control with design features to maximize behavioral diversity. Non-linear rolling joints, anatomical tendon routing, and a low degree-of-freedom, modulating, actuation system, enable rapid production of single-piece 3D printable hands without compromising dexterous behaviors. To demonstrate this, we evaluated the design's low-level behavior range and stability, showing variable stiffness over two orders of magnitude. Additionally, we fabricated three hand designs: human, mirrored human with two thumbs, and aye-aye hands. Manipulation tests evaluate the variation in each hand's proficiency at handling diverse objects, and demonstrate emergent behaviors unique to each design. Overall, we shed light on new possible designs for robotic hands, provide a design space to compare and contrast different hand morphologies and structures, and share a practical and open-source design for exploring embodied manipulation.
<div id='section'>PaperID: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2410.03287.pdf' target='_blank'>https://arxiv.org/pdf/2410.03287.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simone Arreghini, Gabriele Abbate, Alessandro Giusti, Antonio Paolillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03287">A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a service robot that offers chocolate treats to people passing in its proximity: it has the capability of predicting in advance a person's intention to interact, and to actuate an "offering" gesture, subtly extending the tray of chocolates towards a given target. We run the system for more than 5 hours across 3 days and two different crowded public locations; the system implements three possible behaviors that are randomly toggled every few minutes: passive (e.g. never performing the offering gesture); or active, triggered by either a naive distance-based rule, or a smart approach that relies on various behavioral cues of the user. We collect a real-world dataset that includes information on 1777 users with several spontaneous human-robot interactions and study the influence of robot actions on people's behavior. Our comprehensive analysis suggests that users are more prone to engage with the robot when it proactively starts the interaction. We release the dataset and provide insights to make our work reproducible for the community. Also, we report qualitative observations collected during the acquisition campaign and identify future challenges and research directions in the domain of social human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2602.06556.pdf' target='_blank'>https://arxiv.org/pdf/2602.06556.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guodong Wang, Chenkai Zhang, Qingjie Liu, Jinjin Zhang, Jiancheng Cai, Junjie Liu, Xinmin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.06556">LIBERO-X: Robustness Litmus for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.
<div id='section'>PaperID: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2602.05049.pdf' target='_blank'>https://arxiv.org/pdf/2602.05049.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiye Chen, Yanan Jian, Xiaoyi Dong, Shuxin Cao, Jing Wu, Patricio Vela, Benjamin E. Lundell, Dongdong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.05049">VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .
<div id='section'>PaperID: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2602.02533.pdf' target='_blank'>https://arxiv.org/pdf/2602.02533.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kun Wang, Xiao Feng, Mingcheng Qu, Tonghua Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.02533">HMVLA: Hyperbolic Multimodal Fusion for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action (VLA) models have recently shown great potential in bridging multimodal perception with robotic control. However, existing methods often rely on direct fine-tuning of pre-trained Vision-Language Models (VLMs), feeding semantic and visual features directly into a policy network without fully addressing the unique semantic alignment challenges in the VLA domain. In this paper, we propose HMVLA, a novel VLA framework that exploits the inherent hierarchical structures in vision and language for comprehensive semantic alignment. Unlike traditional methods that perform alignment in Euclidean space, our HMVLA embeds multimodal features in hyperbolic space, enabling more effective modeling of the hierarchical relationships present in image text data. Furthermore, we introduce a sparsely gated Mixture of Experts (MoE) mechanism tailored for semantic alignment, which enhances multimodal comprehension between images and text while improving efficiency. Extensive experiments demonstrate that HMVLA surpasses baseline methods in both accuracy and generalization. In addition, we validate its robustness by reconstructing datasets to further test cross domain adaptability.
<div id='section'>PaperID: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2602.00131.pdf' target='_blank'>https://arxiv.org/pdf/2602.00131.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fraser Robinson, Souren Pashangpour, Matthew Lisondra, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00131">PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.
<div id='section'>PaperID: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2601.20321.pdf' target='_blank'>https://arxiv.org/pdf/2601.20321.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li, Jiaming Jiang, Chenxi Xiao, Ziyuan Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20321">Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.
<div id='section'>PaperID: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2601.16870.pdf' target='_blank'>https://arxiv.org/pdf/2601.16870.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guangping Liu, Nicholas Hawkins, Billy Madden, Tipu Sultan, Flavio Esposito, Madi Babaiasl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16870">A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrated control of wheelchairs and wheelchair-mounted robotic arms (WMRAs) has strong potential to increase independence for users with severe motor limitations, yet existing interfaces often lack the flexibility needed for intuitive assistive interaction. Although data-driven AI methods show promise, progress is limited by the lack of multimodal datasets that capture natural Human-Robot Interaction (HRI), particularly conversational ambiguity in dialogue-driven control. To address this gap, we propose a multimodal data collection framework that employs a dialogue-based interaction protocol and a two-room Wizard-of-Oz (WoZ) setup to simulate robot autonomy while eliciting natural user behavior. The framework records five synchronized modalities: RGB-D video, conversational audio, inertial measurement unit (IMU) signals, end-effector Cartesian pose, and whole-body joint states across five assistive tasks. Using this framework, we collected a pilot dataset of 53 trials from five participants and validated its quality through motion smoothness analysis and user feedback. The results show that the framework effectively captures diverse ambiguity types and supports natural dialogue-driven interaction, demonstrating its suitability for scaling to a larger dataset for learning, benchmarking, and evaluation of ambiguity-aware assistive control.
<div id='section'>PaperID: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2601.10796.pdf' target='_blank'>https://arxiv.org/pdf/2601.10796.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junxiang Wang, Cindy Wang, Rana Soltani Zarrin, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.10796">Bidirectional Human-Robot Communication for Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot's planned trajectory -- position, velocity, and force -- in real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot's verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website: https://bidir-comm.github.io/
<div id='section'>PaperID: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2601.09444.pdf' target='_blank'>https://arxiv.org/pdf/2601.09444.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lauri Suomela, Naoki Takahata, Sasanka Kuruppu Arachchige, Harry Edelman, Joni-Kristian Kämäräinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09444">Data Scaling for Navigation in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving. Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.
<div id='section'>PaperID: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2601.06573.pdf' target='_blank'>https://arxiv.org/pdf/2601.06573.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixing Lin, Jiale Wang, Gee Wah Ng, Lee Onn Mak, Chan Zhi Yang Jeriel, Jun Yang Lee, Yaohao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06573">QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) for video-audio understanding have traditionally been evaluated only on shorter videos of a few minutes long. In this paper, we introduce QMAVIS (Q Team-Multimodal Audio Video Intelligent Sensemaking), a novel long video-audio understanding pipeline built through a late fusion of LMMs, Large Language Models, and speech recognition models. QMAVIS addresses the gap in long-form video analytics, particularly for longer videos of a few minutes to beyond an hour long, opening up new potential applications in sensemaking, video content analysis, embodied AI, etc. Quantitative experiments using QMAVIS demonstrated a 38.75% improvement over state-of-the-art video-audio LMMs like VideoLlaMA2 and InternVL2 on the VideoMME (with subtitles) dataset, which comprises long videos with audio information. Evaluations on other challenging video understanding datasets like PerceptionTest and EgoSchema saw up to 2% improvement, indicating competitive performance. Qualitative experiments also showed that QMAVIS is able to extract the nuances of different scenes in a long video audio content while understanding the overarching narrative. Ablation studies were also conducted to ascertain the impact of each component in the fusion pipeline.
<div id='section'>PaperID: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2512.18474.pdf' target='_blank'>https://arxiv.org/pdf/2512.18474.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18474">When Robots Say No: The Empathic Ethical Disobedience Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted.
<div id='section'>PaperID: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2511.15565.pdf' target='_blank'>https://arxiv.org/pdf/2511.15565.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniel Bermuth, Alexander Poeppel, Wolfgang Reif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15565">Scriboora: Rethinking Human Pose Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.
<div id='section'>PaperID: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2511.12896.pdf' target='_blank'>https://arxiv.org/pdf/2511.12896.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jun Huo, Hongge Ru, Bo Yang, Xingjian Chen, Xi Li, Jian Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12896">Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\%$, 2.7$\%$, 5.8$\%$ and 6.7$\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.
<div id='section'>PaperID: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2511.12184.pdf' target='_blank'>https://arxiv.org/pdf/2511.12184.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jun Huo, Kehan Xu, Chengyao Li, Yu Cao, Jie Zuo, Xinxing Chen, Jian Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12184">Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.
<div id='section'>PaperID: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2510.27033.pdf' target='_blank'>https://arxiv.org/pdf/2510.27033.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simindokht Jahangard, Mehrzad Mohammadi, Abhinav Dhall, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27033">A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual reasoning, particularly spatial reasoning, is a challenging cognitive task that requires understanding object relationships and their interactions within complex environments, especially in robotics domain. Existing vision_language models (VLMs) excel at perception tasks but struggle with fine-grained spatial reasoning due to their implicit, correlation-driven reasoning and reliance solely on images. We propose a novel neuro_symbolic framework that integrates both panoramic-image and 3D point cloud information, combining neural perception with symbolic reasoning to explicitly model spatial and logical relationships. Our framework consists of a perception module for detecting entities and extracting attributes, and a reasoning module that constructs a structured scene graph to support precise, interpretable queries. Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superior performance and reliability in crowded, human_built environments while maintaining a lightweight design suitable for robotics and embodied AI applications.
<div id='section'>PaperID: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2510.17369.pdf' target='_blank'>https://arxiv.org/pdf/2510.17369.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haochen Su, Cristian Meo, Francesco Stella, Andrea Peirone, Kai Junge, Josie Hughes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17369">Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $π_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.
<div id='section'>PaperID: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2509.22493.pdf' target='_blank'>https://arxiv.org/pdf/2509.22493.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alberto Olivares-Alarcos, Sergi Foix, JÃºlia BorrÃ s, Gerard Canal, Guillem AlenyÃ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22493">Ontological foundations for contrastive explanatory narration of robot plans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mutual understanding of artificial agents' decisions is key to ensuring a trustworthy and successful human-robot interaction. Hence, robots are expected to make reasonable decisions and communicate them to humans when needed. In this article, the focus is on an approach to modeling and reasoning about the comparison of two competing plans, so that robots can later explain the divergent result. First, a novel ontological model is proposed to formalize and reason about the differences between competing plans, enabling the classification of the most appropriate one (e.g., the shortest, the safest, the closest to human preferences, etc.). This work also investigates the limitations of a baseline algorithm for ontology-based explanatory narration. To address these limitations, a novel algorithm is presented, leveraging divergent knowledge between plans and facilitating the construction of contrastive narratives. Through empirical evaluation, it is observed that the explanations excel beyond the baseline method.
<div id='section'>PaperID: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2509.17760.pdf' target='_blank'>https://arxiv.org/pdf/2509.17760.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Austin Wilson, Sahar Kapasi, Zane Greene, Alexis E. Block
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17760">Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many research groups face challenges when legacy (unsupported) robotic platforms lose manufacturer support and cannot accommodate modern sensing, speech, and interaction capabilities. We present the Enhanced NAO, a revitalized version of Aldebaran's NAO robot that uses upgraded microphones, RGB-D and thermal cameras, and additional compute resources in a fully self-contained package. This system combines cloud and local models for perception and dialogue, while preserving the NAO's expressive body and behaviors. In a pilot validation study, the Enhanced NAO delivered significantly higher conversational quality and stronger user preference compared to the NAO AI Edition, without increasing response latency. Key upgrades, such as beamforming microphones and low-latency audio processing, reduced artifacts like self-hearing and improved multi-party separation. Expanded visual and thermal sensing established a foundation for future interaction capabilities. Beyond the NAO, our framework provides a platform-agnostic strategy for extending the lifespan and research utility of legacy robots, ensuring they remain valuable tools for human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2509.16176.pdf' target='_blank'>https://arxiv.org/pdf/2509.16176.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Lin, Sophie Ziyu Liu, Ran Qi, George Z. Xue, Xinping Song, Chao Qin, Hugh H. -T. Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16176">Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories (ACDC), an autonomous drone cinematography system driven by natural language communication between human directors and drones. The main limitation of previous drone cinematography workflows is that they require manual selection of waypoints and view angles based on predefined human intent, which is labor-intensive and yields inconsistent performance. In this paper, we propose employing large language models (LLMs) and vision foundation models (VFMs) to convert free-form natural language prompts directly into executable indoor UAV video tours. Specifically, our method comprises a vision-language retrieval pipeline for initial waypoint selection, a preference-based Bayesian optimization framework that refines poses using aesthetic feedback, and a motion planner that generates safe quadrotor trajectories. We validate ACDC through both simulation and hardware-in-the-loop experiments, demonstrating that it robustly produces professional-quality footage across diverse indoor scenes without requiring expertise in robotics or cinematography. These results highlight the potential of embodied AI agents to close the loop from open-vocabulary dialogue to real-world autonomous aerial cinematography.
<div id='section'>PaperID: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2509.11791.pdf' target='_blank'>https://arxiv.org/pdf/2509.11791.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lauri Suomela, Sasanka Kuruppu Arachchige, German F. Torres, Harry Edelman, Joni-Kristian KÃ¤mÃ¤rÃ¤inen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11791">Synthetic vs. Real Training Data for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates how the performance of visual navigation policies trained in simulation compares to policies trained with real-world data. Performance degradation of simulator-trained policies is often significant when they are evaluated in the real world. However, despite this well-known sim-to-real gap, we demonstrate that simulator-trained policies can match the performance of their real-world-trained counterparts. Central to our approach is a navigation policy architecture that bridges the sim-to-real appearance gap by leveraging pretrained visual representations and runs real-time on robot hardware. Evaluations on a wheeled mobile robot show that the proposed policy, when trained in simulation, outperforms its real-world-trained version by 31% and the prior state-of-the-art methods by 50% in navigation success rate. Policy generalization is verified by deploying the same model onboard a drone. Our results highlight the importance of diverse image encoder pretraining for sim-to-real generalization, and identify on-policy learning as a key advantage of simulated training over training with real data.
<div id='section'>PaperID: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2509.06917.pdf' target='_blank'>https://arxiv.org/pdf/2509.06917.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiacheng Miao, Joe R. Davis, Jonathan K. Pritchard, James Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06917">Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
<div id='section'>PaperID: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2509.06597.pdf' target='_blank'>https://arxiv.org/pdf/2509.06597.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Frederik Plahl, Georgios Katranis, Ilshat Mamaev, Andrey Morozov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06597">LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LiHRA, a novel dataset designed to facilitate the development of automated, learning-based, or classical risk monitoring (RM) methods for Human-Robot Interaction (HRI) scenarios. The growing prevalence of collaborative robots in industrial environments has increased the need for reliable safety systems. However, the lack of high-quality datasets that capture realistic human-robot interactions, including potentially dangerous events, slows development. LiHRA addresses this challenge by providing a comprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body keypoints, and robot joint states, capturing the complete spatial and dynamic context of human-robot collaboration. This combination of modalities allows for precise tracking of human movement, robot actions, and environmental conditions, enabling accurate RM during collaborative tasks. The LiHRA dataset covers six representative HRI scenarios involving collaborative and coexistent tasks, object handovers, and surface polishing, with safe and hazardous versions of each scenario. In total, the data set includes 4,431 labeled point clouds recorded at 10 Hz, providing a rich resource for training and benchmarking classical and AI-driven RM algorithms. Finally, to demonstrate LiHRA's utility, we introduce an RM method that quantifies the risk level in each scenario over time. This method leverages contextual information, including robot states and the dynamic model of the robot. With its combination of high-resolution LiDAR data, precise human tracking, robot state data, and realistic collision events, LiHRA offers an essential foundation for future research into real-time RM and adaptive safety strategies in human-robot workspaces.
<div id='section'>PaperID: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2509.05433.pdf' target='_blank'>https://arxiv.org/pdf/2509.05433.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rui Chen, Domenico Chiaradia, Antonio Frisoli, Daniele Leonardis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05433">HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Haptic interfaces that can simultaneously modulate multiple physical properties remain a fundamental challenge in human-robot interaction. Existing systems typically allow the rendering of either geometric features or mechanical properties, but rarely both, within wearable form factors. Here, we introduce HapMorph, a pneumatic framework that enables continuous, simultaneous modulation of object size and stiffness through antagonistic fabric-based pneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for hands interaction achieving size variation from 50 to 104 mm, stiffness modulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through systematic characterization, we demonstrate decoupled control of size and stiffness properties via dual-chamber pressure regulation. Human perception studies with 10 participants reveal that users can distinguish nine discrete states across three size categories and three stiffness levels with 89.4% accuracy and 6.7 s average response time. We further demonstrate extended architectures that combine AFPAs with complementary pneumatic structures to enable shape or geometry morphing with concurrent stiffness control. Our results establish antagonistic pneumatic principle as a pathway toward next-generation haptic interfaces, capable of multi-dimensiona rendering properties within practical wearable constraints.
<div id='section'>PaperID: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2509.02983.pdf' target='_blank'>https://arxiv.org/pdf/2509.02983.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02983">DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
<div id='section'>PaperID: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2509.00117.pdf' target='_blank'>https://arxiv.org/pdf/2509.00117.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jared Perlo, Alexander Robey, Fazl Barez, Luciano Floridi, Jakob MÃ¶kander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00117">Embodied AI: Emerging Risks and Opportunities for Policy Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI systems can exist in, learn from, reason about, and act in the physical world. With recent advances in AI models and hardware, EAI systems are becoming increasingly capable across wider operational domains. While EAI systems can offer many benefits, they also pose significant risks, including physical harm from malicious use, mass surveillance, as well as economic and societal disruption. These risks require urgent attention from policymakers, as existing policies governing industrial robots and autonomous vehicles are insufficient to address the full range of concerns EAI systems present. To help address this issue, this paper makes three contributions. First, we provide a taxonomy of the physical, informational, economic, and social risks EAI systems pose. Second, we analyze policies in the US, EU, and UK to assess how existing frameworks address these risks and to identify critical gaps. We conclude by offering policy recommendations for the safe and beneficial deployment of EAI systems, such as mandatory testing and certification schemes, clarified liability frameworks, and strategies to manage EAI's potentially transformative economic and societal impacts.
<div id='section'>PaperID: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2508.13976.pdf' target='_blank'>https://arxiv.org/pdf/2508.13976.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Carlo Mazzola, Hassan Ali, KristÃ­na MalinovskÃ¡, Igor FarkaÅ¡
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13976">Toward an Interaction-Centered Approach to Robot Trustworthiness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots get more integrated into human environments, fostering trustworthiness in embodied robotic agents becomes paramount for an effective and safe human-robot interaction (HRI). To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. In this position paper, we outline an interaction-based framework for building trust through mutual understanding between humans and robots. We emphasize two main pillars: human awareness and transparency, referring to the robot ability to interpret human actions accurately and to clearly communicate its intentions and goals, respectively. By integrating these two pillars, robots can behave in a manner that aligns with human expectations and needs while providing their human partners with both comprehension and control over their actions. We also introduce four components that we think are important for bridging the gap between a human-perceived sense of trust and a robot true capabilities.
<div id='section'>PaperID: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2508.12637.pdf' target='_blank'>https://arxiv.org/pdf/2508.12637.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shankaranarayanan H, Satyapreet Singh Yadav, Adithya Krishna, Ajay Vikram P, Mahesh Mehendale, Chetan Singh Thakur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12637">HOMI: Ultra-Fast EdgeAI platform for Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras offer significant advantages for edge robotics applications due to their asynchronous operation and sparse, event-driven output, making them well-suited for tasks requiring fast and efficient closed-loop control, such as gesture-based human-robot interaction. Despite this potential, existing event processing solutions remain limited, often lacking complete end-to-end implementations, exhibiting high latency, and insufficiently exploiting event data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI accelerator. We have developed hardware-optimized pre-processing pipelines supporting both constant-time and constant-event modes for histogram accumulation, linear and exponential time surfaces. Our general-purpose implementation caters to both accuracy-driven and low-latency applications. HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when configured for high accuracy operation and provides a throughput of 1000 fps for low-latency configuration. The hardware-optimised pipeline maintains a compact memory footprint and utilises only 33% of the available LUT resources on the FPGA, leaving ample headroom for further latency reduction, model parallelisation, multi-task deployments, or integration of more complex architectures.
<div id='section'>PaperID: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2508.10561.pdf' target='_blank'>https://arxiv.org/pdf/2508.10561.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrea Gargano, Jasin Machkour, Mimma Nardelli, Enzo Pasquale Scilingo, Michael Muma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10561">Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Affective Computing, a key challenge lies in reliably linking subjective emotional experiences with objective physiological markers. This preliminary study addresses the issue of reproducibility by identifying physiological features from cardiovascular and electrodermal signals that are associated with continuous self-reports of arousal levels. Using the Continuously Annotated Signal of Emotion dataset, we analyzed 164 features extracted from cardiac and electrodermal signals of 30 participants exposed to short emotion-evoking videos. Feature selection was performed using the Terminating-Random Experiments (T-Rex) method, which performs variable selection systematically controlling a user-defined target False Discovery Rate. Remarkably, among all candidate features, only two electrodermal-derived features exhibited reproducible and statistically significant associations with arousal, achieving a 100\% confirmation rate. These results highlight the necessity of rigorous reproducibility assessments in physiological features selection, an aspect often overlooked in Affective Computing. Our approach is particularly promising for applications in safety-critical environments requiring trustworthy and reliable white box models, such as mental disorder recognition and human-robot interaction systems.
<div id='section'>PaperID: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2508.05294.pdf' target='_blank'>https://arxiv.org/pdf/2508.05294.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge PeÃ±a Queralta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05294">Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those works advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.
<div id='section'>PaperID: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2508.05104.pdf' target='_blank'>https://arxiv.org/pdf/2508.05104.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrej LÃºÄny, Matilde Antonj, Carlo Mazzola, Hana HornÃ¡ÄkovÃ¡, Ana FariÄ, KristÃ­na MalinovskÃ¡, Michal Vavrecka, Igor FarkaÅ¡
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05104">Examining the legibility of humanoid robot arm movements in a pointing task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human--robot interaction requires robots whose actions are legible, allowing humans to interpret, predict, and feel safe around them. This study investigates the legibility of humanoid robot arm movements in a pointing task, aiming to understand how humans predict robot intentions from truncated movements and bodily cues. We designed an experiment using the NICO humanoid robot, where participants observed its arm movements towards targets on a touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or 80\% of their full length, and participants predicted the final target. We tested the multimodal superiority and ocular primacy hypotheses, both of which were supported by the experiment.
<div id='section'>PaperID: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2507.10812.pdf' target='_blank'>https://arxiv.org/pdf/2507.10812.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chuxuan Zhang, Yasaman Etesam, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10812">React to This (RTT): A Nonverbal Turing Test for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an approach to test embodied AI agents for interaction awareness and believability, particularly in scenarios where humans push them to their limits. Turing introduced the Imitation Game as a way to explore the question: "Can machines think?" The Total Turing Test later expanded this concept beyond purely verbal communication, incorporating perceptual and physical interaction. Building on this, we propose a new guiding question: "Can machines react?" and introduce the React to This (RTT) test for nonverbal behaviors, presenting results from an initial experiment.
<div id='section'>PaperID: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2507.01843.pdf' target='_blank'>https://arxiv.org/pdf/2507.01843.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01843">MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixture-of-Experts (MoE) approaches have recently gained traction in robotics applications due to their ability to dynamically allocate computational resources and specialize sub-networks for distinct tasks or environmental contexts, enabling more efficient decision-making. Such systems often comprise sparsely activated experts combined under a single monolithic architecture and require a well-configured internal routing mechanism, which does not allow for selective low-level expert and router customization and requires additional training. We propose MoIRA, an architecture-agnostic modular MoE framework designed to coordinate existing experts with an external text-based router. MoIRA incorporates two zero-shot routing options: embedding-based similarity and prompt-driven language model inference. In our experiments, we choose large Vision-Language-Action models, gr00t-N1 and $Ï_0$, as the underlying experts, and train low-rank adapters for low-overhead inference. We evaluate MoIRA on various GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it consistently outperforms generalist models and competes with other MoE pipelines. Additionally, we analyse the robustness of the proposed approach to the variations of the instructions. While relying solely on textual descriptions of tasks and experts, MoIRA demonstrates the practical viability of modular deployment with precise, low-effort routing and provides an alternative, scalable foundation for future multi-expert robotic systems.
<div id='section'>PaperID: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2506.21732.pdf' target='_blank'>https://arxiv.org/pdf/2506.21732.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ameya Salvi, Venkat Krovi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21732">Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based lane keeping is a topic of significant interest in the robotics and autonomous ground vehicles communities in various on-road and off-road applications. The skid-steered vehicle architecture has served as a useful vehicle platform for human controlled operations. However, systematic modeling, especially of the skid-slip wheel terrain interactions (primarily in off-road settings) has created bottlenecks for automation deployment. End-to-end learning based methods such as imitation learning and deep reinforcement learning, have gained prominence as a viable deployment option to counter the lack of accurate analytical models. However, the systematic formulation and subsequent verification/validation in dynamic operation regimes (particularly for skid-steered vehicles) remains a work in progress. To this end, a novel approach for structured formulation for learning visual navigation is proposed and investigated in this work. Extensive software simulations, hardware evaluations and ablation studies now highlight the significantly improved performance of the proposed approach against contemporary literature.
<div id='section'>PaperID: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2506.08892.pdf' target='_blank'>https://arxiv.org/pdf/2506.08892.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tauhid Tanjim, Jonathan St. George, Kevin Ching, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08892">Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human-robot interaction (HRI) field has recognized the importance of enabling robots to interact with teams. Human teams rely on effective communication for successful collaboration in time-sensitive environments. Robots can play a role in enhancing team coordination through real-time assistance. Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to investigate how multimodal robot communication in action teams affects workload and human perception of robots. We explore team collaboration in a medical training scenario where a robotic crash cart (RCC) provides verbal and non-verbal cues to help users remember to perform iterative tasks and search for supplies. Our findings show that verbal cues for object search tasks and visual cues for task reminders reduce team workload and increase perceived ease of use and perceived usefulness more effectively than a robot with no feedback. Our work contributes to multimodal interaction research in the HRI field, highlighting the need for more human-robot teaming research to understand best practices for integrating collaborative robots in time-sensitive environments such as in hospitals, search and rescue, and manufacturing applications.
<div id='section'>PaperID: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2506.06624.pdf' target='_blank'>https://arxiv.org/pdf/2506.06624.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mojtaba Mollahossein, Farshad Haghgoo Daryakenari, Mohammad Hossein Rohban, Gholamreza Vossoughi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06624">Attention-Based Convolutional Neural Network Model for Human Lower Limb Activity Recognition using sEMG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate classification of lower limb movements using surface electromyography (sEMG) signals plays a crucial role in assistive robotics and rehabilitation systems. In this study, we present a lightweight attention-based deep neural network (DNN) for real-time movement classification using multi-channel sEMG data from the publicly available BASAN dataset. The proposed model consists of only 62,876 parameters and is designed without the need for computationally expensive preprocessing, making it suitable for real-time deployment. We employed a leave-oneout validation strategy to ensure generalizability across subjects, and evaluated the model on three movement classes: walking, standing with knee flexion, and sitting with knee extension. The network achieved 86.74% accuracy on the validation set and 85.38% on the test set, demonstrating strong classification performance under realistic conditions. Comparative analysis with existing models in the literature highlights the efficiency and effectiveness of our approach, especially in scenarios where computational cost and real-time response are critical. The results indicate that the proposed model is a promising candidate for integration into upper-level controllers in human-robot interaction systems.
<div id='section'>PaperID: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2505.20537.pdf' target='_blank'>https://arxiv.org/pdf/2505.20537.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junxiang Wang, Emek BarÄ±Å KÃ¼Ã§Ã¼ktabak, Rana Soltani Zarrin, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20537">CoRI: Communication of Robot Intent for Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot's upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot's image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot's high-level intentions but also crucial details about its motion and any collaborative user action needed. Video and code of our project can be found on our project website: https://cori-phri.github.io/
<div id='section'>PaperID: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2505.20503.pdf' target='_blank'>https://arxiv.org/pdf/2505.20503.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matthew Lisondra, Beno Benhabib, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20503">Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.
<div id='section'>PaperID: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2505.16928.pdf' target='_blank'>https://arxiv.org/pdf/2505.16928.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bosung Kim, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16928">Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.
<div id='section'>PaperID: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2505.00693.pdf' target='_blank'>https://arxiv.org/pdf/2505.00693.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanbang Li, Ziyang Gong, Haoyang Li, Xiaoqi Huang, Haolan Kang, Guangping Bai, Xianzheng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00693">Robotic Visual Instruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision introduces challenges for robotic task definition such as ambiguity and verbosity. Moreover, in some public settings where quiet is required, such as libraries or hospitals, verbal communication with robots is inappropriate. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment,enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Project website: https://robotic-visual-instruction.github.io/
<div id='section'>PaperID: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2504.13370.pdf' target='_blank'>https://arxiv.org/pdf/2504.13370.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiao Jin, Bo Xiao, Huijiang Wang, Wendong Wang, Zhenhua Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13370">Multi-Sensor Fusion-Based Mobile Manipulator Remote Control for Intelligent Smart Home Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a wearable-controlled mobile manipulator system for intelligent smart home assistance, integrating MEMS capacitive microphones, IMU sensors, vibration motors, and pressure feedback to enhance human-robot interaction. The wearable device captures forearm muscle activity and converts it into real-time control signals for mobile manipulation. The wearable device achieves an offline classification accuracy of 88.33\%\ across six distinct movement-force classes for hand gestures by using a CNN-LSTM model, while real-world experiments involving five participants yield a practical accuracy of 83.33\%\ with an average system response time of 1.2 seconds. In Human-Robot synergy in navigation and grasping tasks, the robot achieved a 98\%\ task success rate with an average trajectory deviation of only 3.6 cm. Finally, the wearable-controlled mobile manipulator system achieved a 93.3\%\ gripping success rate, a transfer success of 95.6\%\, and a full-task success rate of 91.1\%\ during object grasping and transfer tests, in which a total of 9 object-texture combinations were evaluated. These three experiments' results validate the effectiveness of MEMS-based wearable sensing combined with multi-sensor fusion for reliable and intuitive control of assistive robots in smart home scenarios.
<div id='section'>PaperID: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2503.16960.pdf' target='_blank'>https://arxiv.org/pdf/2503.16960.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Steve Benford, Eike Schneiders, Juan Pablo Martinez Avila, Praminda Caleb-Solly, Patrick Robert Brundell, Simon Castle-Green, Feng Zhou, Rachael Garrett, Kristina HÃ¶Ã¶k, Sarah Whatley, Kate Marsh, Paul Tennent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16960">Somatic Safety: An Embodied Approach Towards Safe Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots enter the messy human world so the vital matter of safety takes on a fresh complexion with physical contact becoming inevitable and even desirable. We report on an artistic-exploration of how dancers, working as part of a multidisciplinary team, engaged in contact improvisation exercises to explore the opportunities and challenges of dancing with cobots. We reveal how they employed their honed bodily senses and physical skills to engage with the robots aesthetically and yet safely, interleaving improvised physical manipulations with reflections to grow their knowledge of how the robots behaved and felt. We introduce somatic safety, a holistic mind-body approach in which safety is learned, felt and enacted through bodily contact with robots in addition to being reasoned about. We conclude that robots need to be better designed for people to hold them and might recognise tacit safety cues among people.We propose that safety should be learned through iterative bodily experience interleaved with reflection.
<div id='section'>PaperID: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2503.14328.pdf' target='_blank'>https://arxiv.org/pdf/2503.14328.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Renzi Wang, Mathijs Schuurmans, Panagiotis Patrinos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14328">Risk-Sensitive Model Predictive Control for Interaction-Aware Planning -- A Sequential Convexification Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers risk-sensitive model predictive control for stochastic systems with a decision-dependent distribution. This class of systems is commonly found in human-robot interaction scenarios. We derive computationally tractable convex upper bounds to both the objective function, and to frequently used penalty terms for collision avoidance, allowing us to efficiently solve the generally nonconvex optimal control problem as a sequence of convex problems. Simulations of a robot navigating a corridor demonstrate the effectiveness and the computational advantage of the proposed approach.
<div id='section'>PaperID: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2502.14917.pdf' target='_blank'>https://arxiv.org/pdf/2502.14917.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14917">Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.
<div id='section'>PaperID: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2502.02054.pdf' target='_blank'>https://arxiv.org/pdf/2502.02054.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minwoo Kim, Geunsik Bae, Jinwoo Lee, Woojae Shin, Changseung Kim, Myong-Yol Choi, Heejung Shin, Hyondong Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02054">RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.
<div id='section'>PaperID: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2411.10016.pdf' target='_blank'>https://arxiv.org/pdf/2411.10016.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kavindie Katuwandeniya, Leimin Tian, Dana KuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10016">'What did the Robot do in my Absence?' Video Foundation Models to Enhance Intermittent Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of Video Foundation Models (ViFMs) for generating robot data summaries to enhance intermittent human supervision of robot teams. We propose a novel framework that produces both generic and query-driven summaries of long-duration robot vision data in three modalities: storyboards, short videos, and text. Through a user study involving 30 participants, we evaluate the efficacy of these summary methods in allowing operators to accurately retrieve the observations and actions that occurred while the robot was operating without supervision over an extended duration (40 min). Our findings reveal that query-driven summaries significantly improve retrieval accuracy compared to generic summaries or raw data, albeit with increased task duration. Storyboards are found to be the most effective presentation modality, especially for object-related queries. This work represents, to our knowledge, the first zero-shot application of ViFMs for generating multi-modal robot-to-human communication in intermittent supervision contexts, demonstrating both the promise and limitations of these models in human-robot interaction (HRI) scenarios.
<div id='section'>PaperID: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2411.04499.pdf' target='_blank'>https://arxiv.org/pdf/2411.04499.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lei Han, Yu Zhou, Qiongyan Chen, David Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04499">Memory Remedy: An AI-Enhanced Interactive Story Exploring Human-Robot Interaction and Companionship</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our approach to using AI-generated content (AIGC) and multiple media to develop an immersive, game-based, interactive story experience. The narrative of the story, "Memory Remedy", unfolds through flashbacks, allowing the audience to gradually uncover the story and the complex relationship between the robot protagonist and the older adults. This exploration explores important themes such as the journey of life, the profound influence of memories, and the concept of post-human emotional care. By engaging with this AIGC-based interactive story, audiences are encouraged to reflect on the potential role of robotic companionship in the lives of older adults in the future; and to encourage deeper reflection on the complex relationship between artificial intelligence and humanity.
<div id='section'>PaperID: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2411.00007.pdf' target='_blank'>https://arxiv.org/pdf/2411.00007.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohsen Raoufi, Pawel Romanczuk, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00007">LARS: Light Augmented Reality System for Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Light Augmented Reality System LARS as an open-source and cost-effective tool. LARS leverages light-projected visual scenes for indirect robot-robot and human-robot interaction through the real environment. It operates in real-time and is compatible with a range of robotic platforms, from miniature to middle-sized robots. LARS can support researchers in conducting experiments with increased freedom, reliability, and reproducibility. This XR tool makes it possible to enrich the environment with full control by adding complex and dynamic objects while keeping the properties of robots as realistic as they are.
<div id='section'>PaperID: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2410.20496.pdf' target='_blank'>https://arxiv.org/pdf/2410.20496.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dong Hae Mangalindan, Ericka Rovira, Vaibhav Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20496">Trust-Aware Assistance Seeking in Human-Supervised Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to model and experimentally assess trust evolution to predict future beliefs and behaviors of human-robot teams in dynamic environments. Research suggests that maintaining trust among team members in a human-robot team is vital for successful team performance. Research suggests that trust is a multi-dimensional and latent entity that relates to past experiences and future actions in a complex manner. Employing a human-robot collaborative task, we design an optimal assistance-seeking strategy for the robot using a POMDP framework. In the task, the human supervises an autonomous mobile manipulator collecting objects in an environment. The supervisor's task is to ensure that the robot safely executes its task. The robot can either choose to attempt to collect the object or seek human assistance. The human supervisor actively monitors the robot's activities, offering assistance upon request, and intervening if they perceive the robot may fail. In this setting, human trust is the hidden state, and the primary objective is to optimize team performance. We execute two sets of human-robot interaction experiments. The data from the first experiment are used to estimate POMDP parameters, which are used to compute an optimal assistance-seeking policy evaluated in the second experiment. The estimated POMDP parameters reveal that, for most participants, human intervention is more probable when trust is low, particularly in high-complexity tasks. Our estimates suggest that the robot's action of asking for assistance in high-complexity tasks can positively impact human trust. Our experimental results show that the proposed trust-aware policy is better than an optimal trust-agnostic policy. By comparing model estimates of human trust, obtained using only behavioral data, with the collected self-reported trust values, we show that model estimates are isomorphic to self-reported responses.
<div id='section'>PaperID: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2410.12822.pdf' target='_blank'>https://arxiv.org/pdf/2410.12822.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marc Rigter, Tarun Gupta, Agrin Hilmkil, Chao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12822">AVID: Adapting Video Diffusion Models to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale generative models have achieved remarkable success in a number of domains. However, for sequential decision-making problems, such as robotics, action-labelled data is often scarce and therefore scaling-up foundation models for decision-making remains a challenge. A potential solution lies in leveraging widely-available unlabelled videos to train world models that simulate the consequences of actions. If the world model is accurate, it can be used to optimize decision-making in downstream tasks. Image-to-video diffusion models are already capable of generating highly realistic synthetic videos. However, these models are not action-conditioned, and the most powerful models are closed-source which means they cannot be finetuned. In this work, we propose to adapt pretrained video diffusion models to action-conditioned world models, without access to the parameters of the pretrained model. Our approach, AVID, trains an adapter on a small domain-specific dataset of action-labelled videos. AVID uses a learned mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos. We evaluate AVID on video game and real-world robotics data, and show that it outperforms existing baselines for diffusion model adaptation.1 Our results demonstrate that if utilized correctly, pretrained video models have the potential to be powerful tools for embodied AI.
<div id='section'>PaperID: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2410.05756.pdf' target='_blank'>https://arxiv.org/pdf/2410.05756.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuetao Li, Fang Gao, Jun Yu, Shaodong Li, Feng Shuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05756">Learning the Generalizable Manipulation Skills on Soft-body Tasks via Guided Self-attention Behavior Cloning Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI represents a paradigm in AI research where artificial agents are situated within and interact with physical or virtual environments. Despite the recent progress in Embodied AI, it is still very challenging to learn the generalizable manipulation skills that can handle large deformation and topological changes on soft-body objects, such as clay, water, and soil. In this work, we proposed an effective policy, namely GP2E behavior cloning policy, which can guide the agent to learn the generalizable manipulation skills from soft-body tasks, including pouring, filling, hanging, excavating, pinching, and writing. Concretely, we build our policy from three insights:(1) Extracting intricate semantic features from point cloud data and seamlessly integrating them into the robot's end-effector frame; (2) Capturing long-distance interactions in long-horizon tasks through the incorporation of our guided self-attention module; (3) Mitigating overfitting concerns and facilitating model convergence to higher accuracy levels via the introduction of our two-stage fine-tuning strategy. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in the soft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied AI workshop. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their practical applications in real-world scenarios.
<div id='section'>PaperID: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2409.17702.pdf' target='_blank'>https://arxiv.org/pdf/2409.17702.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Leonard BÃ¤rmann, Chad DeChant, Joana Plewnia, Fabian Peller-Konrad, Daniel Bauer, Tamim Asfour, Alex Waibel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17702">Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verbalization of robot experience, i.e., summarization of and question answering about a robot's past, is a crucial ability for improving human-robot interaction. Previous works applied rule-based systems or fine-tuned deep models to verbalize short (several-minute-long) streams of episodic data, limiting generalization and transferability. In our work, we apply large pretrained models to tackle this task with zero or few examples, and specifically focus on verbalizing life-long experiences. For this, we derive a tree-like data structure from episodic memory (EM), with lower levels representing raw perception and proprioception data, and higher levels abstracting events to natural language concepts. Given such a hierarchical representation built from the experience stream, we apply a large language model as an agent to interactively search the EM given a user's query, dynamically expanding (initially collapsed) tree nodes to find the relevant information. The approach keeps computational costs low even when scaling to months of robot experience data. We evaluate our method on simulated household robot data, human egocentric videos, and real-world robot recordings, demonstrating its flexibility and scalability.
<div id='section'>PaperID: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2602.12281.pdf' target='_blank'>https://arxiv.org/pdf/2602.12281.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.12281">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the "intention-action gap." We first characterize the test-time scaling laws for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce CoVer-VLA, a hierarchical test-time verification pipeline using the trained verifier. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses the verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer-VLA achieves 14% gains in task progress and 9% in success rate.
<div id='section'>PaperID: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2602.06243.pdf' target='_blank'>https://arxiv.org/pdf/2602.06243.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guangping Liu, Nicholas Hawkins, Billy Madden, Tipu Sultan, Madi Babaiasl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.06243">A Dialogue-Based Human-Robot Interaction Protocol for Wheelchair and Robotic Arm Integrated Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People with lower and upper body disabilities can benefit from wheelchairs and robotic arms to improve mobility and independence. Prior assistive interfaces, such as touchscreens and voice-driven predefined commands, often remain unintuitive and struggle to capture complex user intent. We propose a natural, dialogue based human robot interaction protocol that simulates an intelligent agent capable of communicating with users to understand intent and execute assistive actions. In a pilot study, five participants completed five assistive tasks (cleaning, drinking, feeding, drawer opening, and door opening) through dialogue-based interaction with a wheelchair and robotic arm. As a baseline, participants were required to open a door using the manual control (a wheelchair joystick and a game controller for the arm) and complete a questionnaire to gather their feedback. By analyzing the post-study questionnaires, we found that most participants enjoyed the dialogue-based interaction and assistive robot autonomy.
<div id='section'>PaperID: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2602.03064.pdf' target='_blank'>https://arxiv.org/pdf/2602.03064.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sandika Biswas, Kian Izadpanah, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03064">JRDB-Pose3D: A Multi-person 3D Human Pose and Shape Estimation Dataset for Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world scenes are inherently crowded. Hence, estimating 3D poses of all nearby humans, tracking their movements over time, and understanding their activities within social and environmental contexts are essential for many applications, such as autonomous driving, robot perception, robot navigation, and human-robot interaction. However, most existing 3D human pose estimation datasets primarily focus on single-person scenes or are collected in controlled laboratory environments, which restricts their relevance to real-world applications. To bridge this gap, we introduce JRDB-Pose3D, which captures multi-human indoor and outdoor environments from a mobile robotic platform. JRDB-Pose3D provides rich 3D human pose annotations for such complex and dynamic scenes, including SMPL-based pose annotations with consistent body-shape parameters and track IDs for each individual over time. JRDB-Pose3D contains, on average, 5-10 human poses per frame, with some scenes featuring up to 35 individuals simultaneously. The proposed dataset presents unique challenges, including frequent occlusions, truncated bodies, and out-of-frame body parts, which closely reflect real-world environments. Moreover, JRDB-Pose3D inherits all available annotations from the JRDB dataset, such as 2D pose, information about social grouping, activities, and interactions, full-scene semantic masks with consistent human- and object-level tracking, and detailed annotations for each individual, such as age, gender, and race, making it a holistic dataset for a wide range of downstream perception and human-centric understanding tasks.
<div id='section'>PaperID: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2602.01158.pdf' target='_blank'>https://arxiv.org/pdf/2602.01158.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniel Yezid Guarnizo Orjuela, Leonardo Scappatura, Veronica Di Gennaro, Riccardo Andrea Izzo, Gianluca Bardaro, Matteo Matteucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.01158">Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.
<div id='section'>PaperID: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2601.22965.pdf' target='_blank'>https://arxiv.org/pdf/2601.22965.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Runhua Zhang, Junyi Hou, Changxu Cheng, Qiyi Chen, Tao Wang, Wuyue Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.22965">Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive "generate-then-filter" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.
<div id='section'>PaperID: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2601.20377.pdf' target='_blank'>https://arxiv.org/pdf/2601.20377.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20377">RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.
<div id='section'>PaperID: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2601.13338.pdf' target='_blank'>https://arxiv.org/pdf/2601.13338.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyi Liu, Xinyi Wang, Shao-Kang Hsia, Chenfei Zhu, Zhengzhe Zhu, Xiyun Hu, Anastasia Kouvaras Ostrowski, Karthik Ramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.13338">Towards Natural Language Environment: Understanding Seamless Natural-Language-Based Human-Multi-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multiple robots are expected to coexist in future households, natural language is increasingly envisioned as a primary medium for human-robot and robot-robot communication. This paper introduces the concept of a Natural Language Environment (NLE), defined as an interaction space in which humans and multiple heterogeneous robots coordinate primarily through natural language. Rather than proposing a deployable system, this work aims to explore the design space of such environments. We first synthesize prior work on language-based human-robot interaction to derive a preliminary design space for NLEs. We then conduct a role-playing study in virtual reality to investigate how people conceptualize, negotiate, and coordinate human-multi-robot interactions within this imagined environment. Based on qualitative and quantitative analysis, we refine the preliminary design space and derive design implications that highlight key tensions and opportunities around task coordination dominance, robot autonomy, and robot personality in Natural Language Environments.
<div id='section'>PaperID: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2601.11421.pdf' target='_blank'>https://arxiv.org/pdf/2601.11421.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Qingbo Hao, Hongliang Lu, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Ye, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Kecheng Zheng, Qian Zhu, Ran Cheng, Yong-Lu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11421">The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.
<div id='section'>PaperID: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2601.03136.pdf' target='_blank'>https://arxiv.org/pdf/2601.03136.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Selma Wanna, Agnes Luhtaru, Jonathan Salfity, Ryan Barron, Juston Moore, Cynthia Matuszek, Mitch Pryor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03136">Limited Linguistic Diversity in Embodied AI Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.
<div id='section'>PaperID: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2601.01705.pdf' target='_blank'>https://arxiv.org/pdf/2601.01705.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kenneth Kwok, Basura Fernando, Qianli Xu, Vigneshwaran Subbaraju, Dongkyu Choi, Boon Kiat Quek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01705">Explicit World Models for Reliable Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible "explicit world model" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.
<div id='section'>PaperID: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2512.14952.pdf' target='_blank'>https://arxiv.org/pdf/2512.14952.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Iddo Yehoshua Wald, Amber Maimon, Shiyao Zhang, Dennis Küster, Robert Porzel, Tanja Schultz, Rainer Malaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14952">Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.
<div id='section'>PaperID: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2512.03828.pdf' target='_blank'>https://arxiv.org/pdf/2512.03828.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dominykas Strazdas, Magnus Jung, Jan Marquenie, Ingo Siegert, Ayoub Al-Hamadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03828">IM HERE: Interaction Model for Human Effort Based Robot Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.
<div id='section'>PaperID: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2512.00547.pdf' target='_blank'>https://arxiv.org/pdf/2512.00547.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00547">Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.
<div id='section'>PaperID: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2511.23215.pdf' target='_blank'>https://arxiv.org/pdf/2511.23215.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Eduardo Sergio Oliveros-Mata, Oleksandr V. Pylypovskyi, Eleonora Raimondo, Rico Illing, Yevhen Zabila, Lin Guo, Guannan Mu, Mónica Navarro López, Xu Wang, Georgios Tzortzinis, Angelos Filippatos, Gilbert Santiago Cañón Bermúdez, Francesca Garescì, Giovanni Finocchio, Denys Makarov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23215">Field-programmable dynamics in a soft magnetic actuator enabling true random number generation and reservoir computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex and even chaotic dynamics, though prevalent in many natural and engineered systems, has been largely avoided in the design of electromechanical systems due to concerns about wear and controlability. Here, we demonstrate that complex dynamics might be particularly advantageous in soft robotics, offering new functionalities beyond motion not easily achievable with traditional actuation methods. We designed and realized resilient magnetic soft actuators capable of operating in a tunable dynamic regime for tens of thousands cycles without fatigue. We experimentally demonstrated the application of these actuators for true random number generation and stochastic computing. {W}e validate soft robots as physical reservoirs capable of performing Mackey--Glass time series prediction. These findings show that exploring the complex dynamics in soft robotics would extend the application scenarios in soft computing, human-robot interaction and collaborative robots as we demonstrate with biomimetic blinking and randomized voice modulation.
<div id='section'>PaperID: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2511.21663.pdf' target='_blank'>https://arxiv.org/pdf/2511.21663.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Naifu Zhang, Wei Tao, Xi Xiao, Qianpu Sun, Yuxin Zheng, Wentao Mo, Peiqiang Wang, Nan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21663">Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.
<div id='section'>PaperID: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2511.15956.pdf' target='_blank'>https://arxiv.org/pdf/2511.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aliyah Smith, Monroe Kennedy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15956">The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.
<div id='section'>PaperID: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2511.12676.pdf' target='_blank'>https://arxiv.org/pdf/2511.12676.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Subin Varghese, Joshua Gao, Asad Ur Rahman, Vedhus Hoskere
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12676">BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery. We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images. Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.
<div id='section'>PaperID: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2511.08935.pdf' target='_blank'>https://arxiv.org/pdf/2511.08935.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ningnan Wang, Weihuang Chen, Liming Chen, Haoxuan Ji, Zhongyu Guo, Xuchong Zhang, Hongbin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08935">Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.
<div id='section'>PaperID: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2511.03576.pdf' target='_blank'>https://arxiv.org/pdf/2511.03576.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aniol Civit, Antonio Andriella, Carles Sierra, Guillem Alenyà
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03576">Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While personalisation in Human-Robot Interaction (HRI) has advanced significantly, most existing approaches focus on single-user adaptation, overlooking scenarios involving multiple stakeholders with potentially conflicting preferences. To address this, we propose the Multi-User Preferences Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user personalisation framework based on Quantitative Bipolar Argumentation Frameworks (QBAFs) that explicitly models and resolves multi-user preference conflicts. Unlike prior work in Argumentation Frameworks, which typically assumes static inputs, our approach is tailored to robotics: it incorporates both users' arguments and the robot's dynamic observations of the environment, allowing the system to adapt over time and respond to changing contexts. Preferences, both positive and negative, are represented as arguments whose strength is recalculated iteratively based on new information. The framework's properties and capabilities are presented and validated through a realistic case study, where an assistive robot mediates between the conflicting preferences of a caregiver and a care recipient during a frailty assessment task. This evaluation further includes a sensitivity analysis of argument base scores, demonstrating how preference outcomes can be shaped by user input and contextual observations. By offering a transparent, structured, and context-sensitive approach to resolving competing user preferences, this work advances the field of multi-user HRI. It provides a principled alternative to data-driven methods, enabling robots to navigate conflicts in real-world environments.
<div id='section'>PaperID: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2510.05425.pdf' target='_blank'>https://arxiv.org/pdf/2510.05425.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marta Lagomarsino, Francesco Tassi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05425">Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Work environments are often inadequate and lack inclusivity for individuals with upper-body disabilities. This paper presents a novel online framework for adaptive human-robot interaction (HRI) that accommodates users' arm mobility impairments, ultimately aiming to promote active work participation. Unlike traditional human-robot collaboration approaches that assume able-bodied users, our method integrates a mobility model for specific joint limitations into a hierarchical optimal controller. This allows the robot to generate reactive, mobility-aware behaviour online and guides the user's impaired limb to exploit residual functional mobility. The framework was tested in handover tasks involving different upper-limb mobility impairments (i.e., emulated elbow and shoulder arthritis, and wrist blockage), under both standing and seated configurations with task constraints using a mobile manipulator, and complemented by quantitative and qualitative comparisons with state-of-the-art ergonomic HRI approaches. Preliminary results indicated that the framework can personalise the interaction to fit within the user's impaired range of motion and encourage joint usage based on the severity of their functional limitations.
<div id='section'>PaperID: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2509.24528.pdf' target='_blank'>https://arxiv.org/pdf/2509.24528.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohamad Amin Mirzaei, Pantea Amoie, Ali Ekhterachian, Matin Mirzababaei, Babak Khalaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24528">CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.
<div id='section'>PaperID: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2509.20656.pdf' target='_blank'>https://arxiv.org/pdf/2509.20656.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junzhe Wang, Jiarui Xie, Pengfei Hao, Zheng Li, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20656">EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable brain-computer interface (BCI) control of robots provides an intuitive and accessible means of human-robot interaction, particularly valuable for individuals with motor impairments. However, existing BCI-Robot systems face major limitations: electroencephalography (EEG) signals are noisy and unstable, target selection is often predefined and inflexible, and most studies remain restricted to simulation without closed-loop validation. These issues hinder real-world deployment in assistive scenarios. To address them, we propose a closed-loop BCI-AR-Robot system that integrates motor imagery (MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic grasping for zero-touch operation. A 14-channel EEG headset enabled individualized MI calibration, a smartphone-based AR interface supported multi-target navigation with direction-congruent feedback to enhance stability, and the robotic arm combined decision outputs with vision-based pose estimation for autonomous grasping. Experiments are conducted to validate the framework: MI training achieved 93.1 percent accuracy with an average information transfer rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2 percent success rate with good efficiency and strong user-reported control. These results show that AR feedback substantially stabilizes EEG-based control and that the proposed framework enables robust zero-touch grasping, advancing assistive robotic applications and future modes of human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2509.07488.pdf' target='_blank'>https://arxiv.org/pdf/2509.07488.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiao Li, Bharat Gandhi, Ming Zhan, Mohit Nehra, Zhicheng Zhang, Yuchen Sun, Meijia Song, Naisheng Zhang, Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07488">Fine-Tuning Vision-Language Models for Visual Navigation Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.
<div id='section'>PaperID: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2509.03842.pdf' target='_blank'>https://arxiv.org/pdf/2509.03842.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guanglu Jia, Ceng Zhang, Gregory S. Chirikjian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03842">INGRID: Intelligent Generative Robotic Design Using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of large language models (LLMs) into robotic systems has accelerated progress in embodied artificial intelligence, yet current approaches remain constrained by existing robotic architectures, particularly serial mechanisms. This hardware dependency fundamentally limits the scope of robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic Design), a framework that enables the automated design of parallel robotic mechanisms through deep integration with reciprocal screw theory and kinematic synthesis methods. We decompose the design challenge into four progressive tasks: constraint analysis, kinematic joint generation, chain construction, and complete mechanism design. INGRID demonstrates the ability to generate novel parallel mechanisms with both fixed and variable mobility, discovering kinematic configurations not previously documented in the literature. We validate our approach through three case studies demonstrating how INGRID assists users in designing task-specific parallel robots based on desired mobility requirements. By bridging the gap between mechanism theory and machine learning, INGRID enables researchers without specialized robotics training to create custom parallel mechanisms, thereby decoupling advances in robotic intelligence from hardware constraints. This work establishes a foundation for mechanism intelligence, where AI systems actively design robotic hardware, potentially transforming the development of embodied AI systems.
<div id='section'>PaperID: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2509.02164.pdf' target='_blank'>https://arxiv.org/pdf/2509.02164.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinshen Zhang, Tongxi Fu, Xu Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02164">Omnidirectional Spatial Modeling from Correlated Panoramas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional scene understanding is vital for various downstream applications, such as embodied AI, autonomous driving, and immersive environments, yet remains challenging due to geometric distortion and complex spatial relations in 360Â° imagery. Existing omnidirectional methods achieve scene understanding within a single frame while neglecting cross-frame correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the \textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas visual question answering in the holistic 360Â° scenes. CFpano consists of over 2700 images together with over 8000 question-answer pairs, and the question types include both multiple choice and open-ended VQA. Building upon our CFpano, we further present \methodname, a multi-modal large language model (MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of tailored reward functions for robust and consistent reasoning with cross-frame correlated panoramas. Benchmark experiments with existing MLLMs are conducted with our CFpano. The experimental results demonstrate that \methodname achieves state-of-the-art performance across both multiple-choice and open-ended VQA tasks, outperforming strong baselines on all major reasoning categories (\textbf{+5.37\%} in overall performance). Our analyses validate the effectiveness of GRPO and establish a new benchmark for panoramic scene understanding.
<div id='section'>PaperID: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2508.10378.pdf' target='_blank'>https://arxiv.org/pdf/2508.10378.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Chen, Shu Miao, Chunyu Wu, Jingsong Mu, Bo OuYang, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10378">A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Upper-limb exoskeletons are primarily designed to provide assistive support by accurately interpreting and responding to human intentions. In home-care scenarios, exoskeletons are expected to adapt their assistive configurations based on the semantic information of the task, adjusting appropriately in accordance with the nature of the object being manipulated. However, existing solutions often lack the ability to understand task semantics or collaboratively plan actions with the user, limiting their generalizability. To address this challenge, this paper introduces a semantic-aware framework that integrates large language models into the task planning framework, enabling the delivery of safe and intent-integrative assistance. The proposed approach begins with the exoskeleton operating in transparent mode to capture the wearer's intent during object grasping. Once semantic information is extracted from the task description, the system automatically configures appropriate assistive parameters. In addition, a diffusion-based anomaly detector is used to continuously monitor the state of human-robot interaction and trigger real-time replanning in response to detected anomalies. During task execution, online trajectory refinement and impedance control are used to ensure safety and regulate human-robot interaction. Experimental results demonstrate that the proposed method effectively aligns with the wearer's cognition, adapts to semantically varying tasks, and responds reliably to anomalies.
<div id='section'>PaperID: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2508.06207.pdf' target='_blank'>https://arxiv.org/pdf/2508.06207.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrea Dal Prete, Seyram Ofori, Chan Yon Sin, Ashwin Narayan, Ding Shuo, Francesco Braghin, Marta Gandolla, Haoyong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06207">Toward Context-Aware Exoskeleton Assistance: Integrating Computer Vision Payload Estimation with a User-Centric Optimization Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Back-support exoskeletons (BSEs) mitigate musculoskeletal strain, yet their efficacy depends on precise, context-aware modulation. This paper introduces a user-centric optimization framework and a vision-based adaptive control strategy for industrial BSEs. First, we constructed a multi-metric optimization space, integrating electromyography reduction, perceived discomfort, and user preference, through baseline experiments with 12 subjects. This revealed a non-linear relationship between optimal assistance and payload. Second, we developed a predictive computer vision pipeline using a Vision Transformer (DINOv2) to estimate payloads before lifting, effectively overcoming actuation latency. Validation with 12 subjects confirmed the system's robustness, achieving over 82% estimation accuracy. Crucially, the adaptive controller reduced peak back muscle activation by up to 23% compared to static baselines while optimizing user comfort. These results validate the proposed framework, demonstrating that pre-lift environmental perception and user-centric optimization significantly enhance physical assistance and human-robot interaction in industrial settings.
<div id='section'>PaperID: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2508.05021.pdf' target='_blank'>https://arxiv.org/pdf/2508.05021.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weifan Zhang, Tingguang Li, Yuzhen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05021">MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation in unknown environments based solely on natural language descriptions is a key capability for intelligent robots. In this work, we propose a navigation framework built upon off-the-shelf Visual Language Models (VLMs), enhanced with two human-inspired mechanisms: perspective-based active grounding, which dynamically adjusts the robot's viewpoint for improved visual inspection, and historical memory backtracking, which enables the system to retain and re-evaluate uncertain observations over time. Unlike existing approaches that passively rely on incidental visual inputs, our method actively optimizes perception and leverages memory to resolve ambiguity, significantly improving vision-language grounding in complex, unseen environments. Our framework operates in a zero-shot manner, achieving strong generalization to diverse and open-ended language descriptions without requiring labeled data or model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show that our method outperforms state-of-the-art approaches in language-driven object navigation. We further demonstrate its practicality through real-world deployment on a quadruped robot, achieving robust and effective navigation performance.
<div id='section'>PaperID: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2508.01651.pdf' target='_blank'>https://arxiv.org/pdf/2508.01651.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanqing Wang, Zhenhao Zhang, Kaiyang Ji, Mingyu Liu, Wenti Yin, Yuchao Chen, Zhirui Liu, Xiangyu Zeng, Tianxiang Gui, Hangxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01651">DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object affordance grounding aims to predict the touchable regions on a 3d object, which is crucial for human-object interaction, human-robot interaction, embodied perception, and robot learning. Recent advances tackle this problem via learning from demonstration images. However, these methods fail to capture the general affordance knowledge within the image, leading to poor generalization. To address this issue, we propose to use text-to-image diffusion models to extract the general affordance knowledge because we find that such models can generate semantically valid HOI images, which demonstrate that their internal representation space is highly correlated with real-world affordance concepts. Specifically, we introduce the DAG, a diffusion-based 3d affordance grounding framework, which leverages the frozen internal representations of the text-to-image diffusion model and unlocks affordance knowledge within the diffusion model to perform 3D affordance grounding. We further introduce an affordance block and a multi-source affordance decoder to endow 3D dense affordance prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization.
<div id='section'>PaperID: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2507.21225.pdf' target='_blank'>https://arxiv.org/pdf/2507.21225.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Annan Zhang, Miguel Flores-Acton, Andy Yu, Anshul Gupta, Maggie Yao, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21225">Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing plays a fundamental role in enabling robots to navigate dynamic and unstructured environments, particularly in applications such as delicate object manipulation, surface exploration, and human-robot interaction. In this paper, we introduce a passive soft robotic fingertip with integrated tactile sensing, fabricated using a 3D-printed elastomer lattice with embedded air channels. This sensorization approach, termed fluidic innervation, transforms the lattice into a tactile sensor by detecting pressure changes within sealed air channels, providing a simple yet robust solution to tactile sensing in robotics. Unlike conventional methods that rely on complex materials or designs, fluidic innervation offers a simple, scalable, single-material fabrication process. We characterize the sensors' response, develop a geometric model to estimate tip displacement, and train a neural network to accurately predict contact location and contact force. Additionally, we integrate the fingertip with an admittance controller to emulate spring-like behavior, demonstrate its capability for environment exploration through tactile feedback, and validate its durability under high impact and cyclic loading conditions. This tactile sensing technique offers advantages in terms of simplicity, adaptability, and durability and opens up new opportunities for versatile robotic manipulation.
<div id='section'>PaperID: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2507.18820.pdf' target='_blank'>https://arxiv.org/pdf/2507.18820.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rachel Ringe, Robin Nolte, Nima Zargham, Robert Porzel, Rainer Malaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18820">MetaMorph -- A Metamodelling Approach For Robot Morphology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot appearance crucially shapes Human-Robot Interaction (HRI) but is typically described via broad categories like anthropomorphic, zoomorphic, or technical. More precise approaches focus almost exclusively on anthropomorphic features, which fail to classify robots across all types, limiting the ability to draw meaningful connections between robot design and its effect on interaction. In response, we present MetaMorph, a comprehensive framework for classifying robot morphology. Using a metamodeling approach, MetaMorph was synthesized from 222 robots in the IEEE Robots Guide, offering a structured method for comparing visual features. This model allows researchers to assess the visual distances between robot models and explore optimal design traits tailored to different tasks and contexts.
<div id='section'>PaperID: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2507.07818.pdf' target='_blank'>https://arxiv.org/pdf/2507.07818.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lu Xu, Jiaqian Yu, Xiongfeng Peng, Yiwei Chen, Weiming Li, Jaewook Yoo, Sunghyun Chunag, Dongwook Lee, Daehyun Ji, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07818">MoSE: Skill-by-Skill Mixture-of-Experts Learning for Embodied Autonomous Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To meet the growing demand for smarter, faster, and more efficient embodied AI solutions, we introduce a novel Mixture-of-Expert (MoE) method that significantly boosts reasoning and learning efficiency for embodied autonomous systems. General MoE models demand extensive training data and complex optimization, which limits their applicability in embodied AI such as autonomous driving (AD) and robotic manipulation. In this work, we propose a skill-oriented MoE called MoSE, which mimics the human learning and reasoning process skill-by-skill, step-by-step. We introduce a skill-oriented routing mechanism that begins with defining and annotating specific skills, enabling experts to identify the necessary competencies for various scenarios and reasoning tasks, thereby facilitating skill-by-skill learning. To better align with multi-step planning in human reasoning and in end-to-end driving models, we build a hierarchical skill dataset and pretrain the router to encourage the model to think step-by-step. Unlike other multi-round dialogues, MoSE integrates valuable auxiliary tasks (e.g. perception-prediction-planning for AD, and high-level and low-level planning for robots) in one single forward process without introducing any extra computational cost. With less than 3B sparsely activated parameters, our model effectively grows more diverse expertise and outperforms models on both AD corner-case reasoning tasks and robot reasoning tasks with less than 40% of the parameters.
<div id='section'>PaperID: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2506.13583.pdf' target='_blank'>https://arxiv.org/pdf/2506.13583.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bernhard Hilpert, Muhan Hou, Kim Baraka, Joost Broekens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13583">Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) agents often exhibit learning behaviors that are not intuitively interpretable by human observers, which can result in suboptimal feedback in collaborative teaching settings. Yet, how humans perceive and interpret RL agent's learning behavior is largely unknown. In a bottom-up approach with two experiments, this work provides a data-driven understanding of the factors of human observers' understanding of the agent's learning process. A novel, observation-based paradigm to directly assess human inferences about agent learning was developed. In an exploratory interview study (\textit{N}=9), we identify four core themes in human interpretations: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second confirmatory study (\textit{N}=34) applied an expanded version of the paradigm across two tasks (navigation/manipulation) and two RL algorithms (tabular/function approximation). Analyses of 816 responses confirmed the reliability of the paradigm and refined the thematic framework, revealing how these themes evolve over time and interrelate. Our findings provide a human-centered understanding of how people make sense of agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.
<div id='section'>PaperID: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2506.09581.pdf' target='_blank'>https://arxiv.org/pdf/2506.09581.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Miguel Ã. GonzÃ¡lez-Santamarta, Francisco J. RodrÃ­guez-Lera, David SobrÃ­n-Hidalgo, Ãngel Manuel Guerrero-Higueras, Vicente MatellÃn-Olivera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09581">Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have experienced great advancements in the last year resulting in an increase of these models in several fields to face natural language tasks. The integration of these models in robotics can also help to improve several aspects such as human-robot interaction, navigation, planning and decision-making. Therefore, this paper introduces llama\_ros, a tool designed to integrate quantized Large Language Models (LLMs) into robotic systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine, llama\_ros enables the efficient execution of quantized LLMs as edge artificial intelligence (AI) in robotics systems with resource-constrained environments, addressing the challenges of computational efficiency and memory limitations. By deploying quantized LLMs, llama\_ros empowers robots to leverage the natural language understanding and generation for enhanced decision-making and interaction which can be paired with prompt engineering, knowledge graphs, ontologies or other tools to improve the capabilities of autonomous robots. Additionally, this paper provides insights into some use cases of using llama\_ros for planning and explainability in robotics.
<div id='section'>PaperID: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2505.24257.pdf' target='_blank'>https://arxiv.org/pdf/2505.24257.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24257">Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An embodied AI assistant operating on egocentric video must integrate spatial cues across time - for instance, determining where an object A, glimpsed a few moments ago lies relative to an object B encountered later. We introduce Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs by posing questions about object pairs that are not co-visible in the same frame. We evaluated seven state-of-the-art VLMs and found that models lag behind human performance by 28%, with steeper declines in accuracy (60% to 30 %) as the temporal gap widens. Our analysis further reveals that providing trajectories or bird's-eye-view projections to VLMs results in only marginal improvements, whereas providing oracle 3D coordinates leads to a substantial 20% performance increase. This highlights a core bottleneck of multi-frame VLMs in constructing and maintaining 3D scene representations over time from visual signals. Disjoint-3DQA therefore sets a clear, measurable challenge for long-horizon spatial reasoning and aims to catalyze future research at the intersection of vision, language, and embodied AI.
<div id='section'>PaperID: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2505.18361.pdf' target='_blank'>https://arxiv.org/pdf/2505.18361.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Trinity Chung, Yuchen Shen, Nathan C. L. Kong, Aran Nayebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18361">Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.
<div id='section'>PaperID: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2505.14366.pdf' target='_blank'>https://arxiv.org/pdf/2505.14366.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14366">Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.
<div id='section'>PaperID: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2505.14197.pdf' target='_blank'>https://arxiv.org/pdf/2505.14197.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinshen Zhang, Zhen Ye, Xu Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14197">Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional images (ODIs), with their 360Â° field of view, provide unparalleled spatial awareness for immersive applications like augmented reality and embodied AI. However, the capability of existing multi-modal large language models (MLLMs) to comprehend and reason about such panoramic scenes remains underexplored. This paper addresses this gap by introducing OmniVQA, the first dataset and conducting the first benchmark for omnidirectional visual question answering. Our evaluation of state-of-the-art MLLMs reveals significant limitations in handling omnidirectional visual question answering, highlighting persistent challenges in object localization, feature extraction, and hallucination suppression within panoramic contexts. These results underscore the disconnect between current MLLM capabilities and the demands of omnidirectional visual understanding, which calls for dedicated architectural or training innovations tailored to 360Â° imagery. Building on the OmniVQA dataset and benchmark, we further introduce a rule-based reinforcement learning method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group relative policy optimization (GRPO) by proposing three novel reward functions: (1) reasoning process similarity reward, (2) answer semantic accuracy reward, and (3) structured format compliance reward. Extensive experiments on our OmniVQA demonstrate the superiority of our proposed method in omnidirectional space (+6% improvement).
<div id='section'>PaperID: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2504.16516.pdf' target='_blank'>https://arxiv.org/pdf/2504.16516.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16516">Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.
<div id='section'>PaperID: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2504.03153.pdf' target='_blank'>https://arxiv.org/pdf/2504.03153.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Natalie Tirabassi, Sathish A. P. Kumar, Sumit Jha, Arvind Ramanathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03153">MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MORAL (a multimodal reinforcement learning framework for decision making in autonomous laboratories) that enhances sequential decision-making in autonomous robotic laboratories through the integration of visual and textual inputs. Using the BridgeData V2 dataset, we generate fine-tuned image captions with a pretrained BLIP-2 vision-language model and combine them with visual features through an early fusion strategy. The fused representations are processed using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents. Experimental results demonstrate that multimodal agents achieve a 20% improvement in task completion rates and significantly outperform visual-only and textual-only baselines after sufficient training. Compared to transformer-based and recurrent multimodal RL models, our approach achieves superior performance in cumulative reward and caption quality metrics (BLEU, METEOR, ROUGE-L). These results highlight the impact of semantically aligned language cues in enhancing agent learning efficiency and generalization. The proposed framework contributes to the advancement of multimodal reinforcement learning and embodied AI systems in dynamic, real-world environments.
<div id='section'>PaperID: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2504.02724.pdf' target='_blank'>https://arxiv.org/pdf/2504.02724.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sammy Christen, David MÃ¼ller, Agon Serifi, Ruben Grandia, Georg Wiedebach, Michael A. Hopkins, Espen Knoop, Moritz BÃ¤cher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02724">Autonomous Human-Robot Interaction via Operator Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperated robotic characters can perform expressive interactions with humans, relying on the operators' experience and social intuition. In this work, we propose to create autonomous interactive robots, by training a model to imitate operator data. Our model is trained on a dataset of human-robot interactions, where an expert operator is asked to vary the interactions and mood of the robot, while the operator commands as well as the pose of the human and robot are recorded. Our approach learns to predict continuous operator commands through a diffusion process and discrete commands through a classifier, all unified within a single transformer architecture. We evaluate the resulting model in simulation and with a user study on the real system. We show that our method enables simple autonomous human-robot interactions that are comparable to the expert-operator baseline, and that users can recognize the different robot moods as generated by our model. Finally, we demonstrate a zero-shot transfer of our model onto a different robotic platform with the same operator interface.
<div id='section'>PaperID: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2503.24110.pdf' target='_blank'>https://arxiv.org/pdf/2503.24110.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>FranÃ§ois Olivier, Zied Bouraoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24110">Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advances in embodied AI, agent reasoning systems still struggle to capture the fundamental conceptual structures that humans naturally use to understand and interact with their environment. To address this, we propose a novel framework that bridges embodied cognition theory and agent systems by leveraging a formal characterization of image schemas, which are defined as recurring patterns of sensorimotor experience that structure human cognition. By customizing LLMs to translate natural language descriptions into formal representations based on these sensorimotor patterns, we will be able to create a neurosymbolic system that grounds the agent's understanding in fundamental conceptual structures. We argue that such an approach enhances both efficiency and interpretability while enabling more intuitive human-agent interactions through shared embodied understanding.
<div id='section'>PaperID: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2503.19941.pdf' target='_blank'>https://arxiv.org/pdf/2503.19941.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Sun, Pengfei Tian, Xiaozhu Hu, Xiaoyu Zhao, Huiying Li, Zhenliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19941">Body Discovery of Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the pursuit of realizing artificial general intelligence (AGI), the importance of embodied artificial intelligence (AI) becomes increasingly apparent. Following this trend, research integrating robots with AGI has become prominent. As various kinds of embodiments have been designed, adaptability to diverse embodiments will become important to AGI. We introduce a new challenge, termed "Body Discovery of Embodied AI", focusing on tasks of recognizing embodiments and summarizing neural signal functionality. The challenge encompasses the precise definition of an AI body and the intricate task of identifying embodiments in dynamic environments, where conventional approaches often prove inadequate. To address these challenges, we apply causal inference method and evaluate it by developing a simulator tailored for testing algorithms with virtual environments. Finally, we validate the efficacy of our algorithms through empirical testing, demonstrating their robust performance in various scenarios based on virtual environments.
<div id='section'>PaperID: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2503.13250.pdf' target='_blank'>https://arxiv.org/pdf/2503.13250.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zejia Zhang, Bo Yang, Xinxing Chen, Weizhuang Shi, Haoyuan Wang, Wei Luo, Jian Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13250">MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System for Implicit Intention Recognition and Task Execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A promising effective human-robot interaction in assistive robotic systems is gaze-based control. However, current gaze-based assistive systems mainly help users with basic grasping actions, offering limited support. Moreover, the restricted intent recognition capability constrains the assistive system's ability to provide diverse assistance functions. In this paper, we propose an open implicit intention recognition framework powered by Large Language Model (LLM) and Vision Foundation Model (VFM), which can process gaze input and recognize user intents that are not confined to predefined or specific scenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot system (MindEye-OmniAssist) that recognizes user's intentions through gaze and assists in completing task. To achieve this, the system utilizes open vocabulary object detector, intention recognition network and LLM to infer their full intentions. By integrating eye movement feedback and LLM, it generates action sequences to assist the user in completing tasks. Real-world experiments have been conducted for assistive tasks, and the system achieved an overall success rate of 41/55 across various undefined tasks. Preliminary results show that the proposed method holds the potential to provide a more user-friendly human-computer interaction interface and significantly enhance the versatility and effectiveness of assistive systems by supporting more complex and diverse task.
<div id='section'>PaperID: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2503.04414.pdf' target='_blank'>https://arxiv.org/pdf/2503.04414.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Davide Tebaldi, Dario Onfiani, Luigi Biagiotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04414">On the Analysis of Stability, Sensitivity and Transparency in Variable Admittance Control for pHRI Enhanced by Virtual Fixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interest in Physical Human-Robot Interaction (pHRI) has significantly increased over the last two decades thanks to the availability of collaborative robots that guarantee user safety during force exchanges. For this reason, stability concerns have been addressed extensively in the literature while proposing new control schemes for pHRI applications. Because of the nonlinear nature of robots, stability analyses generally leverage passivity concepts. On the other hand, the proposed algorithms generally consider ideal models of robot manipulators. For this reason, the primary objective of this paper is to conduct a detailed analysis of the sources of instability for a class of pHRI control schemes, namely proxy-based constrained admittance controllers, by considering parasitic effects such as transmission elasticity, motor velocity saturation, and actuation delay. Next, a sensitivity analysis supported by experimental results is carried out, in order to identify how the control parameters affect the stability of the overall system. Finally, an adaptation technique for the proxy parameters is proposed with the goal of maximizing transparency in pHRI. The proposed adaptation method is validated through both simulations and experimental tests.
<div id='section'>PaperID: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2502.11777.pdf' target='_blank'>https://arxiv.org/pdf/2502.11777.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siddiqui Muhammad Yasir, Hyunsik Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11777">Deep Neural Networks for Accurate Depth Estimation with Latent Space Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth estimation plays a pivotal role in advancing human-robot interactions, especially in indoor environments where accurate 3D scene reconstruction is essential for tasks like navigation and object handling. Monocular depth estimation, which relies on a single RGB camera, offers a more affordable solution compared to traditional methods that use stereo cameras or LiDAR. However, despite recent progress, many monocular approaches struggle with accurately defining depth boundaries, leading to less precise reconstructions. In response to these challenges, this study introduces a novel depth estimation framework that leverages latent space features within a deep convolutional neural network to enhance the precision of monocular depth maps. The proposed model features dual encoder-decoder architecture, enabling both color-to-depth and depth-to-depth transformations. This structure allows for refined depth estimation through latent space encoding. To further improve the accuracy of depth boundaries and local features, a new loss function is introduced. This function combines latent loss with gradient loss, helping the model maintain the integrity of depth boundaries. The framework is thoroughly tested using the NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in complex indoor scenarios. The results clearly show that this approach effectively reduces depth ambiguities and blurring, making it a promising solution for applications in human-robot interaction and 3D scene reconstruction.
<div id='section'>PaperID: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2502.10678.pdf' target='_blank'>https://arxiv.org/pdf/2502.10678.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yate Ge, Meiying Li, Xipeng Huang, Yuanda Hu, Qi Wang, Xiaohua Sun, Weiwei Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10678">GenComUI: Exploring Generative Visual Aids as Medium to Support Task-Oriented Human-Robot Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the integration of generative visual aids in human-robot task communication. We developed GenComUI, a system powered by large language models that dynamically generates contextual visual aids (such as map annotations, path indicators, and animations) to support verbal task communication and facilitate the generation of customized task programs for the robot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communication in spatial tasks. To evaluate its effectiveness, we conducted a user experiment (n = 20) comparing GenComUI with a voice-only baseline. The results demonstrate that generative visual aids, through both qualitative and quantitative analysis, enhance verbal task communication by providing continuous visual feedback, thus promoting natural and effective human-robot communication. Additionally, the study offers a set of design implications, emphasizing how dynamically generated visual aids can serve as an effective communication medium in human-robot interaction. These findings underscore the potential of generative visual aids to inform the design of more intuitive and effective human-robot communication, particularly for complex communication scenarios in human-robot interaction and LLM-based end-user development.
<div id='section'>PaperID: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2502.04012.pdf' target='_blank'>https://arxiv.org/pdf/2502.04012.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Angus B. Clark, Xinran Wang, Alex Ranne, Nicolas Rojas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04012">Malleable Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This chapter is about the fundamentals of fabrication, control, and human-robot interaction of a new type of collaborative robotic manipulators, called malleable robots, which are based on adjustable architectures of varying stiffness for achieving high dexterity with lower mobility arms. Collaborative robots, or cobots, commonly integrate six or more degrees of freedom (DOF) in a serial arm in order to allow positioning in constrained spaces and adaptability across tasks. Increasing the dexterity of robotic arms has been indeed traditionally accomplished by increasing the number of degrees of freedom of the system; however, once a robotic task has been established (e.g., a pick-and-place operation), the motion of the end-effector can be normally achieved using less than 6-DOF (i.e., lower mobility). The aim of malleable robots is to close the technological gap that separates current cobots from achieving flexible, accessible manufacturing automation with a reduced number of actuators.
<div id='section'>PaperID: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2501.07888.pdf' target='_blank'>https://arxiv.org/pdf/2501.07888.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07888">Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8% over GPT-4o and 5.8% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6% performance advantage over GPT-4o and +24.9% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model.
<div id='section'>PaperID: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2412.04700.pdf' target='_blank'>https://arxiv.org/pdf/2412.04700.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Yu, Zebin Huang, Yutong Li, Xinliang Guo, Vincent Crocher, Ignacio Carlucho, Mustafa Suphi Erden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04700">SpasticMyoElbow: Physical Human-Robot Interaction Simulation Framework for Modelling Elbow Spasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic devices hold great potential for efficient and reliable assessment of neuromotor abnormalities in post-stroke patients. However, spasticity caused by stroke is still assessed manually in clinical settings. The limited and variable nature of data collected from patients has long posed a major barrier to quantitatively modelling spasticity with robotic measurements and fully validating robotic assessment techniques. This paper presents a simulation framework developed to support the design and validation of elbow spasticity models and mitigate data problems. The framework consists of a simulation environment of robot-assisted spasticity assessment, two motion controllers for the robot and human models, and a stretch reflex controller. Our framework allows simulation based on synthetic data without experimental data from human subjects. Using this framework, we replicated the constant-velocity stretch experiment typically used in robot-assisted spasticity assessment and evaluated four types of spasticity models. Our results show that a spasticity reflex model incorporating feedback on both muscle fibre velocity and length more accurately captures joint resistance characteristics during passive elbow stretching in spastic patients than a force-dependent model. When integrated with an appropriate spasticity model, this simulation framework has the potential to generate extensive datasets of virtual patients for future research on spasticity assessment.
<div id='section'>PaperID: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2411.17040.pdf' target='_blank'>https://arxiv.org/pdf/2411.17040.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Songtao Li, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17040">Multimodal Alignment and Fusion: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives -- data-level, feature-level, and output-level fusion -- and methodological paradigms -- including statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as cross-modal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.
<div id='section'>PaperID: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2410.13755.pdf' target='_blank'>https://arxiv.org/pdf/2410.13755.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoxiao Cheng, Jonathan Eden, Bastien Berret, Atsushi Takagi, Etienne Burdet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13755">Interacting humans and robots can improve sensory prediction by adapting their viscoelasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To manipulate objects or dance together, humans and robots exchange energy and haptic information. While the exchange of energy in human-robot interaction has been extensively investigated, the underlying exchange of haptic information is not well understood. Here, we develop a computational model of the mechanical and sensory interactions between agents that can tune their viscoelasticity while considering their sensory and motor noise. The resulting stochastic-optimal-information-and-effort (SOIE) controller predicts how the exchange of haptic information and the performance can be improved by adjusting viscoelasticity. This controller was first implemented on a robot-robot experiment with a tracking task which showed its superior performance when compared to either stiff or compliant control. Importantly, the optimal controller also predicts how connected humans alter their muscle activation to improve haptic communication, with differentiated viscoelasticity adjustment to their own sensing noise and haptic perturbations. A human-robot experiment then illustrated the applicability of this optimal control strategy for robots, yielding improved tracking performance and effective haptic communication as the robot adjusted its viscoelasticity according to its own and the user's noise characteristics. The proposed SOIE controller may thus be used to improve haptic communication and collaboration of humans and robots.
<div id='section'>PaperID: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2410.11377.pdf' target='_blank'>https://arxiv.org/pdf/2410.11377.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Theresa Pekarek Rosin, Vanessa Hassouna, Xiaowen Sun, Luca Krohm, Henri-Leon Kordt, Michael Beetz, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11377">A Framework for Adapting Human-Robot Interaction to Diverse User Groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate natural and intuitive interactions with diverse user groups in real-world settings, social robots must be capable of addressing the varying requirements and expectations of these groups while adapting their behavior based on user feedback. While previous research often focuses on specific demographics, we present a novel framework for adaptive Human-Robot Interaction (HRI) that tailors interactions to different user groups and enables individual users to modulate interactions through both minor and major interruptions. Our primary contributions include the development of an adaptive, ROS-based HRI framework with an open-source code base. This framework supports natural interactions through advanced speech recognition and voice activity detection, and leverages a large language model (LLM) as a dialogue bridge. We validate the efficiency of our framework through module tests and system trials, demonstrating its high accuracy in age recognition and its robustness to repeated user inputs and plan changes.
<div id='section'>PaperID: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2410.06472.pdf' target='_blank'>https://arxiv.org/pdf/2410.06472.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rob Royce, Marcel Kaufmann, Jonathan Becktor, Sangwoo Moon, Kalind Carpenter, Kai Pak, Amanda Towler, Rohan Thakker, Shehryar Khattak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06472">Enabling Novel Mission Operations and Interactions with ROSA: The Robot Operating System Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of robotic systems has revolutionized numerous industries, yet their operation often demands specialized technical knowledge, limiting accessibility for non-expert users. This paper introduces ROSA (Robot Operating System Agent), an AI-powered agent that bridges the gap between the Robot Operating System (ROS) and natural language interfaces. By leveraging state-of-the-art language models and integrating open-source frameworks, ROSA enables operators to interact with robots using natural language, translating commands into actions and interfacing with ROS through well-defined tools. ROSA's design is modular and extensible, offering seamless integration with both ROS1 and ROS2, along with safety mechanisms like parameter validation and constraint enforcement to ensure secure, reliable operations. While ROSA is originally designed for ROS, it can be extended to work with other robotics middle-wares to maximize compatibility across missions. ROSA enhances human-robot interaction by democratizing access to complex robotic systems, empowering users of all expertise levels with multi-modal capabilities such as speech integration and visual perception. Ethical considerations are thoroughly addressed, guided by foundational principles like Asimov's Three Laws of Robotics, ensuring that AI integration promotes safety, transparency, privacy, and accountability. By making robotic technology more user-friendly and accessible, ROSA not only improves operational efficiency but also sets a new standard for responsible AI use in robotics and potentially future mission operations. This paper introduces ROSA's architecture and showcases initial mock-up operations in JPL's Mars Yard, a laboratory, and a simulation using three different robots. The core ROSA library is available as open-source.
<div id='section'>PaperID: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2409.15922.pdf' target='_blank'>https://arxiv.org/pdf/2409.15922.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sukai Huang, Shu-Wei Liu, Nir Lipovetzky, Trevor Cohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15922">The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents
<div id='section'>PaperID: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2409.06124.pdf' target='_blank'>https://arxiv.org/pdf/2409.06124.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoxiao Cheng, Shixian Shen, Ekaterina Ivanova, Gerolamo Carboni, Atsushi Takagi, Etienne Burdet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06124">Human Impedance Modulation to Improve Visuo-Haptic Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans activate muscles to shape the mechanical interaction with their environment, but can they harness this control mechanism to best sense the environment? We investigated how participants adapt their muscle activation to visual and haptic information when tracking a randomly moving target with a robotic interface. The results exhibit a differentiated effect of these sensory modalities, where participants' muscle cocontraction increases with the haptic noise and decreases with the visual noise, in apparent contradiction to previous results. These results can be explained, and reconciled with previous findings, when considering muscle spring like mechanics, where stiffness increases with cocontraction to regulate motion guidance. Increasing cocontraction to more closely follow the motion plan favors accurate visual over haptic information, while decreasing it avoids injecting visual noise and relies on accurate haptic information. We formulated this active sensing mechanism as the optimization of visuo-haptic information and effort. This OIE model can explain the adaptation of muscle activity to unimodal and multimodal sensory information when interacting with fixed or dynamic environments, or with another human, and can be used to optimize human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2409.03457.pdf' target='_blank'>https://arxiv.org/pdf/2409.03457.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Changfei Fu, Weinan Chen, Wenjun Xu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03457">FLAF: Focal Line and Feature-constrained Active View Planning for Visual Teach and Repeat</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents FLAF, a focal line and feature-constrained active view planning method for tracking failure avoidance in feature-based visual navigation of mobile robots. Our FLAF-based visual navigation is built upon a feature-based visual teach and repeat (VT\&R) framework, which supports many robotic applications by teaching a robot to navigate on various paths that cover a significant portion of daily autonomous navigation requirements. However, tracking failure in feature-based visual simultaneous localization and mapping (VSLAM) caused by textureless regions in human-made environments is still limiting VT\&R to be adopted in the real world. To address this problem, the proposed view planner is integrated into a feature-based visual SLAM system to build up an active VT\&R system that avoids tracking failure. In our system, a pan-tilt unit (PTU)-based active camera is mounted on the mobile robot. Using FLAF, the active camera-based VSLAM operates during the teaching phase to construct a complete path map and in the repeat phase to maintain stable localization. FLAF orients the robot toward more map points to avoid mapping failures during path learning and toward more feature-identifiable map points beneficial for localization while following the learned trajectory. Experiments in real scenarios demonstrate that FLAF outperforms the methods that do not consider feature-identifiability, and our active VT\&R system performs well in complex environments by effectively dealing with low-texture regions.
<div id='section'>PaperID: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2602.08421.pdf' target='_blank'>https://arxiv.org/pdf/2602.08421.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Farhad Keramat, Salma Salimi, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.08421">Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.
<div id='section'>PaperID: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2602.02745.pdf' target='_blank'>https://arxiv.org/pdf/2602.02745.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minyi Wang, Christoph Bartneck, Michael-John Turp, David Kaber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.02745">Ethical Asymmetry in Human-Robot Interaction - An Empirical Test of Sparrow's Hypothesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ethics of human-robot interaction (HRI) have been discussed extensively based on three traditional frameworks: deontology, consequentialism, and virtue ethics. We conducted a mixed within/between experiment to investigate Sparrow's proposed ethical asymmetry hypothesis in human treatment of robots. The moral permissibility of action (MPA) was manipulated as a subject grouping variable, and virtue type (prudence, justice, courage, and temperance) was controlled as a within-subjects factor. We tested moral stimuli using an online questionnaire with Perceived Moral Permissibility of Action (PMPA) and Perceived Virtue Scores (PVS) as response measures. The PVS measure was based on an adaptation of the established Questionnaire on Cardinal Virtues (QCV), while the PMPA was based on Malle et al. [39] work. We found that the MPA significantly influenced the PMPA and perceived virtue scores. The best-fitting model to describe the relationship between PMPA and PVS was cubic, which is symmetrical in nature. Our study did not confirm Sparrow's asymmetry hypothesis. The adaptation of the QCV is expected to have utility for future studies, pending additional psychometric property assessments.
<div id='section'>PaperID: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2601.08868.pdf' target='_blank'>https://arxiv.org/pdf/2601.08868.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Wang, Yinfeng Yu, Bin Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08868">Residual Cross-Modal Fusion Networks for Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.
<div id='section'>PaperID: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2601.07855.pdf' target='_blank'>https://arxiv.org/pdf/2601.07855.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Subeen Lee, Siyeong Lee, Namil Kim, Jaesik Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07855">An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.
<div id='section'>PaperID: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2512.20014.pdf' target='_blank'>https://arxiv.org/pdf/2512.20014.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sangoh Lee, Sangwoo Mo, Wook-Shin Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20014">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.
<div id='section'>PaperID: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2512.18028.pdf' target='_blank'>https://arxiv.org/pdf/2512.18028.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18028">Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.
<div id='section'>PaperID: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2512.15940.pdf' target='_blank'>https://arxiv.org/pdf/2512.15940.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15940">R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.
<div id='section'>PaperID: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2512.11315.pdf' target='_blank'>https://arxiv.org/pdf/2512.11315.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranav Guruprasad, Sudipta Chowdhury, Harsh Sikka, Mridul Sharma, Helen Lu, Sean Rivera, Aryan Khurana, Hangliang Ren, Yangyue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11315">Benchmarking the Generality of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.
<div id='section'>PaperID: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2511.14161.pdf' target='_blank'>https://arxiv.org/pdf/2511.14161.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoquan Sun, Ruijian Zhang, Kang Pang, Bingchen Miao, Yuxiang Tan, Zhen Yang, Ming Li, Jiayu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14161">RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.
<div id='section'>PaperID: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2511.05683.pdf' target='_blank'>https://arxiv.org/pdf/2511.05683.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Eric Godden, Jacquie Groenewegen, Michael Wheeler, Matthew K. X. J. Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05683">Exploring Immersive Social-Physical Interaction with Virtual Characters through Coordinated Robotic Encountered-Type Contact</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents novel robot-mediated immersive experiences enabled by an encountered-type haptic display (ETHD) that introduces direct physical contact in virtual environments. We focus on social-physical interactions, a class of interaction associated with meaningful human outcomes in prior human-robot interaction (HRI) research. We explore the implementation of this interaction paradigm in immersive virtual environments through an object handover, fist bump, and high five with a virtual character. Extending this HRI paradigm into immersive environments enables the study of how physically grounded robotic contact and virtual augmentation jointly shape these novel social-physical interaction experiences. To support this investigation, we introduce ETHOS (Encountered-Type Haptics for On-demand Social interaction), an experimental platform integrating a torque-controlled manipulator and interchangeable props with a headset-mediated virtual experience. ETHOS enables co-located physical interaction through marker-based physical-virtual registration while concealing the robot behind the virtual environment, decoupling contact from visible robot embodiment. Both technical characterization, through spatial alignment and interaction latency tests, and experiential evaluation, through a 55 participant user study, were completed. Overall, the findings demonstrate the feasibility and experiential value of robot-mediated social-physical interaction in VR and motivate further development of dynamic encountered-type approaches for immersive HRI.
<div id='section'>PaperID: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2511.04994.pdf' target='_blank'>https://arxiv.org/pdf/2511.04994.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xingyuan Zhou, Peter Paik, S. Farokh Atashzar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04994">Encoding Biomechanical Energy Margin into Passivity-based Synchronization for Networked Telerobotic Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining system stability and accurate position tracking is imperative in networked robotic systems, particularly for haptics-enabled human-robot interaction. Recent literature has integrated human biomechanics into the stabilizers implemented for teleoperation, enhancing force preservation while guaranteeing convergence and safety. However, position desynchronization due to imperfect communication and non-passive behaviors remains a challenge. This paper proposes a two-port biomechanics-aware passivity-based synchronizer and stabilizer, referred to as TBPS2. This stabilizer optimizes position synchronization by leveraging human biomechanics while reducing the stabilizer's conservatism in its activation. We provide the mathematical design synthesis of the stabilizer and the proof of stability. We also conducted a series of grid simulations and systematic experiments, comparing their performance with that of state-of-the-art solutions under varying time delays and environmental conditions.
<div id='section'>PaperID: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2511.02225.pdf' target='_blank'>https://arxiv.org/pdf/2511.02225.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fan Feng, Phillip Lippe, Sara Magliacane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02225">Learning Interactive World Model for Object-Centric Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agents that understand objects and their interactions can learn policies that are more robust and transferable. However, most object-centric RL methods factor state by individual objects while leaving interactions implicit. We introduce the Factored Interactive Object-Centric World Model (FIOC-WM), a unified framework that learns structured representations of both objects and their interactions within a world model. FIOC-WM captures environment dynamics with disentangled and modular representations of object interactions, improving sample efficiency and generalization for policy learning. Concretely, FIOC-WM first learns object-centric latents and an interaction structure directly from pixels, leveraging pre-trained vision encoders. The learned world model then decomposes tasks into composable interaction primitives, and a hierarchical policy is trained on top: a high level selects the type and order of interactions, while a low level executes them. On simulated robotic and embodied-AI benchmarks, FIOC-WM improves policy-learning sample efficiency and generalization over world-model baselines, indicating that explicit, modular interaction learning is crucial for robust control.
<div id='section'>PaperID: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2511.01224.pdf' target='_blank'>https://arxiv.org/pdf/2511.01224.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chengmeng Li, Yaxin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01224">Embodiment Transfer Learning for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.
<div id='section'>PaperID: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2510.26588.pdf' target='_blank'>https://arxiv.org/pdf/2510.26588.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gang Li, Chunlei Zhai, Teng Wang, Shaun Li, Shangsong Jiang, Xiangwei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26588">FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation algorithms for quadrotors often exhibit a large variation in performance when transferred across different vehicle platforms and scene geometries, which increases the cost and risk of field deployment. To support systematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity, configurable benchmarking framework that measures how platform kinodynamics and scenario structure jointly affect navigation robustness. FLYINGTRUST models vehicle capability with two compact, physically interpretable indicators: maximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The benchmark pairs a diverse scenario library with a heterogeneous set of real and virtual platforms and prescribes a standardized evaluation protocol together with a composite scoring method that balances scenario importance, platform importance and performance stability. We use FLYINGTRUST to compare representative optimization-based and learning-based navigation approaches under identical conditions, performing repeated trials per platform-scenario combination and reporting uncertainty-aware metrics. The results reveal systematic patterns: navigation success depends predictably on platform capability and scene geometry, and different algorithms exhibit distinct preferences and failure modes across the evaluated conditions. These observations highlight the practical necessity of incorporating both platform capability and scenario structure into algorithm design, evaluation, and selection, and they motivate future work on methods that remain robust across diverse platforms and scenarios.
<div id='section'>PaperID: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2510.16435.pdf' target='_blank'>https://arxiv.org/pdf/2510.16435.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lennart Wachowiak, Andrew Coles, Gerard Canal, Oya Celiktutan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16435">What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the growing use of large language models and conversational interfaces in human-robot interaction, robots' ability to answer user questions is more important than ever. We therefore introduce a dataset of 1,893 user questions for household robots, collected from 100 participants and organized into 12 categories and 70 subcategories. Most work in explainable robotics focuses on why-questions. In contrast, our dataset provides a wide variety of questions, from questions about simple execution details to questions about how the robot would act in hypothetical scenarios -- thus giving roboticists valuable insights into what questions their robot needs to be able to answer. To collect the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots performing varied household tasks. We then asked participants on Prolific what questions they would want to ask the robot in each portrayed situation. In the final dataset, the most frequent categories are questions about task execution details (22.5%), the robot's capabilities (12.7%), and performance assessments (11.3%). Although questions about how robots would handle potentially difficult scenarios and ensure correct behavior are less frequent, users rank them as the most important for robots to be able to answer. Moreover, we find that users who identify as novices in robotics ask different questions than more experienced users. Novices are more likely to inquire about simple facts, such as what the robot did or the current state of the environment. As robots enter environments shared with humans and language becomes central to giving instructions and interaction, this dataset provides a valuable foundation for (i) identifying the information robots need to log and expose to conversational interfaces, (ii) benchmarking question-answering modules, and (iii) designing explanation strategies that align with user expectations.
<div id='section'>PaperID: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2510.12749.pdf' target='_blank'>https://arxiv.org/pdf/2510.12749.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12749">SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scene perception, understanding, and simulation are fundamental techniques for embodied-AI agents, while existing solutions are still prone to segmentation deficiency, dynamic objects' interference, sensor data sparsity, and view-limitation problems. This paper proposes a novel framework, named SPORTS, for holistic scene understanding via tightly integrating Video Panoptic Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into an iterative and unified perspective. Firstly, VPS designs an adaptive attention-based geometric fusion mechanism to align cross-frame features via enrolling the pose, depth, and optical flow modality, which automatically adjust feature maps for different decoding stages. And a post-matching strategy is integrated to improve identities tracking. In VO, panoptic segmentation results from VPS are combined with the optical flow map to improve the confidence estimation of dynamic objects, which enhances the accuracy of the camera pose estimation and completeness of the depth map generation via the learning-based paradigm. Furthermore, the point-based rendering of SR is beneficial from VO, transforming sparse point clouds into neural fields to synthesize high-fidelity RGB views and twin panoptic views. Extensive experiments on three public datasets demonstrate that our attention-based feature fusion outperforms most existing state-of-the-art methods on the odometry, tracking, segmentation, and novel view synthesis tasks.
<div id='section'>PaperID: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2510.09483.pdf' target='_blank'>https://arxiv.org/pdf/2510.09483.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lars Ohnemus, Nils Hantke, Max Weißer, Kai Furmans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09483">FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Scene Graphs (DSGs) provide a structured representation of hierarchical, interconnected environments, but current approaches struggle to capture stochastic dynamics, partial observability, and multi-agent activity. These aspects are critical for embodied AI, where agents must act under uncertainty and delayed perception. We introduce FOGMACHINE , an open-source framework that fuses DSGs with discrete-event simulation to model object dynamics, agent observations, and interactions at scale. This setup enables the study of uncertainty propagation, planning under limited perception, and emergent multi-agent behavior. Experiments in urban scenarios illustrate realistic temporal and spatial patterns while revealing the challenges of belief estimation under sparse observations. By combining structured representations with efficient simulation, FOGMACHINE establishes an effective tool for benchmarking, model training, and advancing embodied AI in complex, uncertain environments.
<div id='section'>PaperID: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2510.06457.pdf' target='_blank'>https://arxiv.org/pdf/2510.06457.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lifei Wang, Natalie Friedman, Chengchao Zhu, Zeshu Zhu, S. Joy Mountford
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06457">Evaluating Node-tree Interfaces for AI Explainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) become ubiquitous in workplace tools and decision-making processes, ensuring explainability and fostering user trust are critical. Although advancements in LLM engineering continue, human-centered design is still catching up, particularly when it comes to embedding transparency and trust into AI interfaces. This study evaluates user experiences with two distinct AI interfaces - node-tree interfaces and chatbot interfaces - to assess their performance in exploratory, follow-up inquiry, decision-making, and problem-solving tasks. Our design-driven approach introduces a node-tree interface that visually structures AI-generated responses into hierarchically organized, interactive nodes, allowing users to navigate, refine, and follow up on complex information. In a comparative study with n=20 business users, we observed that while the chatbot interface effectively supports linear, step-by-step queries, it is the node-tree interface that enhances brainstorming. Quantitative and qualitative findings indicate that node-tree interfaces not only improve task performance and decision-making support but also promote higher levels of user trust by preserving context. Our findings suggest that adaptive AI interfaces capable of switching between structured visualizations and conversational formats based on task requirements can significantly enhance transparency and user confidence in AI-powered systems. This work contributes actionable insights to the fields of human-robot interaction and AI design, particularly for enterprise applications where trust-building is critical for teams.
<div id='section'>PaperID: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2510.01389.pdf' target='_blank'>https://arxiv.org/pdf/2510.01389.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ulas Berk Karli, Ziyao Shangguan, Tesca FItzgerald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01389">INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack introspective mechanisms for anticipating failures and requesting help from a human supervisor. We present \textbf{INSIGHT}, a learning framework for leveraging token-level uncertainty signals to predict when a VLA should request help. Using $π_0$-FAST as the underlying model, we extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based estimates of \emph{aleatoric and epistemic uncertainty}, and train compact transformer classifiers to map these sequences to help triggers. We explore supervision regimes for strong or weak supervision, and extensively compare them across in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though noisier, still support competitive introspection when training and evaluation are aligned, offering a scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal evolution of token-level uncertainty signals with transformers provides far greater predictive power than static sequence-level scores. This study provides the first systematic evaluation of uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time error mitigation through selective human intervention.
<div id='section'>PaperID: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2510.01192.pdf' target='_blank'>https://arxiv.org/pdf/2510.01192.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Isabel Pedersen, Andrea Slane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01192">Better Than "Better Than Nothing": Design Strategies for Enculturated Empathetic AI Robot Companions for Older Adults</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper asserts that emulating empathy in human-robot interaction is a key component to achieve satisfying social, trustworthy, and ethical robot interaction with older people. Following comments from older adult study participants, the paper identifies a gap. Despite the acceptance of robot care scenarios, participants expressed the poor quality of the social aspect. Current human-robot designs, to a certain extent, neglect to include empathy as a theorized design pathway. Using rhetorical theory, this paper defines the socio-cultural expectations for convincing empathetic relationships. It analyzes and then summarizes how society understands, values, and negotiates empathic interaction between human companions in discursive exchanges, wherein empathy acts as a societal value system. Using two public research collections on robots, with one geared specifically to gerontechnology for older people, it substantiates the lack of attention to empathy in public materials produced by robot companies. This paper contends that using an empathetic care vocabulary as a design pathway is a productive underlying foundation for designing humanoid social robots that aim to support older people's goals of aging-in-place. It argues that the integration of affective AI into the sociotechnical assemblages of human-socially assistive robot interaction ought to be scrutinized to ensure it is based on genuine cultural values involving empathetic qualities.
<div id='section'>PaperID: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2509.24559.pdf' target='_blank'>https://arxiv.org/pdf/2509.24559.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marco Molinari, Leonardo Nevali, Saharsha Navani, Omar G. Younis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24559">Emergent World Representations in OpenVLA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action models (VLAs) trained with policy-based reinforcement learning (RL) encode complex behaviors without explicitly modeling environmental dynamics. However, it remains unclear whether VLAs implicitly learn world models, a hallmark of model-based RL. We propose an experimental methodology using embedding arithmetic on state representations to probe whether OpenVLA, the current state of the art in VLAs, contains latent knowledge of state transitions. Specifically, we measure the difference between embeddings of sequential environment states and test whether this transition vector is recoverable from intermediate model activations. Using linear and non linear probes trained on activations across layers, we find statistically significant predictive ability on state transitions exceeding baselines (embeddings), indicating that OpenVLA encodes an internal world model (as opposed to the probes learning the state transitions). We investigate the predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that the world model emerges as training progresses. Finally, we outline a pipeline leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.
<div id='section'>PaperID: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2509.21930.pdf' target='_blank'>https://arxiv.org/pdf/2509.21930.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahui Wang, Changhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21930">DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios. To address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost. Extensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.
<div id='section'>PaperID: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2509.19521.pdf' target='_blank'>https://arxiv.org/pdf/2509.19521.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Najeeb Ahmed Bhuiyan, M. Nasimul Huq, Sakib H. Chowdhury, Rahul Mangharam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19521">A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gesture-based control for mobile manipulators faces persistent challenges in reliability, efficiency, and intuitiveness. This paper presents a dual-hand gesture interface that integrates TinyML, spectral analysis, and sensor fusion within a ROS framework to address these limitations. The system uses left-hand tilt and finger flexion, captured using accelerometer and flex sensors, for mobile base navigation, while right-hand IMU signals are processed through spectral analysis and classified by a lightweight neural network. This pipeline enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3 manipulator. By supporting simultaneous navigation and manipulation, the framework improves efficiency and coordination compared to sequential methods. Key contributions include a bimanual control architecture, real-time low-power gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based implementation. The proposed approach advances Human-Robot Interaction (HRI) for industrial automation, assistive robotics, and hazardous environments, offering a cost-effective, open-source solution with strong potential for real-world deployment and further optimization.
<div id='section'>PaperID: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2509.16032.pdf' target='_blank'>https://arxiv.org/pdf/2509.16032.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michael Faber, Andrey Grishko, Julian Waksberg, David Pardo, Tomer Leivy, Yuval Hazan, Emanuel Talmansky, Benny Megidish, Hadas Erel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16032">A Matter of Height: The Impact of a Robotic Object on Human Compliance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots come in various forms and have different characteristics that may shape the interaction with them. In human-human interactions, height is a characteristic that shapes human dynamics, with taller people typically perceived as more persuasive. In this work, we aspired to evaluate if the same impact replicates in a human-robot interaction and specifically with a highly non-humanoid robotic object. The robot was designed with modules that could be easily added or removed, allowing us to change its height without altering other design features. To test the impact of the robot's height, we evaluated participants' compliance with its request to volunteer to perform a tedious task. In the experiment, participants performed a cognitive task on a computer, which was framed as the main experiment. When done, they were informed that the experiment was completed. While waiting to receive their credits, the robotic object, designed as a mobile robotic service table, entered the room, carrying a tablet that invited participants to complete a 300-question questionnaire voluntarily. We compared participants' compliance in two conditions: A Short robot composed of two modules and 95cm in height and a Tall robot consisting of three modules and 132cm in height. Our findings revealed higher compliance with the Short robot's request, demonstrating an opposite pattern to human dynamics. We conclude that while height has a substantial social impact on human-robot interactions, it follows a unique pattern of influence. Our findings suggest that designers cannot simply adopt and implement elements from human social dynamics to robots without testing them first.
<div id='section'>PaperID: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2509.13948.pdf' target='_blank'>https://arxiv.org/pdf/2509.13948.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Benedict Barrow, Roger K. Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13948">The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trust and the perception of trustworthiness play an important role in decision-making and our behaviour towards others, and this is true not only of human-human interactions but also of human-robot interactions. While significant advances have been made in recent years in the field of social robotics, there is still some way to go before we fully understand the factors that influence human trust in robots. This paper presents the results of a study into the first impressions created by a social robot's facial features, based on the hypothesis that a `babyface' engenders trust. By manipulating the back-projected face of a Furhat robot, the study confirms that eye shape and size have a significant impact on the perception of trustworthiness. The work thus contributes to an understanding of the design choices that need to be made when developing social robots so as to optimise the effectiveness of human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2509.13861.pdf' target='_blank'>https://arxiv.org/pdf/2509.13861.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>GÃ¶rkem KÄ±lÄ±nÃ§ Soylu, Neziha Akalin, Maria Riveiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13861">Using Petri Nets for Context-Adaptive Robot Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction, robots must communicate in a natural and transparent manner to foster trust, which requires adapting their communication to the context. In this paper, we propose using Petri nets (PNs) to model contextual information for adaptive robot explanations. PNs provide a formal, graphical method for representing concurrent actions, causal dependencies, and system states, making them suitable for analyzing dynamic interactions between humans and robots. We demonstrate this approach through a scenario involving a robot that provides explanations based on contextual cues such as user attention and presence. Model analysis confirms key properties, including deadlock-freeness, context-sensitive reachability, boundedness, and liveness, showing the robustness and flexibility of PNs for designing and verifying context-adaptive explanations in human-robot interactions.
<div id='section'>PaperID: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2509.11766.pdf' target='_blank'>https://arxiv.org/pdf/2509.11766.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, Zach Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11766">Igniting VLMs toward the Embodied Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While foundation models show remarkable progress in language and vision, existing vision-language models (VLMs) still have limited spatial and embodiment understanding. Transferring VLMs to embodied domains reveals fundamental mismatches between modalities, pretraining distributions, and training objectives, leaving action comprehension and generation as a central bottleneck on the path to AGI. We introduce WALL-OSS, an end-to-end embodied foundation model that leverages large-scale multimodal pretraining to achieve (1) embodiment-aware vision-language understanding, (2) strong language-action association, and (3) robust manipulation capability. Our approach employs a tightly coupled architecture and multi-strategies training curriculum that enables Unified Cross-Level CoT-seamlessly unifying instruction reasoning, subgoal decomposition, and fine-grained action synthesis within a single differentiable framework. Our results show that WALL-OSS attains high success on complex long-horizon manipulations, demonstrates strong instruction-following capabilities, complex understanding and reasoning, and outperforms strong baselines, thereby providing a reliable and scalable path from VLMs to embodied foundation models.
<div id='section'>PaperID: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2509.11663.pdf' target='_blank'>https://arxiv.org/pdf/2509.11663.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haisheng Wang, Weiming Zhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11663">ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper formulates the Embodied Questions Answering (EQsA) problem, introduces a corresponding benchmark, and proposes a system to tackle the problem. Classical Embodied Question Answering (EQA) is typically formulated as answering one single question by actively exploring a 3D environment. Real deployments, however, often demand handling multiple questions that may arrive asynchronously and carry different urgencies. We formalize this setting as Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group memory module shared among questions to reduce redundant exploration, and a priority-planning module to dynamically schedule questions. To evaluate this setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs) benchmark containing 40 indoor scenes and five questions per scene (200 in total), featuring asynchronous follow-up questions and urgency labels. We further propose metrics for EQsA performance: Direct Answer Rate (DAR), and Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency and responsiveness of this system. ParaEQsA consistently outperforms strong sequential baselines adapted from recent EQA systems, while reducing exploration and delay. Empirical evaluations investigate the relative contributions of priority, urgency modeling, spatial scope, reward estimation, and dependency reasoning within our framework. Together, these results demonstrate that urgency-aware, parallel scheduling is key to making embodied agents responsive and efficient under realistic, multi-question workloads.
<div id='section'>PaperID: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2509.10444.pdf' target='_blank'>https://arxiv.org/pdf/2509.10444.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chaerim Moon, Joohyung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10444">Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supernumerary Robotic Limbs (SRLs) can enhance human capability within close proximity. However, as a wearable device, the generated moment from its operation acts on the human body as an external torque. When the moments increase, more muscle units are activated for balancing, and it can result in reduced muscular null space. Therefore, this paper suggests a concept of a motion planning layer that reduces the generated moment for enhanced Human-Robot Interaction. It modifies given trajectories with desirable angular acceleration and position deviation limits. Its performance to reduce the moment is demonstrated through the simulation, which uses simplified human and robotic system models.
<div id='section'>PaperID: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2509.07942.pdf' target='_blank'>https://arxiv.org/pdf/2509.07942.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>James M. Berzuk, Lauren Corcoran, Brannen McKenzie-Lefurgey, Katie Szilagyi, James E. Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07942">Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contemporary robots are increasingly mimicking human social behaviours to facilitate interaction, such as smiling to signal approachability, or hesitating before taking an action to allow people time to react. Such techniques can activate a person's entrenched social instincts, triggering emotional responses as though they are interacting with a fellow human, and can prompt them to treat a robot as if it truly possesses the underlying life-like processes it outwardly presents, raising significant ethical questions. We engage these issues through the lens of informed consent: drawing upon prevailing legal principles and ethics, we examine how social robots can influence user behaviour in novel ways, and whether under those circumstances users can be appropriately informed to consent to these heightened interactions. We explore the complex circumstances of human-robot interaction and highlight how it differs from more familiar interaction contexts, and we apply legal principles relating to informed consent to social robots in order to reconceptualize the current ethical debates surrounding the field. From this investigation, we synthesize design goals for robot developers to achieve more ethical and informed human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2509.05263.pdf' target='_blank'>https://arxiv.org/pdf/2509.05263.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05263">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
<div id='section'>PaperID: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2509.01547.pdf' target='_blank'>https://arxiv.org/pdf/2509.01547.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01547">FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual SLAM has regained attention due to its ability to provide perceptual capabilities and simulation test data for Embodied AI. However, traditional SLAM methods struggle to meet the demands of high-quality scene reconstruction, and Gaussian SLAM systems, despite their rapid rendering and high-quality mapping capabilities, lack effective pose optimization methods and face challenges in geometric reconstruction. To address these issues, we introduce FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the scene representation to enhance geometric mapping performance. After initial pose estimation, we apply global adjustment to optimize camera poses and sparse point cloud, ensuring robust tracking of our approach. Additionally, we maintain a globally consistent opacity radiance field based on 3D Gaussians and introduce depth distortion and normal consistency terms to refine the scene representation. Furthermore, after constructing tetrahedral grids, we identify level sets to directly extract surfaces from 3D Gaussians. Results across various real-world and large-scale synthetic datasets demonstrate that our method achieves state-of-the-art tracking accuracy and mapping performance.
<div id='section'>PaperID: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2509.00328.pdf' target='_blank'>https://arxiv.org/pdf/2509.00328.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bear HÃ¤on, Kaylene Stocking, Ian Chuang, Claire Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00328">Mechanistic interpretability for steering vision-language-action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.
<div id='section'>PaperID: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2508.13421.pdf' target='_blank'>https://arxiv.org/pdf/2508.13421.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gabrielle Wehr, Reuben Rideaux, Amaya J. Fox, David R. Lightfoot, Jason Tangen, Jason B. Mattingley, Shane E. Ehrhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13421">Virtuous Machines: Towards Artificial General Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.
<div id='section'>PaperID: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2508.11988.pdf' target='_blank'>https://arxiv.org/pdf/2508.11988.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nicolas Mastropasqua, Ignacio Bugueno-Cordova, Rodrigo Verschae, Daniel Acevedo, Pablo Negri, Maria E. Buemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11988">Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-expression analysis has applications in domains such as Human-Robot Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast facial movements remains difficult when relying solely on RGB cameras, due to limitations in temporal resolution and sensitivity to motion blur. Event cameras offer an alternative, with microsecond-level precision, high dynamic range, and low latency. However, public datasets featuring event-based recordings of Action Units are still scarce. In this work, we introduce a novel, preliminary multi-resolution and multi-modal micro-expression dataset recorded with synchronized RGB and event cameras under variable lighting conditions. Two baseline tasks are evaluated to explore the spatial-temporal dynamics of micro-expressions: Action Unit classification using Spiking Neural Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame reconstruction using Conditional Variational Autoencoders, achieving SSIM = 0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising results show that event-based data can be used for micro-expression recognition and frame reconstruction.
<div id='section'>PaperID: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2508.08831.pdf' target='_blank'>https://arxiv.org/pdf/2508.08831.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bo-Hsun Chen, Nevindu M. Batagoda, Dan Negrut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08831">DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam.
<div id='section'>PaperID: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2508.05497.pdf' target='_blank'>https://arxiv.org/pdf/2508.05497.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Federico ScarÃ¬, Olger Siebinga, Arkady Zgonnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05497">Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicle's driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective.
<div id='section'>PaperID: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2508.05208.pdf' target='_blank'>https://arxiv.org/pdf/2508.05208.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Victor Ngo, Rachel, Ramchurn, Roma Patel, Alan Chamberlain, Ayse Kucukyilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05208">Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an evaluation of 18 children's in-the-wild experiences with the autonomous robot arm performer NED (Never-Ending Dancer) within the Thingamabobas installation, showcased across the UK. We detail NED's design, including costume, behaviour, and human interactions, all integral to the installation. Our observational analysis revealed three key challenges in child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of robot expressivity and reciprocity, and 3) Unmet expectations. Our findings show that children are naturally curious, and adept at interacting with a robotic art performer. However, our observations emphasise the critical need to optimise human-robot interaction (HRI) systems through careful consideration of audience's capabilities, perceptions, and expectations, within the performative arts context, to enable engaging and meaningful experiences, especially for young audiences.
<div id='section'>PaperID: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2508.03514.pdf' target='_blank'>https://arxiv.org/pdf/2508.03514.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pavlos Panagiotidis, Victor Zhi Heung Ngo, Sean Myatt, Roma Patel, Rachel Ramchurn, Alan Chamberlain, Ayse Kucukyilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03514">Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose theatre-in-the-loop, a framework for developing expressive robot behaviours tailored to artistic performance through a director-guided puppeteering workflow. Leveraging theatrical methods, we use narrative objectives to direct a puppeteer in generating improvised robotic gestures that convey specific emotions. These improvisations are captured and curated to build a dataset of reusable movement templates for standalone playback in future autonomous performances. Initial trials demonstrate the feasibility of this approach, illustrating how the workflow enables precise sculpting of robotic gestures into coherent emotional arcs while revealing challenges posed by the robot's mechanical constraints. We argue that this practice-led framework provides a model for interdisciplinary teams creating socially expressive robot behaviours, contributing to (1) theatre as an interactive training ground for human-robot interaction and (2) co-creation methodologies between humans and machines.
<div id='section'>PaperID: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2507.17383.pdf' target='_blank'>https://arxiv.org/pdf/2507.17383.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thomas P Zollo, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17383">Confidence Calibration in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.
<div id='section'>PaperID: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2506.10462.pdf' target='_blank'>https://arxiv.org/pdf/2506.10462.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ana MÃ¼ller, Sabina Jeschke, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10462">Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the impact of a group-adaptive conversation design in two socially interactive agents (SIAs) through two real-world studies. Both SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped with a conversational artificial intelligence (CAI) backend combining hybrid retrieval and generative models. The studies were carried out in an in-the-wild setting with a total of $N = 188$ participants who interacted with the SIAs - in dyads, triads or larger groups - at a German museum. Although the results did not reveal a significant effect of the group-sensitive conversation design on perceived satisfaction, the findings provide valuable insights into the challenges of adapting CAI for multi-party interactions and across different embodiments (robot vs.\ virtual agent), highlighting the need for multimodal strategies beyond linguistic pluralization. These insights contribute to the fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and broader Human-Machine Interaction (HMI), providing insights for future research on effective dialogue adaptation in group settings.
<div id='section'>PaperID: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2506.09172.pdf' target='_blank'>https://arxiv.org/pdf/2506.09172.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09172">An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent innovations in multimodal action models represent a promising direction for developing general-purpose agentic systems, combining visual understanding, language comprehension, and action generation. We introduce MultiNet - a novel, fully open-source benchmark and surrounding software ecosystem designed to rigorously evaluate and adapt models across vision, language, and action domains. We establish standardized evaluation protocols for assessing vision-language models (VLMs) and vision-language-action models (VLAs), and provide open source software to download relevant data, models, and evaluations. Additionally, we provide a composite dataset with over 1.3 trillion tokens of image captioning, visual question answering, commonsense reasoning, robotic control, digital game-play, simulated locomotion/manipulation, and many more tasks. The MultiNet benchmark, framework, toolkit, and evaluation harness have been used in downstream research on the limitations of VLA generalization.
<div id='section'>PaperID: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2505.16384.pdf' target='_blank'>https://arxiv.org/pdf/2505.16384.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoming Huang, Musen Zhang, Jianxin Yang, Zhen Li, Jinkai Li, Yao Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16384">MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze can provide rich information on human psychological activities, and has garnered significant attention in the field of Human-Robot Interaction (HRI). However, existing gaze estimation methods merely predict either the gaze direction or the Point-of-Gaze (PoG) on the screen, failing to provide sufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze analysis in 3D space. Moreover, the variations of eye shape and structure among individuals also impede the generalization capability of these methods. In this study, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an efficient calibration module, to predict the 6-DoF gaze information that is applicable for the real-word HRI. Our basic model encodes both the directional and positional features from facial images, and predicts gaze results with dedicated information flow and multiple decoders. To reduce the impact of individual variations, we propose a novel calibration module, namely Easy-Calibration, to fine-tune the basic model with subject-specific data, which is efficient to implement without the need of a screen. Experimental results demonstrate that our method achieves state-of-the-art performance on the public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets.
<div id='section'>PaperID: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2505.11476.pdf' target='_blank'>https://arxiv.org/pdf/2505.11476.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Runze Zuo, Dong Heon Han, Richard Li, Saima Jamal, Daniel Bruder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11476">UMArm: Untethered, Modular, Wearable, Soft Pneumatic Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic arms are essential to modern industries, however, their adaptability to unstructured environments remains limited. Soft robotic arms, particularly those actuated pneumatically, offer greater adaptability in unstructured environments and enhanced safety for human-robot interaction. However, current pneumatic soft arms are constrained by limited degrees of freedom, precision, payload capacity, and reliance on bulky external pressure regulators. In this work, a novel pneumatically driven rigid-soft hybrid arm, ``UMArm'', is presented. The shortcomings of pneumatically actuated soft arms are addressed by densely integrating high-force-to-weight-ratio, self-regulated McKibben actuators onto a lightweight rigid spine structure. The modified McKibben actuators incorporate valves and controllers directly inside, eliminating the need for individual pressure lines and external regulators, significantly reducing system weight and complexity. Full untethered operation, high payload capacity, precision, and directionally tunable compliance are achieved by the UMArm. Portability is demonstrated through a wearable assistive arm experiment, and versatility is showcased by reconfiguring the system into an inchworm robot. The results of this work show that the high-degree-of-freedom, external-regulator-free pneumatically driven arm systems like the UMArm possess great potential for real-world unstructured environments.
<div id='section'>PaperID: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2505.10359.pdf' target='_blank'>https://arxiv.org/pdf/2505.10359.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Le Shi, Yifei Shi, Xin Xu, Tenglong Liu, Junhua Xi, Chengyuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10359">NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep generative models demonstrate unprecedented zero-shot generalization capabilities, offering great potential for robot manipulation in unstructured environments. Given a partial observation of a scene, deep generative models could generate the unseen regions and therefore provide more context, which enhances the capability of robots to generalize across unseen environments. However, due to the visual artifacts in generated images and inefficient integration of multi-modal features in policy learning, this direction remains an open challenge. We introduce NVSPolicy, a generalizable language-conditioned policy learning method that couples an adaptive novel-view synthesis module with a hierarchical policy network. Given an input image, NVSPolicy dynamically selects an informative viewpoint and synthesizes an adaptive novel-view image to enrich the visual context. To mitigate the impact of the imperfect synthesized images, we adopt a cycle-consistent VAE mechanism that disentangles the visual features into the semantic feature and the remaining feature. The two features are then fed into the hierarchical policy network respectively: the semantic feature informs the high-level meta-skill selection, and the remaining feature guides low-level action estimation. Moreover, we propose several practical mechanisms to make the proposed method efficient. Extensive experiments on CALVIN demonstrate the state-of-the-art performance of our method. Specifically, it achieves an average success rate of 90.4\% across all tasks, greatly outperforming the recent methods. Ablation studies confirm the significance of our adaptive novel-view synthesis paradigm. In addition, we evaluate NVSPolicy on a real-world robotic platform to demonstrate its practical applicability.
<div id='section'>PaperID: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2505.09737.pdf' target='_blank'>https://arxiv.org/pdf/2505.09737.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Osher Elhadad, Reuth Mirsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09737">General Dynamic Goal Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding an agent's intent through its behavior is essential in human-robot interaction, interactive AI systems, and multi-agent collaborations. This task, known as Goal Recognition (GR), poses significant challenges in dynamic environments where goals are numerous and constantly evolving. Traditional GR methods, designed for a predefined set of goals, often struggle to adapt to these dynamic scenarios. To address this limitation, we introduce the General Dynamic GR problem - a broader definition of GR - aimed at enabling real-time GR systems and fostering further research in this area. Expanding on this foundation, this paper employs a model-free goal-conditioned RL approach to enable fast adaptation for GR across various changing tasks.
<div id='section'>PaperID: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2505.06278.pdf' target='_blank'>https://arxiv.org/pdf/2505.06278.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tongfei Bian, Mathieu Chollet, Tanaya Guha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06278">Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for social robots and agents to interact and assist humans is growing steadily. To be able to successfully interact with humans, they need to understand and analyse socially interactive scenes from their (robot's) perspective. Works that model social situations between humans and agents are few; and even those existing ones are often too computationally intensive to be suitable for deployment in real time or on real world scenarios with limited available information. We propose a robust knowledge distillation framework that models social interactions through various multimodal cues, yet is robust against incomplete and noisy information during inference. Our teacher model is trained with multimodal input (body, face and hand gestures, gaze, raw images) that transfers knowledge to a student model that relies solely on body pose. Extensive experiments on two publicly available human-robot interaction datasets demonstrate that the our student model achieves an average accuracy gain of 14.75\% over relevant baselines on multiple downstream social understanding task even with up to 51\% of its input being corrupted. The student model is highly efficient: it is $<1$\% in size of the teacher model in terms of parameters and uses $\sim 0.5$\textperthousand~FLOPs of that in the teacher model. Our code will be made public during publication.
<div id='section'>PaperID: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2505.05540.pdf' target='_blank'>https://arxiv.org/pdf/2505.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Harshvardhan Sikka, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05540">Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLMs and VLAs - including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST - on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity; (2) VLAs generally outperforms other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering. We release our benchmark, evaluation framework, and findings to enable the assessment of future VLA models and identify critical areas for improvement in their application to out-of-distribution digital tasks.
<div id='section'>PaperID: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2505.01998.pdf' target='_blank'>https://arxiv.org/pdf/2505.01998.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoliang Chen, Xin Yu, Le Chang, Yunhe Huang, Jiashuai He, Shibo Zhang, Jin Li, Likai Lin, Ziyu Zeng, Xianling Tu, Shuyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01998">A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations, covering far-field localization, weak signal detection, and multilingual speech recognition, demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces.
<div id='section'>PaperID: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2504.05748.pdf' target='_blank'>https://arxiv.org/pdf/2504.05748.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tri Tung Nguyen Nguyen, Quang Tien Dam, Dinh Tuan Tran, Joo-Ho Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05748">When Less Is More: A Sparse Facial Motion Structure For Listening Motion Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human behavior modeling is critical for successful human-robot interaction. Current state-of-the-art approaches for predicting listening head behavior during dyadic conversations employ continuous-to-discrete representations, where continuous facial motion sequence is converted into discrete latent tokens. However, non-verbal facial motion presents unique challenges owing to its temporal variance and multi-modal nature. State-of-the-art discrete motion token representation struggles to capture underlying non-verbal facial patterns making training the listening head inefficient with low-fidelity generated motion. This study proposes a novel method for representing and predicting non-verbal facial motion by encoding long sequences into a sparse sequence of keyframes and transition frames. By identifying crucial motion steps and interpolating intermediate frames, our method preserves the temporal structure of motion while enhancing instance-wise diversity during the learning process. Additionally, we apply this novel sparse representation to the task of listening head prediction, demonstrating its contribution to improving the explanation of facial motion patterns.
<div id='section'>PaperID: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2503.15518.pdf' target='_blank'>https://arxiv.org/pdf/2503.15518.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cheng Tang, Chao Tang, Steven Gong, Thomas M. Kwok, Yue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15518">Robot Character Generation and Adaptive Human-Robot Interaction with Personality Shaping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for designing emotionally agile robots with dynamic personalities and memory-based learning, with the aim of performing adaptive and non-deterministic interactions with humans while conforming to shared social understanding. While existing work has largely focused on emotion recognition and static response systems, many approaches rely on sentiment analysis and action mapping frameworks that are pre-defined with limited dimensionality and fixed configurations, lacking the flexibility of dynamic personality traits and memory-enabled adaptation. Other systems are often restricted to limited modes of expression and fail to develop a causal relationship between human behavior and the robot's proactive physical actions, resulting in constrained adaptability and reduced responsiveness in complex, dynamic interactions. Our methodology integrates the Big Five Personality Traits, Appraisal Theory, and abstracted memory layers through Large Language Models (LLMs). The LLM generates a parameterized robot personality based on the Big Five, processes human language and sentiments, evaluates human behavior using Appraisal Theory, and generates emotions and selects appropriate actions adapted by historical context over time. We validated the framework by testing three robots with distinct personalities in identical background contexts and found that personality, appraisal, and memory influence the adaptability of human-robot interactions. The impact of the individual components was further validated through ablation tests. We conclude that this system enables robots to engage in meaningful and personalized interactions with users, and holds significant potential for applications in domains such as pet robots, assistive robots, educational robots, and collaborative functional robots, where cultivating tailored relationships and enriching user experiences are essential.
<div id='section'>PaperID: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2503.01363.pdf' target='_blank'>https://arxiv.org/pdf/2503.01363.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanghai Zhang, Changyi Liu, Keting Fu, Wenbin Zhou, Qingdu Li, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01363">FABG : End-to-end Imitation Learning for Embodied Affective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes FABG (Facial Affective Behavior Generation), an end-to-end imitation learning system for human-robot interaction, designed to generate natural and fluid facial affective behaviors. In interaction, effectively obtaining high-quality demonstrations remains a challenge. In this work, we develop an immersive virtual reality (VR) demonstration system that allows operators to perceive stereoscopic environments. This system ensures "the operator's visual perception matches the robot's sensory input" and "the operator's actions directly determine the robot's behaviors" - as if the operator replaces the robot in human interaction engagements. We propose a prediction-driven latency compensation strategy to reduce robotic reaction delays and enhance interaction fluency. FABG naturally acquires human interactive behaviors and subconscious motions driven by intuition, eliminating manual behavior scripting. We deploy FABG on a real-world 25-degree-of-freedom (DoF) humanoid robot, validating its effectiveness through four fundamental interaction tasks: expression response, dynamic gaze, foveated attention, and gesture recognition, supported by data collection and policy training. Project website: https://cybergenies.github.io
<div id='section'>PaperID: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2502.13498.pdf' target='_blank'>https://arxiv.org/pdf/2502.13498.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiwei Lian, Feitian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13498">Improving Collision-Free Success Rate For Object Goal Visual Navigation Via Two-Stage Training With Collision Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The object goal visual navigation is the task of navigating to a specific target object using egocentric visual observations. Recent end-to-end navigation models based on deep reinforcement learning have achieved remarkable performance in finding and reaching target objects. However, the collision problem of these models during navigation remains unresolved, since the collision is typically neglected when evaluating the success. Although incorporating a negative reward for collision during training appears straightforward, it results in a more conservative policy, thereby limiting the agent's ability to reach targets. In addition, many of these models utilize only RGB observations, further increasing the difficulty of collision avoidance without depth information. To address these limitations, a new concept -- collision-free success is introduced to evaluate the ability of navigation models to find a collision-free path towards the target object. A two-stage training method with collision prediction is proposed to improve the collision-free success rate of the existing navigation models using RGB observations. In the first training stage, the collision prediction module supervises the agent's collision states during exploration to learn to predict the possible collision. In the second stage, leveraging the trained collision prediction, the agent learns to navigate to the target without collision. The experimental results in the AI2-THOR environment demonstrate that the proposed method greatly improves the collision-free success rate of different navigation models and outperforms other comparable collision-avoidance methods.
<div id='section'>PaperID: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2501.03304.pdf' target='_blank'>https://arxiv.org/pdf/2501.03304.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Evgenii Kruzhkov, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03304">LiLMaps: Learnable Implicit Language Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the current trends in robotics is to employ large language models (LLMs) to provide non-predefined command execution and natural human-robot interaction. It is useful to have an environment map together with its language representation, which can be further utilized by LLMs. Such a comprehensive scene representation enables numerous ways of interaction with the map for autonomously operating robots. In this work, we present an approach that enhances incremental implicit mapping through the integration of vision-language features. Specifically, we (i) propose a decoder optimization technique for implicit language maps which can be used when new objects appear on the scene, and (ii) address the problem of inconsistent vision-language predictions between different viewing positions. Our experiments demonstrate the effectiveness of LiLMaps and solid improvements in performance.
<div id='section'>PaperID: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2501.00785.pdf' target='_blank'>https://arxiv.org/pdf/2501.00785.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Atmaraaj Gopal, Arihiro Yorita, Naoyuki Kubota, Matthias RÃ¤tsch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00785">Natural Multimodal Fusion-Based Human-Robot Interaction: Application With Voice and Deictic Posture via Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing Human-Robot Interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly due to difficulties with complex syntax or sign language. To address the challenge, this paper introduces a multi-modal interaction framework that combines voice and deictic posture information to create a more natural HRI system. The visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity using a Universal Robots UR3e manipulator. Our method demonstrates significantly better performance in HRI in terms of accuracy and robustness. To benefit the research community and the general public, we will make our code and design open-source.
<div id='section'>PaperID: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2412.13726.pdf' target='_blank'>https://arxiv.org/pdf/2412.13726.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuga Yano, Akinobu Mizutani, Yukiya Fukuda, Daiju Kanaoka, Tomohiro Ono, Hakaru Tamukoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13726">Unified Understanding of Environment, Task, and Human for Human-Robot Interaction in Real-World Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate human--robot interaction (HRI) tasks in real-world scenarios, service robots must adapt to dynamic environments and understand the required tasks while effectively communicating with humans. To accomplish HRI in practice, we propose a novel indoor dynamic map, task understanding system, and response generation system. The indoor dynamic map optimizes robot behavior by managing an occupancy grid map and dynamic information, such as furniture and humans, in separate layers. The task understanding system targets tasks that require multiple actions, such as serving ordered items. Task representations that predefine the flow of necessary actions are applied to achieve highly accurate understanding. The response generation system is executed in parallel with task understanding to facilitate smooth HRI by informing humans of the subsequent actions of the robot. In this study, we focused on waiter duties in a restaurant setting as a representative application of HRI in a dynamic environment. We developed an HRI system that could perform tasks such as serving food and cleaning up while communicating with customers. In experiments conducted in a simulated restaurant environment, the proposed HRI system successfully communicated with customers and served ordered food with 90\% accuracy. In a questionnaire administered after the experiment, the HRI system of the robot received 4.2 points out of 5. These outcomes indicated the effectiveness of the proposed method and HRI system in executing waiter tasks in real-world environments.
<div id='section'>PaperID: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2412.13474.pdf' target='_blank'>https://arxiv.org/pdf/2412.13474.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kevin Haninger, Luka Peternel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13474">Planning Human-Robot Co-manipulation with Human Motor Control Objectives and Multi-component Reaching Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For successful goal-directed human-robot interaction, the robot should adapt to the intentions and actions of the collaborating human. This can be supported by musculoskeletal or data-driven human models, where the former are limited to lower-level functioning such as ergonomics, and the latter have limited generalizability or data efficiency. What is missing, is the inclusion of human motor control models that can provide generalizable human behavior estimates and integrate into robot planning methods. We use well-studied models from human motor control based on the speed-accuracy and cost-benefit trade-offs to plan collaborative robot motions. In these models, the human trajectory minimizes an objective function, a formulation we adapt to numerical trajectory optimization. This can then be extended with constraints and new variables to realize collaborative motion planning and goal estimation. We deploy this model, as well as a multi-component movement strategy, in physical collaboration with uncertain goal-reaching and synchronized motion tasks, showing the ability of the approach to produce human-like trajectories over a range of conditions.
<div id='section'>PaperID: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2412.07057.pdf' target='_blank'>https://arxiv.org/pdf/2412.07057.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yichen Li, Chicheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07057">A Note on Sample Complexity of Interactive Imitation Learning with Log Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning (IL) is a general paradigm for learning from experts in sequential decision-making problems. Recent advancements in IL have shown that offline imitation learning, specifically Behavior Cloning (BC) with log loss, is minimax optimal. Meanwhile, its interactive counterpart, DAgger, is shown to suffer from suboptimal sample complexity. In this note, we focus on realizable deterministic expert and revisit interactive imitation learning, particularly DAgger with log loss. We demonstrate: 1. A one-sample-per-round DAgger variant that outperforms BC in state-wise annotation. 2. Without recoverability assumption, DAgger with first-step mixture policies matches the performance of BC. Along the analysis, we introduce a new notion of decoupled Hellinger distance that separates state and action sequences, which can be of independent interest.
<div id='section'>PaperID: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2411.15033.pdf' target='_blank'>https://arxiv.org/pdf/2411.15033.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simone Colombani, Dimitri Ognibene, Giuseppe Boccignone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15033">One to rule them all: natural language to bind communication, perception and action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.
<div id='section'>PaperID: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2411.15027.pdf' target='_blank'>https://arxiv.org/pdf/2411.15027.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simone Colombani, Luca Brini, Dimitri Ognibene, Giuseppe Boccignone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15027">Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly being used in dynamic environments like workplaces, hospitals, and homes. As a result, interactions with robots must be simple and intuitive, with robots perception adapting efficiently to human-induced changes. This paper presents a robot control architecture that addresses key challenges in human-robot interaction, with a particular focus on the dynamic creation and continuous update of the robot state representation. The architecture uses Large Language Models to integrate diverse information sources, including natural language commands, robotic skills representation, real-time dynamic semantic mapping of the perceived scene. This enables flexible and adaptive robotic behavior in complex, dynamic environments. Traditional robotic systems often rely on static, pre-programmed instructions and settings, limiting their adaptability to dynamic environments and real-time collaboration. In contrast, this architecture uses LLMs to interpret complex, high-level instructions and generate actionable plans that enhance human-robot collaboration. At its core, the system Perception Module generates and continuously updates a semantic scene graph using RGB-D sensor data, providing a detailed and structured representation of the environment. A particle filter is employed to ensure accurate object localization in dynamic, real-world settings. The Planner Module leverages this up-to-date semantic map to break down high-level tasks into sub-tasks and link them to robotic skills such as navigation, object manipulation (e.g., PICK and PLACE), and movement (e.g., GOTO). By combining real-time perception, state tracking, and LLM-driven communication and task planning, the architecture enhances adaptability, task efficiency, and human-robot collaboration in dynamic environments.
<div id='section'>PaperID: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2411.06736.pdf' target='_blank'>https://arxiv.org/pdf/2411.06736.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junyeong Park, Junmo Cho, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06736">MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.
<div id='section'>PaperID: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2411.05821.pdf' target='_blank'>https://arxiv.org/pdf/2411.05821.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranav Guruprasad, Harshvardhan Sikka, Jaewoo Song, Yangyue Wang, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05821">Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models represent a promising direction for developing general-purpose robotic systems, demonstrating the ability to combine visual understanding, language comprehension, and action generation. However, systematic evaluation of these models across diverse robotic tasks remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite for assessing VLA models. We profile three state-of-the-art VLM and VLAs - GPT-4o, OpenVLA, and JAT - across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their performance on various manipulation tasks. Our analysis reveals several key insights: 1. current VLA models show significant variation in performance across different tasks and robot platforms, with GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering, 2. all models struggle with complex manipulation tasks requiring multi-step planning, and 3. model performance is notably sensitive to action space characteristics and environmental factors. We release our evaluation framework and findings to facilitate systematic assessment of future VLA models and identify critical areas for improvement in the development of general purpose robotic systems.
<div id='section'>PaperID: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2411.05022.pdf' target='_blank'>https://arxiv.org/pdf/2411.05022.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amar Halilovic, Senka Krivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05022">Towards Probabilistic Planning of Explanations for Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotics, ensuring that autonomous systems are comprehensible and accountable to users is essential for effective human-robot interaction. This paper introduces a novel approach that integrates user-centered design principles directly into the core of robot path planning processes. We propose a probabilistic framework for automated planning of explanations for robot navigation, where the preferences of different users regarding explanations are probabilistically modeled to tailor the stochasticity of the real-world human-robot interaction and the communication of decisions of the robot and its actions towards humans. This approach aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs by anticipating the types of explanations that will satisfy individual users.
<div id='section'>PaperID: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2410.13847.pdf' target='_blank'>https://arxiv.org/pdf/2410.13847.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ariel Slepyan, Dian Li, Aidan Aug, Sriramana Sankar, Trac Tran, Nitish Thakor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13847">Adaptive Compressive Tactile Subsampling: Enabling High Spatiotemporal Resolution in Scalable Robotic Skin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots, like humans, require full-body, high-resolution tactile sensing to operate safely and effectively in unstructured environments, enabling reflexive responses and closed-loop control. However, the high pixel counts necessary for dense, large-area coverage limit readout rates of most tactile arrays to below 100 Hz, hindering their use in high-speed tasks. We introduce Adaptive Compressive Tactile Subsampling (ACTS), a scalable and data-driven method that dramatically enhances the performance of traditional tactile matrices by leveraging sparse recovery and a learned tactile dictionary. Tested on a 1024-pixel tactile sensor array (32X32), ACTS achieved frame rates up to 1,000 Hz, an 18X improvement over conventional raster scanning, with minimal reconstruction error. For the first time, ACTS enables wearable, large-area, high-density tactile sensing systems that can deliver high-speed results. We demonstrate rapid object classification within 20 ms of contact, high-speed projectile detection, ricochet angle estimation, and soft deformation tracking, in tactile and robotics applications, all using flexible, high-density tactile arrays. These include high-resolution tactile gloves, pressure insoles, and full-body configurations covering robotic arms and human-sized mannequins. ACTS transforms standard, low-cost, and robust tactile sensors into high-speed systems, supporting applications from object manipulation to human-robot interaction. By enabling comprehensive, scalable, and efficient tactile coverage for robots and wearables, ACTS advances robotics toward lifelike, responsive, and adaptable operation in dynamic environments.
<div id='section'>PaperID: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2410.09874.pdf' target='_blank'>https://arxiv.org/pdf/2410.09874.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinxin Zhao, Wenzhe Cai, Likun Tang, Teng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09874">ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.
<div id='section'>PaperID: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2410.01071.pdf' target='_blank'>https://arxiv.org/pdf/2410.01071.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan Leusmann, Steeven Villa, Thomas Liang, Chao Wang, Albrecht Schmidt, Sven Mayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01071">An Approach to Elicit Human-Understandable Robot Expressions to Support Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the intentions of robots is essential for natural and seamless human-robot collaboration. Ensuring that robots have means for non-verbal communication is a basis for intuitive and implicit interaction. For this, we contribute an approach to elicit and design human-understandable robot expressions. We outline the approach in the context of non-humanoid robots. We paired human mimicking and enactment with research from gesture elicitation in two phases: first, to elicit expressions, and second, to ensure they are understandable. We present an example application through two studies (N=16 \& N=260) of our approach to elicit expressions for a simple 6-DoF robotic arm. We show that it enabled us to design robot expressions that signal curiosity and interest in getting attention. Our main contribution is an approach to generate and validate understandable expressions for robots, enabling more natural human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2409.18452.pdf' target='_blank'>https://arxiv.org/pdf/2409.18452.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenzhang Xiao, Seung Yun Song, Yu Chen, Mahshid Mansouri, JoÃ£o Ramos, Adam W. Bleakney, William R. Norris, Elizabeth T. Hsiao-Wecksler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18452">Exploiting Physical Human-Robot Interaction to Provide a Unique Rolling Experience with a Riding Ballbot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces the development of hands-free control schemes for a riding ballbot, designed to allow riders including manual wheelchair users to control its movement through torso leaning and twisting. The hardware platform, Personal Unique Rolling Experience (PURE), utilizes a ballbot drivetrain, a dynamically stable mobile robot that uses a ball as its wheel to provide omnidirectional maneuverability. To accommodate users with varying torso motion functions, the hanads-free control scheme should be adjustable based on the rider's torso function and personal preferences. Therefore, concepts of (a) impedance control and (b) admittance control were integrated into the control scheme. A duo-agent optimization framework was utilized to assess the efficiency of this rider-ballbot system for a safety-critical task: braking from 1.4 m/s. The candidate control schemes were further implemented in the physical robot hardware and validated with two experienced users, demonstrating the efficiency and robustness of the hands-free admittance control scheme (HACS). This interface, which utilized physical human-robot interaction (pHRI) as the input, resulted in lower braking effort and shorter braking distance and time. Subsequently, 12 novice participants (six able-bodied users and six manual wheelchair users) with different levels of torso motion capability were then recruited to benchmark the braking performance with HACS. The indoor navigation capability of PURE was further demonstrated with these participants in courses simulating narrow hallways, tight turns, and navigation through static and dynamic obstacles. By exploiting pHRI, the proposed admittance-style control scheme provided effective control of the ballbot via torso motions. This interface enables PURE to provide a personal unique rolling experience to manual wheelchair users for safe and agile indoor navigation.
<div id='section'>PaperID: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2409.10525.pdf' target='_blank'>https://arxiv.org/pdf/2409.10525.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dan Bohus, Sean Andrist, Yuwei Bao, Eric Horvitz, Ann Paradiso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10525">"Is This It?": Towards Ecologically Valid Benchmarks for Situated Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We report initial work towards constructing ecologically valid benchmarks to assess the capabilities of large multimodal models for engaging in situated collaboration. In contrast to existing benchmarks, in which question-answer pairs are generated post hoc over preexisting or synthetic datasets via templates, human annotators, or large language models (LLMs), we propose and investigate an interactive system-driven approach, where the questions are generated by users in context, during their interactions with an end-to-end situated AI system. We illustrate how the questions that arise are different in form and content from questions typically found in existing embodied question answering (EQA) benchmarks and discuss new real-world challenge problems brought to the fore.
<div id='section'>PaperID: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2409.07013.pdf' target='_blank'>https://arxiv.org/pdf/2409.07013.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Chen, Mahshid Mansouri, Chenzhang Xiao, Ze Wang, Elizabeth T. Hsiao-Wecksler, William R. Norris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07013">Enabling Shared-Control for A Riding Ballbot System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a shared-control approach for collision avoidance in a self-balancing riding ballbot, called PURE, marked by its dynamic stability, omnidirectional movement, and hands-free interface. Integrated with a sensor array and a novel Passive Artificial Potential Field (PAPF) method, PURE provides intuitive navigation with deceleration assistance and haptic/audio feedback, effectively mitigating collision risks. This approach addresses the limitations of traditional APF methods, such as control oscillations and unnecessary speed reduction in challenging scenarios. A human-robot interaction experiment, with 20 manual wheelchair users and able-bodied individuals, was conducted to evaluate the performance of indoor navigation and obstacle avoidance with the proposed shared-control algorithm. Results indicated that shared-control significantly reduced collisions and cognitive load without affecting travel speed, offering intuitive and safe operation. These findings highlight the shared-control system's suitability for enhancing collision avoidance in self-balancing mobility devices, a relatively unexplored area in assistive mobility research.
<div id='section'>PaperID: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2409.05593.pdf' target='_blank'>https://arxiv.org/pdf/2409.05593.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muraleekrishna Gopinathan, Jumana Abu-Khalaf, David Suter, Martin Masek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05593">StratXplore: Strategic Novelty-seeking and Instruction-aligned Exploration for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called \textit{StratXplore}, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities.
<div id='section'>PaperID: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2602.06339.pdf' target='_blank'>https://arxiv.org/pdf/2602.06339.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Harold Soh, Eugene Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.06339">Action Hallucination in Generative Visual-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot Foundation Models such as Vision-Language-Action models are rapidly reshaping how robot policies are trained and deployed, replacing hand-designed planners with end-to-end generative action models. While these systems demonstrate impressive generalization, it remains unclear whether they fundamentally resolve the long-standing challenges of robotics. We address this question by analyzing action hallucinations that violate physical constraints and their extension to plan-level failures. Focusing on latent-variable generative policies, we show that hallucinations often arise from structural mismatches between feasible robot behavior and common model architectures. We study three such barriers -- topological, precision, and horizon -- and show how they impose unavoidable tradeoffs. Our analysis provides mechanistic explanations for reported empirical failures of generative robot policies and suggests principled directions for improving reliability and trustworthiness, without abandoning their expressive power.
<div id='section'>PaperID: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2602.02530.pdf' target='_blank'>https://arxiv.org/pdf/2602.02530.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Saurav Singh, Rodney Sanchez, Alexander Ororbia, Jamison Heard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.02530">Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has the potential to transform real-world decision-making systems by enabling autonomous agents to learn from experience. Deploying RL in real-world settings, especially in the context of human-robot interaction, requires defining state representations and reward functions, which are critical for learning efficiency and policy performance. Traditional RL approaches often rely on domain expertise and trial-and-error, necessitating extensive human involvement as well as direct interaction with the environment, which can be costly and impractical, especially in complex and safety-critical applications. This work proposes a novel RL framework that leverages off-policy evaluation (OPE) for state space and reward function selection, using only logged interaction data. This approach eliminates the need for real-time access to the environment or human-in-the-loop feedback, greatly reducing the dependency on costly real-time interactions. The proposed approach systematically evaluates multiple candidate state representations and reward functions by training offline RL agents and applying OPE to estimate policy performance. The optimal state space and reward function are selected based on their ability to produce high-performing policies under OPE metrics. Our method is validated on two environments: the Lunar Lander environment by OpenAI Gym, which provides a controlled setting for assessing state space and reward function selection, and a NASA-MATB-II human subjects study environment, which evaluates the approach's real-world applicability to human-robot teaming scenarios. This work enhances the feasibility and scalability of offline RL for real-world environments by automating critical RL design decisions through a data-driven OPE-based evaluation, enabling more reliable, effective, and sustainable RL formulation for complex human-robot interaction settings.
<div id='section'>PaperID: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2602.00611.pdf' target='_blank'>https://arxiv.org/pdf/2602.00611.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaqi Xu, Tao Huang, Kai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00611">Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments. We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework. We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling. We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.
<div id='section'>PaperID: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2601.21124.pdf' target='_blank'>https://arxiv.org/pdf/2601.21124.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Artem Dementyev, Wazeer Zulfikar, Sinan Hersek, Pascal Getreuer, Anurag Kumar, Vivek Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21124">PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over "Spatial Audio Tokens" produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array.
<div id='section'>PaperID: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2601.12115.pdf' target='_blank'>https://arxiv.org/pdf/2601.12115.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amy Koike, Yuki Okafuji, Sichao Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12115">Practical Insights into Designing Context-Aware Robot Voice Parameters in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voice is an essential modality for human-robot interaction (HRI). The way a robot sounds plays a central role in shaping how humans perceive and engage with it, influencing factors such as intelligibility, understandability, and likability. Although prior work has examined voice design, most studies occur in controlled labs, leaving uncertainty about how results translate to real-world settings. To address this gap, we conducted two naturalistic deployment studies with a guidance robot in a shopping mall: (1) in-depth interviews with six participants, and (2) an eight-day field deployment using a 3x3 design varying speech rate and volume, yielding 725 survey responses. Our results show how real-world context shapes voice perception and inform adaptive, context-aware voice design for social robots in public spaces.
<div id='section'>PaperID: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2601.09954.pdf' target='_blank'>https://arxiv.org/pdf/2601.09954.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09954">The Spatial Blindspot of Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.
<div id='section'>PaperID: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2512.19049.pdf' target='_blank'>https://arxiv.org/pdf/2512.19049.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hwanhee Jung, Seunggwan Lee, Jeongyoon Yoon, SeungHyeon Kim, Giljoo Nam, Qixing Huang, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19049">Decoupled Generative Modeling for Human-Object Interaction Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.
<div id='section'>PaperID: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2512.18712.pdf' target='_blank'>https://arxiv.org/pdf/2512.18712.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maozeng Zhang, Ke Shi, Huijun Li, Tongshu Chen, Jiejun Yan, Aiguo Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18712">DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stroke-induced motor impairment often results in substantial loss of upper-limb function, creating a strong demand for rehabilitation robots that enable safe and transparent physical human-robot interaction (pHRI). Variable stiffness actuators are well suited for such applications. However, in most existing designs, stiffness is coupled with the deflection angle, complicating both modeling and control. To address this limitation, this paper presents a variable stiffness actuator featuring decoupled stiffness and output behavior for rehabilitation robotics. The system integrates a variable stiffness mechanism that combines a variable-length lever with a hypocycloidal straight-line mechanism to achieve a linear torque-deflection relationship and continuous stiffness modulation from near zero to theoretically infinite. It also incorporates a differential transmission mechanism based on a planetary gear system that enables dual-motor load sharing. A cascade PI controller is further developed on the basis of the differential configuration, in which the position-loop term jointly regulates stiffness and deflection angle, effectively suppressing stiffness fluctuations and output disturbances. The performance of prototype was experimentally validated through stiffness calibration, stiffness regulation, torque control, decoupled characteristics, and dual-motor load sharing, indicating the potential for rehabilitation exoskeletons and other pHRI systems.
<div id='section'>PaperID: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2512.15692.pdf' target='_blank'>https://arxiv.org/pdf/2512.15692.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15692">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
<div id='section'>PaperID: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2512.12208.pdf' target='_blank'>https://arxiv.org/pdf/2512.12208.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Indranil Bhattacharjee, Vartika Narayani Srinet, Anirudha Bhattacharjee, Braj Bhushan, Bishakh Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12208">A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.
<div id='section'>PaperID: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2512.01889.pdf' target='_blank'>https://arxiv.org/pdf/2512.01889.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zaid Nasser, Mikhail Iumanov, Tianhao Li, Maxim Popov, Jaafar Mahmoud, Malik Mohrat, Ilya Obrubov, Ekaterina Derevyanka, Ivan Sosin, Sergey Kolyubin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01889">KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.
<div id='section'>PaperID: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2511.17869.pdf' target='_blank'>https://arxiv.org/pdf/2511.17869.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Subramanyam Sahoo, Jared Junkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17869">The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.
<div id='section'>PaperID: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2511.16347.pdf' target='_blank'>https://arxiv.org/pdf/2511.16347.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunyang Li, Zifeng Kang, Junwei Zhang, Zhuo Ma, Anda Cheng, Xinghua Li, Jianfeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16347">The Shawshank Redemption of Embodied AI: Understanding and Benchmarking Indirect Environmental Jailbreaks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The adoption of Vision-Language Models (VLMs) in embodied AI agents, while being effective, brings safety concerns such as jailbreaking. Prior work have explored the possibility of directly jailbreaking the embodied agents through elaborated multi-modal prompts. However, no prior work has studied or even reported indirect jailbreaks in embodied AI, where a black-box attacker induces a jailbreak without issuing direct prompts to the embodied agent. In this paper, we propose, for the first time, indirect environmental jailbreak (IEJ), a novel attack to jailbreak embodied AI via indirect prompt injected into the environment, such as malicious instructions written on a wall. Our key insight is that embodied AI does not ''think twice'' about the instructions provided by the environment -- a blind trust that attackers can exploit to jailbreak the embodied agent. We further design and implement open-source prototypes of two fully-automated frameworks: SHAWSHANK, the first automatic attack generation framework for the proposed attack IEJ; and SHAWSHANK-FORGE, the first automatic benchmark generation framework for IEJ. Then, using SHAWSHANK-FORGE, we automatically construct SHAWSHANK-BENCH, the first benchmark for indirectly jailbreaking embodied agents. Together, our two frameworks and one benchmark answer the questions of what content can be used for malicious IEJ instructions, where they should be placed, and how IEJ can be systematically evaluated. Evaluation results show that SHAWSHANK outperforms eleven existing methods across 3,957 task-scene combinations and compromises all six tested VLMs. Furthermore, current defenses only partially mitigate our attack, and we have responsibly disclosed our findings to all affected VLM vendors.
<div id='section'>PaperID: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2511.13530.pdf' target='_blank'>https://arxiv.org/pdf/2511.13530.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Vesna Poprcova, Iulia Lefter, Matthias Wieser, Martijn Warnier, Frances Brazier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13530">Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.
<div id='section'>PaperID: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2510.21809.pdf' target='_blank'>https://arxiv.org/pdf/2510.21809.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haru Kondoh, Asako Kanezaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21809">Embodied Navigation with Auxiliary Task of Action Description Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of multimodal robot navigation in indoor environments has garnered significant attention in recent years. However, as tasks and methods become more advanced, the action decision systems tend to become more complex and operate as black-boxes. For a reliable system, the ability to explain or describe its decisions is crucial; however, there tends to be a trade-off in that explainable systems can not outperform non-explainable systems in terms of performance. In this paper, we propose incorporating the task of describing actions in language into the reinforcement learning of navigation as an auxiliary task. Existing studies have found it difficult to incorporate describing actions into reinforcement learning due to the absence of ground-truth data. We address this issue by leveraging knowledge distillation from pre-trained description generation models, such as vision-language models. We comprehensively evaluate our approach across various navigation tasks, demonstrating that it can describe actions while attaining high navigation performance. Furthermore, it achieves state-of-the-art performance in the particularly challenging multimodal navigation task of semantic audio-visual navigation.
<div id='section'>PaperID: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2510.21758.pdf' target='_blank'>https://arxiv.org/pdf/2510.21758.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21758">Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has become a foundational approach for enabling intelligent robotic behavior in dynamic and uncertain environments. This work presents an in-depth review of RL principles, advanced deep reinforcement learning (DRL) algorithms, and their integration into robotic and control systems. Beginning with the formalism of Markov Decision Processes (MDPs), the study outlines essential elements of the agent-environment interaction and explores core algorithmic strategies including actor-critic methods, value-based learning, and policy gradients. Emphasis is placed on modern DRL techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving high-dimensional, continuous control tasks. A structured taxonomy is introduced to categorize RL applications across domains such as locomotion, manipulation, multi-agent coordination, and human-robot interaction, along with training methodologies and deployment readiness levels. The review synthesizes recent research efforts, highlighting technical trends, design patterns, and the growing maturity of RL in real-world robotics. Overall, this work aims to bridge theoretical advances with practical implementations, providing a consolidated perspective on the evolving role of RL in autonomous robotic systems.
<div id='section'>PaperID: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2510.13594.pdf' target='_blank'>https://arxiv.org/pdf/2510.13594.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Austin Barret, Meng Cheng Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13594">Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The operation of humanoid robotics is an essential field of research with many practical and competitive applications. Many of these systems, however, do not invest heavily in developing a non-expert-centered graphical user interface (GUI) for operation. The focus of this research is to develop a scalable GUI that is tailored to be simple and intuitive so non-expert operators can control the robot through a FIRA-regulated obstacle course. Using common practices from user interface development (UI) and understanding concepts described in human-robot interaction (HRI) and other related concepts, we will develop a new interface with the goal of a non-expert teleoperation system.
<div id='section'>PaperID: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2508.15119.pdf' target='_blank'>https://arxiv.org/pdf/2508.15119.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15119">Open-Universe Assistance Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents must infer and act in an interpretable way on diverse human goals and preferences that are not predefined. To formalize this setting, we introduce Open-Universe Assistance Games (OU-AGs), a framework where the agent must reason over an unbounded and evolving space of possible goals. In this context, we introduce GOOD (GOals from Open-ended Dialogue), a data-efficient, online method that extracts goals in the form of natural language during an interaction with a human, and infers a distribution over natural language goals. GOOD prompts an LLM to simulate users with different complex intents, using its responses to perform probabilistic inference over candidate goals. This approach enables rich goal representations and uncertainty estimation without requiring large offline datasets. We evaluate GOOD in a text-based grocery shopping domain and in a text-operated simulated household robotics environment (AI2Thor), using synthetic user profiles. Our method outperforms a baseline without explicit goal tracking, as confirmed by both LLM-based and human evaluations.
<div id='section'>PaperID: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2508.13982.pdf' target='_blank'>https://arxiv.org/pdf/2508.13982.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sydney Thompson, Kate Candon, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13982">The Social Context of Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term "social context" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term "social context". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.
<div id='section'>PaperID: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2507.06700.pdf' target='_blank'>https://arxiv.org/pdf/2507.06700.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranav Pandey, Ramviyas Parasuraman, Prashant Doshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06700">Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety in human-robot interaction (HRI) is essential to foster user trust and enable the broader adoption of robotic systems. Traditional safety models primarily rely on sensor-based measures, such as relative distance and velocity, to assess physical safety. However, these models often fail to capture subjective safety perceptions, which are shaped by individual traits and contextual factors. In this paper, we introduce and analyze a parameterized general safety model that bridges the gap between physical and perceived safety by incorporating a personalization parameter, $Ï$, into the safety measurement framework to account for individual differences in safety perception. Through a series of hypothesis-driven human-subject studies in a simulated rescue scenario, we investigate how emotional state, trust, and robot behavior influence perceived safety. Our results show that $Ï$ effectively captures meaningful individual differences, driven by affective responses, trust in task consistency, and clustering into distinct user types. Specifically, our findings confirm that predictable and consistent robot behavior as well as the elicitation of positive emotional states, significantly enhance perceived safety. Moreover, responses cluster into a small number of user types, supporting adaptive personalization based on shared safety models. Notably, participant role significantly shapes safety perception, and repeated exposure reduces perceived safety for participants in the casualty role, emphasizing the impact of physical interaction and experiential change. These findings highlight the importance of adaptive, human-centered safety models that integrate both psychological and behavioral dimensions, offering a pathway toward more trustworthy and effective HRI in safety-critical domains.
<div id='section'>PaperID: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2506.20268.pdf' target='_blank'>https://arxiv.org/pdf/2506.20268.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruben Janssens, Jens De Bock, Sofie Labat, Eva Verhelst, Veronique Hoste, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20268">Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting miscommunication in human-robot interaction is a critical function for maintaining user engagement and trust. While humans effortlessly detect communication errors in conversations through both verbal and non-verbal cues, robots face significant challenges in interpreting non-verbal feedback, despite advances in computer vision for recognizing affective expressions. This research evaluates the effectiveness of machine learning models in detecting miscommunications in robot dialogue. Using a multi-modal dataset of 240 human-robot conversations, where four distinct types of conversational failures were systematically introduced, we assess the performance of state-of-the-art computer vision models. After each conversational turn, users provided feedback on whether they perceived an error, enabling an analysis of the models' ability to accurately detect robot mistakes. Despite using state-of-the-art models, the performance barely exceeds random chance in identifying miscommunication, while on a dataset with more expressive emotional content, they successfully identified confused states. To explore the underlying cause, we asked human raters to do the same. They could also only identify around half of the induced miscommunications, similarly to our model. These results uncover a fundamental limitation in identifying robot miscommunications in dialogue: even when users perceive the induced miscommunication as such, they often do not communicate this to their robotic conversation partner. This knowledge can shape expectations of the performance of computer vision models and can help researchers to design better human-robot conversations by deliberately eliciting feedback where needed.
<div id='section'>PaperID: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2506.17991.pdf' target='_blank'>https://arxiv.org/pdf/2506.17991.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thien Tran, Jonathan Kua, Minh Tran, Honghao Lyu, Thuong Hoang, Jiong Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17991">CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Telerobotics is a key foundation in autonomous Industrial Cyber-Physical Systems (ICPS), enabling remote operations across various domains. However, conventional cloud-based telerobotics suffers from latency, reliability, scalability, and resilience issues, hindering real-time performance in critical applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation (CFA) paradigm to address these limitations by leveraging a distributed Cloud-Edge-Robotics computing architecture, enabling deterministic connectivity, deterministic connected intelligence, and deterministic networked computing. This paper synthesizes recent advancements in CFTel, aiming to highlight its role in facilitating scalable, low-latency, autonomous, and AI-driven telerobotics. We analyze architectural frameworks and technologies that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel has the potential to enhance real-time control, scalability, and autonomy while supporting service-oriented solutions. We also discuss practical challenges, including latency constraints, cybersecurity risks, interoperability issues, and standardization efforts. This work serves as a foundational reference for researchers, stakeholders, and industry practitioners in future telerobotics research.
<div id='section'>PaperID: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2506.15150.pdf' target='_blank'>https://arxiv.org/pdf/2506.15150.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuanlong Ji, Xingbang Yang, Ruoqi Zhao, Qihan Ye, Quan Zheng, Yubo Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15150">Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait phase estimation based on inertial measurement unit (IMU) signals facilitates precise adaptation of exoskeletons to individual gait variations. However, challenges remain in achieving high accuracy and robustness, particularly during periods of terrain changes. To address this, we develop a gait phase estimation neural network based on implicit modeling of human locomotion, which combines temporal convolution for feature extraction with transformer layers for multi-channel information fusion. A channel-wise masked reconstruction pre-training strategy is proposed, which first treats gait phase state vectors and IMU signals as joint observations of human locomotion, thus enhancing model generalization. Experimental results demonstrate that the proposed method outperforms existing baseline approaches, achieving a gait phase RMSE of $2.729 \pm 1.071%$ and phase rate MAE of $0.037 \pm 0.016%$ under stable terrain conditions with a look-back window of 2 seconds, and a phase RMSE of $3.215 \pm 1.303%$ and rate MAE of $0.050 \pm 0.023%$ under terrain transitions. Hardware validation on a hip exoskeleton further confirms that the algorithm can reliably identify gait cycles and key events, adapting to various continuous motion scenarios. This research paves the way for more intelligent and adaptive exoskeleton systems, enabling safer and more efficient human-robot interaction across diverse real-world environments.
<div id='section'>PaperID: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2506.13937.pdf' target='_blank'>https://arxiv.org/pdf/2506.13937.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Caio C. G. Ribeiro, Douglas G. Macharet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13937">Beyond the Plane: A 3D Representation of Human Personal Space for Socially-Aware Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing presence of robots in human environments requires them to exhibit socially appropriate behavior, adhering to social norms. A critical aspect in this context is the concept of personal space, a psychological boundary around an individual that influences their comfort based on proximity. This concept extends to human-robot interaction, where robots must respect personal space to avoid causing discomfort. While much research has focused on modeling personal space in two dimensions, almost none have considered the vertical dimension. In this work, we propose a novel three-dimensional personal space model that integrates both height (introducing a discomfort function along the Z-axis) and horizontal proximity (via a classic XY-plane formulation) to quantify discomfort. To the best of our knowledge, this is the first work to compute discomfort in 3D space at any robot component's position, considering the person's configuration and height.
<div id='section'>PaperID: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2506.12678.pdf' target='_blank'>https://arxiv.org/pdf/2506.12678.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranay Gupta, Henny Admoni, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12678">Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end visuomotor policies trained using behavior cloning have shown a remarkable ability to generate complex, multi-modal low-level robot behaviors. However, at deployment time, these policies still struggle to act reliably when faced with out-of-distribution (OOD) visuals induced by objects, backgrounds, or environment changes. Prior works in interactive imitation learning solicit corrective expert demonstrations under the OOD conditions -- but this can be costly and inefficient. We observe that task success under OOD conditions does not always warrant novel robot behaviors. In-distribution (ID) behaviors can directly be transferred to OOD conditions that share functional similarities with ID conditions. For example, behaviors trained to interact with in-distribution (ID) pens can apply to interacting with a visually-OOD pencil. The key challenge lies in disambiguating which ID observations functionally correspond to the OOD observation for the task at hand. We propose that an expert can provide this OOD-to-ID functional correspondence. Thus, instead of collecting new demonstrations and re-training at every OOD encounter, our method: (1) detects the need for feedback by first checking if current observations are OOD and then identifying whether the most similar training observations show divergent behaviors, (2) solicits functional correspondence feedback to disambiguate between those behaviors, and (3) intervenes on the OOD observations with the functionally corresponding ID observations to perform deployment-time generalization. We validate our method across diverse real-world robotic manipulation tasks with a Franka Panda robotic manipulator. Our results show that test-time functional correspondences can improve the generalization of a vision-based diffusion policy to OOD objects and environment conditions with low feedback.
<div id='section'>PaperID: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2506.04982.pdf' target='_blank'>https://arxiv.org/pdf/2506.04982.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunlong Dong, Xing Liu, Jun Wan, Zelin Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04982">GEX: Democratizing Dexterity with Fully-Actuated Dexterous Hand and Exoskeleton Glove</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces GEX, an innovative low-cost dexterous manipulation system that combines the GX11 tri-finger anthropomorphic hand (11 DoF) with the EX12 tri-finger exoskeleton glove (12 DoF), forming a closed-loop teleoperation framework through kinematic retargeting for high-fidelity control. Both components employ modular 3D-printed finger designs, achieving ultra-low manufacturing costs while maintaining full actuation capabilities. Departing from conventional tendon-driven or underactuated approaches, our electromechanical system integrates independent joint motors across all 23 DoF, ensuring complete state observability and accurate kinematic modeling. This full-actuation architecture enables precise bidirectional kinematic calculations, substantially enhancing kinematic retargeting fidelity between the exoskeleton and robotic hand. The proposed system bridges the cost-performance gap in dexterous manipulation research, providing an accessible platform for acquiring high-quality demonstration data to advance embodied AI and dexterous robotic skill transfer learning.
<div id='section'>PaperID: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2506.03516.pdf' target='_blank'>https://arxiv.org/pdf/2506.03516.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arnab Debnath, Gregory J. Stein, Jana Kosecka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03516">SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal navigation is a fundamental task in embodied AI, where an agent is instructed to locate a target object in an unexplored environment. Traditional learning-based methods rely heavily on large-scale annotated data or require extensive interaction with the environment in a reinforcement learning setting, often failing to generalize to novel environments and limiting scalability. To overcome these challenges, we explore a zero-shot setting where the agent operates without task-specific training, enabling more scalable and adaptable solution. Recent advances in Vision Foundation Models (VFMs) offer powerful capabilities for visual understanding and reasoning, making them ideal for agents to comprehend scenes, identify relevant regions, and infer the likely locations of objects. In this work, we present a zero-shot object goal navigation framework that integrates the perceptual strength of VFMs with a model-based planner that is capable of long-horizon decision making through frontier exploration. We evaluate our approach on the HM3D dataset using the Habitat simulator and demonstrate that our method achieves state-of-the-art performance in terms of success weighted by path length for zero-shot object goal navigation.
<div id='section'>PaperID: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2506.02265.pdf' target='_blank'>https://arxiv.org/pdf/2506.02265.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Samuel Li, Pujith Kachana, Prajwal Chidananda, Saurabh Nair, Yasutaka Furukawa, Matthew Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02265">Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating agent pose and 3D scene structure from multi-camera rigs is a central task in embodied AI applications such as autonomous driving. Recent learned approaches such as DUSt3R have shown impressive performance in multiview settings. However, these models treat images as unstructured collections, limiting effectiveness in scenarios where frames are captured from synchronized rigs with known or inferable structure.
  To this end, we introduce Rig3R, a generalization of prior multiview reconstruction models that incorporates rig structure when available, and learns to infer it when not. Rig3R conditions on optional rig metadata including camera ID, time, and rig poses to develop a rig-aware latent space that remains robust to missing information. It jointly predicts pointmaps and two types of raymaps: a pose raymap relative to a global frame, and a rig raymap relative to a rig-centric frame consistent across time. Rig raymaps allow the model to infer rig structure directly from input images when metadata is missing.
  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose estimation, and rig discovery, outperforming both traditional and learned methods by 17-45% mAA across diverse real-world rig datasets, all in a single forward pass without post-processing or iterative refinement.
<div id='section'>PaperID: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2506.00075.pdf' target='_blank'>https://arxiv.org/pdf/2506.00075.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Diego Pollini, Bruna V. Guterres, Rodrigo S. Guerra, Ricardo B. Grando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00075">Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.
<div id='section'>PaperID: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2505.12707.pdf' target='_blank'>https://arxiv.org/pdf/2505.12707.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yingchen He, Christian D. Weilbach, Martyna E. Wojciechowska, Yuxuan Zhang, Frank Wood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12707">PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\footnote{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.
<div id='section'>PaperID: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2505.12294.pdf' target='_blank'>https://arxiv.org/pdf/2505.12294.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weishang Wu, Yifei Shi, Zhizhong Chen, Zhipong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12294">PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping is a crucial yet challenging task in robotic manipulation. Despite the recent progress, few existing methods address task-oriented grasping with dexterous hands. Dexterous hands provide better precision and versatility, enabling robots to perform task-oriented grasping more effectively. In this paper, we argue that part analysis can enhance dexterous grasping by providing detailed information about the object's functionality. We propose PartDexTOG, a method that generates dexterous task-oriented grasps via language-driven part analysis. Taking a 3D object and a manipulation task represented by language as input, the method first generates the category-level and part-level grasp descriptions w.r.t the manipulation task by LLMs. Then, a category-part conditional diffusion model is developed to generate a dexterous grasp for each part, respectively, based on the generated descriptions. To select the most plausible combination of grasp and corresponding part from the generated ones, we propose a measure of geometric consistency between grasp and part. We show that our method greatly benefits from the open-world knowledge reasoning on object parts by LLMs, which naturally facilitates the learning of grasp generation on objects with different geometry and for different manipulation tasks. Our method ranks top on the OakInk-shape dataset over all previous methods, improving the Penetration Volume, the Grasp Displace, and the P-FID over the state-of-the-art by $3.58\%$, $2.87\%$, and $41.43\%$, respectively. Notably, it demonstrates good generality in handling novel categories and tasks.
<div id='section'>PaperID: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2505.02414.pdf' target='_blank'>https://arxiv.org/pdf/2505.02414.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nicholas Hafner, Chaoran Liu, Carlos Ishi, Hiroshi Ishiguro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02414">Quadrupedal Spine Control Strategies: Exploring Correlations Between System Dynamic Responses and Human Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike their biological cousins, the majority of existing quadrupedal robots are constructed with rigid chassis. This results in motion that is either beetle-like or distinctly robotic, lacking the natural fluidity characteristic of mammalian movements. Existing literature on quadrupedal robots with spinal configurations primarily focuses on energy efficiency and does not consider the effects in human-robot interaction scenarios. Our contributions include an initial investigation into various trajectory generation strategies for a quadrupedal robot with a four degree of freedom spine, and an analysis on the effect that such methods have on human perception of gait naturalness compared to a fixed spine baseline. The strategies were evaluated using videos of walking, trotting and turning simulations. Among the four different strategies developed, the optimised time varying and the foot-tracking strategies were perceived to be more natural than the baseline in a randomised trial with 50 participants. Although none of the strategies demonstrated any energy efficiency improvements over the no-spine baseline, some showed greater footfall consistency at higher speeds. Given the greater likeability drawn from the more natural locomotion patterns, this type of robot displays potential for applications in social robot scenarios such as elderly care, where energy efficiency is not a primary concern.
<div id='section'>PaperID: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2504.17128.pdf' target='_blank'>https://arxiv.org/pdf/2504.17128.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Seyed Yousef Soltanian, Wenlong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17128">PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of a two-player linear quadratic differential game with incomplete information, a scenario commonly encountered in multi-agent control, human-robot interaction (HRI), and approximation methods for solving general-sum differential games. While solutions to such linear differential games are typically obtained through coupled Riccati equations, the complexity increases when agents have incomplete information, particularly when neither is aware of the other's cost function. To tackle this challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework for learning the cost parameters of the other agent. In PACE, each agent treats its peer as a learning agent rather than a stationary optimal agent, models their learning dynamics, and leverages this dynamic to infer the cost function parameters of the other agent. This approach enables agents to infer each other's objective function in real time based solely on their previous state observations and dynamically adapt their control policies. Furthermore, we provide a theoretical guarantee for the convergence of parameter estimation and the stability of system states in PACE. Additionally, in our numerical studies, we demonstrate how modeling the learning dynamics of the other agent benefits PACE, compared to approaches that approximate the other agent as having complete information, particularly in terms of stability and convergence speed.
<div id='section'>PaperID: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2504.05291.pdf' target='_blank'>https://arxiv.org/pdf/2504.05291.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haley N. Green, Tariq Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05291">Using Physiological Measures, Gaze, and Facial Expressions to Model Human Trust in a Robot Partner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With robots becoming increasingly prevalent in various domains, it has become crucial to equip them with tools to achieve greater fluency in interactions with humans. One of the promising areas for further exploration lies in human trust. A real-time, objective model of human trust could be used to maximize productivity, preserve safety, and mitigate failure. In this work, we attempt to use physiological measures, gaze, and facial expressions to model human trust in a robot partner. We are the first to design an in-person, human-robot supervisory interaction study to create a dedicated trust dataset. Using this dataset, we train machine learning algorithms to identify the objective measures that are most indicative of trust in a robot partner, advancing trust prediction in human-robot interactions. Our findings indicate that a combination of sensor modalities (blood volume pulse, electrodermal activity, skin temperature, and gaze) can enhance the accuracy of detecting human trust in a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision Trees classifiers exhibit consistently better performance in measuring the person's trust in the robot partner. These results lay the groundwork for constructing a real-time trust model for human-robot interaction, which could foster more efficient interactions between humans and robots.
<div id='section'>PaperID: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2503.17046.pdf' target='_blank'>https://arxiv.org/pdf/2503.17046.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dongsheng Yang, Qianying Liu, Wataru Sato, Takashi Minato, Chaoran Liu, Shin'ya Nishida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17046">HAPI: A Model for Learning Robot Facial Expressions from Human Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic robotic facial expression generation is crucial for human-robot interaction, as handcrafted methods based on fixed joint configurations often yield rigid and unnatural behaviors. Although recent automated techniques reduce the need for manual tuning, they tend to fall short by not adequately bridging the gap between human preferences and model predictions-resulting in a deficiency of nuanced and realistic expressions due to limited degrees of freedom and insufficient perceptual integration. In this work, we propose a novel learning-to-rank framework that leverages human feedback to address this discrepancy and enhanced the expressiveness of robotic faces. Specifically, we conduct pairwise comparison annotations to collect human preference data and develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese RankNet-based approach that refines expression evaluation. Results obtained via Bayesian Optimization and online expression survey on a 35-DOF android platform demonstrate that our approach produces significantly more realistic and socially resonant expressions of Anger, Happiness, and Surprise than those generated by baseline and expert-designed methods. This confirms that our framework effectively bridges the gap between human preferences and model predictions while robustly aligning robotic expression generation with human affective responses.
<div id='section'>PaperID: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2503.16449.pdf' target='_blank'>https://arxiv.org/pdf/2503.16449.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hangyeol Kang, Thiago Freitas dos Santos, Maher Ben Moussa, Nadia Magnenat-Thalmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16449">Mitigating the Uncanny Valley Effect in Hyper-Realistic Robots: A Student-Centered Study on LLM-Driven Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The uncanny valley effect poses a significant challenge in the development and acceptance of hyper-realistic social robots. This study investigates whether advanced conversational capabilities powered by large language models (LLMs) can mitigate this effect in highly anthropomorphic robots. We conducted a user study with 80 participants interacting with Nadine, a hyper-realistic humanoid robot equipped with LLM-driven communication skills. Through pre- and post-interaction surveys, we assessed changes in perceptions of uncanniness, conversational quality, and overall user experience. Our findings reveal that LLM-enhanced interactions significantly reduce feelings of eeriness while fostering more natural and engaging conversations. Additionally, we identify key factors influencing user acceptance, including conversational naturalness, human-likeness, and interestingness. Based on these insights, we propose design recommendations to enhance the appeal and acceptability of hyper-realistic robots in social contexts. This research contributes to the growing field of human-robot interaction by offering empirical evidence on the potential of LLMs to bridge the uncanny valley, with implications for the future development of social robots.
<div id='section'>PaperID: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2503.14960.pdf' target='_blank'>https://arxiv.org/pdf/2503.14960.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Seungyeon Cho, Tae-Kyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14960">Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based Human Action Recognition (HAR) is a vital technology in robotics and human-robot interaction. However, most existing methods concentrate primarily on full-body movements and often overlook subtle hand motions that are critical for distinguishing fine-grained actions. Recent work leverages a unified graph representation that combines body, hand, and foot keypoints to capture detailed body dynamics. Yet, these models often blur fine hand details due to the disparity between body and hand action characteristics and the loss of subtle features during the spatial-pooling. In this paper, we propose BHaRNet (Body-Hand action Recognition Network), a novel framework that augments a typical body-expert model with a hand-expert model. Our model jointly trains both streams with an ensemble loss that fosters cooperative specialization, functioning in a manner reminiscent of a Mixture-of-Experts (MoE). Moreover, cross-attention is employed via an expertized branch method and a pooling-attention module to enable feature-level interactions and selectively fuse complementary information. Inspired by MMNet, we also demonstrate the applicability of our approach to multi-modal tasks by leveraging RGB information, where body features guide RGB learning to capture richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive actions -- while maintaining fewer GFLOPs and parameters than the relevant unified methods.
<div id='section'>PaperID: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2503.12034.pdf' target='_blank'>https://arxiv.org/pdf/2503.12034.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Enes Erdogan, Eren Erdal Aksoy, Sanem Sariel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12034">Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognition of human manipulation actions in real-time is essential for safe and effective human-robot interaction and collaboration. The challenge lies in developing a model that is both lightweight enough for real-time execution and capable of generalization. While some existing methods in the literature can run in real-time, they struggle with temporal scalability, i.e., they fail to adapt to long-duration manipulations effectively. To address this, leveraging the generalizable scene graph representations, we propose a new Factorized Graph Sequence Encoder network that not only runs in real-time but also scales effectively in the temporal dimension, thanks to its factorized encoder architecture. Additionally, we introduce Hand Pooling operation, a simple pooling operation for more focused extraction of the graph-level embeddings. Our model outperforms the previous state-of-the-art real-time approach, achieving a 14.3\% and 5.6\% improvement in F1-macro score on the KIT Bimanual Action (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively. Moreover, we conduct an extensive ablation study to validate our network design choices. Finally, we compare our model with its architecturally similar RGB-based model on the Bimacs dataset and show the limitations of this model in contrast to ours on such an object-centric manipulation dataset.
<div id='section'>PaperID: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2503.10928.pdf' target='_blank'>https://arxiv.org/pdf/2503.10928.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>David Widhalm, Cory Ohnsted, Corey Knutson, Demetrious Kutzke, Sakshi Singh, Rishi Mukherjee, Grant Schwidder, Ying-Kun Wu, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10928">Design and Development of the MeCO Open-Source Autonomous Underwater Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MeCO, the Medium Cost Open-source autonomous underwater vehicle (AUV), a versatile autonomous vehicle designed to support research and development in underwater human-robot interaction (UHRI) and marine robotics in general. An inexpensive platform to build compared to similarly-capable AUVs, the MeCO design and software are released under open-source licenses, making it a cost effective, extensible, and open platform. It is equipped with UHRI-focused systems, such as front and side facing displays, light-based communication devices, a transducer for acoustic interaction, and stereo vision, in addition to typical AUV sensing and actuation components. Additionally, MeCO is capable of real-time deep learning inference using the latest edge computing devices, while maintaining low-latency, closed-loop control through high-performance microcontrollers. MeCO is designed from the ground up for modularity in internal electronics, external payloads, and software architecture, exploiting open-source robotics and containerarization tools. We demonstrate the diverse capabilities of MeCO through simulated, closed-water, and open-water experiments. All resources necessary to build and run MeCO, including software and hardware design, have been made publicly available.
<div id='section'>PaperID: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2503.09959.pdf' target='_blank'>https://arxiv.org/pdf/2503.09959.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiansheng Li, Haotian Song, Jinni Zhou, Qiang Nie, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09959">RMG: Real-Time Expressive Motion Generation with Self-collision Avoidance for 6-DOF Companion Robotic Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The six-degree-of-freedom (6-DOF) robotic arm has gained widespread application in human-coexisting environments. While previous research has predominantly focused on functional motion generation, the critical aspect of expressive motion in human-robot interaction remains largely unexplored. This paper presents a novel real-time motion generation planner that enhances interactivity by creating expressive robotic motions between arbitrary start and end states within predefined time constraints. Our approach involves three key contributions: first, we develop a mapping algorithm to construct an expressive motion dataset derived from human dance movements; second, we train motion generation models in both Cartesian and joint spaces using this dataset; third, we introduce an optimization algorithm that guarantees smooth, collision-free motion while maintaining the intended expressive style. Experimental results demonstrate the effectiveness of our method, which can generate expressive and generalized motions in under 0.5 seconds while satisfying all specified constraints.
<div id='section'>PaperID: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2503.05152.pdf' target='_blank'>https://arxiv.org/pdf/2503.05152.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonetani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05152">GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to image-goal navigation by integrating 3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal navigation by guiding a robot through a sequence of point-of-view images without requiring metrical localization or environment-specific training. However, constructing a dense and traversable sequence of target viewpoints from start to goal remains a central challenge, particularly when the available image database is sparse. To address these challenges, we propose a 3DGS-based viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints to seamlessly bridge gaps in sparse data while significantly reducing storage overhead. Experimental results in a photorealistic simulator demonstrate that our approach not only enhances navigation efficiency but also exhibits robustness under varying levels of image database sparsity.
<div id='section'>PaperID: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2502.02772.pdf' target='_blank'>https://arxiv.org/pdf/2502.02772.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ravi Tejwani, Karl Velazquez, John Payne, Paolo Bonato, Harry Asada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02772">Cross-modality Force and Language Embeddings for Natural Human-Robot Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A method for cross-modality embedding of force profile and words is presented for synergistic coordination of verbal and haptic communication. When two people carry a large, heavy object together, they coordinate through verbal communication about the intended movements and physical forces applied to the object. This natural integration of verbal and physical cues enables effective coordination. Similarly, human-robot interaction could achieve this level of coordination by integrating verbal and haptic communication modalities. This paper presents a framework for embedding words and force profiles in a unified manner, so that the two communication modalities can be integrated and coordinated in a way that is effective and synergistic. Here, it will be shown that, although language and physical force profiles are deemed completely different, the two can be embedded in a unified latent space and proximity between the two can be quantified. In this latent space, a force profile and words can a) supplement each other, b) integrate the individual effects, and c) substitute in an exchangeable manner. First, the need for cross-modality embedding is addressed, and the basic architecture and key building block technologies are presented. Methods for data collection and implementation challenges will be addressed, followed by experimental results and discussions.
<div id='section'>PaperID: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2502.01256.pdf' target='_blank'>https://arxiv.org/pdf/2502.01256.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rajashekhar V S, Gowdham Prabhakar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01256">Soft is Safe: Human-Robot Interaction for Soft Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the presence of robots increasing in the society, the need for interacting with robots is becoming necessary. The field of Human-Robot Interaction (HRI) has emerged important since more repetitive and tiresome jobs are being done by robots. In the recent times, the field of soft robotics has seen a boom in the field of research and commercialization. The Industry 5.0 focuses on human robot collaboration which also spurs the field of soft robotics. However the HRI for soft robotics is still in the nascent stage. In this work we review and then discuss how HRI is done for soft robots. We first discuss the control, design, materials and manufacturing of soft robots. This will provide an understanding of what is being interacted with. Then we discuss about the various input and output modalities that are used in HRI. The applications where the HRI for soft robots are found in the literature are discussed in detail. Then the limitations of HRI for soft robots and various research opportunities that exist in this field are discussed in detail. It is concluded that there is a huge scope for development for HRI for soft robots.
<div id='section'>PaperID: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2501.11887.pdf' target='_blank'>https://arxiv.org/pdf/2501.11887.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ting-Han Lin, Hannah Dinner, Tsz Long Leung, Bilge Mutlu, J. Gregory Trafton, Sarah Sebo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11887">Connection-Coordination Rapport (CCR) Scale: A Dual-Factor Scale to Measure Human-Robot Rapport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots, particularly in service and companionship roles, must develop positive relationships with people they interact with regularly to be successful. These positive human-robot relationships can be characterized as establishing "rapport," which indicates mutual understanding and interpersonal connection that form the groundwork for successful long-term human-robot interaction. However, the human-robot interaction research literature lacks scale instruments to assess human-robot rapport in a variety of situations. In this work, we developed the 18-item Connection-Coordination Rapport (CCR) Scale to measure human-robot rapport. We first ran Study 1 (N = 288) where online participants rated videos of human-robot interactions using a set of candidate items. Our Study 1 results showed the discovery of two factors in our scale, which we named "Connection" and "Coordination." We then evaluated this scale by running Study 2 (N = 201) where online participants rated a new set of human-robot interaction videos with our scale and an existing rapport scale from virtual agents research for comparison. We also validated our scale by replicating a prior in-person human-robot interaction study, Study 3 (N = 44), and found that rapport is rated significantly greater when participants interacted with a responsive robot (responsive condition) as opposed to an unresponsive robot (unresponsive condition). Results from these studies demonstrate high reliability and validity for the CCR scale, which can be used to measure rapport in both first-person and third-person perspectives. We encourage the adoption of this scale in future studies to measure rapport in a variety of human-robot interactions.
<div id='section'>PaperID: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2412.16093.pdf' target='_blank'>https://arxiv.org/pdf/2412.16093.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Massimiliano Nigro, Emmanuel Akinrintoyo, Nicole Salomons, Micol Spitale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16093">Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Group interactions are a natural part of our daily life, and as robots become more integrated into society, they must be able to socially interact with multiple people at the same time. However, group human-robot interaction (HRI) poses unique computational challenges often overlooked in the current HRI literature. We conducted a scoping review including 44 group HRI papers from the last decade (2015-2024). From these papers, we extracted variables related to perception and behaviour generation challenges, as well as factors related to the environment, group, and robot capabilities that influence these challenges. Our findings show that key computational challenges in perception included detection of groups, engagement, and conversation information, while challenges in behaviour generation involved developing approaching and conversational behaviours. We also identified research gaps, such as improving detection of subgroups and interpersonal relationships, and recommended future work in group HRI to help researchers address these computational challenges
<div id='section'>PaperID: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2412.13569.pdf' target='_blank'>https://arxiv.org/pdf/2412.13569.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sithu Aung, Min-Cheol Sagong, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13569">Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address an advanced challenge of predicting pedestrian occupancy as an extension of multi-view pedestrian detection in urban traffic. To support this, we have created a new synthetic dataset called MVP-Occ, designed for dense pedestrian scenarios in large-scale scenes. Our dataset provides detailed representations of pedestrians using voxel structures, accompanied by rich semantic scene understanding labels, facilitating visual navigation and insights into pedestrian spatial information. Furthermore, we present a robust baseline model, termed OmniOcc, capable of predicting both the voxel occupancy state and panoptic labels for the entire scene from multi-view images. Through in-depth analysis, we identify and evaluate the key elements of our proposed model, highlighting their specific contributions and importance.
<div id='section'>PaperID: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2412.06227.pdf' target='_blank'>https://arxiv.org/pdf/2412.06227.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marsha Mariya Kappan, Eduardo Benitez Sandoval, Erik Meijering, Francisco Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06227">Attention-Enhanced Lightweight Hourglass Network for Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation is a critical task in computer vision with a wide range of applications from activity monitoring to human-robot interaction. However,most of the existing methods are computationally expensive or have complex architecture. Here we propose a lightweight attention based pose estimation network that utilizes depthwise separable convolution and Convolutional Block Attention Module on an hourglass backbone. The network significantly reduces the computational complexity (floating point operations) and the model size (number of parameters) containing only about 10% of parameters of original eight stack Hourglass network. Experiments were conducted on COCO and MPII datasets using a two stack hourglass backbone. The results showed that our model performs well in comparison to six other lightweight pose estimation models with an average precision of 72.07. The model achieves this performance with only 2.3M parameters and 3.7G FLOPs.
<div id='section'>PaperID: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2412.04908.pdf' target='_blank'>https://arxiv.org/pdf/2412.04908.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohammed Althubyani, Zhijin Meng, Shengyuan Xie, Cha Seung, Imran Razzak, Eduardo B. Sandoval, Baki Kocaballi, Francisco Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04908">MERCI: Multimodal Emotional and peRsonal Conversational Interactions Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of conversational agents into our daily lives has become increasingly common, yet many of these agents cannot engage in deep interactions with humans. Despite this, there is a noticeable shortage of datasets that capture multimodal information from human-robot interaction dialogues. To address this gap, we have recorded a novel multimodal dataset (MERCI) that encompasses rich embodied interaction data. The process involved asking participants to complete a questionnaire and gathering their profiles on ten topics, such as hobbies and favorite music. Subsequently, we initiated conversations between the robot and the participants, leveraging GPT-4 to generate contextually appropriate responses based on the participant's profile and emotional state, as determined by facial expression recognition and sentiment analysis. Automatic and user evaluations were conducted to assess the overall quality of the collected data. The results of both evaluations indicated a high level of naturalness, engagement, fluency, consistency, and relevance in the conversation, as well as the robot's ability to provide empathetic responses. It is worth noting that the dataset is derived from genuine interactions with the robot, involving participants who provided personal information and conveyed actual emotions.
<div id='section'>PaperID: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2412.00435.pdf' target='_blank'>https://arxiv.org/pdf/2412.00435.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shipeng Liu, Boshen Zhang, Zhehui Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00435">Benchmark Real-time Adaptation and Communication Capabilities of Embodied Agent in Collaborative Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in Large Language Models (LLMs) have opened transformative possibilities for human-robot interaction, especially in collaborative environments. However, Real-time human-AI collaboration requires agents to adapt to unseen human behaviors while maintaining effective communication dynamically. Existing benchmarks fall short in evaluating such adaptability for embodied agents, focusing mostly on the task performance of the agent itself. To address this gap, we propose a novel benchmark that assesses agents' reactive adaptability and instantaneous communication capabilities at every step. Based on this benchmark, we propose a Monitor-then-Adapt framework (MonTA), combining strong adaptability and communication with real-time execution. MonTA contains three key LLM modules, a lightweight \textit{Monitor} for monitoring the need for adaptation in high frequency, and two proficient \textit{Adapters} for subtask and path adaptation reasoning in low frequency. Our results demonstrate that MonTA outperforms other baseline agents on our proposed benchmark. Further user studies confirm the high reasonability adaptation plan and consistent language instruction provided by our framework.
<div id='section'>PaperID: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2411.13851.pdf' target='_blank'>https://arxiv.org/pdf/2411.13851.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyou Pei, Alexander Chen, Ronak Kaoshik, Ruofei Du, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13851">Arm Robot: AR-Enhanced Embodied Control and Visualization for Intuitive Robot Arm Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied interaction has been introduced to human-robot interaction (HRI) as a type of teleoperation, in which users control robot arms with bodily action via handheld controllers or haptic gloves. Embodied teleoperation has made robot control intuitive to non-technical users, but differences between humans' and robots' capabilities \eg ranges of motion and response time, remain challenging. In response, we present Arm Robot, an embodied robot arm teleoperation system that helps users tackle human-robot discrepancies. Specifically, Arm Robot (1) includes AR visualization as real-time feedback on temporal and spatial discrepancies, and (2) allows users to change observing perspectives and expand action space. We conducted a user study (N=18) to investigate the usability of the Arm Robot and learn how users perceive the embodiment. Our results show users could use Arm Robot's features to effectively control the robot arm, providing insights for continued work in embodied HRI.
<div id='section'>PaperID: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2411.03555.pdf' target='_blank'>https://arxiv.org/pdf/2411.03555.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michael BÃ¼ttner, Jonathan Francis, Helge Rhodin, Andrew Melnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03555">Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.
<div id='section'>PaperID: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2410.23234.pdf' target='_blank'>https://arxiv.org/pdf/2410.23234.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Peide Huang, Yuhan Hu, Nataliya Nechyporenko, Daehwa Kim, Walter Talbott, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23234">EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in humanlike non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of human non-verbal communication. To address this gap, our approach leverages the in-context learning capability of large language models (LLMs) to dynamically generate socially appropriate gesture motion sequences for human-robot interaction. We use this framework to generate 10 different expressive gestures and conduct online user studies comparing the naturalness and understandability of the motions generated by EMOTION and its human-feedback version, EMOTION++, against those by human operators. The results demonstrate that our approach either matches or surpasses human performance in generating understandable and natural robot motions under certain scenarios. We also provide design implications for future research to consider a set of variables when generating expressive robotic gestures.
<div id='section'>PaperID: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2410.21197.pdf' target='_blank'>https://arxiv.org/pdf/2410.21197.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ritam Ghosh, Nibraas Khan, Miroslava Migovich, Judith A. Tate, Cathy Maxwell, Emily Latshaw, Paul Newhouse, Douglas W. Scharre, Alai Tan, Kelley Colopietro, Lorraine C. Mion, Nilanjan Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21197">User-Centered Design of Socially Assistive Robotic Combined with Non-Immersive Virtual Reality-based Dyadic Activities for Older Adults Residing in Long Term Care Facilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Apathy impairs the quality of life for older adults and their care providers. While few pharmacological remedies exist, current non-pharmacologic approaches are resource intensive. To address these concerns, this study utilizes a user-centered design (UCD) process to develop and test a set of dyadic activities that provide physical, cognitive, and social stimuli to older adults residing in long-term care (LTC) communities. Within the design, a novel framework that combines socially assistive robots and non-immersive virtual reality (SAR-VR) emphasizing human-robot interaction (HRI) and human-computer interaction (HCI) is utilized with the roles of the robots being coach and entertainer. An interdisciplinary team of engineers, nurses, and physicians collaborated with an advisory panel comprising LTC activity coordinators, staff, and residents to prototype the activities. The study resulted in four virtual activities: three with the humanoid robot, Nao, and one with the animal robot, Aibo. Fourteen participants tested the acceptability of the different components of the system and provided feedback at different stages of development. Participant approval increased significantly over successive iterations of the system highlighting the importance of stakeholder feedback. Five LTC staff members successfully set up the system with minimal help from the researchers, demonstrating the usability of the system for caregivers. Rationale for activity selection, design changes, and both quantitative and qualitative results on the acceptability and usability of the system have been presented. The paper discusses the challenges encountered in developing activities for older adults in LTCs and underscores the necessity of the UCD process to address them.
<div id='section'>PaperID: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2410.19072.pdf' target='_blank'>https://arxiv.org/pdf/2410.19072.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tyson Jordan, Pranav Pandey, Prashant Doshi, Ramviyas Parasuraman, Adam Goodie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19072">Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation Scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is promising, but existing implementations overlook key insights from human-robot interaction (HRI) research. Studies on human-machine teams demonstrate that human perceptions of a machine teammate are critical in governing the machine's performance. Here, we present a mixed factorial design to assess human perceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants were assigned to the role of casualty (CAS) or bystander (BYS) and subjected to three within-subjects conditions based on the MEDEVAC robot's operating mode: autonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each trial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and transporting them to an ambulance exchange point while avoiding an idle bystander. Following each trial, subjects completed a questionnaire measuring their emotional states, perceived safety, and social compatibility with the robot. Results indicate a consistent main effect of operating mode on reported emotional states and perceived safety. Pairwise analyses suggest that the employment of the AF operating mode negatively impacted perceptions along these dimensions. There were no persistent differences between casualty and bystander responses.
<div id='section'>PaperID: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2410.13957.pdf' target='_blank'>https://arxiv.org/pdf/2410.13957.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13957">Goal Inference from Open-Ended Dialog</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI Agents are quickly becoming important and common tools in society. These embodied agents should be able to learn about and accomplish a wide range of user goals and preferences efficiently and robustly. Large Language Models (LLMs) are often used as they allow for opportunities for rich and open-ended dialog type interaction between the human and agent to accomplish tasks according to human preferences. In this thesis, we argue that for embodied agents that deal with open-ended dialog during task assistance: 1) AI Agents should extract goals from conversations in the form of Natural Language (NL) to be better at capturing human preferences as it is intuitive for humans to communicate their preferences on tasks to agents through natural language. 2) AI Agents should quantify/maintain uncertainty about these goals to ensure that actions are being taken according to goals that the agent is extremely certain about. We present an online method for embodied agents to learn and accomplish diverse user goals. While offline methods like RLHF can represent various goals but require large datasets, our approach achieves similar flexibility with online efficiency. We extract natural language goal representations from conversations with Large Language Models (LLMs). We prompt an LLM to role play as a human with different goals and use the corresponding likelihoods to run Bayesian inference over potential goals. As a result, our method can represent uncertainty over complex goals based on unrestricted dialog. We evaluate in a text-based grocery shopping domain and an AI2Thor robot simulation. We compare our method to ablation baselines that lack either explicit goal representation or probabilistic inference.
<div id='section'>PaperID: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2410.11085.pdf' target='_blank'>https://arxiv.org/pdf/2410.11085.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shahira Ali, Haley N. Green, Tariq Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11085">What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot's perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot. The results suggest that while language fluency may play a more significant role than task competency in the perception of the verbal competency of a robot, both language fluency and task competency contribute to the perception of the intelligence and reliability of the robot. The results also indicate that task competency may play a more significant role than language fluency in the perception of meeting expectations and being a good teammate. The findings of this study highlight the relationship between language fluency and task competency in the context of social HRI and will enable the development of more intelligent robots in the future.
<div id='section'>PaperID: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2410.09081.pdf' target='_blank'>https://arxiv.org/pdf/2410.09081.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nuri Kim, Jeongho Park, Mineui Hong, Songhwai Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09081">Semantic Environment Atlas for Object-Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Semantic Environment Atlas (SEA), a novel mapping approach designed to enhance visual navigation capabilities of embodied agents. The SEA utilizes semantic graph maps that intricately delineate the relationships between places and objects, thereby enriching the navigational context. These maps are constructed from image observations and capture visual landmarks as sparsely encoded nodes within the environment. The SEA integrates multiple semantic maps from various environments, retaining a memory of place-object relationships, which proves invaluable for tasks such as visual localization and navigation. We developed navigation frameworks that effectively leverage the SEA, and we evaluated these frameworks through visual localization and object-goal navigation tasks. Our SEA-based localization framework significantly outperforms existing methods, accurately identifying locations from single query images. Experimental results in Habitat scenarios show that our method not only achieves a success rate of 39.0%, an improvement of 12.4% over the current state-of-the-art, but also maintains robustness under noisy odometry and actuation conditions, all while keeping computational costs low.
<div id='section'>PaperID: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2409.07560.pdf' target='_blank'>https://arxiv.org/pdf/2409.07560.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Houston Claure, Kate Candon, Inyoung Shin, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07560">Dynamic Fairness Perceptions in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People deeply care about how fairly they are treated by robots. The established paradigm for probing fairness in Human-Robot Interaction (HRI) involves measuring the perception of the fairness of a robot at the conclusion of an interaction. However, such an approach is limited as interactions vary over time, potentially causing changes in fairness perceptions as well. To validate this idea, we conducted a 2x2 user study with a mixed design (N=40) where we investigated two factors: the timing of unfair robot actions (early or late in an interaction) and the beneficiary of those actions (either another robot or the participant). Our results show that fairness judgments are not static. They can shift based on the timing of unfair robot actions. Further, we explored using perceptions of three key factors (reduced welfare, conduct, and moral transgression) proposed by a Fairness Theory from Organizational Justice to predict momentary perceptions of fairness in our study. Interestingly, we found that the reduced welfare and moral transgression factors were better predictors than all factors together. Our findings reinforce the idea that unfair robot behavior can shape perceptions of group dynamics and trust towards a robot and pave the path to future research directions on moment-to-moment fairness perceptions
<div id='section'>PaperID: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2409.06503.pdf' target='_blank'>https://arxiv.org/pdf/2409.06503.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sajjad Hussain, Khizer Saeed, Almas Baimagambetov, Shanay Rab, Md Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06503">Advancements in Gesture Recognition Techniques and Machine Learning for Enhanced Human-Robot Interaction: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years robots have become an important part of our day-to-day lives with various applications. Human-robot interaction creates a positive impact in the field of robotics to interact and communicate with the robots. Gesture recognition techniques combined with machine learning algorithms have shown remarkable progress in recent years, particularly in human-robot interaction (HRI). This paper comprehensively reviews the latest advancements in gesture recognition methods and their integration with machine learning approaches to enhance HRI. Furthermore, this paper represents the vision-based gesture recognition for safe and reliable human-robot-interaction with a depth-sensing system, analyses the role of machine learning algorithms such as deep learning, reinforcement learning, and transfer learning in improving the accuracy and robustness of gesture recognition systems for effective communication between humans and robots.
<div id='section'>PaperID: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2409.02522.pdf' target='_blank'>https://arxiv.org/pdf/2409.02522.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiyuan Li, Yanfeng Lu, Yao Mu, Hong Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02522">Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.
<div id='section'>PaperID: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2602.15684.pdf' target='_blank'>https://arxiv.org/pdf/2602.15684.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Feras Kiki, Pouya P. Niaz, Alireza Madani, Cagatay Basdogan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15684">Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.
<div id='section'>PaperID: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2602.10698.pdf' target='_blank'>https://arxiv.org/pdf/2602.10698.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhifeng Rao, Wenlong Chen, Lei Xie, Xia Hua, Dongfu Yin, Zhen Tian, F. Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.10698">AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.
<div id='section'>PaperID: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2602.07243.pdf' target='_blank'>https://arxiv.org/pdf/2602.07243.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siddharth Singh, Ifrah Idrees, Abraham Dauhajre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.07243">Realistic Synthetic Household Data Generation at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions. The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation. We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.
<div id='section'>PaperID: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2602.02787.pdf' target='_blank'>https://arxiv.org/pdf/2602.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simran Saxena, Arpad Kovesdy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.02787">Real-World Applications of AI in LTE and 5G-NR Network Infrastructure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Telecommunications networks generate extensive performance and environmental telemetry, yet most LTE and 5G-NR deployments still rely on static, manually engineered configurations. This limits adaptability in rural, nomadic, and bandwidth-constrained environments where traffic distributions, propagation characteristics, and user behavior fluctuate rapidly. Artificial Intelligence (AI), more specifically Machine Learning (ML) models, provide new opportunities to transition Radio Access Networks (RANs) from rigid, rule-based systems toward adaptive, self-optimizing infrastructures that can respond autonomously to these dynamics. This paper proposes a practical architecture incorporating AI-assisted planning, reinforcement-learning-based RAN optimization, real-time telemetry analytics, and digital-twin-based validation. In parallel, the paper addresses the challenge of delivering embodied-AI healthcare services, educational tools, and large language model (LLM) applications to communities with insufficient backhaul for cloud computing. We introduce an edge-hosted execution model in which applications run directly on LTE/5G-NR base stations using containers, reducing latency and bandwidth consumption while improving resilience. Together, these contributions demonstrate how AI can enhance network performance, reduce operational overhead, and expand access to advanced digital services, aligning with broader goals of sustainable and inclusive network development.
<div id='section'>PaperID: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2601.22948.pdf' target='_blank'>https://arxiv.org/pdf/2601.22948.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nicola Milano, Stefano Nolfi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.22948">Alignment among Language, Vision and Action Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.
<div id='section'>PaperID: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2601.17404.pdf' target='_blank'>https://arxiv.org/pdf/2601.17404.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anke Fischer-Janzen, Thomas M. Wendt, Kristof Van Laerhoven
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17404">Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.
<div id='section'>PaperID: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2601.13252.pdf' target='_blank'>https://arxiv.org/pdf/2601.13252.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mahmud S. Zango, Jianglin Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.13252">Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging "Edge AI" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the "Sim-to-Real" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.
<div id='section'>PaperID: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2601.01067.pdf' target='_blank'>https://arxiv.org/pdf/2601.01067.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenzheng Zhang, Yoshitaka Hara, Sousuke Nakamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01067">Topological Mapping and Navigation using a Monocular Camera based on AnyLoc</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.
<div id='section'>PaperID: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2512.24029.pdf' target='_blank'>https://arxiv.org/pdf/2512.24029.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Takashi Yamamoto, Hiroaki Yaguchi, Shohei Kato, Hiroyuki Okada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24029">Evaluation of Impression Difference of a Domestic Mobile Manipulator with Autonomous and/or Remote Control in Fetch-and-Carry Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A single service robot can present two distinct agencies: its onboard autonomy and an operator-mediated agency, yet users experience them through one physical body. We formalize this dual-agency structure as a User-Robot-Operator triad in an autonomous remote-control setting that integrates teleoperation with autonomous execution and human-in-the-loop remote assistance. Prior to the recent surge of language-based and multimodal interfaces, we developed and evaluated an early-stage prototype in 2020 that combined natural-language text chat with a sketch-based interface enabling freehand on-image annotation over the robot's live camera view to support remote intervention. We evaluated three modes - remote control via teleoperation, autonomous control, and autonomous remote control (a hybrid mode representing different levels of autonomy) - in controlled fetch-and-carry mobile manipulation tasks using a domestic mobile manipulator, the Human Support Robot (HSR), on a World Robot Summit 2020 rule-compliant test field. The results show systematic mode-dependent differences in user-rated affinity and perceived security, indicating that switching or blending agency within one robot measurably shapes human impressions in Human-Robot Interaction (HRI). These findings provide empirical guidance for designing human-in-the-loop mobile manipulation in domestic physical tasks.
<div id='section'>PaperID: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2512.19024.pdf' target='_blank'>https://arxiv.org/pdf/2512.19024.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19024">IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \textbf{IndoorUAV-VLA} subset. Finally, we introduce \textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.
<div id='section'>PaperID: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2512.11472.pdf' target='_blank'>https://arxiv.org/pdf/2512.11472.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>David Wagmann, Matti Krüger, Chao Wang, Jürgen Steimle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11472">Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions.
<div id='section'>PaperID: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2512.11234.pdf' target='_blank'>https://arxiv.org/pdf/2512.11234.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wentang Chen, Shougao Zhang, Yiman Zhang, Tianhao Zhou, Ruihui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11234">RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.
<div id='section'>PaperID: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2512.10235.pdf' target='_blank'>https://arxiv.org/pdf/2512.10235.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hui Li, Akhlak Uz Zaman, Fujian Yan, Hongsheng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10235">Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.
<div id='section'>PaperID: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2512.09592.pdf' target='_blank'>https://arxiv.org/pdf/2512.09592.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Wang, Qijin Song, Yucen Peng, Weibang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09592">CS3D: An Efficient Facial Expression Recognition via Event Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.
<div id='section'>PaperID: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2512.08481.pdf' target='_blank'>https://arxiv.org/pdf/2512.08481.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yixiang Lin, Tiancheng Yang, Jonathan Eden, Ying Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08481">Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\% to 90\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a "trade-off" group that modulated their physical responses according to disturbance likelihood, and an "always-compensate" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.
<div id='section'>PaperID: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2512.02020.pdf' target='_blank'>https://arxiv.org/pdf/2512.02020.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianlei Chang, Ruofeng Mei, Wei Ke, Xiangyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02020">EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
<div id='section'>PaperID: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2512.00883.pdf' target='_blank'>https://arxiv.org/pdf/2512.00883.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahua Wang, Shannan Yan, Leqi Zheng, Jialong Wu, Yaoxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00883">Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.
<div id='section'>PaperID: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2512.00048.pdf' target='_blank'>https://arxiv.org/pdf/2512.00048.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenzheng Zhao, Ran Zhang, Ruth Palan Lopez, Shu-Fen Wung, Fengpei Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00048">Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) faces significant challenges in adaptive healthcare interventions, such as dementia care, where data is scarce, decisions require interpretability, and underlying patient-state dynamic are complex and causal in nature. In this work, we present a novel framework called Causal structure-aware Reinforcement Learning (CRL) that explicitly integrates causal discovery and reasoning into policy optimization. This method enables an agent to learn and exploit a directed acyclic graph (DAG) that describes the causal dependencies between human behavioral states and robot actions, facilitating more efficient, interpretable, and robust decision-making. We validate our approach in a simulated robot-assisted cognitive care scenario, where the agent interacts with a virtual patient exhibiting dynamic emotional, cognitive, and engagement states. The experimental results show that CRL agents outperform conventional model-free RL baselines by achieving higher cumulative rewards, maintaining desirable patient states more consistently, and exhibiting interpretable, clinically-aligned behavior. We further demonstrate that CRL's performance advantage remains robust across different weighting strategies and hyperparameter settings. In addition, we demonstrate a lightweight LLM-based deployment: a fixed policy is embedded into a system prompt that maps inferred states to actions, producing consistent, supportive dialogue without LLM finetuning. Our work illustrates the promise of causal reinforcement learning for human-robot interaction applications, where interpretability, adaptiveness, and data efficiency are paramount.
<div id='section'>PaperID: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2512.00027.pdf' target='_blank'>https://arxiv.org/pdf/2512.00027.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nivedan Yakolli, Avinash Gautam, Abhijit Das, Yuankai Qi, Virendra Singh Shekhawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00027">A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.
<div id='section'>PaperID: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2511.21848.pdf' target='_blank'>https://arxiv.org/pdf/2511.21848.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Eric Leonardis, Akira Nagamori, Ayesha Thanawalla, Yuanjia Yang, Joshua Park, Hutton Saunders, Eiman Azim, Talmo Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21848">Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.
<div id='section'>PaperID: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2511.18960.pdf' target='_blank'>https://arxiv.org/pdf/2511.18960.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lei Xiao, Jifeng Li, Juntao Gao, Feiyang Ye, Yan Jin, Jingjing Qian, Jing Zhang, Yong Wu, Xiaoyuan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18960">AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.
<div id='section'>PaperID: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2511.18950.pdf' target='_blank'>https://arxiv.org/pdf/2511.18950.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juntao Gao, Feiyang Ye, Jing Zhang, Wenjing Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18950">Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.
<div id='section'>PaperID: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2511.11298.pdf' target='_blank'>https://arxiv.org/pdf/2511.11298.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yihao Zhang, Yuankai Qi, Xi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11298">Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models applied in robotics, particularly \textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \textbf{empirical experiences} from benchmarking four representative VLAs -- \textbf{ACT}, \textbf{OpenVLA--OFT}, \textbf{RDT-1B}, and \boldmath{$π_0$} -- across four manipulation tasks conducted in both simulation and on the \textbf{ALOHA Mobile} platform. We establish a \textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \textit{accuracy and efficiency} (success rate and time-to-success), (2) \textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \textit{language instruction-following accuracy}. Through this process, we observe that \boldmath{$π_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.
<div id='section'>PaperID: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2511.09964.pdf' target='_blank'>https://arxiv.org/pdf/2511.09964.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Noah van der Vleuten, Anthony Flores, Shray Mathur, Max Rakitin, Thomas Hopkins, Kevin G. Yager, Esther H. R. Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09964">EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.
<div id='section'>PaperID: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2511.04976.pdf' target='_blank'>https://arxiv.org/pdf/2511.04976.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Nie, Zhiyuan Cheng, Yuan Zhang, Chao Ji, Jiajia Wu, Yuhan Zhang, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04976">iFlyBot-VLM Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used to improve the domain of Embodied Intelligence. The central objective of iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control. To this end, the model abstracts complex visual and spatial information into a body-agnostic and transferable Operational Language, thereby enabling seamless perception-action closed-loop coordination across diverse robotic platforms. The architecture of iFlyBot-VLM is systematically designed to realize four key functional capabilities essential for embodied intelligence: 1) Spatial Understanding and Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and Control Parameter Generation; 4) Task Planning and Skill Sequencing. We envision iFlyBot-VLM as a scalable and generalizable foundation model for embodied AI, facilitating the progression from specialized task-oriented systems toward generalist, cognitively capable agents. We conducted evaluations on 10 current mainstream embodied intelligence-related VLM benchmark datasets, such as Blink and Where2Place, and achieved optimal performance while preserving the model's general capabilities. We will publicly release both the training data and model weights to foster further research and development in the field of Embodied Intelligence.
<div id='section'>PaperID: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2510.27436.pdf' target='_blank'>https://arxiv.org/pdf/2510.27436.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tomoko Yonezawa, Hirotake Yamazoe, Atsuo Fujino, Daigo Suhara, Takaya Tamamoto, Yuto Nishiguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27436">Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction frequently involves physical proximity or contact. In human-human settings, people flexibly accept, reject, or tolerate such approaches depending on the relationship and context. We explore the design of a robot's rejective internal state and corresponding avoidance behaviors, such as withdrawing or pushing away, when a person approaches. We model the accumulation and decay of discomfort as a function of interpersonal distance, and implement tolerance (endurance) and limit-exceeding avoidance driven by the Dominance axis of the PAD affect model. The behaviors and their intensities are realized on an arm robot. Results illustrate a coherent pipeline from internal state parameters to graded endurance motions and, once a limit is crossed, to avoidance actions.
<div id='section'>PaperID: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2510.25713.pdf' target='_blank'>https://arxiv.org/pdf/2510.25713.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Boshi An, Chenyu Yang, Robert Katzschmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25713">Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for dexterous human-robot collaboration with minimal language prompting. Our approach adds (i) FiLM conditioning to visual backbones for task-aware perception, (ii) an auxiliary intent head that predicts collaborator hand pose and target cues, and (iii) action-space post-processing that predicts compact deltas (position/rotation) and PCA-reduced finger joints before mapping to full commands. Using a multi-view, teleoperated Franka and Mimic-hand dataset augmented with MediaPipe hand poses, we demonstrate that delta actions are well-behaved and that four principal components explain ~96% of hand-joint variance. Ablations identify action post-processing as the primary performance driver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is detrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes "pick-up" and "pass" into a long-horizon behavior. We surface "trainer overfitting" to specific demonstrators as the key limitation.
<div id='section'>PaperID: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2510.23902.pdf' target='_blank'>https://arxiv.org/pdf/2510.23902.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jans Solano, Diego Quiroz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23902">Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheeled-legged robots combine the efficiency of wheels with the obstacle negotiation of legs, yet many state-of-the-art systems rely on costly actuators and sensors, and fall-recovery is seldom integrated, especially for wheeled-legged morphologies. This work presents a recovery-aware visual-inertial navigation system on a low-cost wheeled quadruped. The proposed system leverages vision-based perception from a depth camera and deep reinforcement learning policies for robust locomotion and autonomous recovery from falls across diverse terrains. Simulation experiments show agile mobility with low-torque actuators over irregular terrain and reliably recover from external perturbations and self-induced failures. We further show goal directed navigation in structured indoor spaces with low-cost perception. Overall, this approach lowers the barrier to deploying autonomous navigation and robust locomotion policies in budget-constrained robotic platforms.
<div id='section'>PaperID: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2510.22113.pdf' target='_blank'>https://arxiv.org/pdf/2510.22113.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zitiantao Lin, Yongpeng Sang, Yang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22113">RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulators are increasingly used to assist individuals with mobility impairments in object retrieval. However, the predominant joystick-based control interfaces can be challenging due to high precision requirements and unintuitive reference frames. Recent advances in human-robot interaction have explored alternative modalities, yet many solutions still rely on external screens or restrictive control schemes, limiting their intuitiveness and accessibility. To address these challenges, we present an egocentric, gaze-guided robotic manipulation interface that leverages a wearable Mixed Reality (MR) headset. Our system enables users to interact seamlessly with real-world objects using natural gaze fixation from a first-person perspective, while providing augmented visual cues to confirm intent and leveraging a pretrained vision model and robotic arm for intent recognition and object manipulation. Experimental results demonstrate that our approach significantly improves manipulation accuracy, reduces system latency, and achieves single-pass intention and object recognition accuracy greater than 88% across multiple real-world scenarios. These results demonstrate the system's effectiveness in enhancing intuitiveness and accessibility, underscoring its practical significance for assistive robotics applications.
<div id='section'>PaperID: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2510.18373.pdf' target='_blank'>https://arxiv.org/pdf/2510.18373.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wanchen Li, Kahina Chalabi, Sabbah Maxime, Thomas Bousquet, Robin Passama, Sofiane Ramdani, Andrea Cherubini, Vincent Bonnet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18373">Biomechanically consistent real-time action recognition for human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework for real-time human action recognition in industrial contexts, using standard 2D cameras. We introduce a complete pipeline for robust and real-time estimation of human joint kinematics, input to a temporally smoothed Transformer-based network, for action recognition. We rely on a new dataset including 11 subjects performing various actions, to evaluate our approach. Unlike most of the literature that relies on joint center positions (JCP) and is offline, ours uses biomechanical prior, eg. joint angles, for fast and robust real-time recognition. Besides, joint angles make the proposed method agnostic to sensor and subject poses as well as to anthropometric differences, and ensure robustness across environments and subjects. Our proposed learning model outperforms the best baseline model, running also in real-time, along various metrics. It achieves 88% accuracy and shows great generalization ability, for subjects not facing the cameras. Finally, we demonstrate the robustness and usefulness of our technique, through an online interaction experiment, with a simulated robot controlled in real-time via the recognized actions.
<div id='section'>PaperID: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2508.16622.pdf' target='_blank'>https://arxiv.org/pdf/2508.16622.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrew Blair, Peggy Gregory, Mary Ellen Foster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16622">Observations of atypical users from a pilot deployment of a public-space social robot in a church</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though a goal of HRI is the natural integration of social robots into everyday public spaces, real-world studies still occur mostly within controlled environments with predetermined participants. True public spaces present an environment which is largely unconstrained and unpredictable, frequented by a diverse range of people whose goals can often conflict with those of the robot. When combined with the general unfamiliarity most people have with social robots, this leads to unexpected human-robot interactions in these public spaces that are rarely discussed or detected in other contexts. In this paper, we describe atypical users we observed interacting with our robot, and those who did not, during a three-day pilot deployment within a large working church and visitor attraction. We then discuss theoretical future advances in the field that could address these challenges, as well as immediate practical mitigations and strategies to help improve public space human-robot interactions in the present. This work contributes empirical insights into the dynamics of human-robot interaction in public environments and offers actionable guidance for more effective future deployments for social robot designers.
<div id='section'>PaperID: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2508.13901.pdf' target='_blank'>https://arxiv.org/pdf/2508.13901.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yihao Lu, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13901">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional management systems are ill-equipped to handle. In this survey, we first systematically evaluate five storage architectures (Graph Databases, Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series Databases), focusing on their suitability for addressing EAI's core requirements, including physical grounding, low-latency access, and dynamic scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based Optimization), revealing a fundamental tension between achieving long-term semantic coherence and maintaining real-time responsiveness. Based on this comprehensive analysis, we identify key bottlenecks, spanning from the foundational Physical Grounding Gap to systemic challenges in cross-modal integration, dynamic adaptation, and open-world generalization. Finally, we outline a forward-looking research agenda encompassing physics-aware data models, adaptive storage-retrieval co-optimization, and standardized benchmarking, to guide future research toward principled data management solutions for EAI. Our survey is based on a comprehensive review of more than 180 related studies, providing a rigorous roadmap for designing the robust, high-performance data management frameworks essential for the next generation of autonomous embodied systems.
<div id='section'>PaperID: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2508.10399.pdf' target='_blank'>https://arxiv.org/pdf/2508.10399.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, Ping Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10399">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.
<div id='section'>PaperID: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2508.05855.pdf' target='_blank'>https://arxiv.org/pdf/2508.05855.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zixia Wang, Jia Hu, Ronghui Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05855">Safety of Embodied Navigation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.
<div id='section'>PaperID: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2508.01736.pdf' target='_blank'>https://arxiv.org/pdf/2508.01736.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tyrone Justin Sta Maria, Faith Griffin, Jordan Aiko Deja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01736">Set the Stage: Enabling Storytelling with Multiple Robots through Roleplaying Metaphors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are an expressive input modality for controlling multiple robots, but their use is often limited by rigid mappings and recognition constraints. To move beyond these limitations, we propose roleplaying metaphors as a scaffold for designing richer interactions. By introducing three roles: Director, Puppeteer, and Wizard, we demonstrate how narrative framing can guide the creation of diverse gesture sets and interaction styles. These roles enable a variety of scenarios, showing how roleplay can unlock new possibilities for multi-robot systems. Our approach emphasizes creativity, expressiveness, and intuitiveness as key elements for future human-robot interaction design.
<div id='section'>PaperID: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2507.21431.pdf' target='_blank'>https://arxiv.org/pdf/2507.21431.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Victor Liu, Timothy Du, Jordy Sehn, Jack Collier, FranÃ§ois Grondin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21431">Sound Source Localization for Human-Robot Interaction in Outdoor Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a sound source localization strategy that relies on a microphone array embedded in an unmanned ground vehicle and an asynchronous close-talking microphone near the operator. A signal coarse alignment strategy is combined with a time-domain acoustic echo cancellation algorithm to estimate a time-frequency ideal ratio mask to isolate the target speech from interferences and environmental noise. This allows selective sound source localization, and provides the robot with the direction of arrival of sound from the active operator, which enables rich interaction in noisy scenarios. Results demonstrate an average angle error of 4 degrees and an accuracy within 5 degrees of 95\% at a signal-to-noise ratio of 1dB, which is significantly superior to the state-of-the-art localization methods.
<div id='section'>PaperID: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2507.13041.pdf' target='_blank'>https://arxiv.org/pdf/2507.13041.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Julien Wacquez, Elisabetta Zibetti, Joffrey Becker, Lorenzo Aloe, Fabio Amadio, Salvatore Anzalone, Lola CaÃ±amero, Serena Ivaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13041">What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots find their way into more and more aspects of everyday life, questions around trust are becoming increasingly important. What does it mean to trust a robot? And how should we think about trust in relationships that involve both humans and non-human agents? While the field of Human-Robot Interaction (HRI) has made trust a central topic, the concept is often approached in fragmented ways. At the same time, established work in sociology, where trust has long been a key theme, is rarely brought into conversation with developments in robotics. This article argues that we need a more interdisciplinary approach. By drawing on insights from both social sciences and social robotics, we explore how trust is shaped, tested and made visible. Our goal is to open up a dialogue between disciplines and help build a more grounded and adaptable framework for understanding trust in the evolving world of human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2507.10960.pdf' target='_blank'>https://arxiv.org/pdf/2507.10960.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>He Zhu, Ryo Miyoshi, Yuki Okafuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10960">Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior human-robot interaction (HRI) research has primarily focused on single-user interactions, where robots do not need to consider the timing or recipient of their responses. However, in multi-party interactions, such as at malls and hospitals, social robots must understand the context and decide both when and to whom they should respond. In this paper, we propose a Transformer-based multi-task learning framework to improve the decision-making process of social robots, particularly in multi-user environments. Considering the characteristics of HRI, we propose two novel loss functions: one that enforces constraints on active speakers to improve scene modeling, and another that guides response selection towards utterances specifically directed at the robot. Additionally, we construct a novel multi-party HRI dataset that captures real-world complexities, such as gaze misalignment. Experimental results demonstrate that our model achieves state-of-the-art performance in respond decisions, outperforming existing heuristic-based and single-task approaches. Our findings contribute to the development of socially intelligent social robots capable of engaging in natural and context-aware multi-party interactions.
<div id='section'>PaperID: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2507.08831.pdf' target='_blank'>https://arxiv.org/pdf/2507.08831.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Josh Qixuan Sun, Xiaoying Xing, Huaiyuan Weng, Chul Min Yeum, Mark Crowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08831">View Invariant Learning for Vision-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.
<div id='section'>PaperID: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2507.04095.pdf' target='_blank'>https://arxiv.org/pdf/2507.04095.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alireza Mortezapour, Giuliana Vitiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04095">Human-centered AI with focus on Human-robot interaction (Book chapter)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern social robots can be considered the descendants of steam engines from the First Industrial Revolution (IR 1.0) and industrial robotic arms from the Third Industrial Revolution (IR 3.0). As some time has passed since the introduction of these robots during the Fourth Industrial Revolution (IR 4.0), challenges and issues in their interaction with humans have emerged, leading researchers to conclude that, like any other AI-based technology, these robots must also be human-centered to meet the needs of their users. This chapter aims to introduce humans and their needs in interactions with robots, ranging from short-term, one-on-one interactions (micro-level) to long-term, macro-level needs at the societal scale. Building upon the principles of human-centered AI, this chapter presents, for the first time, a new framework of human needs called the Dual Pyramid. This framework encompasses a comprehensive list of human needs in robot interactions, from the most fundamental, robot effectiveness to macro level requirements, such as the collaboration with robots in achieving the United Nations 17 Sustainable Development Goals.
<div id='section'>PaperID: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2507.01206.pdf' target='_blank'>https://arxiv.org/pdf/2507.01206.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kathy Zhuang, Zixun Huang, Yukun Song, Rui Li, Yinuo Zhou, Allen Y. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01206">2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As modern computing advances, new interaction paradigms have emerged, particularly in Augmented Reality (AR), which overlays virtual interfaces onto physical objects. This evolution poses challenges in machine perception, especially for tasks like 3D object pose estimation in complex, dynamic environments. Our project addresses critical issues in human-robot interaction within mobile AR, focusing on non-intrusive, spatially aware interfaces. We present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024 SUITS challenge, targeting future spaceflight needs such as the Artemis missions. URSA integrates three core technologies: a head-mounted AR device (e.g., HoloLens) for intuitive visual feedback, voice control powered by large language models for hands-free interaction, and robot tracking algorithms that enable accurate 3D localization in dynamic settings. To enhance precision, we leverage digital twin localization technologies, using datasets like DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world tracking under noise and occlusion. Our system enables real-time robot control and monitoring via an AR interface, even in the absence of ground-truth sensors--vital for hazardous or remote operations. Key contributions include: (1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5) end-to-end integration for astronaut mission support. This work advances digital twin applications in robotics, offering scalable solutions for both aerospace and industrial domains.
<div id='section'>PaperID: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2506.20134.pdf' target='_blank'>https://arxiv.org/pdf/2506.20134.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ningwei Xie, Zizi Tian, Lei Yang, Xiao-Ping Zhang, Meng Guo, Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20134">From 2D to 3D Cognition: A Brief Survey of General World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have garnered increasing attention in the development of artificial general intelligence (AGI), serving as computational frameworks for learning representations of the external world and forecasting future states. While early efforts focused on 2D visual perception and simulation, recent 3D-aware generative world models have demonstrated the ability to synthesize geometrically consistent, interactive 3D environments, marking a shift toward 3D spatial cognition. Despite rapid progress, the field lacks systematic analysis to categorize emerging techniques and clarify their roles in advancing 3D cognitive world models. This survey addresses this need by introducing a conceptual framework, providing a structured and forward-looking review of world models transitioning from 2D perception to 3D cognition. Within this framework, we highlight two key technological drivers, particularly advances in 3D representations and the incorporation of world knowledge, as fundamental pillars. Building on these, we dissect three core cognitive capabilities that underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction. We further examine the deployment of these capabilities in real-world applications, including embodied AI, autonomous driving, digital twin, and gaming/VR. Finally, we identify challenges across data, modeling, and deployment, and outline future directions for advancing more robust and generalizable 3D world models.
<div id='section'>PaperID: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2506.17639.pdf' target='_blank'>https://arxiv.org/pdf/2506.17639.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxuan Chen, Xiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17639">RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLA) have demonstrated remarkable capabilities and promising potential in solving complex robotic manipulation tasks. However, their substantial parameter sizes and high inference latency pose significant challenges for real-world deployment, particularly on resource-constrained robotic platforms. To address this issue, we begin by conducting an extensive empirical study to explore the effectiveness of model compression techniques when applied to VLAs. Building on the insights gained from these preliminary experiments, we propose RLRC, a three-stage recovery method for compressed VLAs, including structured pruning, performance recovery based on SFT and RL, and further quantization. RLRC achieves up to an 8x reduction in memory usage and a 2.3x improvement in inference throughput, while maintaining or even surpassing the original VLA's task success rate. Extensive experiments show that RLRC consistently outperforms existing compression baselines, demonstrating strong potential for on-device deployment of VLAs. Project website: https://rlrc-vla.github.io
<div id='section'>PaperID: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2506.00220.pdf' target='_blank'>https://arxiv.org/pdf/2506.00220.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xingru Zhou, Sadanand Modak, Yao-Cheng Chan, Zhiyun Deng, Luis Sentis, Maria Esteva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00220">Curate, Connect, Inquire: A System for Findable Accessible Interoperable and Reusable (FAIR) Human-Robot Centered Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of AI in robotics has amplified the need for high-quality, reusable datasets, particularly in human-robot interaction (HRI) and AI-embedded robotics. While more robotics datasets are being created, the landscape of open data in the field is uneven. This is due to a lack of curation standards and consistent publication practices, which makes it difficult to discover, access, and reuse robotics data. To address these challenges, this paper presents a curation and access system with two main contributions: (1) a structured methodology to curate, publish, and integrate FAIR (Findable, Accessible, Interoperable, Reusable) human-centered robotics datasets; and (2) a ChatGPT-powered conversational interface trained with the curated datasets metadata and documentation to enable exploration, comparison robotics datasets and data retrieval using natural language. Developed based on practical experience curating datasets from robotics labs within Texas Robotics at the University of Texas at Austin, the system demonstrates the value of standardized curation and persistent publication of robotics data. The system's evaluation suggests that access and understandability of human-robotics data are significantly improved. This work directly aligns with the goals of the HCRL @ ICRA 2025 workshop and represents a step towards more human-centered access to data for embodied AI.
<div id='section'>PaperID: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2505.14129.pdf' target='_blank'>https://arxiv.org/pdf/2505.14129.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jed Muff, Keiichi Ito, Elijah H. W. Ang, Karine Miras, A. E. Eiben
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14129">Unconventional Hexacopters via Evolution and Learning: Performance Gains and New Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolution and learning have historically been interrelated topics, and their interplay is attracting increased interest lately. The emerging new factor in this trend is morphological evolution, the evolution of physical forms within embodied AI systems such as robots. In this study, we investigate a system of hexacopter-type drones with evolvable morphologies and learnable controllers and make contributions to two fields. For aerial robotics, we demonstrate that the combination of evolution and learning can deliver non-conventional drones that significantly outperform the traditional hexacopter on several tasks that are more complex than previously considered in the literature. For the field of Evolutionary Computing, we introduce novel metrics and perform new analyses into the interaction of morphological evolution and learning, uncovering hitherto unidentified effects. Our analysis tools are domain-agnostic, making a methodological contribution towards building solid foundations for embodied AI systems that integrate evolution and learning.
<div id='section'>PaperID: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2505.10183.pdf' target='_blank'>https://arxiv.org/pdf/2505.10183.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jieke Lin, Wanyu Wang, Longxiang Yin, Yinhe Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10183">KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) systems, such as autonomous robots and intelligent vehicles, are increasingly reliant on diverse heterogeneous accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing and energy-efficiency demands. However, the proliferation of vendor-specific proprietary communication libraries creates significant interoperability barriers, hindering seamless collaboration between different accelerator types and leading to suboptimal resource utilization and performance bottlenecks in distributed AI workloads. This paper introduces KAITIAN, a novel distributed communication framework designed to bridge this gap. KAITIAN provides a unified abstraction layer that intelligently integrates vendor-optimized communication libraries for intra-group efficiency with general-purpose communication protocols for inter-group interoperability. Crucially, it incorporates a load-adaptive scheduling mechanism that dynamically balances computational tasks across heterogeneous devices based on their real-time performance characteristics. Implemented as an extension to PyTorch and rigorously evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN demonstrates significant improvements in resource utilization and scalability for distributed training tasks. Experimental results show that KAITIAN can accelerate training time by up to 42% compared to baseline homogeneous systems, while incurring minimal communication overhead (2.8--4.3%) and maintaining model accuracy. KAITIAN paves the way for more flexible and powerful heterogeneous computing in complex embodied AI applications.
<div id='section'>PaperID: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2505.07710.pdf' target='_blank'>https://arxiv.org/pdf/2505.07710.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yasmin Rafiq, Baslin A. James, Ke Xu, Robert M. Hierons, Sanja Dogramadzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07710">Hybrid Control Strategies for Safe and Adaptive Robot-Assisted Dressing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety, reliability, and user trust are crucial in human-robot interaction (HRI) where the robots must address hazards in real-time. This study presents hazard driven low-level control strategies implemented in robot-assisted dressing (RAD) scenarios where hazards like garment snags and user discomfort in real-time can affect task performance and user safety. The proposed control mechanisms include: (1) Garment Snagging Control Strategy, which detects excessive forces and either seeks user intervention via a chatbot or autonomously adjusts its trajectory, and (2) User Discomfort/Pain Mitigation Strategy, which dynamically reduces velocity based on user feedback and aborts the task if necessary. We used physical dressing trials in order to evaluate these control strategies. Results confirm that integrating force monitoring with user feedback improves safety and task continuity. The findings emphasise the need for hybrid approaches that balance autonomous intervention, user involvement, and controlled task termination, supported by bi-directional interaction and real-time user-driven adaptability, paving the way for more responsive and personalised HRI systems.
<div id='section'>PaperID: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2505.06832.pdf' target='_blank'>https://arxiv.org/pdf/2505.06832.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xueyang Guo, Hongwei Hu, Chengye Song, Jiale Chen, Zilin Zhao, Yu Fu, Bowen Guan, Zhenze Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06832">UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary, task-oriented grasping of specific functional parts, particularly with dual arms, remains a key challenge, as current Vision-Language Models (VLMs), while enhancing task understanding, often struggle with precise grasp generation within defined constraints and effective dual-arm coordination. We innovatively propose UniDiffGrasp, a unified framework integrating VLM reasoning with guided part diffusion to address these limitations. UniDiffGrasp leverages a VLM to interpret user input and identify semantic targets (object, part(s), mode), which are then grounded via open-vocabulary segmentation. Critically, the identified parts directly provide geometric constraints for a Constrained Grasp Diffusion Field (CGDF) using its Part-Guided Diffusion, enabling efficient, high-quality 6-DoF grasps without retraining. For dual-arm tasks, UniDiffGrasp defines distinct target regions, applies part-guided diffusion per arm, and selects stable cooperative grasps. Through extensive real-world deployment, UniDiffGrasp achieves grasp success rates of 0.876 in single-arm and 0.767 in dual-arm scenarios, significantly surpassing existing state-of-the-art methods, demonstrating its capability to enable precise and coordinated open-vocabulary grasping in complex real-world scenarios.
<div id='section'>PaperID: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2505.00055.pdf' target='_blank'>https://arxiv.org/pdf/2505.00055.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuoqi Zeng, Yuxiang Wei, Jiawen Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00055">TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (EAI) addresses autonomous driving challenges in Vehicular Embodied AI Networks (VEANETs) through multi-modal perception, adaptive decision-making, and hardware-software co-scheduling. However, the computational demands of virtual services and the inherent mobility of autonomous vehicles (AVs) necessitate real-time migration of Vehicular Embodied Agent AI Twins (VEAATs) between resource-constrained Roadside Units (RSUs). This paper proposes a novel framework for efficient VEAAT migration in VEANETs, combining a multi-leader multi-follower (MLMF) Stackelberg game-theoretic incentive mechanism with a tiny multi-agent deep reinforcement learning (MADRL) algorithm. First, We propose an virtual immersive experience-driven utility model that captures AV-RSU dynamic interactions by integrating AVs' social influence, service complementarity and substitutability, and RSUs' resource allocation strategies to optimize VEAAT migration decisions. Second, to enhance training efficiency and enable efficient deployment on computation-constrained AVs while preserving exploration-exploitation performance, we propose TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance based on agents' exploration incentives. Numerical results demonstrate that our approach achieves convergence comparable to baseline models and closely approximates the Stackelberg equilibrium.
<div id='section'>PaperID: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2504.12535.pdf' target='_blank'>https://arxiv.org/pdf/2504.12535.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andy Dimnaku, Dominic Yurk, Zhiyuan Gao, Arun Padmanabhan, Mandar Aras, Yaser Abu-Mostafa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12535">Decision-based AI Visual Navigation for Cardiac Ultrasounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound imaging of the heart (echocardiography) is widely used to diagnose cardiac diseases. However, obtaining an echocardiogram requires an expert sonographer and a high-quality ultrasound imaging device, which are generally only available in hospitals. Recently, AI-based navigation models and algorithms have been used to aid novice sonographers in acquiring the standardized cardiac views necessary to visualize potential disease pathologies. These navigation systems typically rely on directional guidance to predict the necessary rotation of the ultrasound probe. This paper demonstrates a novel AI navigation system that builds on a decision model for identifying the inferior vena cava (IVC) of the heart. The decision model is trained offline using cardiac ultrasound videos and employs binary classification to determine whether the IVC is present in a given ultrasound video. The underlying model integrates a novel localization algorithm that leverages the learned feature representations to annotate the spatial location of the IVC in real-time. Our model demonstrates strong localization performance on traditional high-quality hospital ultrasound videos, as well as impressive zero-shot performance on lower-quality ultrasound videos from a more affordable Butterfly iQ handheld ultrasound machine. This capability facilitates the expansion of ultrasound diagnostics beyond hospital settings. Currently, the guidance system is undergoing clinical trials and is available on the Butterfly iQ app.
<div id='section'>PaperID: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2504.01888.pdf' target='_blank'>https://arxiv.org/pdf/2504.01888.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuang Qiu, Zhongcai Pei, Chen Wang, Jing Zhang, Zhiyong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01888">A novel gesture interaction control method for rehabilitation lower extremity exoskeleton</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of Rehabilitation Lower Extremity Robotic Exoskeletons (RLEEX) technology, significant advancements have been made in Human-Robot Interaction (HRI) methods. These include traditional physical HRI methods that are easily recognizable and various bio-electrical signal-based HRI methods that can visualize and predict actions. However, most of these HRI methods are contact-based, facing challenges such as operational complexity, sensitivity to interference, risks associated with implantable devices, and, most importantly, limitations in comfort. These challenges render the interaction less intuitive and natural, which can negatively impact patient motivation for rehabilitation. To address these issues, this paper proposes a novel non-contact gesture interaction control method for RLEEX, based on RGB monocular camera depth estimation. This method integrates three key steps: detecting keypoints, recognizing gestures, and assessing distance, thereby applying gesture information and augmented reality triggering technology to control gait movements of RLEEX. Results indicate that this approach provides a feasible solution to the problems of poor comfort, low reliability, and high latency in HRI for RLEEX platforms. Specifically, it achieves a gesture-controlled exoskeleton motion accuracy of 94.11\% and an average system response time of 0.615 seconds through non-contact HRI. The proposed non-contact HRI method represents a pioneering advancement in control interactions for RLEEX, paving the way for further exploration and development in this field.
<div id='section'>PaperID: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2503.15781.pdf' target='_blank'>https://arxiv.org/pdf/2503.15781.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuci Han, Charles Toth, Alper Yilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15781">UAS Visual Navigation in Large and Unseen Environments via a Meta Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of this work is to develop an approach that enables Unmanned Aerial System (UAS) to efficiently learn to navigate in large-scale urban environments and transfer their acquired expertise to novel environments. To achieve this, we propose a meta-curriculum training scheme. First, meta-training allows the agent to learn a master policy to generalize across tasks. The resulting model is then fine-tuned on the downstream tasks. We organize the training curriculum in a hierarchical manner such that the agent is guided from coarse to fine towards the target task. In addition, we introduce Incremental Self-Adaptive Reinforcement learning (ISAR), an algorithm that combines the ideas of incremental learning and meta-reinforcement learning (MRL). In contrast to traditional reinforcement learning (RL), which focuses on acquiring a policy for a specific task, MRL aims to learn a policy with fast transfer ability to novel tasks. However, the MRL training process is time consuming, whereas our proposed ISAR algorithm achieves faster convergence than the conventional MRL algorithm. We evaluate the proposed methodologies in simulated environments and demonstrate that using this training philosophy in conjunction with the ISAR algorithm significantly improves the convergence speed for navigation in large-scale cities and the adaptation proficiency in novel environments.
<div id='section'>PaperID: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2503.15522.pdf' target='_blank'>https://arxiv.org/pdf/2503.15522.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ana Tanevska, Katie Winkle, Ginevra Castellano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15522">"I don't like things where I do not have control": Participants' Experience of Trustworthy Interaction with Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of autonomous vehicle (AV) technology, AVs are progressively seen as interactive agents with some level of autonomy, as well as some context-dependent social features.
  This introduces new challenges and questions, already relevant in other areas of human-robot interaction (HRI) - namely, if an AV is perceived as a social agent by the human with whom it is interacting, how are the various facets of its design and behaviour impacting its human partner? And how can we foster a successful human-agent interaction (HAI) between the AV and the human, maximizing the human's comfort, acceptance, and trust in the AV?
  In this work, we attempt to understand the various factors that could influence naÃ¯ve participants' acceptance and trust when interacting with an AV in the role of a driver. Through a large-scale online study, we investigate the effect of the AV's autonomy on the human driver, as well as explore which parameters of the interaction have the highest impact on the user's sense of trust in the AV. Finally, we analyze our preliminary findings from the user study within existing guidelines on Trustworthy HAI/HRI.
<div id='section'>PaperID: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2503.15167.pdf' target='_blank'>https://arxiv.org/pdf/2503.15167.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fujian Yan, Hui Li, Hongsheng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15167">Volumetric Reconstruction From Partial Views for Task-Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object affordance and volumetric information are essential in devising effective grasping strategies under task-specific constraints. This paper presents an approach for inferring suitable grasping strategies from limited partial views of an object. To achieve this, a recurrent generative adversarial network (R-GAN) was proposed by incorporating a recurrent generator with long short-term memory (LSTM) units for it to process a variable number of depth scans. To determine object affordances, the AffordPose knowledge dataset is utilized as prior knowledge. Affordance retrieving is defined by the volume similarity measured via Chamfer Distance and action similarities. A Proximal Policy Optimization (PPO) reinforcement learning model is further implemented to refine the retrieved grasp strategies for task-oriented grasping. The retrieved grasp strategies were evaluated on a dual-arm mobile manipulation robot with an overall grasping accuracy of 89% for four tasks: lift, handle grasp, wrap grasp, and press.
<div id='section'>PaperID: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2503.01271.pdf' target='_blank'>https://arxiv.org/pdf/2503.01271.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>An-Chi He, Jungsoo Park, Benjamin Beiter, Bhaben Kalita, Alexander Leonessa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01271">Design and Development of a Locomotion Interface for Virtual Reality Lower-Body Haptic Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents the design, build, control, and preliminary user data of a locomotion interface called ForceBot. It delivers lower-body haptic interaction in virtual reality (VR), enabling users to walk in VR while interacting with various simulated terrains. It utilizes two planar gantries to give each foot two degrees of freedom and passive heel-lifting motion. The design used motion capture data with dynamic simulation for ergonomic human-robot workspace and hardware selection. Its system framework uses open-source robotic software and pairs with a custom-built power delivery system that offers EtherCAT communication with a 1,000 Hz soft real-time computation rate. This system features an admittance controller to regulate physical human-robot interaction (pHRI) alongside a walking algorithm to generate walking motion and simulate virtual terrains. The system's performance is explored through three measurements that evaluate the relationship between user input force and output pHRI motion. Overall, this platform presents a unique approach by utilizing planar gantries to realize VR terrain interaction with an extensive workspace, reasonably compact footprint, and preliminary user data.
<div id='section'>PaperID: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2503.00676.pdf' target='_blank'>https://arxiv.org/pdf/2503.00676.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rishikesh Joshi, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00676">One-Shot Gesture Recognition for Underwater Diver-To-Robot Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable human-robot communication is essential for underwater human-robot interaction (U-HRI), yet traditional methods such as acoustic signaling and predefined gesture-based models suffer from limitations in adaptability and robustness. In this work, we propose One-Shot Gesture Recognition (OSG), a novel method that enables real-time, pose-based, temporal gesture recognition underwater from a single demonstration, eliminating the need for extensive dataset collection or model retraining. OSG leverages shape-based classification techniques, including Hu moments, Zernike moments, and Fourier descriptors, to robustly recognize gestures in visually-challenging underwater environments. Our system achieves high accuracy on real-world underwater data and operates efficiently on embedded hardware commonly found on autonomous underwater vehicles (AUVs), demonstrating its feasibility for deployment on-board robots. Compared to deep learning approaches, OSG is lightweight, computationally efficient, and highly adaptable, making it ideal for diver-to-robot communication. We evaluate OSG's performance on an augmented gesture dataset and real-world underwater video data, comparing its accuracy against deep learning methods. Our results show OSG's potential to enhance U-HRI by enabling the immediate deployment of user-defined gestures without the constraints of predefined gesture languages.
<div id='section'>PaperID: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2503.00576.pdf' target='_blank'>https://arxiv.org/pdf/2503.00576.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gerard GÃ³mez-Izquierdo, Javier Laplaza, Alberto Sanfeliu, AnaÃ­s Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00576">Enhancing Context-Aware Human Motion Prediction for Efficient Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction (HMP) is critical for seamless human-robot collaboration, particularly in handover tasks that require real-time adaptability. Despite the high accuracy of state-of-the-art models, their computational complexity limits practical deployment in real-world robotic applications. In this work, we enhance human motion forecasting for handover tasks by leveraging siMLPe [1], a lightweight yet powerful architecture, and introducing key improvements. Our approach, named IntentMotion incorporates intention-aware conditioning, task-specific loss functions, and a novel intention classifier, significantly improving motion prediction accuracy while maintaining efficiency. Experimental results demonstrate that our method reduces body loss error by over 50%, achieves 200x faster inference, and requires only 3% of the parameters compared to existing state-of-the-art HMP models. These advancements establish our framework as a highly efficient and scalable solution for real-time human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2502.06851.pdf' target='_blank'>https://arxiv.org/pdf/2502.06851.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yerkebulan Massalim, Yermakhan Kassym, Zerde Nurbayeva, Zhanat Kappassov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06851">Survey on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.
<div id='section'>PaperID: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2502.02051.pdf' target='_blank'>https://arxiv.org/pdf/2502.02051.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aimee Allen, Tom Drummond, Dana KuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02051">Sound Judgment: Properties of Consequential Sounds Affecting Human-Perception of Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positive human-perception of robots is critical to achieving sustained use of robots in shared environments. One key factor affecting human-perception of robots are their sounds, especially the consequential sounds which robots (as machines) must produce as they operate. This paper explores qualitative responses from 182 participants to gain insight into human-perception of robot consequential sounds. Participants viewed videos of different robots performing their typical movements, and responded to an online survey regarding their perceptions of robots and the sounds they produce. Topic analysis was used to identify common properties of robot consequential sounds that participants expressed liking, disliking, wanting or wanting to avoid being produced by robots. Alongside expected reports of disliking high pitched and loud sounds, many participants preferred informative and audible sounds (over no sound) to provide predictability of purpose and trajectory of the robot. Rhythmic sounds were preferred over acute or continuous sounds, and many participants wanted more natural sounds (such as wind or cat purrs) in-place of machine-like noise. The results presented in this paper support future research on methods to improve consequential sounds produced by robots by highlighting features of sounds that cause negative perceptions, and providing insights into sound profile changes for improvement of human-perception of robots, thus enhancing human robot interaction.
<div id='section'>PaperID: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2501.19318.pdf' target='_blank'>https://arxiv.org/pdf/2501.19318.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19318">MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.
<div id='section'>PaperID: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2501.17206.pdf' target='_blank'>https://arxiv.org/pdf/2501.17206.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17206">Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores a novel approach to advancing dementia care by integrating socially assistive robotics, reinforcement learning (RL), large language models (LLMs), and clinical domain expertise within a simulated environment. This integration addresses the critical challenge of limited experimental data in socially assistive robotics for dementia care, providing a dynamic simulation environment that realistically models interactions between persons living with dementia (PLWDs) and robotic caregivers. The proposed framework introduces a probabilistic model to represent the cognitive and emotional states of PLWDs, combined with an LLM-based behavior simulation to emulate their responses. We further develop and train an adaptive RL system enabling humanoid robots, such as Pepper, to deliver context-aware and personalized interactions and assistance based on PLWDs' cognitive and emotional states. The framework also generalizes to computer-based agents, highlighting its versatility. Results demonstrate that the RL system, enhanced by LLMs, effectively interprets and responds to the complex needs of PLWDs, providing tailored caregiving strategies. This research contributes to human-computer and human-robot interaction by offering a customizable AI-driven caregiving platform, advancing understanding of dementia-related challenges, and fostering collaborative innovation in assistive technologies. The proposed approach has the potential to enhance the independence and quality of life for PLWDs while alleviating caregiver burden, underscoring the transformative role of interaction-focused AI systems in dementia care.
<div id='section'>PaperID: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2501.16513.pdf' target='_blank'>https://arxiv.org/pdf/2501.16513.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16513">Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.
  Our study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.
<div id='section'>PaperID: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2501.15332.pdf' target='_blank'>https://arxiv.org/pdf/2501.15332.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yinuo Qin, Richard T. Lee, Paul Sajda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15332">Perception of an AI Teammate in an Embodied Control Task Affects Team Performance, Reflected in Human Teammates' Behaviors and Physiological Responses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of artificial intelligence (AI) into human teams is widely expected to enhance performance and collaboration. However, our study reveals a striking and counterintuitive result: human-AI teams performed worse than human-only teams, especially when task difficulty increased. Using a virtual reality-based sensorimotor task, we observed that the inclusion of an active human-like AI teammate disrupted team dynamics, leading to elevated arousal, reduced engagement, and diminished communication intensity among human participants. These effects persisted even as the human teammates' perception of the AI teammate improved over time. These findings challenge prevailing assumptions about the benefits of AI in team settings and highlight the critical need for human-centered AI design to mitigate adverse physiological and behavioral impacts, ensuring more effective human-AI collaboration.
<div id='section'>PaperID: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2501.14099.pdf' target='_blank'>https://arxiv.org/pdf/2501.14099.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jaclyn Molan, Laura Saad, Eileen Roesler, J. Malcolm McCurry, Nathaniel Gyory, J. Gregory Trafton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14099">The Perceived Danger (PD) Scale: Development and Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are currently no psychometrically valid tools to measure the perceived danger of robots. To fill this gap, we provided a definition of perceived danger and developed and validated a 12-item bifactor scale through four studies. An exploratory factor analysis revealed four subdimensions of perceived danger: affective states, physical vulnerability, ominousness, and cognitive readiness. A confirmatory factor analysis confirmed the bifactor model. We then compared the perceived danger scale to the Godspeed perceived safety scale and found that the perceived danger scale is a better predictor of empirical data. We also validated the scale in an in-person setting and found that the perceived danger scale is sensitive to robot speed manipulations, consistent with previous empirical findings. Results across experiments suggest that the perceived danger scale is reliable, valid, and an adequate predictor of both perceived safety and perceived danger in human-robot interaction contexts.
<div id='section'>PaperID: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2501.13996.pdf' target='_blank'>https://arxiv.org/pdf/2501.13996.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ali Farshian Abbasi, Aghil Yousefi-Koma, Soheil Dehghani Firouzabadi, Parisa Rashidi, Alireza Naeini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13996">Integrating Persian Lip Reading in Surena-V Humanoid Robot for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lip reading is vital for robots in social settings, improving their ability to understand human communication. This skill allows them to communicate more easily in crowded environments, especially in caregiving and customer service roles. Generating a Persian Lip-reading dataset, this study integrates Persian lip-reading technology into the Surena-V humanoid robot to improve its speech recognition capabilities. Two complementary methods are explored, an indirect method using facial landmark tracking and a direct method leveraging convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. The indirect method focuses on tracking key facial landmarks, especially around the lips, to infer movements, while the direct method processes raw video data for action and speech recognition. The best-performing model, LSTM, achieved 89\% accuracy and has been successfully implemented into the Surena-V robot for real-time human-robot interaction. The study highlights the effectiveness of these methods, particularly in environments where verbal communication is limited.
<div id='section'>PaperID: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2501.13203.pdf' target='_blank'>https://arxiv.org/pdf/2501.13203.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohsen Amiri, Mehdi Hosseinzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13203">Safe and Efficient Robot Action Planning in the Presence of Unconcerned Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a robot action planning scheme that provides an efficient and probabilistically safe plan for a robot interacting with an unconcerned human -- someone who is either unaware of the robot's presence or unwilling to engage in ensuring safety. The proposed scheme is predictive, meaning that the robot is required to predict human actions over a finite future horizon; such predictions are often inaccurate in real-world scenarios. One possible approach to reduce the uncertainties is to provide the robot with the capability of reasoning about the human's awareness of potential dangers. This paper discusses that by using a binary variable, so-called danger awareness coefficient, it is possible to differentiate between concerned and unconcerned humans, and provides a learning algorithm to determine this coefficient by observing human actions. Moreover, this paper argues how humans rely on predictions of other agents' future actions (including those of robots in human-robot interaction) in their decision-making. It also shows that ignoring this aspect in predicting human's future actions can significantly degrade the efficiency of the interaction, causing agents to deviate from their optimal paths. The proposed robot action planning scheme is verified and validated via extensive simulation and experimental studies on a LoCoBot WidowX-250.
<div id='section'>PaperID: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2501.08946.pdf' target='_blank'>https://arxiv.org/pdf/2501.08946.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gabriel Skantze, Bahar Irfan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08946">Applying General Turn-taking Models to Conversational Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.
<div id='section'>PaperID: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2501.07507.pdf' target='_blank'>https://arxiv.org/pdf/2501.07507.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniele Meli, Paolo Fiorini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07507">Inductive Learning of Robot Task Knowledge from Raw Data and Online Expert Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios.
  In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.
<div id='section'>PaperID: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2501.07213.pdf' target='_blank'>https://arxiv.org/pdf/2501.07213.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohamed Ala Yahyaoui, Mouaad Oujabour, Leila Ben Letaifa, Amine Bohi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07213">Multi-face emotion detection for effective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of dialogue interfaces in mobile devices has become ubiquitous, providing a wide array of services. As technology progresses, humanoid robots designed with human-like features to interact effectively with people are gaining prominence, and the use of advanced human-robot dialogue interfaces is continually expanding. In this context, emotion recognition plays a crucial role in enhancing human-robot interaction by enabling robots to understand human intentions. This research proposes a facial emotion detection interface integrated into a mobile humanoid robot, capable of displaying real-time emotions from multiple individuals on a user interface. To this end, various deep neural network models for facial expression recognition were developed and evaluated under consistent computer-based conditions, yielding promising results. Afterwards, a trade-off between accuracy and memory footprint was carefully considered to effectively implement this application on a mobile humanoid robot.
<div id='section'>PaperID: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2501.06867.pdf' target='_blank'>https://arxiv.org/pdf/2501.06867.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alice Nardelli, Lorenzo Landolfi, Dario Pasquali, Antonio Sgorbissa, Francesco Rea, Carmine Recchiuto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06867">Toward a Universal Concept of Artificial Personality: Implementing Robotic Personality in a Kinova Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fundamental role of personality in shaping interactions is increasingly being exploited in robotics. A carefully designed robotic personality has been shown to improve several key aspects of Human-Robot Interaction (HRI). However, the fragmentation and rigidity of existing approaches reveal even greater challenges when applied to non-humanoid robots. On one hand, the state of the art is very dispersed; on the other hand, Industry 4.0 is moving towards a future where humans and industrial robots are going to coexist. In this context, the proper design of a robotic personality can lead to more successful interactions. This research takes a first step in that direction by integrating a comprehensive cognitive architecture built upon the definition of robotic personality - validated on humanoid robots - into a robotic Kinova Jaco2 arm. The robot personality is defined through the cognitive architecture as a vector in the three-dimensional space encompassing Conscientiousness, Extroversion, and Agreeableness, affecting how actions are executed, the action selection process, and the internal reaction to environmental stimuli. Our main objective is to determine whether users perceive distinct personalities in the robot, regardless of its shape, and to understand the role language plays in shaping these perceptions. To achieve this, we conducted a user study comprising 144 sessions of a collaborative game between a Kinova Jaco2 arm and participants, where the robot's behavior was influenced by its assigned personality. Furthermore, we compared two conditions: in the first, the robot communicated solely through gestures and action choices, while in the second, it also utilized verbal interaction.
<div id='section'>PaperID: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2501.04860.pdf' target='_blank'>https://arxiv.org/pdf/2501.04860.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michael F. Xu, Bilge Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04860">Exploring the Use of Robots for Diary Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.
<div id='section'>PaperID: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2501.04755.pdf' target='_blank'>https://arxiv.org/pdf/2501.04755.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Phillip Richter, Heiko Wersing, Anna-Lisa Vollmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04755">Improving Human-Robot Teaching by Quantifying and Reducing Mental Model Mismatch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of artificial intelligence and robotics has had a significant impact on our lives, with intelligent systems increasingly performing tasks traditionally performed by humans. Efficient knowledge transfer requires matching the mental model of the human teacher with the capabilities of the robot learner. This paper introduces the Mental Model Mismatch (MMM) Score, a feedback mechanism designed to quantify and reduce mismatches by aligning human teaching behavior with robot learning behavior. Using Large Language Models (LLMs), we analyze teacher intentions in natural language to generate adaptive feedback. A study with 150 participants teaching a virtual robot to solve a puzzle game shows that intention-based feedback significantly outperforms traditional performance-based feedback or no feedback. The results suggest that intention-based feedback improves instructional outcomes, improves understanding of the robot's learning process and reduces misconceptions. This research addresses a critical gap in human-robot interaction (HRI) by providing a method to quantify and mitigate discrepancies between human mental models and robot capabilities, with the goal of improving robot learning and human teaching effectiveness.
<div id='section'>PaperID: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2501.00953.pdf' target='_blank'>https://arxiv.org/pdf/2501.00953.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Casey Kennington, Pierre Lison, David Schlangen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00953">Prior Lessons of Incremental Dialogue and Robot Action Management for the Age of Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efforts towards endowing robots with the ability to speak have benefited from recent advancements in natural language processing, in particular large language models. However, current language models are not fully incremental, as their processing is inherently monotonic and thus lack the ability to revise their interpretations or output in light of newer observations. This monotonicity has important implications for the development of dialogue systems for human--robot interaction. In this paper, we review the literature on interactive systems that operate incrementally (i.e., at the word level or below it). We motivate the need for incremental systems, survey incremental modeling of important aspects of dialogue like speech recognition and language generation. Primary focus is on the part of the system that makes decisions, known as the dialogue manager. We find that there is very little research on incremental dialogue management, offer some requirements for practical incremental dialogue management, and the implications of incremental dialogue for embodied, robotic platforms in the age of large language models.
<div id='section'>PaperID: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2412.19595.pdf' target='_blank'>https://arxiv.org/pdf/2412.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shashank Rao Marpally, Pranav Goyal, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19595">SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current social navigation methods and benchmarks primarily focus on proxemics and task efficiency. While these factors are important, qualitative aspects such as perceptions of a robot's social competence are equally crucial for successful adoption and integration into human environments. We propose a more comprehensive evaluation of social navigation through scenario-based testing, where specific human-robot interaction scenarios can reveal key robot behaviors. However, creating such scenarios is often labor-intensive and complex. In this work, we address this challenge by introducing a pipeline that automates the generation of context-, and location-appropriate social navigation scenarios, ready for simulation. Our pipeline transforms simple scenario metadata into detailed textual scenarios, infers pedestrian and robot trajectories, and simulates pedestrian behaviors, which enables more controlled evaluation. We leverage the social reasoning and code-generation capabilities of Large Language Models (LLMs) to streamline scenario generation and translation. Our experiments show that our pipeline produces realistic scenarios and significantly improves scenario translation over naive LLM prompting. Additionally, we present initial feedback from a usability study with social navigation experts and a case-study demonstrating a scenario-based evaluation of three navigation algorithms.
<div id='section'>PaperID: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2412.12990.pdf' target='_blank'>https://arxiv.org/pdf/2412.12990.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Antonios Gasteratos, Stavros N. Moutsis, Konstantinos A. Tsintotas, Yiannis Aloimonos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12990">Future Aspects in Human Action Recognition: Exploring Emerging Techniques and Ethical Influences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-based human action recognition can be found in various application fields, e.g., surveillance systems, sports analytics, medical assistive technologies, or human-robot interaction frameworks, and it concerns the identification and classification of individuals' activities within a video. Since actions typically occur over a sequence of consecutive images, it is particularly challenging due to the inclusion of temporal analysis, which introduces an extra layer of complexity. However, although multiple approaches try to handle temporal analysis, there are still difficulties because of their computational cost and lack of adaptability. Therefore, different types of vision data, containing transition information between consecutive images, provided by next-generation hardware sensors will guide the robotics community in tackling the problem of human action recognition. On the other hand, while there is a plethora of still-image datasets, that researchers can adopt to train new artificial intelligence models, videos representing human activities are of limited capabilities, e.g., small and unbalanced datasets or selected without control from multiple sources. To this end, generating new and realistic synthetic videos is possible since labeling is performed throughout the data creation process, while reinforcement learning techniques can permit the avoidance of considerable dataset dependence. At the same time, human factors' involvement raises ethical issues for the research community, as doubts and concerns about new technologies already exist.
<div id='section'>PaperID: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2412.05024.pdf' target='_blank'>https://arxiv.org/pdf/2412.05024.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thomas Sievers, Nele Russwinkel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05024">Talking Like One of Us: Effects of Using Regional Language in a Humanoid Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are becoming more and more perceptible in public service settings. For engaging people in a natural environment a smooth social interaction as well as acceptance by the users are important issues for future successful Human-Robot Interaction (HRI). The type of verbal communication has a special significance here. In this paper we investigate the effects of spoken language varieties of a non-standard/regional language compared to standard language. More precisely we compare a human dialog with a humanoid social robot Pepper where the robot on the one hand is answering in High German and on the other hand in Low German, a regional language that is understood and partly still spoken in the northern parts of Germany. The content of what the robot says remains the same in both variants. We are interested in the effects that these two different ways of robot talk have on human interlocutors who are more or less familiar with Low German in terms of perceived warmth, competence and possible discomfort in conversation against a background of cultural identity. To measure these factors we use the Robotic Social Attributes Scale (RoSAS) on 17 participants with an age ranging from 19 to 61. Our results show that significantly higher warmth is perceived in the Low German version of the conversation.
<div id='section'>PaperID: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2411.04796.pdf' target='_blank'>https://arxiv.org/pdf/2411.04796.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sayan Paul, Ruddra dev Roychoudhury, Brojeshwar Bhowmick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04796">MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.
<div id='section'>PaperID: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2410.24036.pdf' target='_blank'>https://arxiv.org/pdf/2410.24036.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Niti Parikh, Yiran Zhao, Maria Alinea-Bravo, Tapan Parikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24036">The Communal Loom: Integrating Tangible Interaction and Participatory Data Collection for Assessing Well-Being</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For most health or well-being interventions, the process of evaluation is distinct from the activity itself, both in terms of who is involved, and how the actual data is collected and analyzed. Tangible interaction affords the opportunity to combine direct and embodied collaboration with a holistic approach to data collection and evaluation. We demonstrate this potential by describing our experiences designing and using the Communal Loom, an artifact for art therapy that translates quantitative data to collectively woven artifacts.
<div id='section'>PaperID: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2410.20423.pdf' target='_blank'>https://arxiv.org/pdf/2410.20423.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wentao Gao, Cheng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20423">A Deconfounding Framework for Human Behavior Prediction: Enhancing Robotic Systems in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of human behavior is crucial for effective human-robot interaction (HRI) systems, especially in dynamic environments where real-time decisions are essential. This paper addresses the challenge of forecasting future human behavior using multivariate time series data from wearable sensors, which capture various aspects of human movement. The presence of hidden confounding factors in this data often leads to biased predictions, limiting the reliability of traditional models. To overcome this, we propose a robust predictive model that integrates deconfounding techniques with advanced time series prediction methods, enhancing the model's ability to isolate true causal relationships and improve prediction accuracy. Evaluation on real-world datasets demonstrates that our approach significantly outperforms traditional methods, providing a more reliable foundation for responsive and adaptive HRI systems.
<div id='section'>PaperID: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2410.11564.pdf' target='_blank'>https://arxiv.org/pdf/2410.11564.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shang-Ching Liu, Van Nhiem Tran, Wenkai Chen, Wei-Lun Cheng, Yen-Lin Huang, I-Bin Liao, Yung-Hui Li, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11564">PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance understanding, the task of identifying actionable regions on 3D objects, plays a vital role in allowing robotic systems to engage with and operate within the physical world. Although Visual Language Models (VLMs) have excelled in high-level reasoning and long-horizon planning for robotic manipulation, they still fall short in grasping the nuanced physical properties required for effective human-robot interaction. In this paper, we introduce PAVLM (Point cloud Affordance Vision-Language Model), an innovative framework that utilizes the extensive multimodal knowledge embedded in pre-trained language models to enhance 3D affordance understanding of point cloud. PAVLM integrates a geometric-guided propagation module with hidden embeddings from large language models (LLMs) to enrich visual semantics. On the language side, we prompt Llama-3.1 models to generate refined context-aware text, augmenting the instructional input with deeper semantic cues. Experimental results on the 3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods for both full and partial point clouds, particularly excelling in its generalization to novel open-world affordance tasks of 3D objects. For more information, visit our project site: pavlm-source.github.io.
<div id='section'>PaperID: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2410.06355.pdf' target='_blank'>https://arxiv.org/pdf/2410.06355.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Paul Gajewski, Antonio Galiza Cerdeira Gonzalez, Bipin Indurkhya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06355">Context-Aware Command Understanding for Tabletop Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel hybrid algorithm designed to interpret natural human commands in tabletop scenarios. By integrating multiple sources of information, including speech, gestures, and scene context, the system extracts actionable instructions for a robot, identifying relevant objects and actions. The system operates in a zero-shot fashion, without reliance on predefined object models, enabling flexible and adaptive use in various environments. We assess the integration of multiple deep learning models, evaluating their suitability for deployment in real-world robotic setups. Our algorithm performs robustly across different tasks, combining language processing with visual grounding. In addition, we release a small dataset of video recordings used to evaluate the system. This dataset captures real-world interactions in which a human provides instructions in natural language to a robot, a contribution to future research on human-robot interaction. We discuss the strengths and limitations of the system, with particular focus on how it handles multimodal command interpretation, and its ability to be integrated into symbolic robotic frameworks for safe and explainable decision-making.
<div id='section'>PaperID: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2410.03086.pdf' target='_blank'>https://arxiv.org/pdf/2410.03086.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Danyi Chen, Ravi Prakash, Zacharias Chen, Sarah Dias, Vincent Wang, Leila Bridgeman, Siobhan Oca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03086">Design and Evaluation of a Compliant Quasi Direct Drive End-effector for Safe Robotic Ultrasound Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot-assisted ultrasound scanning promises to advance autonomous and accessible medical imaging. However, ensuring patient safety and compliant human-robot interaction (HRI) during probe contact poses a significant challenge. Most existing systems either have high mechanical stiffness or are compliant but lack sufficient force and precision. This paper presents a novel single-degree-of-freedom end-effector for safe and accurate robotic ultrasound imaging, using a quasi-direct drive actuator to achieve both passive mechanical compliance and precise active force regulation, even during motion. The end-effector demonstrates an effective force control bandwidth of 100 Hz and can apply forces ranging from 2.5N to 15N. To validate the end-effector's performance, we developed a novel ex vivo actuating platform, enabling compliance testing of the end-effector on simulated abdominal breathing and sudden patient movements. Experiments demonstrate that the end-effector can maintain consistent probe contact during simulated respiratory motion at 2.5N, 5N, 10N, and 15N, with an average force tracking RMS error of 0.83N compared to 4.70N on a UR3e robot arm using conventional force control. This system represents the first compliant ultrasound end-effector tested on a tissue platform simulating dynamic movement. The proposed solution provides a novel approach for designing and evaluating compliant robotic ultrasound systems, advancing the path for more compliant and patient-friendly robotic ultrasound systems in clinical settings.
<div id='section'>PaperID: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2409.16465.pdf' target='_blank'>https://arxiv.org/pdf/2409.16465.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juan-Diego Florez, Mehregan Dor, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16465">Initialization of Monocular Visual Navigation for Autonomous Agents Using Modified Structure from Small Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a standalone monocular visual Simultaneous Localization and Mapping (vSLAM) initialization pipeline for autonomous space robots. Our method, a state-of-the-art factor graph optimization pipeline, extends Structure from Small Motion (SfSM) to robustly initialize a monocular agent in spacecraft inspection trajectories, addressing visual estimation challenges such as weak-perspective projection and center-pointing motion, which exacerbates the bas-relief ambiguity, dominant planar geometry, which causes motion estimation degeneracies in classical Structure from Motion, and dynamic illumination conditions, which reduce the survivability of visual information. We validate our approach on realistic, simulated satellite inspection image sequences with a tumbling spacecraft and demonstrate the method's effectiveness over existing monocular initialization procedures.
<div id='section'>PaperID: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2409.15658.pdf' target='_blank'>https://arxiv.org/pdf/2409.15658.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyuan Liu, Jiawei Du, Sicheng Xiang, Zibo Wang, Dingsheng Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15658">Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon embodied planning underpins embodied AI. To accomplish long-horizon tasks, one of the most feasible ways is to decompose abstract instructions into a sequence of actionable steps. Foundation models still face logical errors and hallucinations in long-horizon planning, unless provided with highly relevant examples to the tasks. However, providing highly relevant examples for any random task is unpractical. Therefore, we present ReLEP, a novel framework for Real-time Long-horizon Embodied Planning. ReLEP can complete a wide range of long-horizon tasks without in-context examples by learning implicit logical inference through fine-tuning. The fine-tuned large vision-language model formulates plans as sequences of skill functions. These functions are selected from a carefully designed skill library. ReLEP is also equipped with a Memory module for plan and status recall, and a Robot Configuration module for versatility across robot types. In addition, we propose a data generation pipeline to tackle dataset scarcity. When constructing the dataset, we considered the implicit logical relationships, enabling the model to learn implicit logical relationships and dispel hallucinations. Through comprehensive evaluations across various long-horizon tasks, ReLEP demonstrates high success rates and compliance to execution even on unseen tasks and outperforms state-of-the-art baseline methods.
<div id='section'>PaperID: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2409.01036.pdf' target='_blank'>https://arxiv.org/pdf/2409.01036.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Paolo Magri, Javad Amirian, Mohamed Chetouani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01036">Upgrading Pepper Robot s Social Interaction with Advanced Hardware and Perception Enhancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose hardware and software enhancements for the Pepper robot to improve its human-robot interaction capabilities. This includes the integration of an NVIDIA Jetson GPU to enhance computational capabilities and execute real time algorithms, and a RealSense D435i camera to capture depth images, as well as the computer vision algorithms to detect and localize the humans around the robot and estimate their body orientation and gaze direction. The new stack is implemented on ROS and is running on the extended Pepper hardware, and the communication with the robot s firmware is done through the NAOqi ROS driver API. We have also collected a MoCap dataset of human activities in a controlled environment, together with the corresponding RGB-D data, to validate the proposed perception algorithms.
<div id='section'>PaperID: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2602.16360.pdf' target='_blank'>https://arxiv.org/pdf/2602.16360.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Leonard Günzel, Gabrielė Kasparavičiūtė, Ambjørn Grimsrud Waldum, Bjørn-Magnus Moslått, Abubakar Aliyu Badawi, Celil Yılmaz, Md Shamin Yeasher Yousha, Robert Staven, Martin Ludvigsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.16360">Docking and Persistent Operations for a Resident Underwater Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our understanding of the oceans remains limited by sparse and infrequent observations, primarily because current methods are constrained by the high cost and logistical effort of underwater monitoring, relying either on sporadic surveys across broad areas or on long-term measurements at fixed locations. To overcome these limitations, monitoring systems must enable persistent and autonomous operations without the need for continuous surface support. Despite recent advances, resident underwater vehicles remain uncommon due to persistent challenges in autonomy, robotic resilience, and mechanical robustness, particularly under long-term deployment in harsh and remote environments. This work addresses these problems by presenting the development, deployment, and operation of a resident infrastructure using a docking station with a mini-class Remotely Operated Vehicle (ROV) at 90m depth. The ROVis equipped with enhanced onboard processing and perception, allowing it to autonomously navigate using USBL signals, dock via ArUco marker-based visual localisation fused through an Extended Kalman Filter, and carry out local inspection routines. The system demonstrated a 90% autonomous docking success rate and completed full inspection missions within four minutes, validating the integration of acoustic and visual navigation in real-world conditions. These results show that reliable, untethered operations at depth are feasible, highlighting the potential of resident ROV systems for scalable, cost-effective underwater monitoring.
<div id='section'>PaperID: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2602.15840.pdf' target='_blank'>https://arxiv.org/pdf/2602.15840.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>André Helgert, Carolin Straßmann, Sabrina C. Eimler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15840">A Decade of Human-Robot Interaction Through Immersive Lenses: Reviewing Extended Reality as a Research Instrument in Social Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, Extended Reality (XR), including Virtual, Augmented, and Mixed Reality, gained attention as a research instrument in human-robot interaction studies, but remains underexplored in empirical investigations of social robotics. To map the field, we systematically reviewed empirical studies from 2015 to 2025. Of 6,527 peer-reviewed articles, only 33 met strict inclusion criteria. We examined (1) how XR and virtual social robots are used, focusing on the software and hardware employed and the application contexts in which they are deployed, (2) data collection and analysis methods, (3) demographics of the researchers and participants, and (4) the challenges and future directions. Our findings show that social XR-HRI research is still driven by laboratory simulations, while crucial specifications - such as the hardware, software, and robots used - are often not reported. Robots typically act as passive and hardly interactive visual stimulus, while the rich biosignal (e.g., eye-tracking) and logging (e.g. motion capturing) functions of modern head-mounted displays remain largely untapped. While there are gaps in demographic reporting, the research teams and samples are predominantly tech-centric, Western, young, and male. Key limitations include hardware delays, small homogeneous samples, and short study cycles. We propose a four-phase roadmap to establish social XR-HRI as a reliable research medium, which includes (1) strengthen application contexts, (2) more robust and testable technological iterations, (3) embedding diversity in samples and research teams, and (4) the need for reporting standards, e.g., in form of a suitable taxonomy. Advancing in these directions is essential for XR to mature from a lab prototype into an ecologically valid research instrument for social robotics.
<div id='section'>PaperID: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2602.15813.pdf' target='_blank'>https://arxiv.org/pdf/2602.15813.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haochen Zhang, Nirav Savaliya, Faizan Siddiqui, Enna Sachdeva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15813">FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.
<div id='section'>PaperID: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2602.15513.pdf' target='_blank'>https://arxiv.org/pdf/2602.15513.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ji Li, Jing Xia, Mingyi Li, Shiyan Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.15513">Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.
<div id='section'>PaperID: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2602.13800.pdf' target='_blank'>https://arxiv.org/pdf/2602.13800.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alberto Olivares-Alarcos, Muhammad Ahsan, Satrio Sanjaya, Hsien-I Lin, Guillem Alenyà
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13800">Ontological grounding for sound and natural robot explanations via large language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building effective human-robot interaction requires robots to derive conclusions from their experiences that are both logically sound and communicated in ways aligned with human expectations. This paper presents a hybrid framework that blends ontology-based reasoning with large language models (LLMs) to produce semantically grounded and natural robot explanations. Ontologies ensure logical consistency and domain grounding, while LLMs provide fluent, context-aware and adaptive language generation. The proposed method grounds data from human-robot experiences, enabling robots to reason about whether events are typical or atypical based on their properties. We integrate a state-of-the-art algorithm for retrieving and constructing static contrastive ontology-based narratives with an LLM agent that uses them to produce concise, clear, interactive explanations. The approach is validated through a laboratory study replicating an industrial collaborative task. Empirical results show significant improvements in the clarity and brevity of ontology-based narratives while preserving their semantic accuracy. Initial evaluations further demonstrate the system's ability to adapt explanations to user feedback. Overall, this work highlights the potential of ontology-LLM integration to advance explainable agency, and promote more transparent human-robot collaboration.
<div id='section'>PaperID: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2602.12597.pdf' target='_blank'>https://arxiv.org/pdf/2602.12597.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mahdi Haghighat Joo, Maryam Karimi Jafari, Alireza Taheri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.12597">PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.
<div id='section'>PaperID: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2602.12136.pdf' target='_blank'>https://arxiv.org/pdf/2602.12136.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kaisa Vaananen, Niels van Berkel, Donald McMillan, Thomas Olsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.12136">Embodied AI Agents for Team Collaboration in Co-located Blue-Collar Work</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blue-collar work is often highly collaborative, embodied, and situated in shared physical environments, yet most research on collaborative AI has focused on white-collar work. This position paper explores how the embodied nature of AI agents can support team collaboration and communication in co-located blue-collar workplaces. From the context of our newly started CAI-BLUE research project, we present two speculative scenarios from industrial and maintenance contexts that illustrate how embodied AI agents can support shared situational awareness and facilitate inclusive communication across experience levels. We outline open questions related to embodied AI agent design around worker inclusion, agency, transformation of blue-collar collaboration practices over time, and forms of acceptable AI embodiments. We argue that embodiment is not just an aesthetic choice but should become a socio-material design strategy of AI systems in blue-collar workplaces.
<div id='section'>PaperID: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2602.10891.pdf' target='_blank'>https://arxiv.org/pdf/2602.10891.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Berfin Sakallioglu, Giorgia Nadizar, Eric Medvet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.10891">Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a potential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curriculum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, ranging from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate meaningful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed baselines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications.
<div id='section'>PaperID: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2602.09287.pdf' target='_blank'>https://arxiv.org/pdf/2602.09287.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minja Axelsson, Henry Shevlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.09287">Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.
<div id='section'>PaperID: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2602.08373.pdf' target='_blank'>https://arxiv.org/pdf/2602.08373.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Feiyu Wu, Xu Zheng, Yue Qu, Zhuocheng Wang, Zicheng Feng, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.08373">Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.
<div id='section'>PaperID: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2601.09697.pdf' target='_blank'>https://arxiv.org/pdf/2601.09697.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09697">Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.
<div id='section'>PaperID: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2601.05336.pdf' target='_blank'>https://arxiv.org/pdf/2601.05336.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tracey Yee Hsin Tay, Xu Yan, Jonathan Ouyang, Daniel Wu, William Jiang, Jonathan Kao, Yuchen Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05336">Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing intuitive interfaces for robotic control remains a central challenge in enabling effective human-robot interaction, particularly in assistive care settings. Eye gaze offers a fast, non-intrusive, and intent-rich input modality, making it an attractive channel for conveying user goals. In this work, we present GAMMA (Gaze Assisted Manipulation for Modular Autonomy), a system that leverages ego-centric gaze tracking and a vision-language model to infer user intent and autonomously execute robotic manipulation tasks. By contextualizing gaze fixations within the scene, the system maps visual attention to high-level semantic understanding, enabling skill selection and parameterization without task-specific training. We evaluate GAMMA on a range of table-top manipulation tasks and compare it against baseline gaze-based control without reasoning. Results demonstrate that GAMMA provides robust, intuitive, and generalizable control, highlighting the potential of combining foundation models and gaze for natural and scalable robot autonomy. Project website: https://gamma0.vercel.app/
<div id='section'>PaperID: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2601.03470.pdf' target='_blank'>https://arxiv.org/pdf/2601.03470.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michael C. Darling, Alan H. Hesu, Michael A. Mardikes, Brian C. McGuigan, Reed M. Milewicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03470">Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.
<div id='section'>PaperID: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2601.02378.pdf' target='_blank'>https://arxiv.org/pdf/2601.02378.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Biyuan Liu, Daigang Xu, Lei Jiang, Wenjun Guo, Ping Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02378">Modeling the Mental World for Embodied AI: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice. To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction.
<div id='section'>PaperID: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2601.02205.pdf' target='_blank'>https://arxiv.org/pdf/2601.02205.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Neziha Akalin, Alberto Giaretta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02205">From Chat Control to Robot Control: The Backdoors Left Open for the Sake of Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores how a recent European Union proposal, the so-called Chat Control law, which creates regulatory incentives for providers to implement content detection and communication scanning, could transform the foundations of human-robot interaction (HRI). As robots increasingly act as interpersonal communication channels in care, education, and telepresence, they convey not only speech but also gesture, emotion, and contextual cues. We argue that extending digital surveillance laws to such embodied systems would entail continuous monitoring, embedding observation into the very design of everyday robots. This regulation blurs the line between protection and control, turning companions into potential informants. At the same time, monitoring mechanisms that undermine end-to-end encryption function as de facto backdoors, expanding the attack surface and allowing adversaries to exploit legally induced monitoring infrastructures. This creates a paradox of safety through insecurity: systems introduced to protect users may instead compromise their privacy, autonomy, and trust. This work does not aim to predict the future, but to raise awareness and help prevent certain futures from materialising.
<div id='section'>PaperID: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2601.01196.pdf' target='_blank'>https://arxiv.org/pdf/2601.01196.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shenqi Lu, Liangwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01196">EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.
<div id='section'>PaperID: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2601.00928.pdf' target='_blank'>https://arxiv.org/pdf/2601.00928.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luis Yoichi Morales, Francesco Zanlungo, David M. Woollard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.00928">Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.
<div id='section'>PaperID: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2512.24404.pdf' target='_blank'>https://arxiv.org/pdf/2512.24404.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Soham Pahari, M. Srinivas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24404">Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.
<div id='section'>PaperID: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2512.23570.pdf' target='_blank'>https://arxiv.org/pdf/2512.23570.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amy Ingold, Loong Yi Lee, Richard Suphapol Diteesawat, Ajmal Roshan, Yael Zekaria, Edith-Clare Hall, Enrico Werner, Nahian Rahman, Elaine Czech, Jonathan Rossiter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23570">Soft Robotic Technological Probe for Speculative Fashion Futures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.
<div id='section'>PaperID: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2512.15282.pdf' target='_blank'>https://arxiv.org/pdf/2512.15282.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Martijn IJtsma, Salvatore Hargis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15282">A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.
<div id='section'>PaperID: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2512.11873.pdf' target='_blank'>https://arxiv.org/pdf/2512.11873.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Antonia Yepes, Marie Charbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11873">Audio-Based Tactile Human-Robot Interaction Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.
<div id='section'>PaperID: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2512.11746.pdf' target='_blank'>https://arxiv.org/pdf/2512.11746.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hana Kopecka, Jose Such
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11746">The Influence of Human-like Appearance on Expected Robot Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.
<div id='section'>PaperID: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2512.09105.pdf' target='_blank'>https://arxiv.org/pdf/2512.09105.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Adi Manor, Dan Cohen, Ziv Keidar, Avi Parush, Hadas Erel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09105">Cognitive Trust in HRI: "Pay Attention to Me and I'll Trust You Even if You are Wrong"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.
<div id='section'>PaperID: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2512.02022.pdf' target='_blank'>https://arxiv.org/pdf/2512.02022.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nan Lin, Linrui Zhang, Yuxuan Chen, Zhenrui Chen, Yujun Zhu, Ruoxi Chen, Peichen Wu, Xiaoping Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02022">Reinforcement Learning for Robotic Safe Control with Force Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For the task with complicated manipulation in unstructured environments, traditional hand-coded methods are ineffective, while reinforcement learning can provide more general and useful policy. Although the reinforcement learning is able to obtain impressive results, its stability and reliability is hard to guarantee, which would cause the potential safety threats. Besides, the transfer from simulation to real world also will lead in unpredictable situations. To enhance the safety and reliability of robots, we introduce the force and haptic perception into reinforcement learning. Force and tactual sensation play key roles in robotic dynamic control and human-robot interaction. We demonstrate that the force-based reinforcement learning method can be more adaptive to environment, especially in sim-to-real transfer. Experimental results show in object pushing task, our strategy is safer and more efficient in both simulation and real world, thus it holds prospects for a wide variety of robotic applications.
<div id='section'>PaperID: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2511.21945.pdf' target='_blank'>https://arxiv.org/pdf/2511.21945.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junwei Zhou, Yu-Wing Tai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21945">AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.
<div id='section'>PaperID: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2511.21784.pdf' target='_blank'>https://arxiv.org/pdf/2511.21784.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chi Zhang, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21784">Physics-Informed Spiking Neural Networks via Conservative Flux Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.
<div id='section'>PaperID: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2511.21460.pdf' target='_blank'>https://arxiv.org/pdf/2511.21460.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junjian Wang, Lidan Zhao, Xi Sheryl Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21460">MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.
<div id='section'>PaperID: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2511.21428.pdf' target='_blank'>https://arxiv.org/pdf/2511.21428.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiajie Zhang, Sören Schwertfeger, Alexander Kleiner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21428">From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.
<div id='section'>PaperID: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2511.18746.pdf' target='_blank'>https://arxiv.org/pdf/2511.18746.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Li, Qiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18746">Any4D: Open-Prompt 4D Generation from Natural Language and Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
<div id='section'>PaperID: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2511.14327.pdf' target='_blank'>https://arxiv.org/pdf/2511.14327.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Felipe Ballen-Moreno, Pasquale Ferrentino, Milan Amighi, Bram Vanderborght, Tom Verstraten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14327">Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.
<div id='section'>PaperID: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2511.06202.pdf' target='_blank'>https://arxiv.org/pdf/2511.06202.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shahram Najam Syed, Yatharth Ahuja, Arthur Jakobsson, Jeff Ichnowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06202">ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.
<div id='section'>PaperID: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2508.06767.pdf' target='_blank'>https://arxiv.org/pdf/2508.06767.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arman Dogru, R. Irem Bor-Yaliniz, Nimal Gamini Senarath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06767">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.
<div id='section'>PaperID: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2507.17727.pdf' target='_blank'>https://arxiv.org/pdf/2507.17727.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Robel Mamo, Taeyeong Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17727">CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art visual under-canopy navigation methods are designed with deep learning-based perception models to distinguish traversable space from crop rows. While these models have demonstrated successful performance, they require large amounts of training data to ensure reliability in real-world field deployment. However, data collection is costly, demanding significant human resources for in-field sampling and annotation. To address this challenge, various data augmentation techniques are commonly employed during model training, such as color jittering, Gaussian blur, and horizontal flip, to diversify training data and enhance model robustness. In this paper, we hypothesize that utilizing only these augmentation techniques may lead to suboptimal performance, particularly in complex under-canopy environments with frequent occlusions, debris, and non-uniform spacing of crops. Instead, we propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut) which masks random regions out in input images that are spatially distributed around crop rows on the sides to encourage trained models to capture high-level contextual features even when fine-grained information is obstructed. Our extensive experiments with a public cornfield dataset demonstrate that masking-based augmentations are effective for simulating occlusions and significantly improving robustness in semantic keypoint predictions for visual navigation. In particular, we show that biasing the mask distribution toward crop rows in CA-Cut is critical for enhancing both prediction accuracy and generalizability across diverse environments achieving up to a 36.9% reduction in prediction error. In addition, we conduct ablation studies to determine the number of masks, the size of each mask, and the spatial distribution of masks to maximize overall performance.
<div id='section'>PaperID: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2507.16398.pdf' target='_blank'>https://arxiv.org/pdf/2507.16398.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lavinia Hriscu, Alberto Sanfeliu, Anais Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16398">AI or Human? Understanding Perceptions of Embodied Robots with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of artificial intelligence has long been associated to the the challenge of effectively measuring intelligence. Even if the Turing Test was introduced as a means of assessing a system intelligence, its relevance and application within the field of human-robot interaction remain largely underexplored. This study investigates the perception of intelligence in embodied robots by performing a Turing Test within a robotic platform. A total of 34 participants were tasked with distinguishing between AI- and human-operated robots while engaging in two interactive tasks: an information retrieval and a package handover. These tasks assessed the robot perception and navigation abilities under both static and dynamic conditions. Results indicate that participants were unable to reliably differentiate between AI- and human-controlled robots beyond chance levels. Furthermore, analysis of participant responses reveals key factors influencing the perception of artificial versus human intelligence in embodied robotic systems. These findings provide insights into the design of future interactive robots and contribute to the ongoing discourse on intelligence assessment in AI-driven systems.
<div id='section'>PaperID: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2507.12473.pdf' target='_blank'>https://arxiv.org/pdf/2507.12473.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mia-Katrin Kvalsund, Mikkel Elle LepperÃ¸d
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12473">The Generalist Brain Module: Module Repetition in Neural Networks in Light of the Minicolumn Hypothesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While modern AI continues to advance, the biological brain remains the pinnacle of neural networks in its robustness, adaptability, and efficiency. This review explores an AI architectural path inspired by the brain's structure, particularly the minicolumn hypothesis, which views the neocortex as a distributed system of repeated modules - a structure we connect to collective intelligence (CI). Despite existing work, there is a lack of comprehensive reviews connecting the cortical column to the architectures of repeated neural modules. This review aims to fill that gap by synthesizing historical, theoretical, and methodological perspectives on neural module repetition. We distinguish between architectural repetition - reusing structure - and parameter-shared module repetition, where the same functional unit is repeated across a network. The latter exhibits key CI properties such as robustness, adaptability, and generalization. Evidence suggests that the repeated module tends to converge toward a generalist module: simple, flexible problem solvers capable of handling many roles in the ensemble. This generalist tendency may offer solutions to longstanding challenges in modern AI: improved energy efficiency during training through simplicity and scalability, and robust embodied control via generalization. While empirical results suggest such systems can generalize to out-of-distribution problems, theoretical results are still lacking. Overall, architectures featuring module repetition remain an emerging and unexplored architectural strategy, with significant untapped potential for both efficiency, robustness, and adaptiveness. We believe that a system that adopts the benefits of CI, while adhering to architectural and functional principles of the minicolumns, could challenge the modern AI problems of scalability, energy consumption, and democratization.
<div id='section'>PaperID: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2507.10087.pdf' target='_blank'>https://arxiv.org/pdf/2507.10087.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhammad Tayyab Khan, Ammar Waheed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10087">Foundation Model Driven Robotics: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid emergence of foundation models, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), has introduced a transformative paradigm in robotics. These models offer powerful capabilities in semantic understanding, high-level reasoning, and cross-modal generalization, enabling significant advances in perception, planning, control, and human-robot interaction. This critical review provides a structured synthesis of recent developments, categorizing applications across simulation-driven design, open-world execution, sim-to-real transfer, and adaptable robotics. Unlike existing surveys that emphasize isolated capabilities, this work highlights integrated, system-level strategies and evaluates their practical feasibility in real-world environments. Key enabling trends such as procedural scene generation, policy generalization, and multimodal reasoning are discussed alongside core bottlenecks, including limited embodiment, lack of multimodal data, safety risks, and computational constraints. Through this lens, this paper identifies both the architectural strengths and critical limitations of foundation model-based robotics, highlighting open challenges in real-time operation, grounding, resilience, and trust. The review concludes with a roadmap for future research aimed at bridging semantic reasoning and physical intelligence through more robust, interpretable, and embodied models.
<div id='section'>PaperID: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2507.05607.pdf' target='_blank'>https://arxiv.org/pdf/2507.05607.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chongshan Fan, Shenghai Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05607">Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents Auto-RubikAI, a modular autonomous planning framework that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM), and a large language model (LLM) to solve structured manipulation tasks exemplified by Rubik's Cube restoration. Unlike traditional robot systems based on predefined scripts, or modern approaches relying on pretrained networks and large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step task execution with minimal data requirements and no prior demonstrations. The proposed system employs a KB module to solve group-theoretic restoration steps, overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to construct a semantic 3D scene representation, while the LLM generates structured robotic control code via prompt chaining. This tri-module architecture enables robust performance under spatial uncertainty. We deploy Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic arm, demonstrating effective Sim-to-Real adaptation without retraining. Experiments show a 79% end-to-end task success rate across randomized configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our KB-enhanced method reduces average solution steps while maintaining interpretability and safety. Auto-RubikAI provides a cost-efficient, modular foundation for embodied task planning in smart manufacturing, robotics education, and autonomous execution scenarios. Code, prompts, and hardware modules will be released upon publication.
<div id='section'>PaperID: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2507.02755.pdf' target='_blank'>https://arxiv.org/pdf/2507.02755.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Caleb Rascon, Luis Gato-Diaz, Eduardo GarcÃ­a-AlarcÃ³n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02755">Multi-agent Auditory Scene Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a publicly available framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.
<div id='section'>PaperID: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2507.02016.pdf' target='_blank'>https://arxiv.org/pdf/2507.02016.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cong Wang, Roberto Calandra, Verena KlÃ¶s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02016">Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When robots perform complex and context-dependent tasks in our daily lives, deviations from expectations can confuse users. Explanations of the robot's reasoning process can help users to understand the robot intentions. However, when to provide explanations and what they contain are important to avoid user annoyance. We have investigated user preferences for explanation demand and content for a robot that helps with daily cleaning tasks in a kitchen. Our results show that users want explanations in surprising situations and prefer concise explanations that clearly state the intention behind the confusing action and the contextual factors that were relevant to this decision. Based on these findings, we propose two algorithms to identify surprising actions and to construct effective explanations for Belief-Desire-Intention (BDI) robots. Our algorithms can be easily integrated in the BDI reasoning process and pave the way for better human-robot interaction with context- and user-specific explanations.
<div id='section'>PaperID: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2507.00271.pdf' target='_blank'>https://arxiv.org/pdf/2507.00271.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuochao Peng, Jiaxin Xu, Jun Hu, Haian Xue, Laurens A. G. Kolks, Pieter M. A. Desmet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00271">User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept "Mora" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2506.10172.pdf' target='_blank'>https://arxiv.org/pdf/2506.10172.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yicheng Duan, Kaiyu tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10172">A Navigation Framework Utilizing Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.
<div id='section'>PaperID: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2506.05576.pdf' target='_blank'>https://arxiv.org/pdf/2506.05576.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Valerija Holomjova, Jamie Grech, Dewei Yi, Bruno Yun, Andrew Starkey, Pascal MeiÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05576">TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) is an essential preliminary step for robotic task execution, which involves predicting grasps on regions of target objects that facilitate intended tasks. Existing literature reveals there is a limited availability of TOG datasets for training and benchmarking despite large demand, which are often synthetic or have artifacts in mask annotations that hinder model performance. Moreover, TOG solutions often require affordance masks, grasps, and object masks for training, however, existing datasets typically provide only a subset of these annotations. To address these limitations, we introduce the Top-down Task-oriented Grasping (TD-TOG) dataset, designed to train and evaluate TOG solutions. TD-TOG comprises 1,449 real-world RGB-D scenes including 30 object categories and 120 subcategories, with hand-annotated object masks, affordances, and planar rectangular grasps. It also features a test set for a novel challenge that assesses a TOG solution's ability to distinguish between object subcategories. To contribute to the demand for TOG solutions that can adapt and manipulate previously unseen objects without re-training, we propose a novel TOG framework, Binary-TOG. Binary-TOG uses zero-shot for object recognition, and one-shot learning for affordance recognition. Zero-shot learning enables Binary-TOG to identify objects in multi-object scenes through textual prompts, eliminating the need for visual references. In multi-object settings, Binary-TOG achieves an average task-oriented grasp accuracy of 68.9%. Lastly, this paper contributes a comparative analysis between one-shot and zero-shot learning for object generalization in TOG to be used in the development of future TOG solutions.
<div id='section'>PaperID: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2505.10705.pdf' target='_blank'>https://arxiv.org/pdf/2505.10705.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matej Hoffmann, Shubhan Parag Patni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10705">Embodied AI in Machine Learning -- is it Really Embodied?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the machine learning communities with the goal of leveraging current progress in AI (deep learning, transformers, large language and visual-language models) to empower robots. In this chapter we put this work in the context of "Good Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier 2001). We claim that the AI-powered robots are only weakly embodied and inherit some of the problems of GOFAI. Moreover, we review and critically discuss the possibility of cross-embodiment learning (Padalkar et al. 2024). We identify fundamental roadblocks and propose directions on how to make progress.
<div id='section'>PaperID: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2505.07532.pdf' target='_blank'>https://arxiv.org/pdf/2505.07532.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kajetan RachwaÅ, Maciej Majek, BartÅomiej Boczek, Kacper DÄbrowski, PaweÅ Liberadzki, Adam DÄbrowski, Maria Ganzha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07532">RAI: Flexible Agent Framework for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With an increase in the capabilities of generative language models, a growing interest in embodied AI has followed. This contribution introduces RAI - a framework for creating embodied Multi Agent Systems for robotics. The proposed framework implements tools for Agents' integration with robotic stacks, Large Language Models, and simulations. It provides out-of-the-box integration with state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms for the embodiment of Agents. These mechanisms have been tested on a physical robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid prototyping. Furthermore, these mechanisms have been deployed in two simulations: (1) robot arm manipulator and (2) tractor controller. All of these deployments have been evaluated in terms of their control capabilities, effectiveness of embodiment, and perception ability. The proposed framework has been used successfully to build systems with multiple agents. It has demonstrated effectiveness in all the aforementioned tasks. It also enabled identifying and addressing the shortcomings of the generative models used for embodied AI.
<div id='section'>PaperID: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2505.07460.pdf' target='_blank'>https://arxiv.org/pdf/2505.07460.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Chen, JiaHao Zhao, HaoHao Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07460">A Survey on Collaborative Mechanisms Between Large and Small Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.
<div id='section'>PaperID: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2505.06628.pdf' target='_blank'>https://arxiv.org/pdf/2505.06628.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhongquan Zhou, Shuhao Li, Zixian Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06628">ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI research has traditionally emphasized performance metrics such as success rate and cumulative reward, overlooking critical robustness and safety considerations that emerge during real-world deployment. In actual environments, agents continuously encounter unpredicted situations and distribution shifts, causing seemingly reliable policies to experience catastrophic failures, particularly in manipulation tasks. To address this gap, we introduce four novel safety-centric metrics that quantify an agent's resilience to environmental perturbations. Building on these metrics, we present Adaptive Contrastive Optimization for Robust Manipulation (ACORN), a plug-and-play algorithm that enhances policy robustness without sacrificing performance. ACORN leverages contrastive learning to simultaneously align trajectories with expert demonstrations while diverging from potentially unsafe behaviors. Our approach efficiently generates informative negative samples through structured Gaussian noise injection, employing a double perturbation technique that maintains sample diversity while minimizing computational overhead. Comprehensive experiments across diverse manipulation environments validate ACORN's effectiveness, yielding improvements of up to 23% in safety metrics under disturbance compared to baseline methods. These findings underscore ACORN's significant potential for enabling reliable deployment of embodied agents in safety-critical real-world applications.
<div id='section'>PaperID: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2504.21548.pdf' target='_blank'>https://arxiv.org/pdf/2504.21548.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria MorÃ£o PatrÃ­cio, Anahita Jamshidnejad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21548">Leveraging Systems and Control Theory for Social Robotics: A Model-Based Behavioral Control Approach to Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots (SRs) should autonomously interact with humans, while exhibiting proper social behaviors associated to their role. By contributing to health-care, education, and companionship, SRs will enhance life quality. However, personalization and sustaining user engagement remain a challenge for SRs, due to their limited understanding of human mental states. Accordingly, we leverage a recently introduced mathematical dynamic model of human perception, cognition, and decision-making for SRs. Identifying the parameters of this model and deploying it in behavioral steering system of SRs allows to effectively personalize the responses of SRs to evolving mental states of their users, enhancing long-term engagement and personalization. Our approach uniquely enables autonomous adaptability of SRs by modeling the dynamics of invisible mental states, significantly contributing to the transparency and awareness of SRs. We validated our model-based control system in experiments with 10 participants who interacted with a Nao robot over three chess puzzle sessions, 45 - 90 minutes each. The identified model achieved a mean squared error (MSE) of 0.067 (i.e., 1.675% of the maximum possible MSE) in tracking beliefs, goals, and emotions of participants. Compared to a model-free controller that did not track mental states of participants, our approach increased engagement by 16% on average. Post-interaction feedback of participants (provided via dedicated questionnaires) further confirmed the perceived engagement and awareness of the model-driven robot. These results highlight the unique potential of model-based approaches and control theory in advancing human-SR interactions.
<div id='section'>PaperID: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2504.20109.pdf' target='_blank'>https://arxiv.org/pdf/2504.20109.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rajeev Gupta, Suhani Gupta, Ronak Parikh, Divya Gupta, Amir Javaheri, Jairaj Singh Shaktawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20109">Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired AI, and proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms for edge deployment. We review literature on continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss key neuroscience principles of human learning, including Synaptic Pruning, Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for AI systems. Building on these insights, we outline an AI architecture that features complementary fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates to support on-device lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are provided. We address challenges such as catastrophic forgetting, memory efficiency, and system scalability, and present application scenarios for mobile AI assistants and embodied AI systems like humanoid robots. We conclude with key takeaways and future research directions toward truly continual, personalized AGI on the edge. While the architecture is theoretical, it synthesizes diverse findings and offers a roadmap for future implementation.
<div id='section'>PaperID: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2504.11419.pdf' target='_blank'>https://arxiv.org/pdf/2504.11419.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Li Jin, Liu Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11419">Embodied World Models Emerge from Navigational Task in Open-Ended Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning in partially observable environments has often been approached through passive predictive models, yet theories of embodied cognition suggest that genuinely useful representations arise only when perception is tightly coupled to action. Here we ask whether a recurrent agent, trained solely by sparse rewards to solve procedurally generated planar mazes, can autonomously internalize metric concepts such as direction, distance and obstacle layout. After training, the agent consistently produces near-optimal paths in unseen mazes, behavior that hints at an underlying spatial model. To probe this possibility, we cast the closed agent-environment loop as a hybrid dynamical system, identify stable limit cycles in its state space, and characterize behavior with a Ridge Representation that embeds whole trajectories into a common metric space. Canonical correlation analysis exposes a robust linear alignment between neural and behavioral manifolds, while targeted perturbations of the most informative neural dimensions sharply degrade navigation performance. Taken together, these dynamical, representational, and causal signatures show that sustained sensorimotor interaction is sufficient for the spontaneous emergence of compact, embodied world models, providing a principled path toward interpretable and transferable navigation policies.
<div id='section'>PaperID: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2504.10294.pdf' target='_blank'>https://arxiv.org/pdf/2504.10294.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>J. F. Almeida, J. AndrÃ©, C. P. Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10294">Ankle Exoskeletons in Walking and Load-Carrying Tasks: Insights into Biomechanics and Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Lower limb exoskeletons can enhance quality of life, but widespread adoption is limited by the lack of frameworks to assess their biomechanical and human-robot interaction effects, which are essential for developing adaptive and personalized control strategies. Understanding impacts on kinematics, muscle activity, and HRI dynamics is key to achieve improved usability of wearable robots. Objectives: We propose a systematic methodology evaluate an ankle exoskeleton's effects on human movement during walking and load-carrying (10 kg front pack), focusing on joint kinematics, muscle activity, and HRI torque signals. Materials and Methods: Using Xsens MVN (inertial motion capture), Delsys EMG, and a unilateral exoskeleton, three experiments were conducted: (1) isolated dorsiflexion/plantarflexion; (2) gait analysis (two subjects, passive/active modes); and (3) load-carrying under assistance. Results and Conclusions: The first experiment confirmed that the HRI sensor captured both voluntary and involuntary torques, providing directional torque insights. The second experiment showed that the device slightly restricted ankle range of motion (RoM) but supported normal gait patterns across all assistance modes. The exoskeleton reduced muscle activity, particularly in active mode. HRI torque varied according to gait phases and highlighted reduced synchronization, suggesting a need for improved support. The third experiment revealed that load-carrying increased GM and TA muscle activity, but the device partially mitigated user effort by reducing muscle activity compared to unassisted walking. HRI increased during load-carrying, providing insights into user-device dynamics. These results demonstrate the importance of tailoring exoskeleton evaluation methods to specific devices and users, while offering a framework for future studies on exoskeleton biomechanics and HRI.
<div id='section'>PaperID: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2504.09243.pdf' target='_blank'>https://arxiv.org/pdf/2504.09243.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michael Hagenow, Julie A. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09243">REALM: Real-Time Estimates of Assistance for Learned Models in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are a variety of mechanisms (i.e., input types) for real-time human interaction that can facilitate effective human-robot teaming. For example, previous works have shown how teleoperation, corrective, and discrete (i.e., preference over a small number of choices) input can enable robots to complete complex tasks. However, few previous works have looked at combining different methods, and in particular, opportunities for a robot to estimate and elicit the most effective form of assistance given its understanding of a task. In this paper, we propose a method for estimating the value of different human assistance mechanisms based on the action uncertainty of a robot policy. Our key idea is to construct mathematical expressions for the expected post-interaction differential entropy (i.e., uncertainty) of a stochastic robot policy to compare the expected value of different interactions. As each type of human input imposes a different requirement for human involvement, we demonstrate how differential entropy estimates can be combined with a likelihood penalization approach to effectively balance feedback informational needs with the level of required input. We demonstrate evidence of how our approach interfaces with emergent learning models (e.g., a diffusion model) to produce accurate assistance value estimates through both simulation and a robot user study. Our user study results indicate that the proposed approach can enable task completion with minimal human feedback for uncertain robot behaviors.
<div id='section'>PaperID: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2504.08431.pdf' target='_blank'>https://arxiv.org/pdf/2504.08431.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiafan Lu, Dongcheng Hu, Yitian Ye, Anqi Liu, Zixian Zhang, Xin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08431">The Composite Visual-Laser Navigation Method Applied in Indoor Poultry Farming Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor poultry farms require inspection robots to maintain precise environmental control, which is crucial for preventing the rapid spread of disease and large-scale bird mortality. However, the complex conditions within these facilities, characterized by areas of intense illumination and water accumulation, pose significant challenges. Traditional navigation methods that rely on a single sensor often perform poorly in such environments, resulting in issues like laser drift and inaccuracies in visual navigation line extraction. To overcome these limitations, we propose a novel composite navigation method that integrates both laser and vision technologies. This approach dynamically computes a fused yaw angle based on the real-time reliability of each sensor modality, thereby eliminating the need for physical navigation lines. Experimental validation in actual poultry house environments demonstrates that our method not only resolves the inherent drawbacks of single-sensor systems, but also significantly enhances navigation precision and operational efficiency. As such, it presents a promising solution for improving the performance of inspection robots in complex indoor poultry farming settings.
<div id='section'>PaperID: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2504.08395.pdf' target='_blank'>https://arxiv.org/pdf/2504.08395.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pepita Barnard, Maria J Galvez Trigo, Dominic Price, Sue Cobb, Gisela Reyes-Cruz, Gustavo Berumen, David Branson, Mojtaba A. Khanesar, Mercedes Torres Torres, Michel Valstar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08395">Human strategies for correcting `human-robot' errors during a laundry sorting task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mental models and expectations underlying human-human interaction (HHI) inform human-robot interaction (HRI) with domestic robots. To ease collaborative home tasks by improving domestic robot speech and behaviours for human-robot communication, we designed a study to understand how people communicated when failure occurs. To identify patterns of natural communication, particularly in response to robotic failures, participants instructed Laundrobot to move laundry into baskets using natural language and gestures. Laundrobot either worked error-free, or in one of two error modes. Participants were not advised Laundrobot would be a human actor, nor given information about error modes. Video analysis from 42 participants found speech patterns, included laughter, verbal expressions, and filler words, such as ``oh'' and ``ok'', also, sequences of body movements, including touching one's own face, increased pointing with a static finger, and expressions of surprise. Common strategies deployed when errors occurred, included correcting and teaching, taking responsibility, and displays of frustration. The strength of reaction to errors diminished with exposure, possibly indicating acceptance or resignation. Some used strategies similar to those used to communicate with other technologies, such as smart assistants. An anthropomorphic robot may not be ideally suited to this kind of task. Laundrobot's appearance, morphology, voice, capabilities, and recovery strategies may have impacted how it was perceived. Some participants indicated Laundrobot's actual skills were not aligned with expectations; this made it difficult to know what to expect and how much Laundrobot understood. Expertise, personality, and cultural differences may affect responses, however these were not assessed.
<div id='section'>PaperID: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2504.06189.pdf' target='_blank'>https://arxiv.org/pdf/2504.06189.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Francisco J. RodrÃ­guez Lera, Raquel FernÃ¡ndez HernÃ¡ndez, Sonia Lopez GonzÃ¡lez, Miguel Angel GonzÃ¡lez-Santamarta, Francisco JesÃºs RodrÃ­guez Sedano, Camino Fernandez Llamas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06189">Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI.
<div id='section'>PaperID: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2504.01260.pdf' target='_blank'>https://arxiv.org/pdf/2504.01260.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Roy El-Helou, Matthew K. X. J Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01260">The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores how human perceptions of a non-anthropomorphic robotic manipulator are shaped by two key dimensions of behaviour: arousal, defined as the robot's movement energy and expressiveness, and attention, defined as the robot's capacity to selectively orient toward and engage with a user. We introduce a novel control architecture that integrates a gaze-like attention engine with an arousal-modulated motion system to generate socially meaningful behaviours. In a user study, we find that robots exhibiting high attention -- actively directing their focus toward users -- are perceived as warmer and more competent, intentional, and lifelike. In contrast, high arousal -- characterized by fast, expansive, and energetic motions -- increases perceptions of discomfort and disturbance. Importantly, a combination of focused attention and moderate arousal yields the highest ratings of trust and sociability, while excessive arousal diminishes social engagement. These findings offer design insights for endowing non-humanoid robots with expressive, intuitive behaviours that support more natural human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2503.21232.pdf' target='_blank'>https://arxiv.org/pdf/2503.21232.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ayush Bheemaiah, Seungyong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21232">Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inability of autonomous vehicles (AVs) to infer the material properties of obstacles limits their decision-making capacity. While AVs rely on sensor systems such as cameras, LiDAR, and radar to detect obstacles, this study suggests combining sensors with a knowledge graph (KG)-based world model to improve AVs' comprehension of physical material qualities. Beyond sensor data, AVs can infer qualities such as malleability, density, and elasticity using a semantic KG that depicts the relationships between obstacles and their attributes. Using the CARLA autonomous driving simulator, we evaluated AV performance with and without KG integration. The findings demonstrate that the KG-based method improves obstacle management, which allows AVs to use material qualities to make better decisions about when to change lanes or apply emergency braking. For example, the KG-integrated AV changed lanes for hard impediments like traffic cones and successfully avoided collisions with flexible items such as plastic bags by passing over them. Compared to the control system, the KG framework demonstrated improved responsiveness to obstacles by resolving conflicting sensor data, causing emergency stops for 13.3% more cases. In addition, our method exhibits a 6.6% higher success rate in lane-changing maneuvers in experimental scenarios, particularly for larger, high-impact obstacles. While we focus particularly on autonomous driving, our work demonstrates the potential of KG-based world models to improve decision-making in embodied AI systems and scale to other domains, including robotics, healthcare, and environmental simulation.
<div id='section'>PaperID: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2503.17730.pdf' target='_blank'>https://arxiv.org/pdf/2503.17730.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Francisco J. RodrÃ­guez Lera, Yoana Pita Lorenzo, David SobrÃ­n Hidalgo, Laura FernÃ¡ndez Becerra, Irene GonzÃ¡lez FernÃ¡ndez, Jose Miguel Guerrero HernÃ¡ndez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17730">Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en robÃ³tica y sistemas autÃ³nomos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybersecurity in robotics stands out as a key aspect within Regulation (EU) 2024/1689, also known as the Artificial Intelligence Act, which establishes specific guidelines for intelligent and automated systems. A fundamental distinction in this regulatory framework is the difference between robots with Artificial Intelligence (AI) and those that operate through automation systems without AI, since the former are subject to stricter security requirements due to their learning and autonomy capabilities. This work analyzes cybersecurity tools applicable to advanced robotic systems, with special emphasis on the protection of knowledge bases in cognitive architectures. Furthermore, a list of basic tools is proposed to guarantee the security, integrity, and resilience of these systems, and a practical case is presented, focused on the analysis of robot knowledge management, where ten evaluation criteria are defined to ensure compliance with the regulation and reduce risks in human-robot interaction (HRI) environments.
<div id='section'>PaperID: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2503.16467.pdf' target='_blank'>https://arxiv.org/pdf/2503.16467.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anargh Viswanath, Lokesh Veeramacheneni, Hendrik Buschmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16467">Enhancing Explainability with Multimodal Context Representations for Smarter Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence (AI) has significantly advanced in recent years, driving innovation across various fields, especially in robotics. Even though robots can perform complex tasks with increasing autonomy, challenges remain in ensuring explainability and user-centered design for effective interaction. A key issue in Human-Robot Interaction (HRI) is enabling robots to effectively perceive and reason over multimodal inputs, such as audio and vision, to foster trust and seamless collaboration. In this paper, we propose a generalized and explainable multimodal framework for context representation, designed to improve the fusion of speech and vision modalities. We introduce a use case on assessing 'Relevance' between verbal utterances from the user and visual scene perception of the robot. We present our methodology with a Multimodal Joint Representation module and a Temporal Alignment module, which can allow robots to evaluate relevance by temporally aligning multimodal inputs. Finally, we discuss how the proposed framework for context representation can help with various aspects of explainability in HRI.
<div id='section'>PaperID: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2503.14931.pdf' target='_blank'>https://arxiv.org/pdf/2503.14931.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ehud Nahum, Yael Edan, Tal Oron-Gilad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14931">Advancing a taxonomy for proxemics in robot social navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying robots in human environments requires effective social robot navigation. This article focuses on proxemics, proposing a new taxonomy and suggesting future directions through an analysis of state-of-the-art studies and the identification of research gaps. The various factors that affect the dynamic properties of proxemics patterns in human-robot interaction are thoroughly explored. To establish a coherent proxemics framework, we identified and organized the key parameters and attributes that shape proxemics behavior. Building on this framework, we introduce a novel approach to define proxemics in robot navigation, emphasizing the significant attributes that influence its structure and size. This leads to the development of a new taxonomy that serves as a foundation for guiding future research and development. Our findings underscore the complexity of defining personal distance, revealing it as a complex, multi-dimensional challenge. Furthermore, we highlight the flexible and dynamic nature of personal zone boundaries, which should be adaptable to different contexts and circumstances. Additionally, we propose a new layer for implementing proxemics in the navigation of social robots.
<div id='section'>PaperID: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2503.14719.pdf' target='_blank'>https://arxiv.org/pdf/2503.14719.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Miguel S. Soriano-GarcÃ­a, Diego A. Mercado-Ravell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14719">ViVa-SAFELAND: a New Freeware for Safe Validation of Vision-based Navigation in Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ViVa-SAFELAND is an open source software library, aimed to test and evaluate vision-based navigation strategies for aerial vehicles, with special interest in autonomous landing, while complying with legal regulations and people's safety. It consists of a collection of high definition aerial videos, focusing on real unstructured urban scenarios, recording moving obstacles of interest, such as cars and people. Then, an Emulated Aerial Vehicle (EAV) with a virtual moving camera is implemented in order to ``navigate" inside the video, according to high-order commands. ViVa-SAFELAND provides a new, safe, simple and fair comparison baseline to evaluate and compare different visual navigation solutions under the same conditions, and to randomize variables along several trials. It also facilitates the development of autonomous landing and navigation strategies, as well as the generation of image datasets for different training tasks. Moreover, it is useful for training either human of autonomous pilots using deep learning. The effectiveness of the framework for validating vision algorithms is demonstrated through two case studies, detection of moving objects and risk assessment segmentation. To our knowledge, this is the first safe validation framework of its kind, to test and compare visual navigation solution for aerial vehicles, which is a crucial aspect for urban deployment in complex real scenarios.
<div id='section'>PaperID: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2503.08174.pdf' target='_blank'>https://arxiv.org/pdf/2503.08174.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Veronica Bot, Zheyuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08174">Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated unprecedented capability in reasoning with natural language. Coupled with this development is the emergence of embodied AI in robotics. Despite showing promise for verbal and written reasoning tasks, it remains unknown whether LLMs are capable of navigating complex spatial tasks with physical actions in the real world. To this end, it is of interest to investigate applying LLMs to robotics in zero-shot learning scenarios, and in the absence of fine-tuning - a feat which could significantly improve human-robot interaction, alleviate compute cost, and eliminate low-level programming tasks associated with robot tasks.
  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot in Webots engine for an object search task. We evaluate the effectiveness of three reasoning strategies based on Chain-of-Thought (CoT) sub-task list generation with the Socratic method (SocraCoT) (in order of increasing rigor): (1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was measured in terms of the proportion of tasks successfully completed and execution time (N = 20). Our preliminary results show that when combined with chain-of-thought reasoning, the Socratic method can be used for code generation for robotic tasks that require spatial awareness. In extension of this finding, we propose EVINCE-LoC; a modified EVINCE method that could further enhance performance in highly complex and or dynamic testing scenarios.
<div id='section'>PaperID: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2503.05251.pdf' target='_blank'>https://arxiv.org/pdf/2503.05251.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lorenzo Scarciglia, Antonio Paolillo, Daniele Palossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05251">A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently entered the drone racing scenario, where they are tasked to avoid obstacles and navigate as fast as possible through gates. However, in contrast with their bigger counterparts, i.e., kg-scale drones, nano-drones expose three orders of magnitude less onboard memory and compute power, demanding more efficient and lightweight vision-based pipelines to win the race. This work presents a map-free vision-based (using only a monocular camera) autonomous nano-drone that combines a real-time deep learning gate detection front-end with a classic yet elegant and effective visual servoing control back-end, only relying on onboard resources. Starting from two state-of-the-art tiny deep learning models, we adapt them for our specific task, and after a mixed simulator-real-world training, we integrate and deploy them aboard our nano-drone. Our best-performing pipeline costs of only 24M multiply-accumulate operations per frame, resulting in a closed-loop control performance of 30 Hz, while achieving a gate detection root mean square error of 1.4 pixels, on our ~20k real-world image dataset. In-field experiments highlight the capability of our nano-drone to successfully navigate through 15 gates in 4 min, never crashing and covering a total travel distance of ~100m, with a peak flight speed of 1.9 m/s. Finally, to stress the generalization capability of our system, we also test it in a never-seen-before environment, where it navigates through gates for more than 4 min.
<div id='section'>PaperID: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2503.04296.pdf' target='_blank'>https://arxiv.org/pdf/2503.04296.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Filippo Cantucci, Marco Marini, Rino Falcone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04296">The Role of Robot Competence, Autonomy, and Personality on Trust Formation in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human trust in social robots is a complex attitude based on cognitive and emotional evaluations, as well as a behavior, like task delegation. While previous research explored the features of robots that influence overall trust attitude, it remains unclear whether these features affect behavioral trust. Additionally, there is limited investigation into which features of robots influence cognitive and emotional attitudes, and how these attitudes impact humans' willingness to delegate new tasks to robots. This study examines the interplay between competence, autonomy, and personality traits of robots and their impact on trust attitudes (cognitive and affective trust) and trust behavior (task delegation), within the context of task-oriented Human-Robot Interaction. Our findings indicate that robot competence is a key determinant of trust, influencing cognitive, affective, and behavioral trust. In contrast, robot personality traits significantly impact only affective trust without affecting cognitive trust or trust behavior. In addition, autonomy was found to moderate the relationship between competence and cognitive trust, as well as between personality and affective trust. Finally, cognitive trust was found to positively influence task delegation, whereas affective trust did not show a significant effect. This paper contributes to the literature on Human-Robot Trust by providing novel evidence that enhances the acceptance and effectiveness of social robots in collaborative scenarios.
<div id='section'>PaperID: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2503.02525.pdf' target='_blank'>https://arxiv.org/pdf/2503.02525.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Martin Cooney, Alexey Vinel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02525">Magic in Human-Robot Interaction (HRI)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"Magic" is referred to here and there in the robotics literature, from "magical moments" afforded by a mobile bubble machine, to "spells" intended to entertain and motivate children--but what exactly could this concept mean for designers? Here, we present (1) some theoretical discussion on how magic could inform interaction designs based on reviewing the literature, followed by (2) a practical description of using such ideas to develop a simplified prototype, which received an award in an international robot magic competition. Although this topic can be considered unusual and some negative connotations exist (e.g., unrealistic thinking can be referred to as magical), our results seem to suggest that magic, in the experiential, supernatural, and illusory senses of the term, could be useful to consider in various robot design contexts, also for artifacts like home assistants and autonomous vehicles--thus, inviting further discussion and exploration.
<div id='section'>PaperID: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2503.00284.pdf' target='_blank'>https://arxiv.org/pdf/2503.00284.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maaz Qureshi, Kerstin Dautenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00284">Human-Robot Collaboration: A Non-Verbal Approach with the NAO Humanoid Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots, particularly NAO, are gaining prominence for their potential to revolutionize human-robot collaboration, especially in domestic settings like kitchens. Leveraging the advantages of NAO, this research explores non-verbal communications role in enhancing human-robot interaction during meal preparation tasks. By employing gestures, body movements, and visual cues, NAO provides feedback to users, improving comprehension and safety. Our study investigates user perceptions of NAO feedback and its anthropomorphic attributes. Findings suggest that combining various non-verbal cues enhances communication effectiveness, although achieving full anthropomorphic likeness remains a challenge. Insights from this research inform the design of future robotic systems for improved human-robot collaboration.
<div id='section'>PaperID: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2502.15252.pdf' target='_blank'>https://arxiv.org/pdf/2502.15252.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amartaivan Sanjjamts, Hiroshi Morita, Togootogtokh Enkhtogtokh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15252">Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding collective pedestrian movement is crucial for applications in crowd management, autonomous navigation, and human-robot interaction. This paper investigates the use of sequential deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers, for real-time flock detection in multi-pedestrian trajectories. Our proposed approach consists of a two-stage process: first, a pre-trained binary classification model is used for pairwise trajectory classification, and second, the learned representations are applied to identify multi-agent flocks dynamically.
  We validate our method using real-world group movement datasets, demonstrating its robustness across varying sequence lengths and diverse movement patterns. Experimental results indicate that our model consistently detects pedestrian flocks with high accuracy and stability, even in dynamic and noisy environments. Furthermore, we extend our approach to identify other forms of collective motion, such as convoys and swarms, paving the way for more comprehensive multi-agent behavior analysis.
<div id='section'>PaperID: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2502.02967.pdf' target='_blank'>https://arxiv.org/pdf/2502.02967.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bastien Muraccioli, Mathieu Celerier, Mehdi Benallegue, Gentiane Venture
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02967">Demonstrating a Control Framework for Physical Human-Robot Interaction Toward Industrial Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0, which focuses on human-centric approaches. However, few studies explore the practical alignment of pHRI to industrial-grade performance. This paper introduces a versatile control framework designed to bridge this gap by incorporating the torque-based control modes: compliance control, null-space compliance, and dual compliance, all in static and dynamic scenarios. Thanks to our second-order Quadratic Programming (QP) formulation, strict kinematic and collision constraints are integrated into the system as safety features, and a weighted hierarchy guarantees singularity-robust task tracking performance. The framework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped with a Bota force/torque sensor. A DualShock 4 game controller is attached to the robot's end-effector to demonstrate the framework's capabilities. This setup enables seamless dynamic switching between the modes, and real-time adjustments of parameters, such as transitioning between position and torque control or selecting a more robust custom-developed low-level torque controller over the default one. Built on the open-source robotic control software mc_rtc, our framework ensures reproducibility for both research and industrial deployment, this framework demonstrates a step toward industrial-grade performance and repeatability, showcasing its potential as a robust pHRI control system for industrial environments.
<div id='section'>PaperID: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2502.02443.pdf' target='_blank'>https://arxiv.org/pdf/2502.02443.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zi-Qi Yang, Miaomiao Wang, Mehrdad R. Kermani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02443">A Null Space Compliance Approach for Maintaining Safety and Tracking Performance in Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the focus on developing robot manipulators has shifted towards prioritizing safety in Human-Robot Interaction (HRI). Impedance control is a typical approach for interaction control in collaboration tasks. However, such a control approach has two main limitations: 1) the end-effector (EE)'s limited compliance to adapt to unknown physical interactions, and 2) inability of the robot body to compliantly adapt to unknown physical interactions. In this work, we present an approach to address these drawbacks. We introduce a modified Cartesian impedance control method combined with a Dynamical System (DS)-based motion generator, aimed at enhancing the interaction capability of the EE without compromising main task tracking performance. This approach enables human coworkers to interact with the EE on-the-fly, e.g. tool changeover, after which the robot compliantly resumes its task. Additionally, combining with a new null space impedance control method enables the robot body to exhibit compliant behaviour in response to interactions, avoiding serious injuries from accidental contact while mitigating the impact on main task tracking performance. Finally, we prove the passivity of the system and validate the proposed approach through comprehensive comparative experiments on a 7 Degree-of-Freedom (DOF) KUKA LWR IV+ robot.
<div id='section'>PaperID: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2501.00541.pdf' target='_blank'>https://arxiv.org/pdf/2501.00541.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Adnan Rashid, Sa'ed Abed, Osman Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00541">Formalization of Biological Circuit Block Diagrams for formally analyzing Biomedical Control Systems in pHRI Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The control of Biomedical Systems in Physical Human-Robot Interaction (pHRI) plays a pivotal role in achieving the desired behavior by ensuring the intended transfer function and stability of subsystems within the overall system. Traditionally, the control aspects of biomedical systems have been analyzed using manual proofs and computer based analysis tools. However, these approaches provide inaccurate results due to human error in manual proofs and unverified algorithms and round-off errors in computer-based tools. We argue using Interactive reasoning, or frequently called theorem proving, to analyze control systems of biomedical engineering applications, specifically in the context of Physical Human-Robot Interaction (pHRI). Our methodology involves constructing mathematical models of the control components using Higher-order Logic (HOL) and analyzing them through deductive reasoning in the HOL Light theorem prover. We propose to model these control systems in terms of their block diagram representations, which in turn utilize the corresponding differential equations and their transfer function-based representation using the Laplace Transform (LT). These formally represented block diagrams are then analyzed through logical reasoning in the trusted environment of a theorem prover to ensure the correctness of the results. For illustration, we present a real-world case study by analyzing the control system of the ultrafilteration dialysis process.
<div id='section'>PaperID: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2412.20632.pdf' target='_blank'>https://arxiv.org/pdf/2412.20632.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jordan Sinclair, Christopher Reardon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20632">EVOLVE: Emotion and Visual Output Learning via LLM Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human acceptance of social robots is greatly effected by empathy and perceived understanding. This necessitates accurate and flexible responses to various input data from the user. While systems such as this can become increasingly complex as more states or response types are included, new research in the application of large language models towards human-robot interaction has allowed for more streamlined perception and reaction pipelines. LLM-selected actions and emotional expressions can help reinforce the realism of displayed empathy and allow for improved communication between the robot and user. Beyond portraying empathy in spoken or written responses, this shows the possibilities of using LLMs in actuated, real world scenarios. In this work we extend research in LLM-driven nonverbal behavior for social robots by considering more open-ended emotional response selection leveraging new advances in vision-language models, along with emotionally aligned motion and color pattern selections that strengthen conveyance of meaning and empathy.
<div id='section'>PaperID: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2412.18601.pdf' target='_blank'>https://arxiv.org/pdf/2412.18601.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fernando Jia, Jade Zheng, Florence Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18601">Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of GameFi, a fusion of gaming and decentralized finance (DeFi), there exists a critical need to enhance player engagement and economic interaction within gaming ecosystems. Our GameFi ecosystem aims to fundamentally transform this landscape by integrating advanced embodied AI agents into GameFi platforms. These AI agents, developed using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI, are capable of proactive, adaptive, and contextually rich interactions with players. By going beyond traditional scripted responses, these agents become integral participants in the game's narrative and economic systems, directly influencing player strategies and in-game economies. We address the limitations of current GameFi platforms, which often lack immersive AI interactions and mechanisms for community engagement or creator monetization. Through the deep integration of AI agents with blockchain technology, we establish a consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers creators to monetize their contributions and fosters democratic collaboration among players and creators. Furthermore, by embedding DeFi mechanisms into the gaming experience, we enhance economic participation and provide new opportunities for financial interactions within the game. Our approach enhances player immersion and retention and advances the GameFi ecosystem by bridging traditional gaming with Web3 technologies. By integrating sophisticated AI and DeFi elements, we contribute to the development of more engaging, economically robust, and community-centric gaming environments. This project represents a significant advancement in the state-of-the-art in GameFi, offering insights and methodologies that can be applied throughout the gaming industry.
<div id='section'>PaperID: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2412.15462.pdf' target='_blank'>https://arxiv.org/pdf/2412.15462.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ammar N. Abbas, Csaba Beleznai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15462">TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through Large/Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TalkWithMachines aims to enhance human-robot interaction by contributing to interpretable industrial robotic systems, especially for safety-critical applications. The presented paper investigates recent advancements in Large Language Models (LLMs) and Vision Language Models (VLMs), in combination with robotic perception and control. This integration allows robots to understand and execute commands given in natural language and to perceive their environment through visual and/or descriptive inputs. Moreover, translating the LLM's internal states and reasoning into text that humans can easily understand ensures that operators gain a clearer insight into the robot's current state and intentions, which is essential for effective and safe operation. Our paper outlines four LLM-assisted simulated robotic control workflows, which explore (i) low-level control, (ii) the generation of language-based feedback that describes the robot's internal states, (iii) the use of visual information as additional input, and (iv) the use of robot structure information for generating task plans and feedback, taking the robot's physical capabilities and limitations into account. The proposed concepts are presented in a set of experiments, along with a brief discussion. Project description, videos, and supplementary materials will be available on the project website: https://talk-machines.github.io.
<div id='section'>PaperID: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2412.14837.pdf' target='_blank'>https://arxiv.org/pdf/2412.14837.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qihang Cao, Huangxun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14837">ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging Scenes with Subtly Distinguished Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding is an important task, and there has been a recent surge of research interest in aligning 3D representations of point clouds with text to empower embodied AI. However, due to the lack of comprehensive 3D benchmarks, the capabilities of 3D models in real-world scenes, particularly those that are challenging with subtly distinguished objects, remain insufficiently investigated. To facilitate a more thorough evaluation of 3D models' capabilities, we propose a scheme, ObjVariantEnsemble, to systematically introduce more scenes with specified object classes, colors, shapes, quantities, and spatial relationships to meet model evaluation needs. More importantly, we intentionally construct scenes with similar objects to a certain degree and design an LLM-VLM-cooperated annotator to capture key distinctions as annotations. The resultant benchmark can better challenge 3D models, reveal their shortcomings in understanding, and potentially aid in the further development of 3D models.
<div id='section'>PaperID: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2412.10599.pdf' target='_blank'>https://arxiv.org/pdf/2412.10599.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nikunj Sanghai, Nik Bear Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10599">Advances in Transformers for Robotic Applications: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of Transformers architecture has brought about significant breakthroughs in Deep Learning (DL), particularly within Natural Language Processing (NLP). Since their inception, Transformers have outperformed many traditional neural network architectures due to their "self-attention" mechanism and their scalability across various applications. In this paper, we cover the use of Transformers in Robotics. We go through recent advances and trends in Transformer architectures and examine their integration into robotic perception, planning, and control for autonomous systems. Furthermore, we review past work and recent research on use of Transformers in Robotics as pre-trained foundation models and integration of Transformers with Deep Reinforcement Learning (DRL) for autonomous systems. We discuss how different Transformer variants are being adapted in robotics for reliable planning and perception, increasing human-robot interaction, long-horizon decision-making, and generalization. Finally, we address limitations and challenges, offering insight and suggestions for future research directions.
<div id='section'>PaperID: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2412.06469.pdf' target='_blank'>https://arxiv.org/pdf/2412.06469.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Craig Vear, Johann Benerradi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06469">Jess+: designing embodied AI for interactive music-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we discuss the conceptualisation and design of embodied AI within an inclusive music-making project. The central case study is Jess+ an intelligent digital score system for shared creativity with a mixed ensemble of non-disabled and disabled musicians. The overarching aim is that the digital score enables disabled musicians to thrive in a live music conversation with other musicians regardless of the potential barriers of disability and music-making. After defining what we mean by embodied AI and how this approach supports the aims of the Jess+ project, we outline the main design features of the system. This includes several novel approaches such as its modular design, an AI Factory based on an embodied musicking dataset, and an embedded belief system. Our findings showed that the implemented design decisions and embodied-AI approach led to rich experiences for the musicians which in turn transformed their practice as an inclusive ensemble.
<div id='section'>PaperID: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2412.04820.pdf' target='_blank'>https://arxiv.org/pdf/2412.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Charles Dietzel, Patrick J. Martin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04820">Assessing Similarity Measures for the Evaluation of Human-Robot Motion Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One key area of research in Human-Robot Interaction is solving the human-robot correspondence problem, which asks how a robot can learn to reproduce a human motion demonstration when the human and robot have different dynamics and kinematic structures. Evaluating these correspondence problem solutions often requires the use of qualitative surveys that can be time consuming to design and administer. Additionally, qualitative survey results vary depending on the population of survey participants. In this paper, we propose the use of heterogeneous time-series similarity measures as a quantitative evaluation metric for evaluating motion correspondence to complement these qualitative surveys. To assess the suitability of these measures, we develop a behavioral cloning-based motion correspondence model, and evaluate it with a qualitative survey as well as quantitative measures. By comparing the resulting similarity scores with the human survey results, we identify Gromov Dynamic Time Warping as a promising quantitative measure for evaluating motion correspondence.
<div id='section'>PaperID: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2412.02655.pdf' target='_blank'>https://arxiv.org/pdf/2412.02655.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pranav Doma, Aliasghar Arab, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02655">LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous navigation guided by natural language instructions is essential for improving human-robot interaction and enabling complex operations in dynamic environments. While large language models (LLMs) are not inherently designed for planning, they can significantly enhance planning efficiency by providing guidance and informing constraints to ensure safety. This paper introduces a planning framework that integrates LLMs with 2D occupancy grid maps and natural language commands to improve spatial reasoning and task execution in resource-limited settings. By decomposing high-level commands and real-time environmental data, the system generates structured navigation plans for pick-and-place tasks, including obstacle avoidance, goal prioritization, and adaptive behaviors. The framework dynamically recalculates paths to address environmental changes and aligns with implicit social norms for seamless human-robot interaction. Our results demonstrates the potential of LLMs to design context-aware system to enhance navigation efficiency and safety in industrial and dynamic environments.
<div id='section'>PaperID: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2411.18587.pdf' target='_blank'>https://arxiv.org/pdf/2411.18587.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Suzanne Oliver, Tomoko Kitago, Adam Buchwald, S. Farokh Atashzar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18587">EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot Interaction: Modulating Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User engagement, cognitive participation, and motivation during task execution in physical human-robot interaction are crucial for motor learning. These factors are especially important in contexts like robotic rehabilitation, where neuroplasticity is targeted. However, traditional robotic rehabilitation systems often face challenges in maintaining user engagement, leading to unpredictable therapeutic outcomes. To address this issue, various techniques, such as assist-as-needed controllers, have been developed to prevent user slacking and encourage active participation. In this paper, we introduce a new direction through a novel multi-modal robotic interaction designed to enhance user engagement by synergistically integrating visual, motor, cognitive, and auditory (speech recognition) tasks into a single, comprehensive activity. To assess engagement quantitatively, we compared multiple electroencephalography (EEG) biomarkers between this multi-modal protocol and a traditional motor-only protocol. Fifteen healthy adult participants completed 100 trials of each task type. Our findings revealed that EEG biomarkers, particularly relative alpha power, showed statistically significant improvements in engagement during the multi-modal task compared to the motor-only task. Moreover, while engagement decreased over time in the motor-only task, the multi-modal protocol maintained consistent engagement, suggesting that users could remain engaged for longer therapy sessions. Our observations on neural responses during interaction indicate that the proposed multi-modal approach can effectively enhance user engagement, which is critical for improving outcomes. This is the first time that objective neural response highlights the benefit of a comprehensive robotic intervention combining motor, cognitive, and auditory functions in healthy subjects.
<div id='section'>PaperID: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2411.16723.pdf' target='_blank'>https://arxiv.org/pdf/2411.16723.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mitchell Rosser, Marc. G Carmichael
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16723">Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent development of natural language generation models - termed as large language models (LLMs) - a potential use case has opened up to improve the way that humans interact with robot assistants. These LLMs should be able to leverage their large breadth of understanding to interpret natural language commands into effective, task appropriate and safe robot task executions. However, in reality, these models suffer from hallucinations, which may cause safety issues or deviations from the task. In other domains, these issues have been improved through the use of collaborative AI systems where multiple LLM agents can work together to collectively plan, code and self-check outputs. In this research, multiple collaborative AI systems were tested against a single independent AI agent to determine whether the success in other domains would translate into improved human-robot interaction performance. The results show that there is no defined trend between the number of agents and the success of the model. However, it is clear that some collaborative AI agent architectures can exhibit a greatly improved capacity to produce error-free code and to solve abstract problems.
<div id='section'>PaperID: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2411.05122.pdf' target='_blank'>https://arxiv.org/pdf/2411.05122.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Leanne Oon Hui Yee, Siew Sui Fun, Thit Sar Zin, Zar Nie Aung, Kian Meng Yap, Jiehan Teoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05122">Socially Assistive Robots: A Technological Approach to Emotional Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's high-pressure and isolated society, the demand for emotional support has surged, necessitating innovative solutions. Socially Assistive Robots (SARs) offer a technological approach to providing emotional assistance by leveraging advanced robotics, artificial intelligence, and sensor technologies. This study explores the development of an emotional support robot designed to detect and respond to human emotions, particularly sadness, through facial recognition and gesture analysis. Utilising the Lego Mindstorms Robotic Kit, Raspberry Pi 4, and various Python libraries, the robot is capable of delivering empathetic interactions, including comforting hugs and AI-generated conversations. Experimental findings highlight the robot's effective facial recognition accuracy, user interaction, and hug feedback mechanisms. These results demonstrate the feasibility of using SARs for emotional support, showcasing their potential features and functions. This research underscores the promise of SARs in providing innovative emotional assistance and enhancing human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2411.01120.pdf' target='_blank'>https://arxiv.org/pdf/2411.01120.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tengyu Hou, Hanming Bai, Ye Ding, Han Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01120">Generation of Conservative Dynamical Systems Based on Stiffness Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamical systems (DSs) provide a framework for high flexibility, robustness, and control reliability and are widely used in motion planning and physical human-robot interaction. The properties of the DS directly determine the robot's specific motion patterns and the performance of the closed-loop control system. In this paper, we establish a quantitative relationship between stiffness properties and DS. We propose a stiffness encoding framework to modulate DS properties by embedding specific stiffnesses. In particular, from the perspective of the closed-loop control system's passivity, a conservative DS is learned by encoding a conservative stiffness. The generated DS has a symmetric attraction behavior and a variable stiffness profile. The proposed method is applicable to demonstration trajectories belonging to different manifolds and types (e.g., closed and self-intersecting trajectories), and the closed-loop control system is always guaranteed to be passive in different cases. For controllers tracking the general DS, the passivity of the system needs to be guaranteed by the energy tank. We further propose a generic vector field decomposition strategy based on conservative stiffness, which effectively slows down the decay rate of energy in the energy tank and improves the stability margin of the control system. Finally, a series of simulations in various scenarios and experiments on planar and curved motion tasks demonstrate the validity of our theory and methodology.
<div id='section'>PaperID: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2410.19564.pdf' target='_blank'>https://arxiv.org/pdf/2410.19564.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Liyou Zhou, Oleg Sinavski, Athanasios Polydoros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19564">Robotic Learning in your Backyard: A Neural Simulator from Open Source Components</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of 3D Gaussian Splatting for fast and high-quality novel view synthesize has opened up the possibility to construct photo-realistic simulations from video for robotic reinforcement learning. While the approach has been demonstrated in several research papers, the software tools used to build such a simulator remain unavailable or proprietary. We present SplatGym, an open source neural simulator for training data-driven robotic control policies. The simulator creates a photorealistic virtual environment from a single video. It supports ego camera view generation, collision detection, and virtual object in-painting. We demonstrate training several visual navigation policies via reinforcement learning. SplatGym represents a notable first step towards an open-source general-purpose neural environment for robotic learning. It broadens the range of applications that can effectively utilise reinforcement learning by providing convenient and unrestricted tooling, and by eliminating the need for the manual development of conventional 3D environments.
<div id='section'>PaperID: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2410.14969.pdf' target='_blank'>https://arxiv.org/pdf/2410.14969.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marie Roald, Magnus Breder Birkenes, Lars GunnarsÃ¸nn BagÃ¸ien Johnsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14969">Visual Navigation of Digital Libraries: Retrieval and Classification of Images in the National Library of Norway's Digitised Book Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital tools for text analysis have long been essential for the searchability and accessibility of digitised library collections. Recent computer vision advances have introduced similar capabilities for visual materials, with deep learning-based embeddings showing promise for analysing visual heritage. Given that many books feature visuals in addition to text, taking advantage of these breakthroughs is critical to making library collections open and accessible. In this work, we present a proof-of-concept image search application for exploring images in the National Library of Norway's pre-1900 books, comparing Vision Transformer (ViT), Contrastive Language-Image Pre-training (CLIP), and Sigmoid loss for Language-Image Pre-training (SigLIP) embeddings for image retrieval and classification. Our results show that the application performs well for exact image retrieval, with SigLIP embeddings slightly outperforming CLIP and ViT in both retrieval and classification tasks. Additionally, SigLIP-based image classification can aid in cleaning image datasets from a digitisation pipeline.
<div id='section'>PaperID: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2410.14337.pdf' target='_blank'>https://arxiv.org/pdf/2410.14337.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chinmaya Mishra, Gabriel Skantze, Peter Hagoort, Rinus Verdonschot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14337">Perception of Emotions in Human and Robot Faces: Is the Eye Region Enough?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increased interest in developing next-gen social robots has raised questions about the factors affecting the perception of robot emotions. This study investigates the impact of robot appearances (humanlike, mechanical) and face regions (full-face, eye-region) on human perception of robot emotions. A between-subjects user study (N = 305) was conducted where participants were asked to identify the emotions being displayed in videos of robot faces, as well as a human baseline. Our findings reveal three important insights for effective social robot face design in Human-Robot Interaction (HRI): Firstly, robots equipped with a back-projected, fully animated face - regardless of whether they are more human-like or more mechanical-looking - demonstrate a capacity for emotional expression comparable to that of humans. Secondly, the recognition accuracy of emotional expressions in both humans and robots declines when only the eye region is visible. Lastly, within the constraint of only the eye region being visible, robots with more human-like features significantly enhance emotion recognition.
<div id='section'>PaperID: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2410.04173.pdf' target='_blank'>https://arxiv.org/pdf/2410.04173.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Richard C. Rodriguez, Jonah Elijah P. Bardos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04173">Fast Object Detection with a Machine Learning Edge Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This machine learning study investigates a lowcost edge device integrated with an embedded system having computer vision and resulting in an improved performance in inferencing time and precision of object detection and classification. A primary aim of this study focused on reducing inferencing time and low-power consumption and to enable an embedded device of a competition-ready autonomous humanoid robot and to support real-time object recognition, scene understanding, visual navigation, motion planning, and autonomous navigation of the robot. This study compares processors for inferencing time performance between a central processing unit (CPU), a graphical processing unit (GPU), and a tensor processing unit (TPU). CPUs, GPUs, and TPUs are all processors that can be used for machine learning tasks. Related to the aim of supporting an autonomous humanoid robot, there was an additional effort to observe whether or not there was a significant difference in using a camera having monocular vision versus stereo vision capability. TPU inference time results for this study reflect a 25% reduction in time over the GPU, and a whopping 87.5% reduction in inference time compared to the CPU. Much information in this paper is contributed to the final selection of Google's Coral brand, Edge TPU device. The Arduino Nano 33 BLE Sense Tiny ML Kit was also considered for comparison but due to initial incompatibilities and in the interest of time to complete this study, a decision was made to review the kit in a future experiment.
<div id='section'>PaperID: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2410.00517.pdf' target='_blank'>https://arxiv.org/pdf/2410.00517.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Oscar Gil Viyuela, Alberto Sanfeliu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00517">Human-Robot Collaborative Minimum Time Search through Sub-priors in Ant Colony Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-Robot Collaboration (HRC) has evolved into a highly promising issue owing to the latest breakthroughs in Artificial Intelligence (AI) and Human-Robot Interaction (HRI), among other reasons. This emerging growth increases the need to design multi-agent algorithms that can manage also human preferences. This paper presents an extension of the Ant Colony Optimization (ACO) meta-heuristic to solve the Minimum Time Search (MTS) task, in the case where humans and robots perform an object searching task together. The proposed model consists of two main blocks. The first one is a convolutional neural network (CNN) that provides the prior probabilities about where an object may be from a segmented image. The second one is the Sub-prior MTS-ACO algorithm (SP-MTS-ACO), which takes as inputs the prior probabilities and the particular search preferences of the agents in different sub-priors to generate search plans for all agents. The model has been tested in real experiments for the joint search of an object through a Vizanti web-based visualization in a tablet computer. The designed interface allows the communication between a human and our humanoid robot named IVO. The obtained results show an improvement in the search perception of the users without loss of efficiency.
<div id='section'>PaperID: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2409.15305.pdf' target='_blank'>https://arxiv.org/pdf/2409.15305.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juan M. Deniz, Andre S. Kelboucas, Ricardo Bedin Grando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15305">Real-time Robotics Situation Awareness for Accident Prevention in Industry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores human-robot interaction (HRI) based on a mobile robot and YOLO to increase real-time situation awareness and prevent accidents in the workplace. Using object segmentation, we propose an approach that is capable of analyzing these situations in real-time and providing useful information to avoid critical working situations. In the industry, ensuring the safety of workers is paramount, and solutions based on robots and AI can provide a safer environment. For that, we proposed a methodology evaluated with two different YOLO versions (YOLOv8 and YOLOv5) alongside a LoCoBot robot for supervision and to perform the interaction with a user. We show that our proposed approach is capable of navigating a test scenario and issuing alerts via Text-to-Speech when dangerous situations are faced, such as when hardhats and safety vests are not detected. Based on the results gathered, we can conclude that our system is capable of detecting and informing risk situations such as helmet/no helmet and safety vest/no safety vest situations.
<div id='section'>PaperID: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2409.11906.pdf' target='_blank'>https://arxiv.org/pdf/2409.11906.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Youssef Mohamed, Severin Lemaignan, Arzu Guneysu, Patric Jensfelt, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11906">Fusion in Context: A Multimodal Approach to Affective State Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recognition of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. However, emotional expressions can be influenced by contextual factors, leading to misinterpretations if context is not considered. Multimodal fusion, combining modalities like facial expressions, speech, and physiological signals, has shown promise in improving affect recognition. This paper proposes a transformer-based multimodal fusion approach that leverages facial thermal data, facial action units, and textual context information for context-aware emotion recognition. We explore modality-specific encoders to learn tailored representations, which are then fused using additive fusion and processed by a shared transformer encoder to capture temporal dependencies and interactions. The proposed method is evaluated on a dataset collected from participants engaged in a tangible tabletop Pacman game designed to induce various affective states. Our results demonstrate the effectiveness of incorporating contextual information and multimodal fusion for affective state recognition.
<div id='section'>PaperID: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2409.10687.pdf' target='_blank'>https://arxiv.org/pdf/2409.10687.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10687">Personalized Speech Emotion Recognition in Human-Robot Interaction using Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotions are an essential element in verbal communication, so understanding individuals' affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT/BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs.
<div id='section'>PaperID: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2409.10048.pdf' target='_blank'>https://arxiv.org/pdf/2409.10048.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wessel Ledder, Yuzhen Qin, Kiki van der Heijden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10048">Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep reinforcement learning (DRL) approaches in audio signal processing have seen substantial progress in recent years, audio-driven DRL for tasks such as navigation, gaze control and head-orientation control in the context of human-robot interaction have received little attention. Here, we propose an audio-driven DRL framework in which we utilise deep Q-learning to develop an autonomous agent that orients towards a talker in the acoustic environment based on stereo speech recordings. Our results show that the agent learned to perform the task at a near perfect level when trained on speech segments in anechoic environments (that is, without reverberation). The presence of reverberation in naturalistic acoustic environments affected the agent's performance, although the agent still substantially outperformed a baseline, randomly acting agent. Finally, we quantified the degree of generalization of the proposed DRL approach across naturalistic acoustic environments. Our experiments revealed that policies learned by agents trained on medium or high reverb environments generalized to low reverb environments, but policies learned by agents trained on anechoic or low reverb environments did not generalize to medium or high reverb environments. Taken together, this study demonstrates the potential of audio-driven DRL for tasks such as head-orientation control and highlights the need for training strategies that enable robust generalization across environments for real-world audio-driven DRL applications.
<div id='section'>PaperID: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2409.09429.pdf' target='_blank'>https://arxiv.org/pdf/2409.09429.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Damian Hostettler, Simon Mayer, Jan Liam Albert, Kay Erik Jenss, Christian Hildebrand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09429">Real-Time Adaptive Industrial Robots: Improving Safety And Comfort In Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial robots become increasingly prevalent, resulting in a growing need for intuitive, comforting human-robot collaboration. We present a user-aware robotic system that adapts to operator behavior in real time while non-intrusively monitoring physiological signals to create a more responsive and empathetic environment. Our prototype dynamically adjusts robot speed and movement patterns while measuring operator pupil dilation and proximity. Our user study compares this adaptive system to a non-adaptive counterpart, and demonstrates that the adaptive system significantly reduces both perceived and physiologically measured cognitive load while enhancing usability. Participants reported increased feelings of comfort, safety, trust, and a stronger sense of collaboration when working with the adaptive robot. This highlights the potential of integrating real-time physiological data into human-robot interaction paradigms. This novel approach creates more intuitive and collaborative industrial environments where robots effectively 'read' and respond to human cognitive states, and we feature all data and code for future use.
<div id='section'>PaperID: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2409.08253.pdf' target='_blank'>https://arxiv.org/pdf/2409.08253.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08253">The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The question of how cyber-physical systems should interact with human partners that can take over control or exert oversight is becoming more pressing, as these systems are deployed for an ever larger range of tasks. Drawing on the literatures on handing over control during semi-autonomous driving and human-robot interaction, we propose a design of a take-over request that combines an abstract pre-alert with an informative TOR: Relevant sensor information is highlighted on the controller's display, while a spoken message verbalizes the reason for the TOR. We conduct our study in the context of a semi-autonomous drone control scenario as our testbed. The goal of our online study is to assess in more detail what form a language-based TOR should take. Specifically, we compare a full sentence condition to shorter fragments, and test whether the visual highlighting should be done synchronously or asynchronously with the speech. Participants showed a higher accuracy in choosing the correct solution with our bi-modal TOR and felt that they were better able to recognize the critical situation. Using only fragments in the spoken message rather than full sentences did not lead to improved accuracy or faster reactions. Also, synchronizing the visual highlighting with the spoken message did not result in better accuracy and response times were even increased in this condition.
<div id='section'>PaperID: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2409.05010.pdf' target='_blank'>https://arxiv.org/pdf/2409.05010.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiyi Tang, Christian Dondrup
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05010">Gesture Generation from Trimodal Context for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural co-speech gestures are essential components to improve the experience of Human-robot interaction (HRI). However, current gesture generation approaches have many limitations of not being natural, not aligning with the speech and content, or the lack of diverse speaker styles. Therefore, this work aims to repoduce the work by Yoon et,al generating natural gestures in simulation based on tri-modal inputs and apply this to a robot. During evaluation, ``motion variance'' and ``Frechet Gesture Distance (FGD)'' is employed to evaluate the performance objectively. Then, human participants were recruited to subjectively evaluate the gestures. Results show that the movements in that paper have been successfully transferred to the robot and the gestures have diverse styles and are correlated with the speech. Moreover, there is a significant likeability and style difference between different gestures.
<div id='section'>PaperID: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2601.08355.pdf' target='_blank'>https://arxiv.org/pdf/2601.08355.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guo Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08355">Semantic Misalignment in Vision-Language Models under Perceptual Degradation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.
<div id='section'>PaperID: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2601.02779.pdf' target='_blank'>https://arxiv.org/pdf/2601.02779.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ting Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02779">Hierarchical Preemptive Holistic Collaborative Systems for Embodied Multi-Agent Systems: Framework, Hybrid Stability, and Scalability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The coordination of Embodied Multi-Agent Systems in constrained physical environments requires a rigorous balance between safety, scalability, and efficiency. Traditional decentralized approaches, e.g., reactive collision avoidance, are prone to local minima or reciprocal yielding standoffs due to the lack of future intent awareness. In contrast, centralized planning suffers from intractable computational complexity and single-point-of-failure vulnerabilities. To address these limitations, we propose the Hierarchical Preemptive Holistic Collaborative (Prollect) framework, which generalizes the Preemptive Holistic Collaborative System (PHCS) by decomposing the global coordination problem into topologically connected subspace optimizations. We formalize the system as a Hybrid Automaton and introduce a three-stage receding horizon mechanism (frozen execution, preliminary planning, proactive look-ahead windows) with explicit padding to prevent races between coordination dissemination and intent updates. Notably, we design a robust timing protocol with a mandatory Idle Buffer that acts as a dwell-time constraint to eliminate Zeno behaviors and ensure computational stability under jitter. Furthermore, we formalize a Shadow Agent protocol to guarantee seamless trajectory consistency across subspace boundaries, which we treat as an Input-to-State Stability (ISS) problem.
<div id='section'>PaperID: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2512.00783.pdf' target='_blank'>https://arxiv.org/pdf/2512.00783.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Libo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00783">Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named "Sigma" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.
<div id='section'>PaperID: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2511.03733.pdf' target='_blank'>https://arxiv.org/pdf/2511.03733.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pratham Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03733">HACI: A Haptic-Audio Code Interface to Improve Educational Outcomes for Visually Impaired Introductory Programming Students</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This thesis introduces the Haptic-Audio Code Interface (HACI), an educational tool designed to enhance programming education for visually impaired (VI) students by integrating haptic and audio feedback to compensate for the absence of visual cues. HACI consists of a non-resource-intensive web application supporting JavaScript program development, execution, and debugging, connected via a cable to an Arduino-powered glove with six integrated haptic motors to provide physical feedback to VI programmers. Motivated by the need to provide equitable educational opportunities in computer science, HACI aims to improve non-visual code navigation, comprehension, summarizing, editing, and debugging for students with visual impairments while minimizing cognitive load. This work details HACI's design principles, technical implementation, and a preliminary evaluation through a pilot study conducted with undergraduate Computer Science students. Findings indicate that HACI aids in the non-visual navigation and understanding of programming constructs, although challenges remain in refining feedback mechanisms to ensure consistency and reliability, as well as supplementing the current functionality with a more feature-reach and customizable accessible learning experience which will allow visually impaired students to fully utilize interleaved haptic and audio feedback. The study underscores the transformative potential of haptic and audio feedback in educational practices for the visually impaired, setting a foundation for future research and development in accessible programming education. This thesis contributes to the field of accessible technology by demonstrating how tactile and auditory feedback can be effectively integrated into educational tools, thereby broadening accessibility in STEM education.
<div id='section'>PaperID: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2511.02838.pdf' target='_blank'>https://arxiv.org/pdf/2511.02838.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Goran Bubas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02838">How ChatGPT and Gemini View the Elements of Communication Competence of Large Language Models: A Pilot Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A concise overview is provided of selected theoretical models of communication competence in the fields of linguistics, interpersonal communication, second language use, and human-robot interaction. The following practical research consisted of two case studies with the goals of investigating how advanced AI tools like ChatGPT and Gemini interpret elements of two communication competence theories in the context of Large Language Model (LLM) interactions with users. The focus was on these theoretical approaches: (1) an integrated linguistic-interpersonal model and (2) an interpersonal "human-humanoid" interaction model. The conclusion is that both approaches are suitable for a better understanding of LLM-user interaction.
<div id='section'>PaperID: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2510.10823.pdf' target='_blank'>https://arxiv.org/pdf/2510.10823.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniel Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10823">The Irrational Machine: Neurosis and the Limits of Algorithmic Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for characterizing neurosis in embodied AI: behaviors that are internally coherent yet misaligned with reality, arising from interactions among planning, uncertainty handling, and aversive memory. In a grid navigation stack we catalogue recurrent modalities including flip-flop, plan churn, perseveration loops, paralysis and hypervigilance, futile search, belief incoherence, tie break thrashing, corridor thrashing, optimality compulsion, metric mismatch, policy oscillation, and limited-visibility variants. For each we give lightweight online detectors and reusable escape policies (short commitments, a margin to switch, smoothing, principled arbitration). We then show that durable phobic avoidance can persist even under full visibility when learned aversive costs dominate local choice, producing long detours despite globally safe routes. Using First/Second/Third Law as engineering shorthand for safety latency, command compliance, and resource efficiency, we argue that local fixes are insufficient; global failures can remain. To surface them, we propose genetic-programming based destructive testing that evolves worlds and perturbations to maximize law pressure and neurosis scores, yielding adversarial curricula and counterfactual traces that expose where architectural revision, not merely symptom-level patches, is required.
<div id='section'>PaperID: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2509.18200.pdf' target='_blank'>https://arxiv.org/pdf/2509.18200.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Ti Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18200">Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational agents must translate egocentric utterances (e.g., "on my right") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.
<div id='section'>PaperID: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2509.04731.pdf' target='_blank'>https://arxiv.org/pdf/2509.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04731">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
<div id='section'>PaperID: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2507.21589.pdf' target='_blank'>https://arxiv.org/pdf/2507.21589.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21589">Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied intelligence posits that cognitive capabilities fundamentally emerge from - and are shaped by - an agent's real-time sensorimotor interactions with its environment. Such adaptive behavior inherently requires continuous inference under uncertainty. Bayesian statistics offers a principled probabilistic framework to address this challenge by representing knowledge as probability distributions and updating beliefs in response to new evidence. The core computational processes underlying embodied intelligence - including perception, action selection, learning, and even higher-level cognition - can be effectively understood and modeled as forms of Bayesian inference. Despite the deep conceptual connection between Bayesian statistics and embodied intelligence, Bayesian principles have not been widely or explicitly applied in today's embodied intelligence systems. In this work, we examine both Bayesian and contemporary embodied intelligence approaches through two fundamental lenses: search and learning - the two central themes in modern AI, as highlighted in Rich Sutton's influential essay "The Bitter Lesson". This analysis sheds light on why Bayesian inference has not played a central role in the development of modern embodied intelligence. At the same time, it reveals that current embodied intelligence systems remain largely confined to closed-physical-world environments, and highlights the potential for Bayesian methods to play a key role in extending these systems toward truly open physical-world embodied intelligence.
<div id='section'>PaperID: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2507.20395.pdf' target='_blank'>https://arxiv.org/pdf/2507.20395.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hafsteinn Einarsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20395">MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Large Language Models (LLMs) increasingly power autonomous agents in robotics and embodied AI, understanding their spatial reasoning capabilities becomes crucial for ensuring reliable real-world deployment. Despite advances in language understanding, current research lacks evaluation of how LLMs perform spatial navigation without visual cues, a fundamental requirement for agents operating with limited sensory information. This paper addresses this gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our methodology employs a function-calling interface where models navigate mazes of varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate feedback and distance-to-wall information, excluding visual input to test fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across identical mazes in both English and Icelandic to assess cross-linguistic transfer of spatial abilities. Our findings reveal striking disparities: while OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$, other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100% of failures attributed to excessive looping behavior where models revisit a cell at least 10 times. We document a significant performance degradation in Icelandic, with models solving mazes 3-4 sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These results have important implications for global deployment of LLM-powered autonomous systems, showing spatial intelligence remains fundamentally constrained by training data availability and highlighting the need for architectural innovations to achieve reliable navigation across linguistic contexts.
<div id='section'>PaperID: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2507.09217.pdf' target='_blank'>https://arxiv.org/pdf/2507.09217.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>GÃ¶rkay Aydemir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09217">Online Long-term Point Tracking in the Foundation Model Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point tracking aims to identify the same physical point across video frames and serves as a geometry-aware representation of motion. This representation supports a wide range of applications, from robotics to augmented reality, by enabling accurate modeling of dynamic environments. Most existing long-term tracking approaches operate in an offline setting, where future frames are available to refine predictions and recover from occlusions. However, real-world scenarios often demand online predictions: the model must operate causally, using only current and past frames. This constraint is critical in streaming video and embodied AI, where decisions must be made immediately based on past observations. Under such constraints, viewpoint invariance becomes essential. Visual foundation models, trained on diverse large-scale datasets, offer the potential for robust geometric representations. While they lack temporal reasoning on their own, they can be integrated into tracking pipelines to enrich spatial features. In this thesis, we address the problem of long-term point tracking in an online setting, where frames are processed sequentially without access to future information or sliding windows. We begin by evaluating the suitability of visual foundation models for this task and find that they can serve as useful initializations and be integrated into tracking pipelines. However, to enable long-term tracking in an online setting, a dedicated design is still required. In particular, maintaining coherence over time in this causal regime requires memory to propagate appearance and context across frames. To address this, we introduce Track-On, a transformer-based model that treats each tracked point as a query and processes video frames one at a time. Track-On sets a new state of the art across seven public benchmarks, demonstrating the feasibility of long-term tracking without future access.
<div id='section'>PaperID: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2507.04160.pdf' target='_blank'>https://arxiv.org/pdf/2507.04160.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Subasish Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04160">HyperSumm-RL: A Dialogue Summarization Framework for Modeling Leadership Perception in Social Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces HyperSumm-RL, a hypertext-aware summarization and interaction analysis framework designed to investigate human perceptions of social robot leadership through long-form dialogue. The system utilizes a structured Natural Language Processing (NLP) workflow that combines transformer-based long dialogue summarization, leadership style modeling, and user response analysis, enabling scalable evaluation of social robots in complex human-robot interaction (HRI) settings. Unlike prior work that focuses on static or task-oriented HRI, HyperSumm-RL captures and hypertextually organizes dynamic conversational exchanges into navigable, semantically rich representations which allows researchers to trace interaction threads, identify influence cues, and analyze leadership framing over time. The contributions of this study are threefold: (1) we present a novel infrastructure for summarizing and linking long, multi-turn dialogues using leadership-style taxonomies; (2) we propose an interactive hypertext model that supports relational navigation across conversational themes, participant responses, and robot behavior modes; and (3) we demonstrate the utility of this system in interpreting participant trust, engagement, and expectation shifts during social robot leadership scenarios. The findings reveal how hypertextual workflows can augment HRI research by enabling transparent, interpretable, and semantically grounded analysis of emergent social dynamics.
<div id='section'>PaperID: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2506.07286.pdf' target='_blank'>https://arxiv.org/pdf/2506.07286.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aditya Chakravarty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07286">Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown remarkable flexibility for solving inverse problems without task-specific retraining. However, existing approaches such as Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update per denoising step, limiting restoration fidelity and robustness, especially in embedded or out-of-distribution settings. In this work, we introduce a multistep optimization strategy within each denoising timestep, significantly enhancing image quality, perceptual accuracy, and generalization. Our experiments on super-resolution and Gaussian deblurring demonstrate that increasing the number of gradient updates per step improves LPIPS and PSNR with minimal latency overhead. Notably, we validate this approach on a Jetson Orin Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally trained on face datasets, generalizes effectively to natural and aerial scenes. Our findings highlight MPGD's potential as a lightweight, plug-and-play restoration module for real-time visual perception in embodied AI agents such as drones and mobile robots.
<div id='section'>PaperID: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2505.12312.pdf' target='_blank'>https://arxiv.org/pdf/2505.12312.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12312">Visuospatial Cognitive Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
<div id='section'>PaperID: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2505.07668.pdf' target='_blank'>https://arxiv.org/pdf/2505.07668.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Davide Torielli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07668">Intuitive Human-Robot Interfaces Leveraging on Autonomy Features for the Control of Highly-redundant Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>[...] With the TelePhysicalOperation interface, the user can teleoperate the different capabilities of a robot (e.g., single/double arm manipulation, wheel/leg locomotion) by applying virtual forces on selected robot body parts. This approach emulates the intuitiveness of physical human-robot interaction, but at the same time it permits to teleoperate the robot from a safe distance, in a way that resembles a "Marionette" interface. The system is further enhanced with wearable haptic feedback functions to align better with the "Marionette" metaphor, and a user study has been conducted to validate its efficacy with and without the haptic channel enabled. Considering the importance of robot independence, the TelePhysicalOperation interface incorporates autonomy modules to face, for example, the teleoperation of dual-arm mobile base robots for bimanual object grasping and transportation tasks.
  With the laser-guided interface, the user can indicate points of interest to the robot through the utilization of a simple but effective laser emitter device. With a neural network-based vision system, the robot tracks the laser projection in real time, allowing the user to indicate not only fixed goals, like objects, but also paths to follow. With the implemented autonomous behavior, a mobile manipulator employs its locomanipulation abilities to follow the indicated goals. The behavior is modeled using Behavior Trees, exploiting their reactivity to promptly respond to changes in goal positions, and their modularity to adapt the motion planning to the task needs. The proposed laser interface has also been employed in an assistive scenario. In this case, users with upper limbs impairments can control an assistive manipulator by directing a head-worn laser emitter to the point of interests, to collaboratively address activities of everyday life. [...]
<div id='section'>PaperID: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2505.04897.pdf' target='_blank'>https://arxiv.org/pdf/2505.04897.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Taisuke Kobayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04897">CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive imitation learning makes an agent's control policy robust by stepwise supervisions from an expert. The recent algorithms mostly employ expert-agent switching systems to reduce the expert's burden by limitedly selecting the supervision timing. However, the precise selection is difficult and such a switching causes abrupt changes in actions, damaging the dynamic stability. This paper therefore proposes a novel method, so-called CubeDAgger, which improves robustness while reducing dynamic stability violations by making three improvements to a baseline method, EnsembleDAgger. The first improvement adds a regularization to explicitly activate the threshold for deciding the supervision timing. The second transforms the expert-agent switching system to an optimal consensus system of multiple action candidates. Third, autoregressive colored noise to the actions is introduced to make the stochastic exploration consistent over time. These improvements are verified by simulations, showing that the learned policies are sufficiently robust while maintaining dynamic stability during interaction.
<div id='section'>PaperID: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2505.03500.pdf' target='_blank'>https://arxiv.org/pdf/2505.03500.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Quanyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03500">Task Reconstruction and Extrapolation for $Ï_0$ using Text Latent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) often achieve high performance on demonstrated tasks but struggle significantly when required to extrapolate, combining skills learned from different tasks in novel ways. For instance, VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet. In this work, we demonstrate that behaviors from distinct tasks can be effectively recombined by manipulating the VLA's internal representations at inference time. Concretely, we identify the text latent by averaging the text tokens' hidden states across all demonstrated trajectories for a specific base task. For executing an extrapolated task, we can temporally interpolate the text latent of the two base tasks and add it back to the text hidden states, so sub-behaviors from the two tasks will be activated sequentially. We evaluate this approach using the newly created libero-ood benchmark, featuring 20 tasks extrapolated from standard LIBERO suites. The results on libero-ood show that all SOTA VLAs achieve < 15% success rate, while $\pi0$ with text latent interpolation reaches an 83% success rate. Further qualitative analysis reveals a tendency for VLAs to exhibit spatial overfitting, mapping object names to demonstrated locations rather than achieving genuine object and goal understanding. Additionally, we find that decoding the text latent yields human-unreadable prompts that can nevertheless instruct the VLA to achieve a 70% success rate on standard LIBERO suites, enabling private instruction or backdoor attacks.
<div id='section'>PaperID: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2505.00935.pdf' target='_blank'>https://arxiv.org/pdf/2505.00935.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Roberto Bigazzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00935">Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.
<div id='section'>PaperID: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2503.16469.pdf' target='_blank'>https://arxiv.org/pdf/2503.16469.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>S M Taslim Uddin Raju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16469">Enhancing Human-Robot Interaction in Healthcare: A Study on Nonverbal Communication Cues and Trust Dynamics with NAO Robot Caregivers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the population of older adults increases, so will the need for both human and robot care providers. While traditional practices involve hiring human caregivers to serve meals and attend to basic needs, older adults often require continuous companionship and health monitoring. However, hiring human caregivers for this job costs a lot of money. However, using a robot like Nao could be cheaper and still helpful. This study explores the integration of humanoid robots, particularly Nao, in health monitoring and caregiving for older adults. Using a mixed-methods approach with a within-subject factorial design, we investigated the effectiveness of nonverbal communication modalities, including touch, gestures, and LED patterns, in enhancing human-robot interactions. Our results indicate that Nao's touch-based health monitoring was well-received by participants, with positive ratings across various dimensions. LED patterns were perceived as more effective and accurate compared to hand and head gestures. Moreover, longer interactions were associated with higher trust levels and perceived empathy, highlighting the importance of prolonged engagement in fostering trust in human-robot interactions. Despite limitations, our study contributes valuable insights into the potential of humanoid robots to improve health monitoring and caregiving for older adults.
<div id='section'>PaperID: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2503.15693.pdf' target='_blank'>https://arxiv.org/pdf/2503.15693.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Meng Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15693">Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning (SL) and reinforcement learning (RL) are both widely used to train general-purpose agents for complex tasks, yet their generalization capabilities and underlying mechanisms are not yet fully understood. In this paper, we provide a direct comparison between SL and RL in terms of zero-shot generalization. Using the Habitat visual navigation task as a testbed, we evaluate Proximal Policy Optimization (PPO) and Behavior Cloning (BC) agents across two levels of generalization: state-goal pair generalization within seen environments and generalization to unseen environments. Our experiments show that PPO consistently outperforms BC across both zero-shot settings and performance metrics-success rate and SPL. Interestingly, even though additional optimal training data enables BC to match PPO's zero-shot performance in SPL, it still falls significantly behind in success rate. We attribute this to a fundamental difference in how models trained by these algorithms generalize: BC-trained models generalize by imitating successful trajectories, whereas TD-based RL-trained models generalize through combinatorial experience stitching-leveraging fragments of past trajectories (mostly failed ones) to construct solutions for new tasks. This allows RL to efficiently find solutions in vast state space and discover novel strategies beyond the scope of human knowledge. Besides providing empirical evidence and understanding, we also propose practical guidelines for improving the generalization capabilities of RL and SL through algorithm design.
<div id='section'>PaperID: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2503.04879.pdf' target='_blank'>https://arxiv.org/pdf/2503.04879.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sammy Christen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04879">Modeling Dynamic Hand-Object Interactions with Applications to Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans frequently grasp, manipulate, and move objects. Interactive systems assist humans in these tasks, enabling applications in Embodied AI, human-robot interaction, and virtual reality. However, current methods in hand-object synthesis often neglect dynamics and focus on generating static grasps. The first part of this dissertation introduces dynamic grasp synthesis, where a hand grasps and moves an object to a target pose. We approach this task using physical simulation and reinforcement learning. We then extend this to bimanual manipulation and articulated objects, requiring fine-grained coordination between hands. In the second part of this dissertation, we study human-to-robot handovers. We integrate captured human motion into simulation and introduce a student-teacher framework that adapts to human behavior and transfers from sim to real. To overcome data scarcity, we generate synthetic interactions, increasing training diversity by 100x. Our user study finds no difference between policies trained on synthetic vs. real motions.
<div id='section'>PaperID: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2501.18726.pdf' target='_blank'>https://arxiv.org/pdf/2501.18726.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Canxuan Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18726">Strong and Controllable 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a significant pursuit in generative computer vision with widespread applications in film-making, video games, AR/VR, and human-robot interaction. Current methods mainly utilize either diffusion-based generative models or autoregressive models for text-to-motion generation. However, they face two significant challenges: (1) The generation process is time-consuming, posing a major obstacle for real-time applications such as gaming, robot manipulation, and other online settings. (2) These methods typically learn a relative motion representation guided by text, making it difficult to generate motion sequences with precise joint-level control. These challenges significantly hinder progress and limit the real-world application of human motion generation techniques. To address this gap, we propose a simple yet effective architecture consisting of two key components. Firstly, we aim to improve hardware efficiency and computational complexity in transformer-based diffusion models for human motion generation. By customizing flash linear attention, we can optimize these models specifically for generating human motion efficiently. Furthermore, we will customize the consistency model in the motion latent space to further accelerate motion generation. Secondly, we introduce Motion ControlNet, which enables more precise joint-level control of human motion compared to previous text-to-motion generation methods. These contributions represent a significant advancement for text-to-motion generation, bringing it closer to real-world applications.
<div id='section'>PaperID: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2501.08944.pdf' target='_blank'>https://arxiv.org/pdf/2501.08944.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fouad Bousetouane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08944">Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vertical AI Agents are revolutionizing industries by delivering domain-specific intelligence and tailored solutions. However, many sectors, such as manufacturing, healthcare, and logistics, demand AI systems capable of extending their intelligence into the physical world, interacting directly with objects, environments, and dynamic conditions. This need has led to the emergence of Physical AI Agents--systems that integrate cognitive reasoning, powered by specialized LLMs, with precise physical actions to perform real-world tasks.
  This work introduces Physical AI Agents as an evolution of shared principles with Vertical AI Agents, tailored for physical interaction. We propose a modular architecture with three core blocks--perception, cognition, and actuation--offering a scalable framework for diverse industries. Additionally, we present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern, which connects physical intelligence to industry-specific LLMs for real-time decision-making and reporting informed by physical context.
  Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG framework are transforming industries like autonomous vehicles, warehouse robotics, healthcare, and manufacturing, offering businesses a pathway to integrate embodied AI for operational efficiency and innovation.
<div id='section'>PaperID: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2412.12542.pdf' target='_blank'>https://arxiv.org/pdf/2412.12542.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Katie Seaborn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12542">Bots against Bias: Critical Next Steps for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We humans are biased - and our robotic creations are biased, too. Bias is a natural phenomenon that drives our perceptions and behavior, including when it comes to socially expressive robots that have humanlike features. Recognizing that we embed bias, knowingly or not, within the design of such robots is crucial to studying its implications for people in modern societies. In this chapter, I consider the multifaceted question of bias in the context of humanoid, AI-enabled, and expressive social robots: Where does bias arise, what does it look like, and what can (or should) we do about it. I offer observations on human-robot interaction (HRI) along two parallel tracks: (1) robots designed in bias-conscious ways and (2) robots that may help us tackle bias in the human world. I outline a curated selection of cases for each track drawn from the latest HRI research and positioned against social, legal, and ethical factors. I also propose a set of critical next steps to tackle the challenges and opportunities on bias within HRI research and practice.
<div id='section'>PaperID: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2412.11632.pdf' target='_blank'>https://arxiv.org/pdf/2412.11632.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juncheng Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11632">Multi-Scale Incremental Modeling for Enhanced Human Motion Prediction in Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction is crucial for safe human-robot collaboration but remains challenging due to the complexity of modeling intricate and variable human movements. This paper presents Parallel Multi-scale Incremental Prediction (PMS), a novel framework that explicitly models incremental motion across multiple spatio-temporal scales to capture subtle joint evolutions and global trajectory shifts. PMS encodes these multi-scale increments using parallel sequence branches, enabling iterative refinement of predictions. A multi-stage training procedure with a full-timeline loss integrates temporal context. Extensive experiments on four datasets demonstrate substantial improvements in continuity, biomechanical consistency, and long-term forecast stability by modeling inter-frame increments. PMS achieves state-of-the-art performance, increasing prediction accuracy by 16.3%-64.2% over previous methods. The proposed multi-scale incremental approach provides a powerful technique for advancing human motion prediction capabilities critical for seamless human-robot interaction.
<div id='section'>PaperID: <span id='pid'>1534, <a href='https://arxiv.org/pdf/2412.03619.pdf' target='_blank'>https://arxiv.org/pdf/2412.03619.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Teng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03619">A Teleoperation System with Impedance Control and Disturbance Observer for Robot-Assisted Rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical movement therapy is a crucial method of rehabilitation aimed at reinstating mobility among patients facing motor dysfunction due to neurological conditions or accidents. Such therapy is usually featured as patient-specific, repetitive, and labor-intensive. The conventional method, where therapists collaborate with patients to conduct repetitive physical training, proves strenuous due to these characteristics. The concept of robot-assisted rehabilitation, assisting therapists with robotic systems, has gained substantial popularity. However, building such systems presents challenges, such as diverse task demands, uncertainties in dynamic models, and safety issues. To address these concerns, in this paper, we proposed a bilateral teleoperation system for rehabilitation. The control scheme of the system is designed as an integrated framework of impedance control and disturbance observer where the former can ensure compliant human-robot interaction without the need for force sensors while the latter can compensate for dynamic uncertainties when only a roughly identified dynamic model is available. Furthermore, the scheme allows free switching between tracking tasks and physical human-robot interaction (pHRI). The presented system can execute a wide array of pre-defined trajectories with varying patterns, adaptable to diverse needs. Moreover, the system can capture therapists' demonstrations, replaying them as many times as necessary. The effectiveness of the teleoperation system is experimentally evaluated and demonstrated.
<div id='section'>PaperID: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2409.18982.pdf' target='_blank'>https://arxiv.org/pdf/2409.18982.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haresh Karnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18982">Aligning Robot Navigation Behaviors with Human Intentions and Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in the field of machine learning have led to new ways for mobile robots to acquire advanced navigational capabilities. However, these learning-based methods raise the possibility that learned navigation behaviors may not align with the intentions and preferences of people, a problem known as value misalignment. To mitigate this risk, this dissertation aims to answer the question: "How can we use machine learning methods to align the navigational behaviors of autonomous mobile robots with human intentions and preferences?" First, this dissertation addresses this question by introducing a new approach to learning navigation behaviors by imitating human-provided demonstrations of the intended navigation task. This contribution allows mobile robots to acquire autonomous visual navigation capabilities through imitation, using a novel objective function that encourages the agent to align with the human's navigation objectives and penalizes misalignment. Second, this dissertation introduces two algorithms to enhance terrain-aware off-road navigation for mobile robots by learning visual terrain awareness in a self-supervised manner. This contribution enables mobile robots to respect a human operator's preferences for navigating different terrains in urban outdoor environments, while extrapolating these preferences to visually novel terrains by leveraging multi-modal representations. Finally, in the context of robot navigation in human-occupied environments, this dissertation introduces a dataset and an algorithm for robot navigation in a socially compliant manner in both indoor and outdoor environments. In summary, the contributions in this dissertation take significant steps toward addressing the value alignment problem in autonomous navigation, enabling mobile robots to navigate autonomously with objectives that align with human intentions and preferences.
<div id='section'>PaperID: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2409.14692.pdf' target='_blank'>https://arxiv.org/pdf/2409.14692.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiyang Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14692">Dynamic Realms: 4D Content Analysis, Recovery and Generation with Geometric, Topological and Physical Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>My research focuses on the analysis, recovery, and generation of 4D content, where 4D includes three spatial dimensions (x, y, z) and a temporal dimension t, such as shape and motion. This focus goes beyond static objects to include dynamic changes over time, providing a comprehensive understanding of both spatial and temporal variations. These techniques are critical in applications like AR/VR, embodied AI, and robotics. My research aims to make 4D content generation more efficient, accessible, and higher in quality by incorporating geometric, topological, and physical priors. I also aim to develop effective methods for 4D content recovery and analysis using these priors.
