<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2509.19936.pdf' target='_blank'>https://arxiv.org/pdf/2509.19936.pdf</a></span>   <span><a href='https://github.com/toukapy/capsStare' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miren Samaniego, Igor Rodriguez, Elena Lazkano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19936">CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze estimation that integrates a ConvNeXt backbone, capsule formation with attention routing, and dual GRU decoders specialized for slow and rapid gaze dynamics. This modular design enables efficient part-whole reasoning and disentangled temporal modeling, achieving state-of-the-art performance on ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference (< 10 ms). The model also generalizes well to unconstrained conditions in Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76), outperforming or matching existing methods with fewer parameters and greater interpretability. These results demonstrate that CapStARE offers a practical and robust solution for real-time gaze estimation in interactive systems. The related code and results for this article can be found on: https://github.com/toukapy/capsStare
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2509.15333.pdf' target='_blank'>https://arxiv.org/pdf/2509.15333.pdf</a></span>   <span><a href='https://github.com/LeapLabTHU/AdaptiveNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulin Wang, Yang Yue, Yang Yue, Huanqian Wang, Haojun Jiang, Yizeng Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao, Zhuofan Xia, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15333">Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2509.11895.pdf' target='_blank'>https://arxiv.org/pdf/2509.11895.pdf</a></span>   <span><a href='https://github.com/m4renz/incremental-scene-graph-prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marian Renz, Felix Igelbrink, Martin Atzmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11895">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D semantic scene graphs (3DSSG) provide compact structured representations of environments by explicitly modeling objects, attributes, and relationships. While 3DSSGs have shown promise in robotics and embodied AI, many existing methods rely mainly on sensor data, not integrating further information from semantically rich environments. Additionally, most methods assume access to complete scene reconstructions, limiting their applicability in real-world, incremental settings. This paper introduces a novel heterogeneous graph model for incremental 3DSSG prediction that integrates additional, multi-modal information, such as prior observations, directly into the message-passing process. Utilizing multiple layers, the model flexibly incorporates global and local scene representations without requiring specialized modules or full scene reconstructions. We evaluate our approach on the 3DSSG dataset, showing that GNNs enriched with multi-modal information such as semantic embeddings (e.g., CLIP) and prior observations offer a scalable and generalizable solution for complex, real-world environments. The full source code of the presented architecture will be made available at https://github.com/m4renz/incremental-scene-graph-prediction.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2509.11617.pdf' target='_blank'>https://arxiv.org/pdf/2509.11617.pdf</a></span>   <span><a href='https://github.com/cristina304/AssemMate.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zheng, Chaoran Zhang, Zijian Liang, EnTe Lin, Shubo Cui, Qinghongbing Xie, Zhaobo Xu, Long Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11617">AssemMate: Graph-Based LLM for Robotic Assembly Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM)-based robotic assembly assistance has gained significant research attention. It requires the injection of domain-specific knowledge to guide the assembly process through natural language interaction with humans. Despite some progress, existing methods represent knowledge in the form of natural language text. Due to the long context and redundant content, they struggle to meet the robots' requirements for real-time and precise reasoning. In order to bridge this gap, we present AssemMate, which utilizes the graph\textemdash a concise and accurate form of knowledge representation\textemdash as input. This graph-based LLM enables knowledge graph question answering (KGQA), supporting human-robot interaction and assembly task planning for specific products. Beyond interactive QA, AssemMate also supports sensing stacked scenes and executing grasping to assist with assembly. Specifically, a self-supervised Graph Convolutional Network (GCN) encodes knowledge graph entities and relations into a latent space and aligns them with LLM's representation, enabling the LLM to understand graph information. In addition, a vision-enhanced strategy is employed to address stacked scenes in grasping. Through training and evaluation, AssemMate outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster inference, and 28 times shorter context length, while demonstrating strong generalization ability on random graphs. And our approach further demonstrates superiority through robotic grasping experiments in both simulated and real-world settings. More details can be found on the project page: https://github.com/cristina304/AssemMate.git
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2509.10884.pdf' target='_blank'>https://arxiv.org/pdf/2509.10884.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/Nav-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10884">Nav-R1: Reasoning and Navigation in Embodied Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2509.08699.pdf' target='_blank'>https://arxiv.org/pdf/2509.08699.pdf</a></span>   <span><a href='https://github.com/podgorki/TANGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08699">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2509.05031.pdf' target='_blank'>https://arxiv.org/pdf/2509.05031.pdf</a></span>   <span><a href='https://github.com/lucamuellercode/MMITF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Müller, Hassan Ali, Philipp Allgeuer, Lukáš Gajdošech, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05031">Pointing-Guided Target Estimation via Transformer-Based Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2509.03383.pdf' target='_blank'>https://arxiv.org/pdf/2509.03383.pdf</a></span>   <span><a href='https://github.com/RLCLab/Annie' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03383">ANNIE: Be Careful of Your Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2508.15354.pdf' target='_blank'>https://arxiv.org/pdf/2508.15354.pdf</a></span>   <span><a href='https://github.com/Franky-X/Awesome-Embodied-Navigation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoran Xiong, Yulong Huang, Fangwen Yu, Changhao Chen, Yue Wang, Songpengchen Xia, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15354">Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation (EN) advances traditional navigation by enabling robots to perform complex egocentric tasks through sensing, social, and motion intelligence. In contrast to classic methodologies that rely on explicit localization and pre-defined maps, EN leverages egocentric perception and human-like interaction strategies. This survey introduces a comprehensive EN formulation structured into five stages: Transition, Observation, Fusion, Reward-policy construction, and Action (TOFRA). The TOFRA framework serves to synthesize the current state of the art, provide a critical review of relevant platforms and evaluation metrics, and identify critical open research challenges. A list of studies is available at https://github.com/Franky-X/Awesome-Embodied-Navigation.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2508.13073.pdf' target='_blank'>https://arxiv.org/pdf/2508.13073.pdf</a></span>   <span><a href='https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13073">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2508.09621.pdf' target='_blank'>https://arxiv.org/pdf/2508.09621.pdf</a></span>   <span><a href='https://github.com/snt-arg/robot_suite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingrid MaÃ©va Chekam, Ines Pastor-Martinez, Ali Tourani, Jose Andres Millan-Romera, Laura Ribeiro, Pedro Miguel Bastos Soares, Holger Voos, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09621">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at https://github.com/snt-arg/robot_suite.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2508.09547.pdf' target='_blank'>https://arxiv.org/pdf/2508.09547.pdf</a></span>   <span><a href='https://github.com/F1y1113/GoViG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09547">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2508.08252.pdf' target='_blank'>https://arxiv.org/pdf/2508.08252.pdf</a></span>   <span><a href='https://github.com/heshuting555/ReferSplat' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/heshuting555/ReferSplat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08252">ReferSplat: Referring Segmentation in 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2508.08189.pdf' target='_blank'>https://arxiv.org/pdf/2508.08189.pdf</a></span>   <span><a href='https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08189">Reinforcement Learning in Vision: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2508.06206.pdf' target='_blank'>https://arxiv.org/pdf/2508.06206.pdf</a></span>   <span><a href='https://github.com/hq-King/Affordance-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06206">Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance grounding focuses on predicting the specific regions of objects that are associated with the actions to be performed by robots. It plays a vital role in the fields of human-robot interaction, human-object interaction, embodied manipulation, and embodied perception. Existing models often neglect the affordance shared among different objects because they lack the Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD) generalization and explicit reasoning capabilities. To address these challenges, we propose Affordance-R1, the first unified affordance grounding framework that integrates cognitive CoT guided Group Relative Policy Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we designed a sophisticated affordance function, which contains format, perception, and cognition rewards to effectively guide optimization directions. Furthermore, we constructed a high-quality affordance-centric reasoning dataset, ReasonAff, to support training. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Affordance-R1 achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Comprehensive experiments demonstrate that our model outperforms well-established methods and exhibits open-world generalization. To the best of our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with reasoning into affordance reasoning. The code of our method and our dataset is released on https://github.com/hq-King/Affordance-R1.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2508.05614.pdf' target='_blank'>https://arxiv.org/pdf/2508.05614.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/OmniEmbodied' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05614">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2508.01766.pdf' target='_blank'>https://arxiv.org/pdf/2508.01766.pdf</a></span>   <span><a href='https://github.com/farlit/VPN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Feng, Zihan Wang, Yuchen Li, Rui Kong, Hengyi Cai, Shuaiqiang Wang, Gim Hee Lee, Piji Li, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01766">VPN: Visual Prompt Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While natural language is commonly used to guide embodied agents, the inherent ambiguity and verbosity of language often hinder the effectiveness of language-guided navigation in complex environments. To this end, we propose Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate using only user-provided visual prompts within 2D top-view maps. This visual prompt primarily focuses on marking the visual navigation trajectory on a top-down view of a scene, offering intuitive and spatially grounded guidance without relying on language instructions. It is more friendly for non-expert users and reduces interpretive ambiguity. We build VPN tasks in both discrete and continuous navigation settings, constructing two new datasets, R2R-VP and R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network to handle the VPN tasks, with two data augmentation strategies: view-level augmentation (altering initial headings and prompt orientations) and trajectory-level augmentation (incorporating diverse trajectories from large-scale 3D scenes), to enhance navigation performance. Extensive experiments evaluate how visual prompt forms, top-view map formats, and data augmentation strategies affect the performance of visual prompt navigation. The code is available at https://github.com/farlit/VPN.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2508.00400.pdf' target='_blank'>https://arxiv.org/pdf/2508.00400.pdf</a></span>   <span><a href='https://github.com/upeee/sari-sandbox-env' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Janika Deborah Gajo, Gerarld Paul Merales, Jerome Escarcha, Brenden Ashley Molina, Gian Nartea, Emmanuel G. Maminta, Juan Carlos Roldan, Rowel O. Atienza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00400">Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks. Addressing a gap in retail-specific sim environments for embodied agent training, Sari Sandbox features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. We also introduce SariBench, a dataset of annotated human demonstrations across varied task difficulties. Our sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance. We conclude with benchmarks, performance analysis, and recommendations for enhancing realism and scalability. The source code can be accessed via https://github.com/upeee/sari-sandbox-env.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2508.00097.pdf' target='_blank'>https://arxiv.org/pdf/2508.00097.pdf</a></span>   <span><a href='https://github.com/XR-Robotics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhigen Zhao, Liuchuan Yu, Ke Jing, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00097">XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2507.22522.pdf' target='_blank'>https://arxiv.org/pdf/2507.22522.pdf</a></span>   <span><a href='https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Peiming Li, Hong Liu, Zhichao Deng, Can Wang, Jun Liu, Junsong Yuan, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22522">Recognizing Actions from Robotic View for Natural Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural Human-Robot Interaction (N-HRI) requires robots to recognize human actions at varying distances and states, regardless of whether the robot itself is in motion or stationary. This setup is more flexible and practical than conventional human action recognition tasks. However, existing benchmarks designed for traditional action recognition fail to address the unique complexities in N-HRI due to limited data, modalities, task categories, and diversity of subjects and environments. To address these challenges, we introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored specifically for perception-centric robotic views prevalent in mobile service robots. ACTIVE comprises 30 composite action categories, 80 participants, and 46,868 annotated video instances, covering both RGB and point cloud modalities. Participants performed various human actions in diverse environments at distances ranging from 3m to 50m, while the camera platform was also mobile, simulating real-world scenarios of robot perception with varying camera heights due to uneven ground. This comprehensive and challenging benchmark aims to advance action and attribute recognition research in N-HRI. Furthermore, we propose ACTIVE-PC, a method that accurately perceives human actions at long distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic Ellipse Query, and precise decoupling of kinematic interference from human actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our code is available at: https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2507.21045.pdf' target='_blank'>https://arxiv.org/pdf/2507.21045.pdf</a></span>   <span><a href='https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowen Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21045">Reconstructing 4D Spatial Intelligence: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2507.17294.pdf' target='_blank'>https://arxiv.org/pdf/2507.17294.pdf</a></span>   <span><a href='https://github.com/jxbi1010/VLA-Touch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxin Bi, Kevin Yuchen Ma, Ce Hao, Mike Zheng Shou, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17294">VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile feedback is generally recognized to be crucial for effective interaction with the physical world. However, state-of-the-art Vision-Language-Action (VLA) models lack the ability to interpret and use tactile signals, limiting their effectiveness in contact-rich tasks. Incorporating tactile feedback into these systems is challenging due to the absence of large multi-modal datasets. We present VLA-Touch, an approach that enhances generalist robot policies with tactile sensing \emph{without fine-tuning} the base VLA. Our method introduces two key innovations: (1) a pipeline that leverages a pretrained tactile-language model that provides semantic tactile feedback for high-level task planning, and (2) a diffusion-based controller that refines VLA-generated actions with tactile signals for contact-rich manipulation. Through real-world experiments, we demonstrate that our dual-level integration of tactile feedback improves task planning efficiency while enhancing execution precision. Code is open-sourced at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2507.14049.pdf' target='_blank'>https://arxiv.org/pdf/2507.14049.pdf</a></span>   <span><a href='https://github.com/kscalelabs/evla' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>PaweÅ Budzianowski, Wesley Maa, Matthew Freed, Jingxiang Mo, Winston Hsiao, Aaron Xie, Tomasz MÅoduchowski, Viraj Tipnis, Benjamin Bolte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14049">EdgeVLA: Efficient Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have emerged as a promising approach to address the data scarcity challenge in robotics, enabling the development of generalizable visuomotor control policies. While models like OpenVLA showcase the potential of this paradigm, deploying large-scale VLMs on resource-constrained mobile manipulation systems remains a significant hurdle. This paper introduces Edge VLA (EVLA), a novel approach designed to significantly enhance the inference speed of Vision-Language-Action (VLA) models. EVLA maintains the representational power of these models while enabling real-time performance on edge devices. We achieve this through two key innovations: 1) Eliminating the autoregressive requirement for end-effector position prediction, leading to a 7x speedup in inference, and 2) Leveraging the efficiency of Small Language Models (SLMs), demonstrating comparable training performance to larger models with significantly reduced computational demands. Our early results demonstrate that EVLA achieves comparable training characteristics to OpenVLA while offering substantial gains in inference speed and memory efficiency. We release our model checkpoints and training \href{https://github.com/kscalelabs/evla }{codebase} to foster further research.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2507.08496.pdf' target='_blank'>https://arxiv.org/pdf/2507.08496.pdf</a></span>   <span><a href='https://github.com/sunshibo1234/LLaPa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shibo Sun, Xue Li, Donglin Di, Mingjie Wei, Lanshun Nie, Wei-Nan Zhang, Dechen Zhan, Yang Song, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08496">LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) have advanced procedural planning for embodied AI systems through strong reasoning abilities, the integration of multimodal inputs and counterfactual reasoning remains underexplored. To tackle these challenges, we introduce LLaPa, a vision-language model framework designed for multimodal procedural planning. LLaPa generates executable action sequences from textual task descriptions and visual environmental images using vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary modules to improve procedural planning. The first module, the Task-Environment Reranker (TER), leverages task-oriented segmentation to create a task-sensitive feature space, aligning textual descriptions with visual environments and emphasizing critical regions for procedural execution. The second module, the Counterfactual Activities Retriever (CAR), identifies and emphasizes potential counterfactual conditions, enhancing the model's reasoning capability in counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED benchmarks demonstrate that LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models. The code and models are available https://github.com/sunshibo1234/LLaPa.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2507.07781.pdf' target='_blank'>https://arxiv.org/pdf/2507.07781.pdf</a></span>   <span><a href='https://github.com/liziwennba/SUPRISE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Huang, Ziwen Li, Hanlve Zhang, Runnan Chen, Xiao He, Yandong Guo, Wenping Wang, Tongliang Liu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07781">SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of language and 3D perception is critical for embodied AI and robotic systems to perceive, understand, and interact with the physical world. Spatial reasoning, a key capability for understanding spatial relationships between objects, remains underexplored in current 3D vision-language research. Existing datasets often mix semantic cues (e.g., object name) with spatial context, leading models to rely on superficial shortcuts rather than genuinely interpreting spatial relationships. To address this gap, we introduce S\textsc{urprise}3D, a novel dataset designed to evaluate language-guided spatial reasoning segmentation in complex 3D scenes. S\textsc{urprise}3D consists of more than 200k vision language pairs across 900+ detailed indoor scenes from ScanNet++ v2, including more than 2.8k unique object classes. The dataset contains 89k+ human-annotated spatial queries deliberately crafted without object name, thereby mitigating shortcut biases in spatial understanding. These queries comprehensively cover various spatial reasoning skills, such as relative position, narrative perspective, parametric perspective, and absolute distance reasoning. Initial benchmarks demonstrate significant challenges for current state-of-the-art expert 3D visual grounding methods and 3D-LLMs, underscoring the necessity of our dataset and the accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite. S\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially aware AI, paving the way for effective embodied interaction and robotic planning. The code and datasets can be found in https://github.com/liziwennba/SUPRISE.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2507.00917.pdf' target='_blank'>https://arxiv.org/pdf/2507.00917.pdf</a></span>   <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00917">A Survey: Learning Embodied Intelligence from Physical Simulators and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2506.24044.pdf' target='_blank'>https://arxiv.org/pdf/2506.24044.pdf</a></span>   <span><a href='https://github.com/JohnsonJiang1996/Awesome-VLA4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicong Jiang, Zilin Huang, Kangan Qian, Ziang Luo, Tianze Zhu, Yang Zhong, Yihong Tang, Menglin Kong, Yunlong Wang, Siwen Jiao, Hao Ye, Zihao Sheng, Xin Zhao, Tuopu Wen, Zheng Fu, Sikai Chen, Kun Jiang, Diange Yang, Seongjin Choi, Lijun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24044">A Survey on Vision-Language-Action Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2506.23852.pdf' target='_blank'>https://arxiv.org/pdf/2506.23852.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/RGC-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianing Jin, Jiangyong Ying, Huiyu Duan, Liu Yang, Sijing Wu, Yunhao Li, Yushuo Zheng, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23852">RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As camera-equipped robotic platforms become increasingly integrated into daily life, robotic-generated videos have begun to appear on streaming media platforms, enabling us to envision a future where humans and robots coexist. We innovatively propose the concept of Robotic-Generated Content (RGC) to term these videos generated from egocentric perspective of robots. The perceptual quality of RGC videos is critical in human-robot interaction scenarios, and RGC videos exhibit unique distortions and visual requirements that differ markedly from those of professionally-generated content (PGC) videos and user-generated content (UGC) videos. However, dedicated research on quality assessment of RGC videos is still lacking. To address this gap and to support broader robotic applications, we establish the first Robotic-Generated Content Database (RGCD), which contains a total of 2,100 videos drawn from three robot categories and sourced from diverse platforms. A subjective VQA experiment is conducted subsequently to assess human visual perception of robotic-generated videos. Finally, we conduct a benchmark experiment to evaluate the performance of 11 state-of-the-art VQA models on our database. Experimental results reveal significant limitations in existing VQA models when applied to complex, robotic-generated content, highlighting a critical need for RGC-specific VQA models. Our RGCD is publicly available at: https://github.com/IntMeGroup/RGC-VQA.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2506.23135.pdf' target='_blank'>https://arxiv.org/pdf/2506.23135.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/RoboScape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23135">RoboScape: Physics-informed Embodied World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2506.20566.pdf' target='_blank'>https://arxiv.org/pdf/2506.20566.pdf</a></span>   <span><a href='https://github.com/interaction-lab/HRIBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghao Shi, Enyu Zhao, Nathaniel Dennler, Jingzhen Wang, Xinyang Xu, Kaleen Shrestha, Mengxue Fu, Daniel Seita, Maja MatariÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20566">HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time human perception is crucial for effective human-robot interaction (HRI). Large vision-language models (VLMs) offer promising generalizable perceptual capabilities but often suffer from high latency, which negatively impacts user experience and limits VLM applicability in real-world scenarios. To systematically study VLM capabilities in human perception for HRI and performance-latency trade-offs, we introduce HRIBench, a visual question-answering (VQA) benchmark designed to evaluate VLMs across a diverse set of human perceptual tasks critical for HRI. HRIBench covers five key domains: (1) non-verbal cue understanding, (2) verbal instruction understanding, (3) human-robot object relationship understanding, (4) social navigation, and (5) person identification. To construct HRIBench, we collected data from real-world HRI environments to curate questions for non-verbal cue understanding, and leveraged publicly available datasets for the remaining four domains. We curated 200 VQA questions for each domain, resulting in a total of 1000 questions for HRIBench. We then conducted a comprehensive evaluation of both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench. Our results show that, despite their generalizability, current VLMs still struggle with core perceptual capabilities essential for HRI. Moreover, none of the models within our experiments demonstrated a satisfactory performance-latency trade-off suitable for real-time deployment, underscoring the need for future research on developing smaller, low-latency VLMs with improved human perception capabilities. HRIBench and our results can be found in this Github repository: https://github.com/interaction-lab/HRIBench.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2506.20487.pdf' target='_blank'>https://arxiv.org/pdf/2506.20487.pdf</a></span>   <span><a href='https://github.com/yuanmingqi/awesome-bfm-papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingqi Yuan, Tao Yu, Wenqi Ge, Xiuyong Yao, Huijiang Wang, Jiayu Chen, Xin Jin, Bo Li, Hua Chen, Wei Zhang, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20487">A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and long-term list of BFM papers and projects to facilitate more subsequent research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2506.20066.pdf' target='_blank'>https://arxiv.org/pdf/2506.20066.pdf</a></span>   <span><a href='https://github.com/hsiangwei0903/ToSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Wenhao Chai, Kuang-Ming Chen, Cheng-Yen Yang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20066">ToSA: Token Merging with Spatial Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Token merging has emerged as an effective strategy to accelerate Vision Transformers (ViT) by reducing computational costs. However, existing methods primarily rely on the visual token's feature similarity for token merging, overlooking the potential of integrating spatial information, which can serve as a reliable criterion for token merging in the early layers of ViT, where the visual tokens only possess weak visual information. In this paper, we propose ToSA, a novel token merging method that combines both semantic and spatial awareness to guide the token merging process. ToSA leverages the depth image as input to generate pseudo spatial tokens, which serve as auxiliary spatial information for the visual token merging process. With the introduced spatial awareness, ToSA achieves a more informed merging strategy that better preserves critical scene structure. Experimental results demonstrate that ToSA outperforms previous token merging methods across multiple benchmarks on visual and embodied question answering while largely reducing the runtime of the ViT, making it an efficient solution for ViT acceleration. The code will be available at: https://github.com/hsiangwei0903/ToSA
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2506.18904.pdf' target='_blank'>https://arxiv.org/pdf/2506.18904.pdf</a></span>   <span><a href='https://github.com/Linketic/TC-Light' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Chuanchen Luo, Zimo Tang, Yingyan Li, Yuran Yang, Yuanyong Ning, Lue Fan, Zhaoxiang Zhang, Junran Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18904">TC-Light: Temporally Coherent Generative Rendering for Realistic World Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Illumination and texture editing are critical dimensions for world-to-world transfer, which is valuable for applications including sim2real and real2real visual data scaling up for embodied AI. Existing techniques generatively re-render the input video to realize the transfer, such as video relighting models and conditioned world generation models. Nevertheless, these models are predominantly limited to the domain of training data (e.g., portrait) or fall into the bottleneck of temporal consistency and computation efficiency, especially when the input video involves complex dynamics and long durations. In this paper, we propose TC-Light, a novel generative renderer to overcome these problems. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible re-rendering results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2506.16402.pdf' target='_blank'>https://arxiv.org/pdf/2506.16402.pdf</a></span>   <span><a href='https://github.com/AI45Lab/IS-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoya Lu, Zeren Chen, Xuhao Hu, Yijin Zhou, Weichen Zhang, Dongrui Liu, Lu Sheng, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16402">IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under [this https URL](https://github.com/AI45Lab/IS-Bench).
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2506.16012.pdf' target='_blank'>https://arxiv.org/pdf/2506.16012.pdf</a></span>   <span><a href='https://github.com/ds199895/DualTHOR.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyu Li, Siyuan He, Hang Xu, Haoqi Yuan, Yu Zang, Liwei Hu, Junpeng Yue, Zhenxiong Jiang, Pengbo Hu, BÃ¶rje F. Karlsson, Yehui Tang, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16012">DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains a fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present a physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, a task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce a contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables a more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2506.13045.pdf' target='_blank'>https://arxiv.org/pdf/2506.13045.pdf</a></span>   <span><a href='https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13045">Continual Learning for Generative AI: From LLMs to MLLMs and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative models has empowered modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models are fundamentally constrained by \emph{catastrophic forgetting}, \ie~a persistent challenge where models experience performance degradation on previously learned tasks when adapting to new tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative AI in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative AI models, encompassing large language models, multimodal large language models, vision-language-action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, thereby providing deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2506.10389.pdf' target='_blank'>https://arxiv.org/pdf/2506.10389.pdf</a></span>   <span><a href='https://github.com/UNITES-Lab/EQA-RM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Chen, Zhen Tan, Tianlong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10389">EQA-RM: A Generative Embodied Reward Model with Test-time Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward Models (RMs), vital for large model alignment, are underexplored for complex embodied tasks like Embodied Question Answering (EQA) where nuanced evaluation of agents' spatial, temporal, and logical understanding is critical yet not considered by generic approaches. We introduce EQA-RM, a novel generative multimodal reward model specifically architected for EQA, trained via our innovative Contrastive Group Relative Policy Optimization (C-GRPO) strategy to learn fine-grained behavioral distinctions. The generative nature of EQA-RM provides interpretable, structured reward feedback (beyond simple scalars), uniquely enabling test-time scaling to dynamically adjust evaluation granularity, from concise scores to detailed critiques of reasoning and grounding, at inference without retraining. Concurrently, we introduce EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700 samples, outperforming strong proprietary baselines, including Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art models such as RoVRM and VisualPRM. The code and dataset can be found here https://github.com/UNITES-Lab/EQA-RM.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2506.09623.pdf' target='_blank'>https://arxiv.org/pdf/2506.09623.pdf</a></span>   <span><a href='https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lipei Xie, Yingxin Li, Huiping Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09623">Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied foundation models are crucial for Artificial Intelligence (AI) interacting with the physical world by integrating multi-modal inputs, such as proprioception, vision and language, to understand human intentions and generate actions to control robots. While these models demonstrate strong generalization and few-shot learning capabilities, they face significant challenges in continually acquiring new skills without forgetting previously learned skills, a problem known as catastrophic forgetting. To address this issue, we propose the Analytic Task Scheduler (ATS), a novel framework for continual learning in embodied foundation models. ATS consists of a task-specific model library, where each model is fine-tuned independently on a single task, and an analytic scheduler trained using recursive least squares (RLS) to learn the mapping between language instructions and task-specific models. This architecture enables accurate task recognition and dynamic model selection while fundamentally avoiding parameter interference across tasks. The scheduler updates its parameters incrementally using only statistics (autocorrelation and cross-correlation matrices), enabling forgetting-resistant learning without the need to revisit historical data. We validate ATS on a real-world robot platform (RM65B), demonstrating superior resistance to forgetting and strong adaptability to task variations. The results highlight ATS as an effective, scalable, and deployable solution for continual learning in embodied foundation models operating in complex, dynamic environments. Our code will be available at https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2506.09176.pdf' target='_blank'>https://arxiv.org/pdf/2506.09176.pdf</a></span>   <span><a href='https://github.com/metadriverse/AIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyuan Cai, Zhenghao Peng, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09176">Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency. Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2506.07530.pdf' target='_blank'>https://arxiv.org/pdf/2506.07530.pdf</a></span>   <span><a href='https://github.com/ustcwhy/BitVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Wang, Chuyan Xiong, Ruiping Wang, Xilin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07530">BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2506.03613.pdf' target='_blank'>https://arxiv.org/pdf/2506.03613.pdf</a></span>   <span><a href='https://github.com/airs-admin/HEAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoshan Liu, Fan Wang, Hongjun Zhou, Yuanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03613">Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While theory and practice are often seen as separate domains, this article shows that theoretical insight is essential for overcoming real-world engineering barriers. We begin with a practical challenge: training a cross-morphology embodied AI policy that generalizes across diverse robot morphologies. We formalize this as the Heterogeneous Embodied Agent Training (HEAT) problem and prove it reduces to a structured Partially Observable Markov Decision Process (POMDP) that is PSPACE-complete. This result explains why current reinforcement learning pipelines break down under morphological diversity, due to sequential training constraints, memory-policy coupling, and data incompatibility. We further explore Collective Adaptation, a distributed learning alternative inspired by biological systems. Though NEXP-complete in theory, it offers meaningful scalability and deployment benefits in practice. This work illustrates how computational theory can illuminate system design trade-offs and guide the development of more robust, scalable embodied AI. For practitioners and researchers to explore this problem, the implementation code of this work has been made publicly available at https://github.com/airs-admin/HEAT
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2506.03350.pdf' target='_blank'>https://arxiv.org/pdf/2506.03350.pdf</a></span>   <span><a href='https://github.com/eliotjones1/robogcg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03350">Adversarial Attacks on Robotic Vision Language Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2506.03097.pdf' target='_blank'>https://arxiv.org/pdf/2506.03097.pdf</a></span>   <span><a href='https://github.com/adityavavre/VidEgoVLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashwin Vinod, Shrey Pandit, Aditya Vavre, Linshen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03097">EgoVLM: Policy Optimization for Egocentric Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2506.01551.pdf' target='_blank'>https://arxiv.org/pdf/2506.01551.pdf</a></span>   <span><a href='https://github.com/expectorlin/EvolveNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01551">EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2506.00927.pdf' target='_blank'>https://arxiv.org/pdf/2506.00927.pdf</a></span>   <span><a href='https://github.com/Alice01010101/TASU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianrui Pan, Jie Liu, Zewen Huang, Jie Tang, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00927">In-the-wild Audio Spatialization with Flexible Text-guided Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enhance immersive experiences, binaural audio offers spatial awareness of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes flexible text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of premium and large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio. It outperforms existing methods on both simulated and real-recorded datasets, demonstrating superior generalization and accuracy. Besides, we develop an assessment model based on Llama-3.1-8B, which evaluates the spatial semantic coherence between our generated binaural audio and text prompts through a spatial reasoning task. Results demonstrate that text prompts provide flexible and interactive control to generate binaural audio with excellent quality and semantic consistency in spatial locations. Dataset is available at \href{https://github.com/Alice01010101/TASU}
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2505.23757.pdf' target='_blank'>https://arxiv.org/pdf/2505.23757.pdf</a></span>   <span><a href='https://github.com/ahydchh/Impromptu-VLA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ahydchh/Impromptu-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23757">Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2505.19510.pdf' target='_blank'>https://arxiv.org/pdf/2505.19510.pdf</a></span>   <span><a href='https://github.com/docworlds/tsg-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19510">LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs' ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs' ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://github.com/docworlds/tsg-bench.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2505.16815.pdf' target='_blank'>https://arxiv.org/pdf/2505.16815.pdf</a></span>   <span><a href='https://github.com/lcysyzxdxc/EmbodiedIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16815">Perceptual Quality Assessment for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: https://github.com/lcysyzxdxc/EmbodiedIQA
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2505.10010.pdf' target='_blank'>https://arxiv.org/pdf/2505.10010.pdf</a></span>   <span><a href='https://github.com/LAMDA-RL/ImagineBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10010">ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge in reinforcement learning (RL) is its dependence on extensive real-world interaction data to learn task-specific policies. While recent work demonstrates that large language models (LLMs) can mitigate this limitation by generating synthetic experience (noted as imaginary rollouts) for mastering novel tasks, progress in this emerging field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ImagineBench, the first comprehensive benchmark for evaluating offline RL algorithms that leverage both real rollouts and LLM-imaginary rollouts. The key features of ImagineBench include: (1) datasets comprising environment-collected and LLM-imaginary rollouts; (2) diverse domains of environments covering locomotion, robotic manipulation, and navigation tasks; and (3) natural language task instructions with varying complexity levels to facilitate language-conditioned policy learning. Through systematic evaluation of state-of-the-art offline RL algorithms, we observe that simply applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving 35.44% success rate in hard tasks in contrast to 64.37% of method training on real rollouts for hard tasks. This result highlights the need for algorithm advancements to better leverage LLM-imaginary rollouts. Additionally, we identify key opportunities for future research: including better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks. Our code is publicly available at https://github.com/LAMDA-RL/ImagineBench.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2505.09694.pdf' target='_blank'>https://arxiv.org/pdf/2505.09694.pdf</a></span>   <span><a href='https://github.com/AgibotTech/EWMBench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AgibotTech/EWMBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, Guanghui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09694">EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2505.08854.pdf' target='_blank'>https://arxiv.org/pdf/2505.08854.pdf</a></span>   <span><a href='https://github.com/taco-group/GenAI4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuping Wang, Shuo Xing, Cui Can, Renjie Li, Hongyuan Hua, Kexin Tian, Zhaobin Mo, Xiangbo Gao, Keshu Wu, Sulong Zhou, Hengxu You, Juntong Peng, Junge Zhang, Zehao Wang, Rui Song, Mingxuan Yan, Walter Zimmer, Xingcheng Zhou, Peiran Li, Zhaohan Lu, Chia-Ju Chen, Yue Huang, Ryan A. Rossi, Lichao Sun, Hongkai Yu, Zhiwen Fan, Frank Hao Yang, Yuhao Kang, Ross Greer, Chenxi Liu, Eun Hak Lee, Xuan Di, Xinyue Ye, Liu Ren, Alois Knoll, Xiaopeng Li, Shuiwang Ji, Masayoshi Tomizuka, Marco Pavone, Tianbao Yang, Jing Du, Ming-Hsuan Yang, Hua Wei, Ziran Wang, Yang Zhou, Jiachen Li, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08854">Generative AI for Autonomous Driving: Frontiers and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2505.05622.pdf' target='_blank'>https://arxiv.org/pdf/2505.05622.pdf</a></span>   <span><a href='https://github.com/VinceOuti/CityNavAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Chen Gao, Shiquan Yu, Ruiying Peng, Baining Zhao, Qian Zhang, Jinqiang Cui, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05622">CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \href{https://github.com/VinceOuti/CityNavAgent}{link}.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2505.05474.pdf' target='_blank'>https://arxiv.org/pdf/2505.05474.pdf</a></span>   <span><a href='https://github.com/hzxie/Awesome-3D-Scene-Generation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hzxie/Awesome-3D-Scene-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05474">3D Scene Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2505.03792.pdf' target='_blank'>https://arxiv.org/pdf/2505.03792.pdf</a></span>   <span><a href='https://github.com/langfengQ/CoSo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, Bo An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03792">Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2504.21769.pdf' target='_blank'>https://arxiv.org/pdf/2504.21769.pdf</a></span>   <span><a href='https://github.com/Tubicor/LLM-iTeach' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Werner, Kun Chu, Cornelius Weber, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21769">LLM-based Interactive Imitation Learning for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in machine learning provide methods to train autonomous agents capable of handling the increasing complexity of sequential decision-making in robotics. Imitation Learning (IL) is a prominent approach, where agents learn to control robots based on human demonstrations. However, IL commonly suffers from violating the independent and identically distributed (i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL) achieves improved performance by allowing agents to learn from interactive feedback from human teachers. Despite these improvements, both approaches come with significant costs due to the necessity of human involvement. Leveraging the emergent capabilities of Large Language Models (LLMs) in reasoning and generating human-like responses, we introduce LLM-iTeach -- a novel IIL framework that utilizes an LLM as an interactive teacher to enhance agent performance while alleviating the dependence on human resources. Firstly, LLM-iTeach uses a hierarchical prompting strategy that guides the LLM in generating a policy in Python code. Then, with a designed similarity-based feedback mechanism, LLM-iTeach provides corrective and evaluative feedback interactively during the agent's training. We evaluate LLM-iTeach against baseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a state-of-the-art IIL method using a human teacher, on various robotic manipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the success rate and achieves or even outscores that of CEILing, highlighting the potential of LLMs as cost-effective, human-like teachers in interactive learning environments. We further demonstrate the method's potential for generalization by evaluating it on additional tasks. The code and prompts are provided at: https://github.com/Tubicor/LLM-iTeach.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2504.18317.pdf' target='_blank'>https://arxiv.org/pdf/2504.18317.pdf</a></span>   <span><a href='https://github.com/fangzr/TOC-Edge-Aerial' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18317">Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support the Low Altitude Economy (LAE), it is essential to achieve precise localization of unmanned aerial vehicles (UAVs) in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available at: github.com/fangzr/TOC-Edge-Aerial.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2504.10808.pdf' target='_blank'>https://arxiv.org/pdf/2504.10808.pdf</a></span>   <span><a href='https://github.com/hasan-rakibul/TFMPathy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rakibul Hasan, Md Zakir Hossain, Aneesh Krishna, Shafin Rahman, Tom Gedeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10808">TFMPathy: Tabular Foundation Model for Privacy-Aware, Generalisable Empathy Detection from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting empathy from video interactions is an emerging area of research, particularly in healthcare and social robotics. However, privacy and ethical concerns often prevent the release of raw video data, with many datasets instead shared as pre-extracted tabular features. Previous work on such datasets has established classical tree-based models as the state of the art. Motivated by recent successes of large-scale foundation models for text, we investigate the potential of tabular foundation models (TFMs) for empathy detection from video-derived tabular data. Our proposed system, TFMPathy, is demonstrated with two recent TFMs (TabPFN v2 and TabICL) under both in-context learning and fine-tuning paradigms. On a public human-robot interaction benchmark, TFMPathy significantly improves empathy detection accuracy reported in the literature. While the established evaluation protocol in the literature does not ensure cross-subject generalisation, our evaluation scheme also captures such generalisation. We show that TFMPathy under a fine-tuning setup has better cross-subject generalisation capacity over baseline methods (accuracy: $0.590 \rightarrow 0.730$; AUC: $0.564 \rightarrow 0.669$). Given the ongoing privacy and ethical constraints around raw video sharing, the proposed TFMPathy system provides a practical and scalable path toward building AI systems dependent on human-centred video datasets. Our code is publicly available at https://github.com/hasan-rakibul/TFMPathy (will be made available upon acceptance of this paper).
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2504.10041.pdf' target='_blank'>https://arxiv.org/pdf/2504.10041.pdf</a></span>   <span><a href='https://github.com/hren20/NaiviBridger' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ren, Yiming Zeng, Zetong Bi, Zhaoliang Wan, Junlong Huang, Hui Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10041">Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion-based imitation learning, which show impressive performance in modeling multimodal distributions and training stability, have led to substantial progress in various robot learning tasks. In visual navigation, previous diffusion-based policies typically generate action sequences by initiating from denoising Gaussian noise. However, the target action distribution often diverges significantly from Gaussian noise, leading to redundant denoising steps and increased learning complexity. Additionally, the sparsity of effective action distributions makes it challenging for the policy to generate accurate actions without guidance. To address these issues, we propose a novel, unified visual navigation framework leveraging the denoising diffusion bridge models named NaviBridger. This approach enables action generation by initiating from any informative prior actions, enhancing guidance and efficiency in the denoising process. We explore how diffusion bridges can enhance imitation learning in visual navigation tasks and further examine three source policies for generating prior actions. Extensive experiments in both simulated and real-world indoor and outdoor scenarios demonstrate that NaviBridger accelerates policy inference and outperforms the baselines in generating target action sequences. Code is available at https://github.com/hren20/NaiviBridger.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2504.10003.pdf' target='_blank'>https://arxiv.org/pdf/2504.10003.pdf</a></span>   <span><a href='https://github.com/SYSU-RoboticsLab/NaviD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zeng, Hao Ren, Shuhang Wang, Junlong Huang, Hui Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10003">NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation, a fundamental challenge in mobile robotics, demands versatile policies to handle diverse environments. Classical methods leverage geometric solutions to minimize specific costs, offering adaptability to new scenarios but are prone to system errors due to their multi-modular design and reliance on hand-crafted rules. Learning-based methods, while achieving high planning success rates, face difficulties in generalizing to unseen environments beyond the training data and often require extensive training. To address these limitations, we propose a hybrid approach that combines the strengths of learning-based methods and classical approaches for RGB-only visual navigation. Our method first trains a conditional diffusion model on diverse path-RGB observation pairs. During inference, it integrates the gradients of differentiable scene-specific and task-level costs, guiding the diffusion model to generate valid paths that meet the constraints. This approach alleviates the need for retraining, offering a plug-and-play solution. Extensive experiments in both indoor and outdoor settings, across simulated and real-world scenarios, demonstrate zero-shot transfer capability of our approach, achieving higher success rates and fewer collisions compared to baseline methods. Code will be released at https://github.com/SYSU-RoboticsLab/NaviD.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2504.08646.pdf' target='_blank'>https://arxiv.org/pdf/2504.08646.pdf</a></span>   <span><a href='https://github.com/RISELabPurdue/MBE-ARI/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Noronha, Advait Prasad Jawaji, Juan Camilo Soto, Jiajun An, Yan Gu, Upinder Kaur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08646">MBE-ARI: A Multimodal Dataset Mapping Bi-directional Engagement in Animal-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animal-robot interaction (ARI) remains an unexplored challenge in robotics, as robots struggle to interpret the complex, multimodal communication cues of animals, such as body language, movement, and vocalizations. Unlike human-robot interaction, which benefits from established datasets and frameworks, animal-robot interaction lacks the foundational resources needed to facilitate meaningful bidirectional communication. To bridge this gap, we present the MBE-ARI (Multimodal Bidirectional Engagement in Animal-Robot Interaction), a novel multimodal dataset that captures detailed interactions between a legged robot and cows. The dataset includes synchronized RGB-D streams from multiple viewpoints, annotated with body pose and activity labels across interaction phases, offering an unprecedented level of detail for ARI research. Additionally, we introduce a full-body pose estimation model tailored for quadruped animals, capable of tracking 39 keypoints with a mean average precision (mAP) of 92.7%, outperforming existing benchmarks in animal pose estimation. The MBE-ARI dataset and our pose estimation framework lay a robust foundation for advancing research in animal-robot interaction, providing essential tools for developing perception, reasoning, and interaction frameworks needed for effective collaboration between robots and animals. The dataset and resources are publicly available at https://github.com/RISELabPurdue/MBE-ARI/, inviting further exploration and development in this critical area.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2504.00954.pdf' target='_blank'>https://arxiv.org/pdf/2504.00954.pdf</a></span>   <span><a href='https://github.com/BwLiu01/IDMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, Chaochao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00954">IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal retrieval systems are becoming increasingly vital for cutting-edge AI technologies, such as embodied AI and AI-driven digital content industries. However, current multimodal retrieval tasks lack sufficient complexity and demonstrate limited practical application value. It spires us to design Instance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires models to retrieve images containing the same instance as a query image while matching a text-described scenario. Unlike existing retrieval tasks focused on global image similarity or category-level matching, IDMR demands fine-grained instance-level consistency across diverse contexts. To benchmark this capability, we develop IDMR-bench using real-world object tracking and first-person video data. Addressing the scarcity of training data, we propose a cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets. Our Multimodal Large Language Model (MLLM) based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and our zero-shot IDMR-bench. Experimental results demonstrate previous models' limitations in instance-aware retrieval and highlight the potential of MLLM for advanced retrieval applications. The whole training dataset, codes and models, with wide ranges of sizes, are available at https://github.com/BwLiu01/IDMR.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2503.19851.pdf' target='_blank'>https://arxiv.org/pdf/2503.19851.pdf</a></span>   <span><a href='https://github.com/Sampson-Lee/OnlineMMSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, Yapeng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19851">Towards Online Multi-Modal Social Interaction Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal social interaction understanding (MMSI) is critical in human-robot interaction systems. In real-world scenarios, AI agents are required to provide real-time feedback. However, existing models often depend on both past and future contexts, which hinders them from applying to real-world problems. To bridge this gap, we propose an online MMSI setting, where the model must resolve MMSI tasks using only historical information, such as recorded dialogues and video streams. To address the challenges of missing the useful future context, we develop a novel framework, named Online-MMSI-VLM, that leverages two complementary strategies: multi-party conversation forecasting and social-aware visual prompting with multi-modal large language models. First, to enrich linguistic context, the multi-party conversation forecasting simulates potential future utterances in a coarse-to-fine manner, anticipating upcoming speaker turns and then generating fine-grained conversational details. Second, to effectively incorporate visual social cues like gaze and gesture, social-aware visual prompting highlights the social dynamics in video with bounding boxes and body keypoints for each person and frame. Extensive experiments on three tasks and two datasets demonstrate that our method achieves state-of-the-art performance and significantly outperforms baseline models, indicating its effectiveness on Online-MMSI. The code and pre-trained models will be publicly released at: https://github.com/Sampson-Lee/OnlineMMSI.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2503.19397.pdf' target='_blank'>https://arxiv.org/pdf/2503.19397.pdf</a></span>   <span><a href='https://github.com/clee-jaist/QFAAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Li, Razvan Beuran, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19397">Quality-focused Active Adversarial Policy for Safe Grasping in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-guided robot grasping methods based on Deep Neural Networks (DNNs) have achieved remarkable success in handling unknown objects, attributable to their powerful generalizability. However, these methods with this generalizability tend to recognize the human hand and its adjacent objects as graspable targets, compromising safety during Human-Robot Interaction (HRI). In this work, we propose the Quality-focused Active Adversarial Policy (QFAAP) to solve this problem. Specifically, the first part is the Adversarial Quality Patch (AQP), wherein we design the adversarial quality patch loss and leverage the grasp dataset to optimize a patch with high quality scores. Next, we construct the Projected Quality Gradient Descent (PQGD) and integrate it with the AQP, which contains only the hand region within each real-time frame, endowing the AQP with fast adaptability to the human hand shape. Through AQP and PQGD, the hand can be actively adversarial with the surrounding objects, lowering their quality scores. Therefore, further setting the quality score of the hand to zero will reduce the grasping priority of both the hand and its adjacent objects, enabling the robot to grasp other objects away from the hand without emergency stops. We conduct extensive experiments on the benchmark datasets and a cobot, showing the effectiveness of QFAAP. Our code and demo videos are available here: https://github.com/clee-jaist/QFAAP.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2503.13424.pdf' target='_blank'>https://arxiv.org/pdf/2503.13424.pdf</a></span>   <span><a href='https://github.com/Intern-Nexus/Infinite-Mobility' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Lian, Zichao Yu, Ruiming Liang, Yitong Wang, Li Ray Luo, Kaixu Chen, Yuanzhen Zhou, Qihong Tang, Xudong Xu, Zhaoyang Lyu, Bo Dai, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13424">Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2503.12955.pdf' target='_blank'>https://arxiv.org/pdf/2503.12955.pdf</a></span>   <span><a href='https://github.com/ZJHTerry18/HumanInScene' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Zhao, Ruibing Hou, Zejie Tian, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12955">HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new task to benchmark human-in-scene understanding for embodied agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors, reason about its surrounding environment, and answer human-related questions within the scene. To support this new task, we present HIS-Bench, a multimodal benchmark that systematically evaluates HIS understanding across a broad spectrum, from basic perception to commonsense reasoning and planning. Our evaluation of various vision-language models on HIS-Bench reveals significant limitations in their ability to handle HIS-QA tasks. To this end, we propose HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates 3D scene context and human motion dynamics into large language models while incorporating specialized mechanisms to capture human-scene interactions. Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on HIS-QA tasks. We hope this work inspires future research on human behavior analysis in 3D scenes, advancing embodied AI and world models. The codes and data: https://github.com/ZJHTerry18/HumanInScene.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2503.09335.pdf' target='_blank'>https://arxiv.org/pdf/2503.09335.pdf</a></span>   <span><a href='https://github.com/laiyuzhi/NVP-HRI.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Thomas Weber, Matthias RÃ¤tsch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09335">NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective Human-Robot Interaction (HRI) is crucial for future service robots in aging societies. Existing solutions are biased toward only well-trained objects, creating a gap when dealing with new objects. Currently, HRI systems using predefined gestures or language tokens for pretrained objects pose challenges for all individuals, especially elderly ones. These challenges include difficulties in recalling commands, memorizing hand gestures, and learning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI paradigm that combines voice commands and deictic posture. NVP-HRI utilizes the Segment Anything Model (SAM) to analyze visual cues and depth data, enabling precise structural object representation. Through a pre-trained SAM network, NVP-HRI allows interaction with new objects via zero-shot prediction, even without prior knowledge. NVP-HRI also integrates with a large language model (LLM) for multimodal commands, coordinating them with object selection and scene distribution in real time for collision-free trajectory solutions. We also regulate the action sequence with the essential control syntax to reduce LLM hallucination risks. The evaluation of diverse real-world tasks using a Universal Robot showcased up to 59.2\% efficiency improvement over traditional gesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our code and design will be openly available at https://github.com/laiyuzhi/NVP-HRI.git.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2503.03984.pdf' target='_blank'>https://arxiv.org/pdf/2503.03984.pdf</a></span>   <span><a href='https://github.com/Qianzhong-Chen/grad_nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03984">GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous visual navigation is an essential element in robot autonomy. Reinforcement learning (RL) offers a promising policy training paradigm. However existing RL methods suffer from high sample complexity, poor sim-to-real transfer, and limited runtime adaptability to navigation scenarios not seen during training. These problems are particularly challenging for drones, with complex nonlinear and unstable dynamics, and strong dynamic coupling between control and perception. In this paper, we propose a novel framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep reinforcement learning (DDRL) to train vision-based drone navigation policies. By leveraging high-fidelity 3D scene representations and differentiable simulation, our method improves sample efficiency and sim-to-real transfer. Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt to environmental variations at runtime. Moreover, by curriculum training in a mixture of different surrounding environments, we achieve in-task generalization, the ability to solve new instances of a task not seen during training. Drone hardware experiments demonstrate our method's high training efficiency compared to state-of-the-art RL methods, zero shot sim-to-real transfer for real robot deployment without fine tuning, and ability to adapt to new instances within the same task class (e.g. to fly through a gate at different locations with different distractors in the environment). Our simulator and training framework are open-sourced at: https://github.com/Qianzhong-Chen/grad_nav.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2502.19090.pdf' target='_blank'>https://arxiv.org/pdf/2502.19090.pdf</a></span>   <span><a href='https://github.com/TianCuteQY/EndoMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19090">EndoMamba: An Efficient Foundation Model for Endoscopic Videos via Hierarchical Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Endoscopic video-based tasks, such as visual navigation and surgical phase recognition, play a crucial role in minimally invasive surgeries by providing real-time assistance. While recent video foundation models have shown promise, their applications are hindered by (1) computational inefficiencies and (2) suboptimal performance caused by limited data for pre-training in endoscopy. To address these issues, we present EndoMamba, a foundation model designed for real-time inference while learning generalized spatiotemporal representations. First, to mitigate computational inefficiencies, we propose the EndoMamba backbone, optimized for real-time inference. Inspired by recent advancements in state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial modeling within individual frames and vanilla Mamba blocks for past-to-present reasoning across the temporal domain. This design enables both strong spatiotemporal modeling and efficient inference in online video streams. Second, we propose a self-supervised hierarchical pre-training diagram to enhance EndoMamba's representation learning using endoscopic videos and incorporating general video domain knowledge. Specifically, our approach combines masked reconstruction with auxiliary supervision, leveraging low-level reconstruction to capture spatial-temporal structures and high-level alignment to transfer broader knowledge from a pretrained general-video domain foundation model. Extensive experiments on four downstream tasks--classification, segmentation, surgical phase recognition, and localization--demonstrate that EndoMamba outperforms existing foundation models and task-specific methods while maintaining real-time inference speed. The source code is available at https://github.com/TianCuteQY/EndoMamba.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2502.12861.pdf' target='_blank'>https://arxiv.org/pdf/2502.12861.pdf</a></span>   <span><a href='https://github.com/icleveston/InstructRobot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Iury Cleveston, Alana C. Santana, Paula D. P. Costa, Ricardo R. Gudwin, Alexandre S. SimÃµes, Esther L. Colombini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12861">InstructRobot: A Model-Free Framework for Mapping Natural Language Instructions into Robot Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to communicate with robots using natural language is a significant step forward in human-robot interaction. However, accurately translating verbal commands into physical actions is promising, but still presents challenges. Current approaches require large datasets to train the models and are limited to robots with a maximum of 6 degrees of freedom. To address these issues, we propose a framework called InstructRobot that maps natural language instructions into robot motion without requiring the construction of large datasets or prior knowledge of the robot's kinematics model. InstructRobot employs a reinforcement learning algorithm that enables joint learning of language representations and inverse kinematics model, simplifying the entire learning process. The proposed framework is validated using a complex robot with 26 revolute joints in object manipulation tasks, demonstrating its robustness and adaptability in realistic environments. The framework can be applied to any task or domain where datasets are scarce and difficult to create, making it an intuitive and accessible solution to the challenges of training robots using linguistic communication. Open source code for the InstructRobot framework and experiments can be accessed at https://github.com/icleveston/InstructRobot.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2502.12532.pdf' target='_blank'>https://arxiv.org/pdf/2502.12532.pdf</a></span>   <span><a href='https://github.com/BiluYong/CityEQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12532">CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2502.12449.pdf' target='_blank'>https://arxiv.org/pdf/2502.12449.pdf</a></span>   <span><a href='https://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Yang, Miao Wang, Quan Zhou, Jiangchuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12449">YUNet: Improved YOLOv11 Network for Skyline Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skyline detection plays an important role in geolocalizaion, flight control, visual navigation, port security, etc. The appearance of the sky and non-sky areas are variable, because of different weather or illumination environment, which brings challenges to skyline detection. In this research, we proposed the YUNet algorithm, which improved the YOLOv11 architecture to segment the sky region and extract the skyline in complicated and variable circumstances. To improve the ability of multi-scale and large range contextual feature fusion, the YOLOv11 architecture is extended as an UNet-like architecture, consisting of an encoder, neck and decoder submodule. The encoder extracts the multi-scale features from the given images. The neck makes fusion of these multi-scale features. The decoder applies the fused features to complete the prediction rebuilding. To validate the proposed approach, the YUNet was tested on Skyfinder and CH1 datasets for segmentation and skyline detection respectively. Our test shows that the IoU of YUnet segmentation can reach 0.9858, and the average error of YUnet skyline detection is just 1.36 pixels. The implementation is published at https://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2502.00392.pdf' target='_blank'>https://arxiv.org/pdf/2502.00392.pdf</a></span>   <span><a href='https://github.com/sunzc-sunny/refdrone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Sun, Yepeng Liu, Huachao Zhu, Yuliang Gu, Yuda Zou, Zelong Liu, Gui-Song Xia, Bo Du, Yongchao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00392">RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code are be publicly at https://github.com/sunzc-sunny/refdrone.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2502.00379.pdf' target='_blank'>https://arxiv.org/pdf/2502.00379.pdf</a></span>   <span><a href='https://github.com/dunnolab/laom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00379">Latent Action Learning Requires Supervision in the Presence of Distractors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2501.16899.pdf' target='_blank'>https://arxiv.org/pdf/2501.16899.pdf</a></span>   <span><a href='https://github.com/shadynasrat/RDMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shady Nasrat, Myungsu Kim, Seonil Lee, Jiho Lee, Yeoncheol Jang, Seung-joon Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16899">RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with Enhanced Contextual Awareness in Specific Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) represent a significant advancement in integrating physical robots with AI-driven systems. We showcase the capabilities of our framework within the context of the real-world household competition. This research introduces a framework that utilizes RDMM (Robotics Decision-Making Models), which possess the capacity for decision-making within domain-specific contexts, as well as an awareness of their personal knowledge and capabilities. The framework leverages information to enhance the autonomous decision-making of the system. In contrast to other approaches, our focus is on real-time, on-device solutions, successfully operating on hardware with as little as 8GB of memory. Our framework incorporates visual perception models equipping robots with understanding of their environment. Additionally, the framework has integrated real-time speech recognition capabilities, thus enhancing the human-robot interaction experience. Experimental results demonstrate that the RDMM framework can plan with an 93\% accuracy. Furthermore, we introduce a new dataset consisting of 27k planning instances, as well as 1.3k text-image annotated samples derived from the competition. The framework, benchmarks, datasets, and models developed in this work are publicly available on our GitHub repository at https://github.com/shadynasrat/RDMM.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2501.13416.pdf' target='_blank'>https://arxiv.org/pdf/2501.13416.pdf</a></span>   <span><a href='https://github.com/AbrarAnwar/masked-social-signals/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Tang, Abrar Anwar, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13416">M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding social signals in multi-party conversations is important for human-robot interaction and artificial social intelligence. Social signals include body pose, head pose, speech, and context-specific activities like acquiring and taking bites of food when dining. Past work in multi-party interaction tends to build task-specific models for predicting social signals. In this work, we address the challenge of predicting multimodal social signals in multi-party settings in a single model. We introduce M3PT, a causal transformer architecture with modality and temporal blockwise attention masking to simultaneously process multiple social cues across multiple participants and their temporal interactions. We train and evaluate M3PT on the Human-Human Commensality Dataset (HHCD), and demonstrate that using multiple modalities improves bite timing and speaking status prediction. Source code: https://github.com/AbrarAnwar/masked-social-signals/.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2501.11858.pdf' target='_blank'>https://arxiv.org/pdf/2501.11858.pdf</a></span>   <span><a href='https://github.com/thunlp/EmbodiedEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11858">EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2501.10105.pdf' target='_blank'>https://arxiv.org/pdf/2501.10105.pdf</a></span>   <span><a href='https://github.com/2toinf/UniAct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10105">Universal Actions for Enhanced Embodied Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training on diverse, internet-scale data is a key factor in the success of recent large foundation models. Yet, using the same recipe for building embodied agents has faced noticeable difficulties. Despite the availability of many crowd-sourced embodied datasets, their action spaces often exhibit significant heterogeneity due to distinct physical embodiment and control interfaces for different robots, causing substantial challenges in developing embodied foundation models using cross-domain data. In this paper, we introduce UniAct, a new embodied foundation modeling framework operating in a Universal Action Space. Our learned universal actions capture the generic atomic behaviors across diverse robots by exploiting their shared structural features, and enable enhanced cross-domain data utilization and cross-embodiment generalizations by eliminating the notorious heterogeneity. The universal actions can be efficiently translated back to heterogeneous actionable commands by simply adding embodiment-specific details, from which fast adaptation to new robots becomes simple and straightforward. Our 0.5B instantiation of UniAct outperforms 14X larger SOTA embodied foundation models in extensive evaluations on various real-world and simulation robots, showcasing exceptional cross-embodiment control and adaptation capability, highlighting the crucial benefit of adopting universal actions. Project page: https://github.com/2toinf/UniAct
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2501.07051.pdf' target='_blank'>https://arxiv.org/pdf/2501.07051.pdf</a></span>   <span><a href='https://github.com/CHRI-Lab/ROSAnnotator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Haoqi Li, Ramtin Tabatabaei, Wafa Johal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07051">ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) is an interdisciplinary field that utilises both quantitative and qualitative methods. While ROSBags, a file format within the Robot Operating System (ROS), offer an efficient means of collecting temporally synched multimodal data in empirical studies with real robots, there is a lack of tools specifically designed to integrate qualitative coding and analysis functions with ROSBags. To address this gap, we developed ROSAnnotator, a web-based application that incorporates a multimodal Large Language Model (LLM) to support both manual and automated annotation of ROSBag data. ROSAnnotator currently facilitates video, audio, and transcription annotations and provides an open interface for custom ROS messages and tools. By using ROSAnnotator, researchers can streamline the qualitative analysis process, create a more cohesive analysis pipeline, and quickly access statistical summaries of annotations, thereby enhancing the overall efficiency of HRI data analysis. https://github.com/CHRI-Lab/ROSAnnotator
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2412.18774.pdf' target='_blank'>https://arxiv.org/pdf/2412.18774.pdf</a></span>   <span><a href='https://github.com/Jianbo-maker/EPD_benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Zhang, Chunyi Li, Jie Hao, Jun Jia, Huiyu Duan, Guoquan Zheng, Liang Yuan, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18774">Embodied Image Quality Assessment for Robotic Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) of User-Generated Content (UGC) is a critical technique for human Quality of Experience (QoE). However, does the the image quality of Robot-Generated Content (RGC) demonstrate traits consistent with the Moravec paradox, potentially conflicting with human perceptual norms? Human subjective scoring is more based on the attractiveness of the image. Embodied agent are required to interact and perceive in the environment, and finally perform specific tasks. Visual images as inputs directly influence downstream tasks. In this paper, we explore the perception mechanism of embodied robots for image quality. We propose the first Embodied Preference Database (EPD), which contains 12,500 distorted image annotations. We establish assessment metrics based on the downstream tasks of robot. In addition, there is a gap between UGC and RGC. To address this, we propose a novel Multi-scale Attention Embodied Image Quality Assessment called MA-EIQA. For the proposed EPD dataset, this is the first no-reference IQA model designed for embodied robot. Finally, the performance of mainstream IQA algorithms on EPD dataset is verified. The experiments demonstrate that quality assessment of embodied images is different from that of humans. We sincerely hope that the EPD can contribute to the development of embodied AI by focusing on image quality assessment. The benchmark is available at https://github.com/Jianbo-maker/EPD_benchmark.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2412.13729.pdf' target='_blank'>https://arxiv.org/pdf/2412.13729.pdf</a></span>   <span><a href='https://github.com/tmralmeida/thor-magni-actions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiago Rodrigues de Almeida, Tim Schreiter, Andrey Rudenko, Luigi Palmieiri, Johannes A. Stork, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13729">THÃR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the THÃR-MAGNI Act dataset, a substantial extension of the THÃR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. THÃR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided THÃR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of THÃR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of THÃR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2412.08467.pdf' target='_blank'>https://arxiv.org/pdf/2412.08467.pdf</a></span>   <span><a href='https://github.com/wz0919/VLN-SRDF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08467">Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2412.01179.pdf' target='_blank'>https://arxiv.org/pdf/2412.01179.pdf</a></span>   <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Hong Liu, Yingxuan You, Ti Wang, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01179">Dual-Branch Graph Transformer Network for 3D Human Mesh Reconstruction from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Mesh Reconstruction (HMR) from monocular video plays an important role in human-robot interaction and collaboration. However, existing video-based human mesh reconstruction methods face a trade-off between accurate reconstruction and smooth motion. These methods design networks based on either RNNs or attention mechanisms to extract local temporal correlations or global temporal dependencies, but the lack of complementary long-term information and local details limits their performance. To address this problem, we propose a \textbf{D}ual-branch \textbf{G}raph \textbf{T}ransformer network for 3D human mesh \textbf{R}econstruction from video, named DGTR. DGTR employs a dual-branch network including a Global Motion Attention (GMA) branch and a Local Details Refine (LDR) branch to parallelly extract long-term dependencies and local crucial information, helping model global human motion and local human details (e.g., local motion, tiny movement). Specifically, GMA utilizes a global transformer to model long-term human motion. LDR combines modulated graph convolutional networks and the transformer framework to aggregate local information in adjacent frames and extract crucial information of human details. Experiments demonstrate that our DGTR outperforms state-of-the-art video-based methods in reconstruction accuracy and maintains competitive motion smoothness. Moreover, DGTR utilizes fewer parameters and FLOPs, which validate the effectiveness and efficiency of the proposed DGTR. Code is publicly available at \href{https://github.com/TangTao-PKU/DGTR}{\textcolor{myBlue}{https://github.com/TangTao-PKU/DGTR}}.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2411.15714.pdf' target='_blank'>https://arxiv.org/pdf/2411.15714.pdf</a></span>   <span><a href='https://github.com/harrytea/ROOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonghui Wang, Shi-Yong Chen, Zhenxing Zhou, Siyi Li, Haoran Li, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15714">ROOT: VLM based System for Indoor Scene Understanding and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vision Language Models (VLMs) have experienced significant advancements, yet these models still face challenges in spatial hierarchical reasoning within indoor scenes. In this study, we introduce ROOT, a VLM-based system designed to enhance the analysis of indoor scenes. Specifically, we first develop an iterative object perception algorithm using GPT-4V to detect object entities within indoor scenes. This is followed by employing vision foundation models to acquire additional meta-information about the scene, such as bounding boxes. Building on this foundational data, we propose a specialized VLM, SceneVLM, which is capable of generating spatial hierarchical scene graphs and providing distance information for objects within indoor environments. This information enhances our understanding of the spatial arrangement of indoor scenes. To train our SceneVLM, we collect over 610,000 images from various public indoor datasets and implement a scene data generation pipeline with a semi-automated technique to establish relationships and estimate distances among indoor objects. By utilizing this enriched data, we conduct various training recipes and finish SceneVLM. Our experiments demonstrate that \rootname facilitates indoor scene understanding and proves effective in diverse downstream applications, such as 3D scene generation and embodied AI. The code will be released at \url{https://github.com/harrytea/ROOT}.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2411.13674.pdf' target='_blank'>https://arxiv.org/pdf/2411.13674.pdf</a></span>   <span><a href='https://github.com/knowledgetechnologyuhh/FabuLight-ASD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Carneiro, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13674">FabuLight-ASD: Unveiling Speech Activity via Body Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active speaker detection (ASD) in multimodal environments is crucial for various applications, from video conferencing to human-robot interaction. This paper introduces FabuLight-ASD, an advanced ASD model that integrates facial, audio, and body pose information to enhance detection accuracy and robustness. Our model builds upon the existing Light-ASD framework by incorporating human pose data, represented through skeleton graphs, which minimises computational overhead. Using the Wilder Active Speaker Detection (WASD) dataset, renowned for reliable face and body bounding box annotations, we demonstrate FabuLight-ASD's effectiveness in real-world scenarios. Achieving an overall mean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD, which has an overall mAP of 93.7% across various challenging scenarios. The incorporation of body pose information shows a particularly advantageous impact, with notable improvements in mAP observed in scenarios with speech impairment, face occlusion, and human voice background noise. Furthermore, efficiency analysis indicates only a modest increase in parameter count (27.3%) and multiply-accumulate operations (up to 2.4%), underscoring the model's efficiency and feasibility. These findings validate the efficacy of FabuLight-ASD in enhancing ASD performance through the integration of body pose data. FabuLight-ASD's code and model weights are available at https://github.com/knowledgetechnologyuhh/FabuLight-ASD.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2411.13587.pdf' target='_blank'>https://arxiv.org/pdf/2411.13587.pdf</a></span>   <span><a href='https://github.com/William-wAng618/roboticAttack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taowen Wang, Cheng Han, James Chenhao Liang, Wenhao Yang, Dongfang Liu, Luna Xinyu Zhang, Qifan Wang, Jiebo Luo, Ruixiang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13587">Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. Despite their significant capabilities, VLA models introduce new attack surfaces. This paper systematically evaluates their robustness. Recognizing the unique demands of robotic execution, our attack objectives target the inherent spatial and functional characteristics of robotic systems. In particular, we introduce two untargeted attack objectives that leverage spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory. Additionally, we design an adversarial patch generation approach that places a small, colorful patch within the camera's view, effectively executing the attack in both digital and physical environments. Our evaluation reveals a marked degradation in task success rates, with up to a 100\% reduction across a suite of simulated robotic tasks, highlighting critical security gaps in current VLA architectures. By unveiling these vulnerabilities and proposing actionable evaluation metrics, we advance both the understanding and enhancement of safety for VLA-based robotic systems, underscoring the necessity for continuously developing robust defense strategies prior to physical-world deployments.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2411.05902.pdf' target='_blank'>https://arxiv.org/pdf/2411.05902.pdf</a></span>   <span><a href='https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05902">Autoregressive Models in Vision: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2410.22997.pdf' target='_blank'>https://arxiv.org/pdf/2410.22997.pdf</a></span>   <span><a href='https://github.com/AIS-Bonn/Prompt_Engineering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Bode, Bastian PÃ¤tzold, Raphael Memmesheimer, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22997">A Comparison of Prompt Engineering Techniques for Task Planning and Execution in Service Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in LLM have been instrumental in autonomous robot control and human-robot interaction by leveraging their vast general knowledge and capabilities to understand and reason across a wide range of tasks and scenarios. Previous works have investigated various prompt engineering techniques for improving the performance of LLM to accomplish tasks, while others have proposed methods that utilize LLMs to plan and execute tasks based on the available functionalities of a given robot platform. In this work, we consider both lines of research by comparing prompt engineering techniques and combinations thereof within the application of high-level task planning and execution in service robotics. We define a diverse set of tasks and a simple set of functionalities in simulation, and measure task completion accuracy and execution time for several state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2410.20263.pdf' target='_blank'>https://arxiv.org/pdf/2410.20263.pdf</a></span>   <span><a href='https://github.com/chengkaiAcademyCity/EfficientEQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Cheng, Zhengyuan Li, Xingpeng Sun, Byung-Cheol Min, Amrit Singh Bedi, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20263">EfficientEQA: An Efficient Approach to Open-Vocabulary Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) is an essential yet challenging task for robot assistants. Large vision-language models (VLMs) have shown promise for EQA, but existing approaches either treat it as static video question answering without active exploration or restrict answers to a closed set of choices. These limitations hinder real-world applicability, where a robot must explore efficiently and provide accurate answers in open-vocabulary settings. To overcome these challenges, we introduce EfficientEQA, a novel framework that couples efficient exploration with free-form answer generation. EfficientEQA features three key innovations: (1) Semantic-Value-Weighted Frontier Exploration (SFE) with Verbalized Confidence (VC) from a black-box VLM to prioritize semantically important areas to explore, enabling the agent to gather relevant information faster; (2) a BLIP relevancy-based mechanism to stop adaptively by flagging highly relevant observations as outliers to indicate whether the agent has collected enough information; and (3) a Retrieval-Augmented Generation (RAG) method for the VLM to answer accurately based on pertinent images from the agent's observation history without relying on predefined choices. Our experimental results show that EfficientEQA achieves over 15% higher answer accuracy and requires over 20% fewer exploration steps than state-of-the-art methods. Our code is available at: https://github.com/chengkaiAcademyCity/EfficientEQA
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2410.16411.pdf' target='_blank'>https://arxiv.org/pdf/2410.16411.pdf</a></span>   <span><a href='https://github.com/clmoro/Robotics-RL-FMs-Integration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelo Moroncelli, Vishal Soni, Marco Forgione, Dario Piga, Blerina Spahiu, Loris Roveda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16411">The Duality of Generative AI and Reinforcement Learning in Robotics: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.
  Lastly, we identify open challenges accounting for model scalability, adaptation and grounding, giving recommendations and insights on future research directions. We reflect on which generative AI models best fit the RL tasks and why. On the other side, we reflect on important issues inherent to RL-enhanced generative policies, such as safety concerns and failure modes, and what are the limitations of current methods. A curated collection of relevant research papers is maintained on our GitHub repository, serving as a resource for ongoing research and development in this field: https://github.com/clmoro/Robotics-RL-FMs-Integration.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2410.02751.pdf' target='_blank'>https://arxiv.org/pdf/2410.02751.pdf</a></span>   <span><a href='https://github.com/aielawady/relic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Elawady, Gunjan Chhablani, Ram Ramrakhya, Karmesh Yadav, Dhruv Batra, Zsolt Kira, Andrew Szot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02751">ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called "partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2409.20188.pdf' target='_blank'>https://arxiv.org/pdf/2409.20188.pdf</a></span>   <span><a href='https://github.com/bigzen/Active-Listener' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bishal Ghosh, Emma Li, Tanaya Guha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20188">Active Listener: Continuous Generation of Listener's Head Motion Response in Dyadic Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key component of dyadic spoken interactions is the contextually relevant non-verbal gestures, such as head movements that reflect a listener's response to the interlocutor's speech. Although significant progress has been made in the context of generating co-speech gestures, generating listener's response has remained a challenge. We introduce the task of generating continuous head motion response of a listener in response to the speaker's speech in real time. To this end, we propose a graph-based end-to-end crossmodal model that takes interlocutor's speech audio as input and directly generates head pose angles (roll, pitch, yaw) of the listener in real time. Different from previous work, our approach is completely data-driven, does not require manual annotations or oversimplify head motion to merely nods and shakes. Extensive evaluation on the dyadic interaction sessions on the IEMOCAP dataset shows that our model produces a low overall error (4.5 degrees) and a high frame rate, thereby indicating its deployability in real-world human-robot interaction systems. Our code is available at - https://github.com/bigzen/Active-Listener
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2409.17004.pdf' target='_blank'>https://arxiv.org/pdf/2409.17004.pdf</a></span>   <span><a href='https://github.com/IrmakDogan/ExpressionDataset' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/IrmakDogan/ExpressionDataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fethiye Irmak Dogan, Maithili Patel, Weiyu Liu, Iolanda Leite, Sonia Chernova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17004">A Model-Agnostic Approach for Semantically Driven Disambiguation in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ambiguities are inevitable in human-robot interaction, especially when a robot follows user instructions in a large, shared space. For example, if a user asks the robot to find an object in a home environment with underspecified instructions, the object could be in multiple locations depending on missing factors. For instance, a bowl might be in the kitchen cabinet or on the dining room table, depending on whether it is clean or dirty, full or empty, and the presence of other objects around it. Previous works on object search have assumed that the queried object is immediately visible to the robot or have predicted object locations using one-shot inferences, which are likely to fail for ambiguous or partially understood instructions. This paper focuses on these gaps and presents a novel model-agnostic approach leveraging semantically driven clarifications to enhance the robot's ability to locate queried objects in fewer attempts. Specifically, we leverage different knowledge embedding models, and when ambiguities arise, we propose an informative clarification method, which follows an iterative prediction process. The user experiment evaluation of our method shows that our approach is applicable to different custom semantic encoders as well as LLMs, and informative clarifications improve performances, enabling the robot to locate objects on its first attempts. The user experiment data is publicly available at https://github.com/IrmakDogan/ExpressionDataset.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2409.15560.pdf' target='_blank'>https://arxiv.org/pdf/2409.15560.pdf</a></span>   <span><a href='https://github.com/exponentialR/QUB-PHEO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Adebayo, SeÃ¡n McLoone, Joost C. Dessing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15560">QUB-PHEO: A Visual-Based Dyadic Multi-View Dataset for Intention Inference in Collaborative Assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>QUB-PHEO introduces a visual-based, dyadic dataset with the potential of advancing human-robot interaction (HRI) research in assembly operations and intention inference. This dataset captures rich multimodal interactions between two participants, one acting as a 'robot surrogate,' across a variety of assembly tasks that are further broken down into 36 distinct subtasks. With rich visual annotations, such as facial landmarks, gaze, hand movements, object localization, and more for 70 participants, QUB-PHEO offers two versions: full video data for 50 participants and visual cues for all 70. Designed to improve machine learning models for HRI, QUB-PHEO enables deeper analysis of subtle interaction cues and intentions, promising contributions to the field. The dataset will be available at https://github.com/exponentialR/QUB-PHEO subject to an End-User License Agreement (EULA).
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2409.14908.pdf' target='_blank'>https://arxiv.org/pdf/2409.14908.pdf</a></span>   <span><a href='https://github.com/WZX0Swarm0Robotics/KARMA/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14908">KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates long-term and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. KARMA distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dual-memory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data. Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by 1.3x and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by 3.4x and 62.7x. Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms.Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient. The experimental videos from the work can be found at https://youtu.be/4BT7fnw9ehs. Our code is available at https://github.com/WZX0Swarm0Robotics/KARMA/tree/master.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2409.10071.pdf' target='_blank'>https://arxiv.org/pdf/2409.10071.pdf</a></span>   <span><a href='https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Chen, Jiawei Tu, Chao Qi, Yonghao Dang, Feng Zhou, Wei Wei, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10071">Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant advancements in embodied vision navigation have raised concerns about its susceptibility to adversarial attacks exploiting deep neural networks. Investigating the adversarial robustness of embodied vision navigation is crucial, especially given the threat of 3D physical attacks that could pose risks to human safety. However, existing attack methods for embodied vision navigation often lack physical feasibility due to challenges in transferring digital perturbations into the physical world. Moreover, current physical attacks for object detection struggle to achieve both multi-view effectiveness and visual naturalness in navigation scenarios. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches to objects, where both opacity and textures are learnable. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which optimizes the patch's texture based on feedback from the vision-based perception model used in navigation. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, in which opacity is fine-tuned after texture optimization. Experimental results demonstrate that our adversarial patches decrease the navigation success rate by an average of 22.39%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2409.09016.pdf' target='_blank'>https://arxiv.org/pdf/2409.09016.pdf</a></span>   <span><a href='https://github.com/OpenDriveLab/CLOVER' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/OpenDriveLab/CLOVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09016">Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2409.05655.pdf' target='_blank'>https://arxiv.org/pdf/2409.05655.pdf</a></span>   <span><a href='https://github.com/DLR-RM/interactive-incremental-learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Knauer, Alin Albu-SchÃ¤ffer, Freek Stulp, JoÃ£o SilvÃ©rio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05655">Interactive incremental learning of generalizable skills with local trajectory modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of generalization in learning from demonstration (LfD) has received considerable attention over the years, particularly within the context of movement primitives, where a number of approaches have emerged. Recently, two important approaches have gained recognition. While one leverages via-points to adapt skills locally by modulating demonstrated trajectories, another relies on so-called task-parameterized models that encode movements with respect to different coordinate systems, using a product of probabilities for generalization. While the former are well-suited to precise, local modulations, the latter aim at generalizing over large regions of the workspace and often involve multiple objects. Addressing the quality of generalization by leveraging both approaches simultaneously has received little attention. In this work, we propose an interactive imitation learning framework that simultaneously leverages local and global modulations of trajectory distributions. Building on the kernelized movement primitives (KMP) framework, we introduce novel mechanisms for skill modulation from direct human corrective feedback. Our approach particularly exploits the concept of via-points to incrementally and interactively 1) improve the model accuracy locally, 2) add new objects to the task during execution and 3) extend the skill into regions where demonstrations were not provided. We evaluate our method on a bearing ring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2409.05583.pdf' target='_blank'>https://arxiv.org/pdf/2409.05583.pdf</a></span>   <span><a href='https://github.com/gmuraleekrishna/SAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muraleekrishna Gopinathan, Martin Masek, Jumana Abu-Khalaf, David Suter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05583">Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI aims to develop robots that can \textit{understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or \textit{Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at \url{https://github.com/gmuraleekrishna/SAS}.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2409.01559.pdf' target='_blank'>https://arxiv.org/pdf/2409.01559.pdf</a></span>   <span><a href='https://github.com/pr2-humanoid/PR2-Platform,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangxin Liu, Qi Xie, Zeyu Zhang, Tao Yuan, Song Wang, Zaijin Wang, Xiaokun Leng, Lining Sun, Jingwen Zhang, Zhicheng He, Yao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01559">PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in Competition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the development of a Physics-realistic and Photo-realistic humanoid robot testbed, PR2, to facilitate collaborative research between Embodied Artificial Intelligence (Embodied AI) and robotics. PR2 offers high-quality scene rendering and robot dynamic simulation, enabling (i) the creation of diverse scenes using various digital assets, (ii) the integration of advanced perception or foundation models, and (iii) the implementation of planning and control algorithms for dynamic humanoid robot behaviors based on environmental feedback. The beta version of PR2 has been deployed for the simulation track of a nationwide full-size humanoid robot competition for college students, attracting 137 teams and over 400 participants within four months. This competition covered traditional tasks in bipedal walking, as well as novel challenges in loco-manipulation and language-instruction-based object search, marking a first for public college robotics competitions. A retrospective analysis of the competition suggests that future events should emphasize the integration of locomotion with manipulation and perception. By making the PR2 testbed publicly available at https://github.com/pr2-humanoid/PR2-Platform, we aim to further advance education and training in humanoid robotics. Video demonstration: https://pr2-humanoid.github.io/
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2408.11537.pdf' target='_blank'>https://arxiv.org/pdf/2408.11537.pdf</a></span>   <span><a href='https://github.com/RayYoh/OCRM_survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, Lap-Pui Chau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11537">A Survey of Embodied Learning for Object-Centric Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM_survey.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2408.04449.pdf' target='_blank'>https://arxiv.org/pdf/2408.04449.pdf</a></span>   <span><a href='https://github.com/zihao-ai/EARBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Zhu, Bingzhe Wu, Zhengyou Zhang, Lei Han, Qingshan Liu, Baoyuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04449">EARBench: Towards Evaluating Physical Risk Awareness for Task Planning of Foundation Model-based Embodied AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied artificial intelligence (EAI) integrates advanced AI models into physical entities for real-world interaction. The emergence of foundation models as the "brain" of EAI agents for high-level task planning has shown promising results. However, the deployment of these agents in physical environments presents significant safety challenges. For instance, a housekeeping robot lacking sufficient risk awareness might place a metal container in a microwave, potentially causing a fire. To address these critical safety concerns, comprehensive pre-deployment risk assessments are imperative. This study introduces EARBench, a novel framework for automated physical risk assessment in EAI scenarios. EAIRiskBench employs a multi-agent cooperative system that leverages various foundation models to generate safety guidelines, create risk-prone scenarios, make task planning, and evaluate safety systematically. Utilizing this framework, we construct EARDataset, comprising diverse test cases across various domains, encompassing both textual and visual scenarios. Our comprehensive evaluation of state-of-the-art foundation models reveals alarming results: all models exhibit high task risk rates (TRR), with an average of 95.75% across all evaluated models. To address these challenges, we further propose two prompting-based risk mitigation strategies. While these strategies demonstrate some efficacy in reducing TRR, the improvements are limited, still indicating substantial safety concerns. This study provides the first large-scale assessment of physical risk awareness in EAI agents. Our findings underscore the critical need for enhanced safety measures in EAI systems and provide valuable insights for future research directions in developing safer embodied artificial intelligence system. Data and code are available at https://github.com/zihao-ai/EARBench.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2407.14758.pdf' target='_blank'>https://arxiv.org/pdf/2407.14758.pdf</a></span>   <span><a href='https://github.com/AllenXuuu/DISCO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14758">DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building a general-purpose intelligent home-assistant agent skilled in diverse tasks by human commands is a long-term blueprint of embodied AI research, which poses requirements on task planning, environment modeling, and object interaction. In this work, we study primitive mobile manipulations for embodied agents, i.e. how to navigate and interact based on an instructed verb-noun pair. We propose DISCO, which features non-trivial advancements in contextualized scene modeling and efficient controls. In particular, DISCO incorporates differentiable scene representations of rich semantics in object and affordance, which is dynamically learned on the fly and facilitates navigation planning. Besides, we propose dual-level coarse-to-fine action controls leveraging both global and local cues to accomplish mobile manipulation tasks efficiently. DISCO easily integrates into embodied tasks such as embodied instruction following. To validate our approach, we take the ALFRED benchmark of large-scale long-horizon vision-language navigation and interaction tasks as a test bed. In extensive experiments, we make comprehensive evaluations and demonstrate that DISCO outperforms the art by a sizable +8.6% success rate margin in unseen scenes, even without step-by-step instructions. Our code is publicly released at https://github.com/AllenXuuu/DISCO.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2407.11954.pdf' target='_blank'>https://arxiv.org/pdf/2407.11954.pdf</a></span>   <span><a href='https://github.com/olga-zats/GTDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Zatsarynna, Emad Bahrami, Yazan Abu Farha, Gianpiero Francesca, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11954">Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term action anticipation has become an important task for many applications such as autonomous driving and human-robot interaction. Unlike short-term anticipation, predicting more actions into the future imposes a real challenge with the increasing uncertainty in longer horizons. While there has been a significant progress in predicting more actions into the future, most of the proposed methods address the task in a deterministic setup and ignore the underlying uncertainty. In this paper, we propose a novel Gated Temporal Diffusion (GTD) network that models the uncertainty of both the observation and the future predictions. As generator, we introduce a Gated Anticipation Network (GTAN) to model both observed and unobserved frames of a video in a mutual representation. On the one hand, using a mutual representation for past and future allows us to jointly model ambiguities in the observation and future, while on the other hand GTAN can by design treat the observed and unobserved parts differently and steer the information flow between them. Our model achieves state-of-the-art results on the Breakfast, Assembly101 and 50Salads datasets in both stochastic and deterministic settings. Code: https://github.com/olga-zats/GTDA .
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2407.11915.pdf' target='_blank'>https://arxiv.org/pdf/2407.11915.pdf</a></span>   <span><a href='https://github.com/dingdingding60/Humanoids2024HRI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bosong Ding, Murat Kirtay, Giacomo Spigler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11915">Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head movements are crucial for social human-human interaction. They can transmit important cues (e.g., joint attention, speaker detection) that cannot be achieved with verbal interaction alone. This advantage also holds for human-robot interaction. Even though modeling human motions through generative AI models has become an active research area within robotics in recent years, the use of these methods for producing head movements in human-robot interaction remains underexplored. In this work, we employed a generative AI pipeline to produce human-like head movements for a Nao humanoid robot. In addition, we tested the system on a real-time active-speaker tracking task in a group conversation setting. Overall, the results show that the Nao robot successfully imitates human head movements in a natural manner while actively tracking the speakers during the conversation. Code and data from this study are available at https://github.com/dingdingding60/Humanoids2024HRI
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2407.11325.pdf' target='_blank'>https://arxiv.org/pdf/2407.11325.pdf</a></span>   <span><a href='https://github.com/cilinyan/VISA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, Efstratios Gavves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11325">VISA: Reasoning Video Object Segmentation via Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Video Object Segmentation (VOS) relies on explicit user instructions, such as categories, masks, or short phrases, restricting their ability to perform complex video segmentation requiring reasoning with world knowledge. In this paper, we introduce a new task, Reasoning Video Object Segmentation (ReasonVOS). This task aims to generate a sequence of segmentation masks in response to implicit text queries that require complex reasoning abilities based on world knowledge and video contexts, which is crucial for structured environment understanding and object-centric interactions, pivotal in the development of embodied AI. To tackle ReasonVOS, we introduce VISA (Video-based large language Instructed Segmentation Assistant), to leverage the world knowledge reasoning capabilities of multi-modal LLMs while possessing the ability to segment and track objects in videos with a mask decoder. Moreover, we establish a comprehensive benchmark consisting of 35,074 instruction-mask sequence pairs from 1,042 diverse videos, which incorporates complex world knowledge reasoning into segmentation tasks for instruction-tuning and evaluation purposes of ReasonVOS models. Experiments conducted on 8 datasets demonstrate the effectiveness of VISA in tackling complex reasoning segmentation and vanilla referring segmentation in both video and image domains. The code and dataset are available at https://github.com/cilinyan/VISA.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2407.10943.pdf' target='_blank'>https://arxiv.org/pdf/2407.10943.pdf</a></span>   <span><a href='https://github.com/OpenRobotLab/GRUtopia' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, Peizhou Cao, Wenye Yu, Zichao Ye, Jialun Li, Junfeng Long, Zirui Wang, Huiling Wang, Ying Zhao, Zhongying Tu, Yu Qiao, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10943">GRUtopia: Dream General Robots in a City at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have been exploring the scaling laws in the field of Embodied AI. Given the prohibitive costs of collecting real-world data, we believe the Simulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the learning of embodied models. This paper introduces project GRUtopia, the first simulated interactive 3D society designed for various robots. It features several advancements: (a) The scene dataset, GRScenes, includes 100k interactive, finely annotated scenes, which can be freely combined into city-scale environments. In contrast to previous works mainly focusing on home, GRScenes covers 89 diverse scene categories, bridging the gap of service-oriented environments where general robots would be initially deployed. (b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC) system that is responsible for social interaction, task generation, and task assignment, thus simulating social scenarios for embodied AI applications. (c) The benchmark, GRBench, supports various robots but focuses on legged robots as primary agents and poses moderately challenging tasks involving Object Loco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that this work can alleviate the scarcity of high-quality data in this field and provide a more comprehensive assessment of Embodied AI research. The project is available at https://github.com/OpenRobotLab/GRUtopia.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2407.07061.pdf' target='_blank'>https://arxiv.org/pdf/2407.07061.pdf</a></span>   <span><a href='https://github.com/OpenBMB/IoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07061">Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \url{https://github.com/OpenBMB/IoA}.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2407.06886.pdf' target='_blank'>https://arxiv.org/pdf/2407.06886.pdf</a></span>   <span><a href='https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06886">Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2406.19741.pdf' target='_blank'>https://arxiv.org/pdf/2406.19741.pdf</a></span>   <span><a href='https://github.com/huawei-noah/HEBO/tree/master/ROSLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher E. Mower, Yuhui Wan, Hongzhan Yu, Antoine Grosnit, Jonas Gonzalez-Billandon, Matthieu Zimmer, Jinlong Wang, Xinyu Zhang, Yao Zhao, Anbang Zhai, Puze Liu, Daniel Palenicek, Davide Tateo, Cesar Cadena, Marco Hutter, Jan Peters, Guangjian Tian, Yuzheng Zhuang, Kun Shao, Xingyue Quan, Jianye Hao, Jun Wang, Haitham Bou-Ammar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19741">ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for intuitive robot programming by non-experts, leveraging natural language prompts and contextual information from the Robot Operating System (ROS). Our system integrates large language models (LLMs), enabling non-experts to articulate task requirements to the system through a chat interface. Key features of the framework include: integration of ROS with an AI agent connected to a plethora of open-source and commercial LLMs, automatic extraction of a behavior from the LLM output and execution of ROS actions/services, support for three behavior modes (sequence, behavior tree, state machine), imitation learning for adding new robot actions to the library of possible actions, and LLM reflection via human and environment feedback. Extensive experiments validate the framework, showcasing robustness, scalability, and versatility in diverse scenarios, including long-horizon tasks, tabletop rearrangements, and remote supervisory control. To facilitate the adoption of our framework and support the reproduction of our results, we have made our code open-source. You can access it at: https://github.com/huawei-noah/HEBO/tree/master/ROSLLM.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2406.13807.pdf' target='_blank'>https://arxiv.org/pdf/2406.13807.pdf</a></span>   <span><a href='https://github.com/alanaai/EVUD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Suglia, Claudio Greco, Katie Baker, Jose L. Part, Ioannis Papaioannou, Arash Eshghi, Ioannis Konstas, Oliver Lemon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13807">AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI personal assistants deployed via robots or wearables require embodied understanding to collaborate with humans effectively. However, current Vision-Language Models (VLMs) primarily focus on third-person view videos, neglecting the richness of egocentric perceptual experience. To address this gap, we propose three key contributions. First, we introduce the Egocentric Video Understanding Dataset (EVUD) for training VLMs on video captioning and question answering tasks specific to egocentric videos. Second, we present AlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD. Finally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging benchmark for embodied video question answering. Our model achieves state-of-the-art performance, outperforming open-source models including strong Socratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform Claude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to Gemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning. This research paves the way for building efficient VLMs that can be deployed in robots or wearables, leveraging embodied video understanding to collaborate seamlessly with humans in everyday tasks, contributing to the next generation of Embodied AI.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2406.13642.pdf' target='_blank'>https://arxiv.org/pdf/2406.13642.pdf</a></span>   <span><a href='https://github.com/BAAI-DCAI/SpatialBot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13642">SpatialBot: Precise Spatial Understanding with Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models (VLMs) have achieved impressive performance in 2D image understanding, however they are still struggling with spatial understanding which is the foundation of Embodied AI. In this paper, we propose SpatialBot for better spatial understanding by feeding both RGB and depth images. Additionally, we have constructed the SpatialQA dataset, which involves multi-level depth-related questions to train VLMs for depth understanding. Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities in spatial understanding at different levels. Extensive experiments on our spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks, demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2406.07500.pdf' target='_blank'>https://arxiv.org/pdf/2406.07500.pdf</a></span>   <span><a href='https://github.com/vpulab/SPIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier Montalvo, Juan Ignacio Bravo PÃ©rez-Villar, Ãlvaro GarcÃ­a-MartÃ­n, Pablo Carballeira, JesÃºs BescÃ³s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07500">SPIN: Spacecraft Imagery for Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of data acquired under actual space operational conditions poses a significant challenge for developing learning-based visual navigation algorithms crucial for autonomous spacecraft navigation. This data shortage is primarily due to the prohibitive costs and inherent complexities of space operations. While existing datasets, predominantly relying on computer-simulated data, have partially addressed this gap, they present notable limitations. Firstly, these datasets often utilize proprietary image generation tools, restricting the evaluation of navigation methods in novel, unseen scenarios. Secondly, they provide limited ground-truth data, typically focusing solely on the spacecraft's translation and rotation relative to the camera. To address these limitations, we present SPIN (SPacecraft Imagery for Navigation), an open-source spacecraft image generation tool designed to support a wide range of visual navigation scenarios in space, with a particular focus on relative navigation tasks. SPIN provides multiple modalities of ground-truth data and allows researchers to employ custom 3D models of satellites, define specific camera-relative poses, and adjust settings such as camera parameters or environmental illumination conditions. We also propose a method for exploiting our tool as a data augmentation module. We validate our tool on the spacecraft pose estimation task by training with a SPIN-generated replica of SPEED+, reaching a 47% average error reduction on SPEED+ testbed data (that simulates realistic space conditions), further reducing it to a 60% error reduction when using SPIN as a data augmentation method. Both the SPIN tool (and source code) and our SPIN-generated version of SPEED+ will be publicly released upon paper acceptance on GitHub. https://github.com/vpulab/SPIN
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2405.18170.pdf' target='_blank'>https://arxiv.org/pdf/2405.18170.pdf</a></span>   <span><a href='https://github.com/renchizhhhh/OpenChessRobot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renchi Zhang, Joost de Winter, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18170">An Open-Source Reproducible Chess Robot for Human-Robot Interaction Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in AI have accelerated the evolution of versatile robot designs. Chess provides a standardized environment for evaluating the impact of robot behavior on human behavior. This article presents an open-source chess robot for human-robot interaction (HRI) research, specifically focusing on verbal and non-verbal interactions. The OpenChessRobot recognizes chess pieces using computer vision, executes moves, and interacts with the human player through voice and robotic gestures. We detail the software design, provide quantitative evaluations of the efficacy of the robot, and offer a guide for its reproducibility. An online survey examining people's views of the robot in three possible scenarios was conducted with 597 participants. The robot received the highest ratings in the robotics education and the chess coach scenarios, while the home entertainment scenario received the lowest scores. The code is accessible on GitHub: https://github.com/renchizhhhh/OpenChessRobot
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2405.14093.pdf' target='_blank'>https://arxiv.org/pdf/2405.14093.pdf</a></span>   <span><a href='https://github.com/yueen-ma/Awesome-VLA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yueen-ma/Awesome-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14093">A Survey on Vision-Language-Action Models for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is widely recognized as a key element of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. In recent years, a myriad of VLAs have been developed, making it imperative to capture the rapidly evolving landscape through a comprehensive survey. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs and outline promising future directions in embodied AI. We have created a project associated with this survey, which is available at https://github.com/yueen-ma/Awesome-VLA.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2404.10675.pdf' target='_blank'>https://arxiv.org/pdf/2404.10675.pdf</a></span>   <span><a href='https://github.com/KubeEdge4Robotics/ScaleNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Chen, Yuecheng Liu, Yuzheng Zhuang, Sitong Mao, Kaiwen Xue, Shunbo Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10675">SCALE: Self-Correcting Visual Navigation for Mobile Robots via Anti-Novelty Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task. Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation. This significantly drops the success rates and even induces collision. In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention. Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL). When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND). A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places. We collect offline data and conduct evaluation experiments in three real-world urban scenarios. Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention. Code is available at https://github.com/KubeEdge4Robotics/ScaleNav.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2404.06243.pdf' target='_blank'>https://arxiv.org/pdf/2404.06243.pdf</a></span>   <span><a href='https://github.com/rana2149/ActNetFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06243">ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more. Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire. This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition. Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples. We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer. The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames. By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action. This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures. Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data. The official website of this work is available at: https://github.com/rana2149/ActNetFormer.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2404.00838.pdf' target='_blank'>https://arxiv.org/pdf/2404.00838.pdf</a></span>   <span><a href='https://github.com/3M-OS/3MOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Ye, Xichao Teng, Shuo Chen, Yijie Bian, Tao Tan, Zhang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00838">3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical-SAR image matching is a fundamental task for image fusion and visual navigation. However, all large-scale open SAR dataset for methods development are collected from single platform, resulting in limited satellite types and spatial resolutions. Since images captured by different sensors vary significantly in both geometric and radiometric appearance, existing methods may fail to match corresponding regions containing the same content. Besides, most of existing datasets have not been categorized based on the characteristics of different scenes. To encourage the design of more general multi-modal image matching methods, we introduce a large-scale Multi-sources,Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching(3MOS). It consists of 155K optical-SAR image pairs, including SAR data from six commercial satellites, with resolutions ranging from 1.25m to 12.5m. The data has been classified into eight scenes including urban, rural, plains, hills, mountains, water, desert, and frozen earth. Extensively experiments show that none of state-of-the-art methods achieve consistently superior performance across different sources, resolutions and scenes. In addition, the distribution of data has a substantial impact on the matching capability of deep learning models, this proposes the domain adaptation challenge in optical-SAR image matching. Our data and code will be available at:https://github.com/3M-OS/3MOS.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2403.12670.pdf' target='_blank'>https://arxiv.org/pdf/2403.12670.pdf</a></span>   <span><a href='https://github.com/library87/OpenRoboExp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boren Li, Hang Li, Hangxin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12670">Driving Animatronic Robot Facial Expression From Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: \url{https://github.com/library87/OpenRoboExp}.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2403.11625.pdf' target='_blank'>https://arxiv.org/pdf/2403.11625.pdf</a></span>   <span><a href='https://github.com/XiaohanLei/GaussNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11625">GaussNav: Gaussian Splatting for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment. The primary challenge of IIN arises from the need to recognize the target object across varying viewpoints while ignoring potential distractors. Existing map-based navigation methods typically use Bird's Eye View (BEV) maps, which lack detailed texture representation of a scene. Consequently, while BEV maps are effective for semantic-level visual navigation, they are struggling for instance-level tasks. To this end, we propose a new framework for IIN, Gaussian Splatting for Visual Navigation (GaussNav), which constructs a novel map representation based on 3D Gaussian Splatting (3DGS). The GaussNav framework enables the agent to memorize both the geometry and semantic information of the scene, as well as retain the textural features of objects. By matching renderings of similar objects with the target, the agent can accurately identify, ground, and navigate to the specified object. Our GaussNav framework demonstrates a significant performance improvement, with Success weighted by Path Length (SPL) increasing from 0.347 to 0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset. The source code is publicly available at the link: https://github.com/XiaohanLei/GaussNav.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2403.08109.pdf' target='_blank'>https://arxiv.org/pdf/2403.08109.pdf</a></span>   <span><a href='https://github.com/mhnazeri/VANP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Nazeri, Junzhe Wang, Amirreza Payandeh, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08109">VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects -- not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders. Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2403.07376.pdf' target='_blank'>https://arxiv.org/pdf/2403.07376.pdf</a></span>   <span><a href='https://github.com/expectorlin/NavCoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07376">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2402.19161.pdf' target='_blank'>https://arxiv.org/pdf/2402.19161.pdf</a></span>   <span><a href='https://github.com/ZJULiHongxin/MemoNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19161">MemoNav: Working Memory Model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation. The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map. Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes. Qualitative results further illustrate that MemoNav plans more efficient routes.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2402.16068.pdf' target='_blank'>https://arxiv.org/pdf/2402.16068.pdf</a></span>   <span><a href='https://github.com/lcastri/roscausal.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola Bellotto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16068">ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2402.05655.pdf' target='_blank'>https://arxiv.org/pdf/2402.05655.pdf</a></span>   <span><a href='https://github.com/Oliverbansk/Holistic-Robot-Pose-Estimation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shikun Ban, Juling Fan, Xiaoxuan Ma, Wentao Zhu, Yu Qiao, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05655">Real-time Holistic Robot Pose Estimation with Unknown States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles. However, this assumption is not always valid in practical situations. In real-world applications such as multi-robot collaboration or human-robot interaction, the robot joint states might not be shared or could be unreliable. On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications. This work introduces an efficient framework for real-time robot pose estimation from RGB images without requiring known robot states. Our method estimates camera-to-robot rotation, robot state parameters, keypoint locations, and root depth, employing a neural network module for each task to facilitate learning and sim-to-real transfer. Notably, it achieves inference in a single feed-forward pass without iterative optimization. Our approach offers a 12-time speed increase with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time. Code and models are available at https://github.com/Oliverbansk/Holistic-Robot-Pose-Estimation.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2402.00290.pdf' target='_blank'>https://arxiv.org/pdf/2402.00290.pdf</a></span>   <span><a href='https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00290">MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the surge in the development of large language models, embodied intelligence has attracted increasing attention. Nevertheless, prior works on embodied intelligence typically encode scene or historical memory in an unimodal manner, either visual or linguistic, which complicates the alignment of the model's action planning with embodied control. To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions. Specifically, we propose a novel Multimodal Environment Memory (MEM) module, facilitating the integration of embodied control with large models through the visual-language memory of scenes. This capability enables MEIA to generate executable action plans based on diverse requirements and the robot's capabilities. Furthermore, we construct an embodied question answering dataset based on a dynamic virtual cafe environment with the help of the large language model. In this virtual environment, we conduct several experiments, utilizing multiple large models through zero-shot learning, and carefully design scenarios for various situations. The experimental results showcase the promising performance of our MEIA in various embodied interactive tasks.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2401.16699.pdf' target='_blank'>https://arxiv.org/pdf/2401.16699.pdf</a></span>   <span><a href='https://github.com/jxu124/TiO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Xu, Hanbo Zhang, Qingyi Si, Yifeng Li, Xuguang Lan, Tao Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16699">Towards Unified Interactive Visual Grounding in The Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages. It requires robots to disambiguate the user input by active information gathering. Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios. In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction. Benefiting from a unified formulation of visual dialogue and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios. In the experiments, we validate TiO on GuessWhat?! and InViG benchmarks, setting new state-of-the-art performance by a clear margin. Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms. Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate. Codes and demos are available at https://github.com/jxu124/TiO.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2401.11838.pdf' target='_blank'>https://arxiv.org/pdf/2401.11838.pdf</a></span>   <span><a href='https://github.com/LinusNEP/TCC_IRoNL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linus Nwankwo, Elmar Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11838">The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, autonomous agents have surged in real-world environments such as our homes, offices, and public spaces. However, natural human-robot interaction remains a key challenge. In this paper, we introduce an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue. We leveraged the LLMs to decode the high-level natural language instructions from humans and abstract them into precise robot actionable commands or queries. Further, we utilised the VLMs to provide a visual and semantic understanding of the robot's task environment. Our results with 99.13% command recognition accuracy and 97.96% commands execution success show that our approach can enhance human-robot interaction in real-world applications. The video demonstrations of this paper can be found at https://osf.io/wzyf6 and the code is available at our GitHub repository (https://github.com/LinusNEP/TCC_IRoNL.git).
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2312.16170.pdf' target='_blank'>https://arxiv.org/pdf/2312.16170.pdf</a></span>   <span><a href='https://github.com/OpenRobotLab/EmbodiedScan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16170">EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions. This necessitates the ability to fully understand 3D scenes given their first-person observations and contextualize them into language for interaction. However, traditional research focuses more on scene-level input and output setups from a global view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding. It encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories. Building upon this database, we introduce a baseline framework named Embodied Perceptron. It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and in the wild. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2312.15144.pdf' target='_blank'>https://arxiv.org/pdf/2312.15144.pdf</a></span>   <span><a href='https://github.com/zhshj0110/CSRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Zhang, Jianqin Yin, Yonghao Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15144">A Generically Contrastive Spatiotemporal Representation Enhancement for 3D Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition is a central task in computer vision and human-robot interaction. However, most previous methods suffer from overlooking the explicit exploitation of the latent data distributions (i.e., the intra-class variations and inter-class relations), thereby leading to confusion about ambiguous samples and sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a Contrastive Spatiotemporal Representation Enhancement (CSRE) framework to obtain more discriminative representations from the sequences, which can be incorporated into various previous skeleton encoders and can be removed when testing. Specifically, we decompose the representation into spatial- and temporal-specific features to explore fine-grained motion patterns along the corresponding dimensions. Furthermore, to explicitly exploit the latent data distributions, we employ the attentive features to contrastive learning, which models the cross-sequence semantic relations by pulling together the features from the positive pairs and pushing away the negative pairs. Extensive experiments show that CSRE with five various skeleton encoders (HCN, 2S-AGCN, CTR-GCN, Hyperformer, and BlockGCN) achieves solid improvements on five benchmarks. The code will be released at https://github.com/zhshj0110/CSRE.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2312.01097.pdf' target='_blank'>https://arxiv.org/pdf/2312.01097.pdf</a></span>   <span><a href='https://github.com/joeyy5588/planning-as-inpainting' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Fu Yang, Haoyang Xu, Te-Lin Wu, Xiaofeng Gao, Kai-Wei Chang, Feng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01097">Planning as In-Painting: A Diffusion-Based Embodied Task Planning Framework for Environments under Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task planning for embodied AI has been one of the most challenging problems where the community does not meet a consensus in terms of formulation. In this paper, we aim to tackle this problem with a unified framework consisting of an end-to-end trainable method and a planning algorithm. Particularly, we propose a task-agnostic method named 'planning as in-painting'. In this method, we use a Denoising Diffusion Model (DDM) for plan generation, conditioned on both language instructions and perceptual inputs under partially observable environments. Partial observation often leads to the model hallucinating the planning. Therefore, our diffusion-based method jointly models both state trajectory and goal estimation to improve the reliability of the generated plan, given the limited available information at each step. To better leverage newly discovered information along the plan execution for a higher success rate, we propose an on-the-fly planning algorithm to collaborate with the diffusion-based planner. The proposed framework achieves promising performances in various embodied AI tasks, including vision-language navigation, object manipulation, and task planning in a photorealistic virtual environment. The code is available at: https://github.com/joeyy5588/planning-as-inpainting.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2311.13373.pdf' target='_blank'>https://arxiv.org/pdf/2311.13373.pdf</a></span>   <span><a href='https://github.com/ZJLAB-AMMI/LLM4Teach' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, Bin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13373">Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have uncovered the potential of Large Language Models (LLMs) in addressing complex sequential decision-making tasks through the provision of high-level instructions. However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming. On the other hand, reinforcement learning (RL) approaches train agents that specialize in the target task but often suffer from low sampling efficiency and high exploration costs. In this paper, we introduce a novel framework that addresses these challenges by training a smaller, specialized student RL agent using instructions from an LLM-based teacher agent. By incorporating the guidance from the teacher agent, the student agent can distill the prior knowledge of the LLM into its own model. Consequently, the student agent can be trained with significantly less data. Moreover, through further training with environment feedback, the student agent surpasses the capabilities of its teacher for completing the target task. We conducted experiments on challenging MiniGrid and Habitat environments, specifically designed for embodied AI research, to evaluate the effectiveness of our framework. The results clearly demonstrate that our approach achieves superior performance compared to strong baseline methods. Our code is available at https://github.com/ZJLAB-AMMI/LLM4Teach.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2311.00530.pdf' target='_blank'>https://arxiv.org/pdf/2311.00530.pdf</a></span>   <span><a href='https://github.com/Rongtao-Xu/Awesome-LLM-EN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhou Lin, Han Gao, Xuxiang Feng, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00530">Advances in Embodied Navigation Using Large Language Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2310.13398.pdf' target='_blank'>https://arxiv.org/pdf/2310.13398.pdf</a></span>   <span><a href='https://github.com/Fudan-ProjectTitan/OpenAnnotate3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijie Zhou, Likun Cai, Xianhui Cheng, Zhongxue Gan, Xiangyang Xue, Wenchao Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13398">OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2310.12547.pdf' target='_blank'>https://arxiv.org/pdf/2310.12547.pdf</a></span>   <span><a href='https://github.com/JHKim-snu/PGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12547">PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user's environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant efficiency while previous LCRG systems rely on resource-intensive human annotations -- necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA's real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2310.12344.pdf' target='_blank'>https://arxiv.org/pdf/2310.12344.pdf</a></span>   <span><a href='https://github.com/joeyy5588/LACMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Fu Yang, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, Yu-Chiang Frank Wang, Kai-Wei Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12344">LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent's insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent's hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent's low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents. The code is available at: https://github.com/joeyy5588/LACMA.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2310.09883.pdf' target='_blank'>https://arxiv.org/pdf/2310.09883.pdf</a></span>   <span><a href='https://github.com/SmartAndCleverRobot/ICRA-CIRN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinting Li, Shiguang Zhang, Yue LU, Kerry Dang, Lingyan Ran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09883">Zero-Shot Object Goal Visual Navigation With Class-Independent Relationship Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the zero-shot object goal visual navigation problem. In the object goal visual navigation task, the agent needs to locate navigation targets from its egocentric visual input. "Zero-shot" means that the target the agent needs to find is not trained during the training phase. To address the issue of coupling navigation ability with target features during training, we propose the Class-Independent Relationship Network (CIRN). This method combines target detection information with the relative semantic similarity between the target and the navigation target, and constructs a brand new state representation based on similarity ranking, this state representation does not include target feature or environment feature, effectively decoupling the agent's navigation ability from target features. And a Graph Convolutional Network (GCN) is employed to learn the relationships between different objects based on their similarities. During testing, our approach demonstrates strong generalization capabilities, including zero-shot navigation tasks with different targets and environments. Through extensive experiments in the AI2-THOR virtual environment, our method outperforms the current state-of-the-art approaches in the zero-shot object goal visual navigation task. Furthermore, we conducted experiments in more challenging cross-target and cross-scene settings, which further validate the robustness and generalization ability of our method. Our code is available at: https://github.com/SmartAndCleverRobot/ICRA-CIRN.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2310.08588.pdf' target='_blank'>https://arxiv.org/pdf/2310.08588.pdf</a></span>   <span><a href='https://github.com/dongyh20/Octopus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08588">Octopus: Embodied Vision-Language Programmer from Environmental Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. When integrated into an embodied agent, existing embodied VLM works either output detailed action sequences at the manipulation level or only provide plans at an abstract level, leaving a gap between high-level planning and real-world manipulation. To bridge this gap, we introduce Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation. Octopus is designed to 1) proficiently comprehend an agent's visual and textual task objectives, 2) formulate intricate action sequences, and 3) generate executable code. To facilitate Octopus model development, we introduce OctoVerse: a suite of environments tailored for benchmarking vision-based code generators on a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games such as Grand Theft Auto (GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an explorative agent that generates training data, i.e., action blueprints and corresponding executable code. We also collect feedback that enables an enhanced training scheme called Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's functionality and present compelling results, showing that the proposed RLEF refines the agent's decision-making. By open-sourcing our simulation environments, dataset, and model architecture, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2310.01824.pdf' target='_blank'>https://arxiv.org/pdf/2310.01824.pdf</a></span>   <span><a href='https://github.com/StanfordVL/mini_behavior' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, Roberto MartÃ­n-MartÃ­n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01824">Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and development of solutions, simplifying their assessment and development while advancing the field of embodied AI. Code is publicly available at https://github.com/StanfordVL/mini_behavior.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2309.10436.pdf' target='_blank'>https://arxiv.org/pdf/2309.10436.pdf</a></span>   <span><a href='https://github.com/TIERS/ws-lidar-as-camera-odom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haizhou Zhang, Xianjia Yu, Sier Ha, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10436">LiDAR-Generated Images Derived Keypoints Assisted Point Cloud Registration Scheme in Odometry Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Keypoint detection and description play a pivotal role in various robotics and autonomous applications including visual odometry (VO), visual navigation, and Simultaneous localization and mapping (SLAM). While a myriad of keypoint detectors and descriptors have been extensively studied in conventional camera images, the effectiveness of these techniques in the context of LiDAR-generated images, i.e. reflectivity and ranges images, has not been assessed. These images have gained attention due to their resilience in adverse conditions such as rain or fog. Additionally, they contain significant textural information that supplements the geometric information provided by LiDAR point clouds in the point cloud registration phase, especially when reliant solely on LiDAR sensors. This addresses the challenge of drift encountered in LiDAR Odometry (LO) within geometrically identical scenarios or where not all the raw point cloud is informative and may even be misleading. This paper aims to analyze the applicability of conventional image key point extractors and descriptors on LiDAR-generated images via a comprehensive quantitative investigation. Moreover, we propose a novel approach to enhance the robustness and reliability of LO. After extracting key points, we proceed to downsample the point cloud, subsequently integrating it into the point cloud registration phase for the purpose of odometry estimation. Our experiment demonstrates that the proposed approach has comparable accuracy but reduced computational overhead, higher odometry publishing rate, and even superior performance in scenarios prone to drift by using the raw point cloud. This, in turn, lays a foundation for subsequent investigations into the integration of LiDAR-generated images with LO. Our code is available on GitHub: https://github.com/TIERS/ws-lidar-as-camera-odom.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2309.10309.pdf' target='_blank'>https://arxiv.org/pdf/2309.10309.pdf</a></span>   <span><a href='https://github.com/wzcai99/Pixel-Navigator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Cai, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10309">Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of home-assistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real-world environments validate the effectiveness of our proposed navigation strategy. Code and video demos are available at https://github.com/wzcai99/Pixel-Navigator.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2309.07918.pdf' target='_blank'>https://arxiv.org/pdf/2309.07918.pdf</a></span>   <span><a href='https://github.com/OpenRobotLab/UniHSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07918">Unified Human-Scene Interaction via Prompted Chain-of-Contacts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. This framework is built upon the definition of interaction as Chain of Contacts (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. The project page is at https://github.com/OpenRobotLab/UniHSI .
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2309.07164.pdf' target='_blank'>https://arxiv.org/pdf/2309.07164.pdf</a></span>   <span><a href='https://github.com/AnshulRanjan2004/PyHMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anshul Ranjan, Kaushik Jegadeesan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07164">Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel hybrid Automatic Speech Recognition (ASR) system designed specifically for resource-constrained robots. The proposed approach combines Hidden Markov Models (HMMs) with deep learning models and leverages socket programming to distribute processing tasks effectively. In this architecture, the HMM-based processing takes place within the robot, while a separate PC handles the deep learning model. This synergy between HMMs and deep learning enhances speech recognition accuracy significantly. We conducted experiments across various robotic platforms, demonstrating real-time and precise speech recognition capabilities. Notably, the system exhibits adaptability to changing acoustic conditions and compatibility with low-power hardware, making it highly effective in environments with limited computational resources. This hybrid ASR paradigm opens up promising possibilities for seamless human-robot interaction. In conclusion, our research introduces a pioneering dimension to ASR techniques tailored for robotics. By employing socket programming to distribute processing tasks across distinct devices and strategically combining HMMs with deep learning models, our hybrid ASR system showcases its potential to enable robots to comprehend and respond to spoken language adeptly, even in environments with restricted computational resources. This paradigm sets a innovative course for enhancing human-robot interaction across a wide range of real-world scenarios.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2309.05073.pdf' target='_blank'>https://arxiv.org/pdf/2309.05073.pdf</a></span>   <span><a href='https://github.com/wangjiongw/FreeMan_API' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiong Wang, Fengyu Yang, Wenbo Gou, Bingliang Li, Danqi Yan, Ailing Zeng, Yijun Gao, Junle Wang, Yanqing Jing, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05073">FreeMan: Towards Benchmarking 3D Human Pose Estimation under Real-World Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the 3D structure of the human body from natural scenes is a fundamental aspect of visual perception. 3D human pose estimation is a vital step in advancing fields like AIGC and human-robot interaction, serving as a crucial technique for understanding and interacting with human actions in real-world settings. However, the current datasets, often collected under single laboratory conditions using complex motion capture equipment and unvarying backgrounds, are insufficient. The absence of datasets on variable conditions is stalling the progress of this crucial task. To facilitate the development of 3D pose estimation, we present FreeMan, the first large-scale, multi-view dataset collected under the real-world conditions. FreeMan was captured by synchronizing 8 smartphones across diverse scenarios. It comprises 11M frames from 8000 sequences, viewed from different perspectives. These sequences cover 40 subjects across 10 different scenarios, each with varying lighting conditions. We have also established an semi-automated pipeline containing error detection to reduce the workload of manual check and ensure precise annotation. We provide comprehensive evaluation baselines for a range of tasks, underlining the significant challenges posed by FreeMan. Further evaluations of standard indoor/outdoor human sensing datasets reveal that FreeMan offers robust representation transferability in real and complex scenes. Code and data are available at https://wangjiongw.github.io/freeman.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2309.00438.pdf' target='_blank'>https://arxiv.org/pdf/2309.00438.pdf</a></span>   <span><a href='https://github.com/martinfleis/urban-block-artifacts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Fleischmann, Anastassia Vybornova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00438">A shape-based heuristic for the detection of urban block artifacts in street networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Street networks are ubiquitous components of cities, guiding their development and enabling movement from place to place; street networks are also the critical components of many urban analytical methods. However, their graph representation is often designed primarily for transportation purposes. This representation is less suitable for other use cases where transportation networks need to be simplified as a mandatory pre-processing step, e.g., in the case of morphological analysis, visual navigation, or drone flight routing. While the urgent demand for automated pre-processing methods comes from various fields, it is still an unsolved challenge. In this article, we tackle this challenge by proposing a cheap computational heuristic for the identification of "face artifacts", i.e., geometries that are enclosed by transportation edges but do not represent urban blocks. The heuristic is based on combining the frequency distributions of shape compactness metrics and area measurements of street network face polygons. We test our method on 131 globally sampled large cities and show that it successfully identifies face artifacts in 89\% of analyzed cities. Our heuristic of detecting artifacts caused by data being collected for another purpose is the first step towards an automated street network simplification workflow. Moreover, the proposed face artifact index uncovers differences in structural rules guiding the development of cities in different world regions.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2308.14965.pdf' target='_blank'>https://arxiv.org/pdf/2308.14965.pdf</a></span>   <span><a href='https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Umar Khalid, Hasan Iqbal, Saeed Vahidian, Jing Hua, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14965">CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) is a rapidly growing field that encompasses social and industrial applications. Machine learning plays a vital role in industrial HRI by enhancing the adaptability and autonomy of robots in complex environments. However, data privacy is a crucial concern in the interaction between humans and robots, as companies need to protect sensitive data while machine learning algorithms require access to large datasets. Federated Learning (FL) offers a solution by enabling the distributed training of models without sharing raw data. Despite extensive research on Federated learning (FL) for tasks such as natural language processing (NLP) and image classification, the question of how to use FL for HRI remains an open research problem. The traditional FL approach involves transmitting large neural network parameter matrices between the server and clients, which can lead to high communication costs and often becomes a bottleneck in FL. This paper proposes a communication-efficient FL framework for human-robot interaction (CEFHRI) to address the challenges of data heterogeneity and communication costs. The framework leverages pre-trained models and introduces a trainable spatiotemporal adapter for video understanding tasks in HRI. Experimental results on three human-robot interaction benchmark datasets: HRI30, InHARD, and COIN demonstrate the superiority of CEFHRI over full fine-tuning in terms of communication costs. The proposed methodology provides a secure and efficient approach to HRI federated learning, particularly in industrial environments with data privacy concerns and limited communication bandwidth. Our code is available at https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2308.12537.pdf' target='_blank'>https://arxiv.org/pdf/2308.12537.pdf</a></span>   <span><a href='https://github.com/dzcgaara/HuBo-VLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichao Dong, Weikun Zhang, Xufeng Huang, Hang Ji, Xin Zhan, Junbo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12537">HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human robot interaction is an exciting task, which aimed to guide robots following instructions from human. Since huge gap lies between human natural language and machine codes, end to end human robot interaction models is fair challenging. Further, visual information receiving from sensors of robot is also a hard language for robot to perceive. In this work, HuBo-VLM is proposed to tackle perception tasks associated with human robot interaction including object detection and visual grounding by a unified transformer based vision language model. Extensive experiments on the Talk2Car benchmark demonstrate the effectiveness of our approach. Code would be publicly available in https://github.com/dzcgaara/HuBo-VLM.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2308.00640.pdf' target='_blank'>https://arxiv.org/pdf/2308.00640.pdf</a></span>   <span><a href='https://github.com/luyh20/VL-Grasp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Lu, Yixuan Fan, Beixing Deng, Fangfu Liu, Yali Li, Shengjin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00640">VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic grasping faces new challenges in human-robot-interaction scenarios. We consider the task that the robot grasps a target object designated by human's language directives. The robot not only needs to locate a target based on vision-and-language information, but also needs to predict the reasonable grasp pose candidate at various views and postures. In this work, we propose a novel interactive grasp policy, named Visual-Lingual-Grasp (VL-Grasp), to grasp the target specified by human language. First, we build a new challenging visual grounding dataset to provide functional training data for robotic interactive perception in indoor environments. Second, we propose a 6-Dof interactive grasp policy combined with visual grounding and 6-Dof grasp pose detection to extend the universality of interactive grasping. Third, we design a grasp pose filter module to enhance the performance of the policy. Experiments demonstrate the effectiveness and extendibility of the VL-Grasp in real world. The VL-Grasp achieves a success rate of 72.5\% in different indoor scenes. The code and dataset is available at https://github.com/luyh20/VL-Grasp.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2307.14332.pdf' target='_blank'>https://arxiv.org/pdf/2307.14332.pdf</a></span>   <span><a href='https://github.com/DaniDeniz/EventVisionTransformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Deniz, Cornelia Fermuller, Eduardo Ros, Manuel Rodriguez-Alvarez, Francisco Barranco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14332">Event-based Vision for Early Prediction of Manipulation Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene. These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing. In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events. There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible. Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction. Our Transformer network uses events to predict manipulation actions as they occur, using online inference. The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification. Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model. Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues. Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition. Code will be available at https://github.com/DaniDeniz/EventVisionTransformer.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2307.11934.pdf' target='_blank'>https://arxiv.org/pdf/2307.11934.pdf</a></span>   <span><a href='https://github.com/shengnanh20/LAMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11934">LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric visual understanding is an important desideratum for effective human-robot interaction. In order to navigate crowded public places, social robots must be able to interpret the activity of the surrounding humans. This paper addresses one key aspect of human-centric visual understanding, multi-person pose estimation. Achieving good performance on multi-person pose estimation in crowded scenes is difficult due to the challenges of occluded joints and instance separation. In order to tackle these challenges and overcome the limitations of image features in representing invisible body parts, we propose a novel prompt-based pose inference strategy called LAMP (Language Assisted Multi-person Pose estimation). By utilizing the text representations generated by a well-trained language model (CLIP), LAMP can facilitate the understanding of poses on the instance and joint levels, and learn more robust visual representations that are less susceptible to occlusion. This paper demonstrates that language-supervised training boosts the performance of single-stage multi-person pose estimation, and both instance-level and joint-level prompts are valuable for training. The code is available at https://github.com/shengnanh20/LAMP.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2307.11343.pdf' target='_blank'>https://arxiv.org/pdf/2307.11343.pdf</a></span>   <span><a href='https://github.com/xtli12/GXU-LIPE.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Gao, XueTao Li, Jun Yu, Feng Shaung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11343">A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of Chat-GPT has led to a surge of interest in Embodied AI. However, many existing Embodied AI models heavily rely on massive interactions with training environments, which may not be practical in real-world situations. To this end, the Maniskill2 has introduced a full-physics simulation benchmark for manipulating various 3D objects. This benchmark enables agents to be trained using diverse datasets of demonstrations and evaluates their ability to generalize to unseen scenarios in testing environments. In this paper, we propose a novel two-stage fine-tuning strategy that aims to further enhance the generalization capability of our model based on the Maniskill2 benchmark. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in all three tracks of the ManiSkill2 Challenge. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their ractical applications in real-world scenarios. All codes and models of our solution is available at https://github.com/xtli12/GXU-LIPE.git
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2307.09027.pdf' target='_blank'>https://arxiv.org/pdf/2307.09027.pdf</a></span>   <span><a href='https://github.com/connorlee77/uav-thermal-water-segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Connor Lee, Jonathan Gustafsson Frennert, Lu Gan, Matthew Anderson, Soon-Jo Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09027">Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals. This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night. Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods. In this work, we curate the first aerial thermal near-shore dataset, show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform. Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2307.07469.pdf' target='_blank'>https://arxiv.org/pdf/2307.07469.pdf</a></span>   <span><a href='https://github.com/Necolizer/ISTA-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Wen, Zixuan Tang, Yunsheng Pang, Beichen Ding, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07469">Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2307.00123.pdf' target='_blank'>https://arxiv.org/pdf/2307.00123.pdf</a></span>   <span><a href='https://github.com/aliayub7/cl_hri' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ayub, Jainish Mehta, Zachary De Francesco, Patrick Holthaus, Kerstin Dautenhahn, Chrystopher L. Nehaniv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00123">How Do Human Users Teach a Continual Learning Robot in Repeated Interactions?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual learning (CL) has emerged as an important avenue of research in recent years, at the intersection of Machine Learning (ML) and Human-Robot Interaction (HRI), to allow robots to continually learn in their environments over long-term interactions with humans. Most research in continual learning, however, has been robot-centered to develop continual learning algorithms that can quickly learn new information on static datasets. In this paper, we take a human-centered approach to continual learning, to understand how humans teach continual learning robots over the long term and if there are variations in their teaching styles. We conducted an in-person study with 40 participants that interacted with a continual learning robot in 200 sessions. In this between-participant study, we used two different CL models deployed on a Fetch mobile manipulator robot. An extensive qualitative and quantitative analysis of the data collected in the study shows that there is significant variation among the teaching styles of individual users indicating the need for personalized adaptation to their distinct teaching styles. The results also show that although there is a difference in the teaching styles between expert and non-expert users, the style does not have an effect on the performance of the continual learning robot. Finally, our analysis shows that the constrained experimental setups that have been widely used to test most continual learning techniques are not adequate, as real users interact with and teach continual learning robots in a variety of ways. Our code is available at https://github.com/aliayub7/cl_hri.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2306.00923.pdf' target='_blank'>https://arxiv.org/pdf/2306.00923.pdf</a></span>   <span><a href='https://github.com/StanfordVL/Sonicverse' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/StanfordVL/sonicverse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruohan Gao, Hao Li, Gokul Dharan, Zhuzhu Wang, Chengshu Li, Fei Xia, Silvio Savarese, Li Fei-Fei, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00923">Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing embodied agents in simulation has been a key research topic in recent years. Exciting new tasks, algorithms, and benchmarks have been developed in various simulators. However, most of them assume deaf agents in silent environments, while we humans perceive the world with multiple senses. We introduce Sonicverse, a multisensory simulation platform with integrated audio-visual simulation for training household agents that can both see and hear. Sonicverse models realistic continuous audio rendering in 3D environments in real-time. Together with a new audio-visual VR interface that allows humans to interact with agents with audio, Sonicverse enables a series of embodied AI tasks that need audio-visual perception. For semantic audio-visual navigation in particular, we also propose a new multi-task learning model that achieves state-of-the-art performance. In addition, we demonstrate Sonicverse's realism via sim-to-real transfer, which has not been achieved by other simulators: an agent trained in Sonicverse can successfully perform audio-visual navigation in real-world environments. Sonicverse is available at: https://github.com/StanfordVL/Sonicverse.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2305.17245.pdf' target='_blank'>https://arxiv.org/pdf/2305.17245.pdf</a></span>   <span><a href='https://github.com/Hamoon1987/meshConfidence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamoon Jafarian, Faisal Z. Qureshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17245">Error Estimation for Single-Image Human Body Mesh Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose and shape estimation methods continue to suffer in situations where one or more parts of the body are occluded. More importantly, these methods cannot express when their predicted pose is incorrect. This has serious consequences when these methods are used in human-robot interaction scenarios, where we need methods that can evaluate their predictions and flag situations where they might be wrong. This work studies this problem. We propose a method that combines information from OpenPose and SPIN -- two popular human pose and shape estimation methods -- to highlight regions on the predicted mesh that are least reliable. We have evaluated the proposed approach on 3DPW, 3DOH, and Human3.6M datasets, and the results demonstrate our model's effectiveness in identifying inaccurate regions of the human body mesh. Our code is available at https://github.com/Hamoon1987/meshConfidence.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2304.03047.pdf' target='_blank'>https://arxiv.org/pdf/2304.03047.pdf</a></span>   <span><a href='https://github.com/MarSaKi/ETPNav' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MarSaKi/ETPNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03047">ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language navigation is a task that requires an agent to follow instructions to navigate in environments. It becomes increasingly crucial in the field of embodied AI, with potential applications in autonomous navigation, search and rescue, and human-robot interaction. In this paper, we propose to address a more practical yet challenging counterpart setting - vision-language navigation in continuous environments (VLN-CE). To develop a robust VLN-CE agent, we propose a new navigation framework, ETPNav, which focuses on two critical skills: 1) the capability to abstract environments and generate long-range navigation plans, and 2) the ability of obstacle-avoiding control in continuous environments. ETPNav performs online topological mapping of environments by self-organizing predicted waypoints along a traversed path, without prior environmental experience. It privileges the agent to break down the navigation procedure into high-level planning and low-level control. Concurrently, ETPNav utilizes a transformer-based cross-modal planner to generate navigation plans based on topological maps and instructions. The plan is then performed through an obstacle-avoiding controller that leverages a trial-and-error heuristic to prevent navigation from getting stuck in obstacles. Experimental results demonstrate the effectiveness of the proposed method. ETPNav yields more than 10% and 20% improvements over prior state-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is available at https://github.com/MarSaKi/ETPNav.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2303.01859.pdf' target='_blank'>https://arxiv.org/pdf/2303.01859.pdf</a></span>   <span><a href='https://github.com/proroklab/popgym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01859">POPGym: Benchmarking Partially Observable Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines -- the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2302.03385.pdf' target='_blank'>https://arxiv.org/pdf/2302.03385.pdf</a></span>   <span><a href='https://github.com/DRL-CASIA/NeuronsGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Li, Shasha Liu, Mingjun Ma, Guangzheng Hu, Yaran Chen, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03385">NeuronsGym: A Hybrid Framework and Benchmark for Robot Tasks with Sim2Real Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of embodied AI has greatly improved the possibility of general mobile agent systems. At present, many evaluation platforms with rich scenes, high visual fidelity and various application scenarios have been developed. In this paper, we present a hybrid framework named NeuronsGym that can be used for policy learning of robot tasks, covering a simulation platform for training policy, and a physical system for studying sim2real problems. Unlike most current single-task, slow-moving robotic platforms, our framework provides agile physical robots with a wider range of speeds, and can be employed to train robotic navigation and confrontation policies. At the same time, in order to evaluate the safety of robot navigation, we propose a safety-weighted path length (SFPL) to improve the safety evaluation in the current mobile robot navigation. Based on this platform, we build a new benchmark for navigation and confrontation tasks under this platform by comparing the current mainstream sim2real methods, and hold the 2022 IEEE Conference on Games (CoG) RoboMaster sim2real challenge. We release the codes of this framework\footnote{\url{https://github.com/DRL-CASIA/NeuronsGym}} and hope that this platform can promote the development of more flexible and agile general mobile agent algorithms.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2301.05223.pdf' target='_blank'>https://arxiv.org/pdf/2301.05223.pdf</a></span>   <span><a href='https://github.com/xavierpuigf/online_watch_and_help' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Puig, Tianmin Shu, Joshua B. Tenenbaum, Antonio Torralba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05223">NOPA: Neurally-guided Online Probabilistic Assistance for Building Socially Intelligent Home Assistants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study how to build socially intelligent robots to assist people in their homes. In particular, we focus on assistance with online goal inference, where robots must simultaneously infer humans' goals and how to help them achieve those goals. Prior assistance methods either lack the adaptivity to adjust helping strategies (i.e., when and how to help) in response to uncertainty about goals or the scalability to conduct fast inference in a large goal space. Our NOPA (Neurally-guided Online Probabilistic Assistance) method addresses both of these challenges. NOPA consists of (1) an online goal inference module combining neural goal proposals with inverse planning and particle filtering for robust inference under uncertainty, and (2) a helping planner that discovers valuable subgoals to help with and is aware of the uncertainty in goal inference. We compare NOPA against multiple baselines in a new embodied AI assistance challenge: Online Watch-And-Help, in which a helper agent needs to simultaneously watch a main agent's action, infer its goal, and help perform a common household task faster in realistic virtual home environments. Experiments show that our helper agent robustly updates its goal inference and adapts its helping plans to the changing level of uncertainty.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2210.13984.pdf' target='_blank'>https://arxiv.org/pdf/2210.13984.pdf</a></span>   <span><a href='https://github.com/LUNAProject22/AAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Clement Tan, Chai Kiat Yeo, Cheston Tan, Basura Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13984">Inferring Past Human Actions in Homes with Abductive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Abductive reasoning aims to make the most likely inference for a given set of incomplete observations. In this paper, we introduce "Abductive Past Action Inference", a novel research task aimed at identifying the past actions performed by individuals within homes to reach specific states captured in a single image, using abductive inference. The research explores three key abductive inference problems: past action set prediction, past action sequence prediction, and abductive past action verification. We introduce several models tailored for abductive past action inference, including a relational graph neural network, a relational bilinear pooling model, and a relational transformer model. Notably, the newly proposed object-relational bilinear graph encoder-decoder (BiGED) model emerges as the most effective among all methods evaluated, demonstrating good proficiency in handling the intricacies of the Action Genome dataset. The contributions of this research significantly advance the ability of deep learning models to reason about current scene evidence and make highly plausible inferences about past human actions. This advancement enables a deeper understanding of events and behaviors, which can enhance decision-making and improve system capabilities across various real-world applications such as Human-Robot Interaction and Elderly Care and Health Monitoring. Code and data available at https://github.com/LUNAProject22/AAR
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2210.06887.pdf' target='_blank'>https://arxiv.org/pdf/2210.06887.pdf</a></span>   <span><a href='https://github.com/cmower/ros_pybullet_interface' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher E. Mower, Theodoros Stouraitis, JoÃ£o Moura, Christian Rauch, Lei Yan, Nazanin Zamani Behabadi, Michael Gienger, Tom Vercauteren, Christos Bergeles, Sethu Vijayakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06887">ROS-PyBullet Interface: A Framework for Reliable Contact Simulation and Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable contact simulation plays a key role in the development of (semi-)autonomous robots, especially when dealing with contact-rich manipulation scenarios, an active robotics research topic. Besides simulation, components such as sensing, perception, data collection, robot hardware control, human interfaces, etc. are all key enablers towards applying machine learning algorithms or model-based approaches in real world systems. However, there is a lack of software connecting reliable contact simulation with the larger robotics ecosystem (i.e. ROS, Orocos), for a more seamless application of novel approaches, found in the literature, to existing robotic hardware. In this paper, we present the ROS-PyBullet Interface, a framework that provides a bridge between the reliable contact/impact simulator PyBullet and the Robot Operating System (ROS). Furthermore, we provide additional utilities for facilitating Human-Robot Interaction (HRI) in the simulated environment. We also present several use-cases that highlight the capabilities and usefulness of our framework. Please check our video, source code, and examples included in the supplementary material. Our full code base is open source and can be found at https://github.com/cmower/ros_pybullet_interface.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2208.00759.pdf' target='_blank'>https://arxiv.org/pdf/2208.00759.pdf</a></span>   <span><a href='https://github.com/proroklab/sensor-guided-visual-nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Blumenkamp, Qingbiao Li, Binyu Wang, Zhe Liu, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.00759">See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate to the target along the shortest path. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to 2.0x improvement in SPL (Success weighted by Path Length) when compared to a communication-free baseline. This is done without requiring a global map, positioning data, nor pre-calibration of the sensor network. Second, we perform a zero-shot transfer of our model from simulation to the real world. Laboratory experiments demonstrate the feasibility of our approach in various cluttered environments. Finally, we showcase examples of successful navigation to the target while both the sensor network layout as well as obstacles are dynamically reconfigured as the robot navigates. We provide a video demo, the dataset, trained models, and source code.
  https://www.youtube.com/watch?v=kcmr6RUgucw https://github.com/proroklab/sensor-guided-visual-nav
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2206.07423.pdf' target='_blank'>https://arxiv.org/pdf/2206.07423.pdf</a></span>   <span><a href='https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianfan Zhao, Lu Zhang, Bin He, Hong Qiao, Zhiyong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.07423">Zero-shot object goal visual navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal visual navigation is a challenging task that aims to guide a robot to find the target object based on its visual observation, and the target is limited to the classes pre-defined in the training stage. However, in real households, there may exist numerous target classes that the robot needs to deal with, and it is hard for all of these classes to be contained in the training stage. To address this challenge, we study the zero-shot object goal visual navigation task, which aims at guiding robots to find targets belonging to novel classes without any training samples. To this end, we also propose a novel zero-shot object navigation framework called semantic similarity network (SSNet). Our framework use the detection results and the cosine similarity between semantic word embeddings as input. Such type of input data has a weak correlation with classes and thus our framework has the ability to generalize the policy to novel classes. Extensive experiments on the AI2-THOR platform show that our model outperforms the baseline models in the zero-shot object navigation task, which proves the generalization ability of our model. Our code is available at: https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2502.09624.pdf' target='_blank'>https://arxiv.org/pdf/2502.09624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Kang, Jiana Liao, Runquan Gao, Jinbo Wen, Huawei Huang, Maomao Zhang, Changyan Yi, Tao Zhang, Dusit Niyato, Zibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09624">Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By synergistically integrating mobile networks and embodied artificial intelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of vehicular messages or vulnerability to malicious tampering, potentially causing severe traffic accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the miner trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2410.01176.pdf' target='_blank'>https://arxiv.org/pdf/2410.01176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhong, Jiawen Kang, Jinbo Wen, Dongdong Ye, Jiangtian Nie, Dusit Niyato, Xiaozheng Gao, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01176">Generative Diffusion-based Contract Design for Efficient AI Twins Migration in Vehicular Embodied AI Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is a rapidly advancing field that bridges the gap between cyberspace and physical space, enabling a wide range of applications. This evolution has led to the development of the Vehicular Embodied AI NETwork (VEANET), where advanced AI capabilities are integrated into vehicular systems to enhance autonomous operations and decision-making. Embodied agents, such as Autonomous Vehicles (AVs), are autonomous entities that can perceive their environment and take actions to achieve specific goals, actively interacting with the physical world. Embodied twins are digital models of these embodied agents, with various embodied AI twins for intelligent applications in cyberspace. In VEANET, embodied AI twins act as in-vehicle AI assistants to perform diverse tasks supporting autonomous driving using generative AI models. Due to limited computational resources of AVs, these AVs often offload computationally intensive tasks, such as constructing and updating embodied AI twins, to nearby RSUs. However, since the rapid mobility of AVs and the limited provision coverage of a single RSU, embodied AI twins require dynamic migrations from current RSU to other RSUs in real-time, resulting in the challenge of selecting suitable RSUs for efficient embodied AI twins migrations. Given information asymmetry, AVs cannot know the detailed information of RSUs. To this end, in this paper, we construct a multi-dimensional contract theoretical model between AVs and alternative RSUs. Considering that AVs may exhibit irrational behavior, we utilize prospect theory instead of expected utility theory to model the actual utilities of AVs. Finally, we employ a generative diffusion model-based algorithm to identify the optimal contract designs. Compared with traditional deep reinforcement learning algorithms, numerical results demonstrate the effectiveness of the proposed scheme.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2501.01141.pdf' target='_blank'>https://arxiv.org/pdf/2501.01141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruichen Zhang, Changyuan Zhao, Hongyang Du, Dusit Niyato, Jiacheng Wang, Suttinee Sawadsitang, Xuemin Shen, Dong In Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01141">Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language Models and Reinforcement Learning Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating large language models (LLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90\% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model's focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36\% compared to DDPG and accelerates convergence by reducing required steps by up to 47\% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4\% improvement in QoE when scaling from 4 to 8 vehicles.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2509.07496.pdf' target='_blank'>https://arxiv.org/pdf/2509.07496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayano Miyamichi, Moju Zhao, Kazuki Sugihara, Junichiro Sugihara, Masanori Konishi, Kunio Kojima, Kei Okada, Masayuki Inaba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07496">Flexible Morphing Aerial Robot with Inflatable Structure for Perching-based Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Birds in nature perform perching not only for rest but also for interaction with human such as the relationship with falconers. Recently, researchers achieve perching-capable aerial robots as a way to save energy, and deformable structure demonstrate significant advantages in efficiency of perching and compactness of configuration. However, ensuring flight stability remains challenging for deformable aerial robots due to the difficulty of controlling flexible arms. Furthermore, perching for human interaction requires high compliance along with safety. Thus, this study aims to develop a deformable aerial robot capable of perching on humans with high flexibility and grasping ability. To overcome the challenges of stability of both flight and perching, we propose a hybrid morphing structure that combines a unilateral flexible arm and a pneumatic inflatable actuators. This design allows the robot's arms to remain rigid during flight and soft while perching for more effective grasping. We also develop a pneumatic control system that optimizes pressure regulation while integrating shock absorption and adjustable grasping forces, enhancing interaction capabilities and energy efficiency. Besides, we focus on the structural characteristics of the unilateral flexible arm and identify sufficient conditions under which standard quadrotor modeling and control remain effective in terms of flight stability. Finally, the developed prototype demonstrates the feasibility of compliant perching maneuvers on humans, as well as the robust recovery even after arm deformation caused by thrust reductions during flight. To the best of our knowledge, this work is the first to achieve an aerial robot capable of perching on humans for interaction.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2508.06553.pdf' target='_blank'>https://arxiv.org/pdf/2508.06553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Xiao, Jianbo Zhang, BoWen Yan, Shengyu Guo, Tongrui Ye, Kaiwei Zhang, Zicheng Zhang, Xiaohong Liu, Zhengxue Cheng, Lei Fan, Chuyi Li, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06553">Static and Plugged: Make Embodied Evaluation Simple</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied intelligence is advancing rapidly, driving the need for efficient evaluation. Current benchmarks typically rely on interactive simulated environments or real-world setups, which are costly, fragmented, and hard to scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play benchmark that enables unified evaluation using static scene representations. Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and comprehensive assessment through a simple interface. Furthermore, we evaluate 19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs), establishing the first unified static leaderboard for Embodied intelligence. Moreover, we release a subset of 200 samples from our benchmark to accelerate the development of embodied intelligence.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2409.11279.pdf' target='_blank'>https://arxiv.org/pdf/2409.11279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11279">P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2506.19613.pdf' target='_blank'>https://arxiv.org/pdf/2506.19613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sha Zhang, Suorong Yang, Tong Xie, Xiangyuan Xue, Zixuan Hu, Rui Li, Wenxi Qu, Zhenfei Yin, Tianfan Fu, Di Hu, Andres M Bran, Nian Ran, Bram Hoex, Wangmeng Zuo, Philippe Schwaller, Wanli Ouyang, Lei Bai, Yanyong Zhang, Lingyu Duan, Shixiang Tang, Dongzhan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19613">Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific discovery has long been constrained by human limitations in expertise, physical capability, and sleep cycles. The recent rise of AI scientists and automated laboratories has accelerated both the cognitive and operational aspects of research. However, key limitations persist: AI systems are often confined to virtual environments, while automated laboratories lack the flexibility and autonomy to adaptively test new hypotheses in the physical world. Recent advances in embodied AI, such as generalist robot foundation models, diffusion-based action policies, fine-grained manipulation learning, and sim-to-real transfer, highlight the promise of integrating cognitive and embodied intelligence. This convergence opens the door to closed-loop systems that support iterative, autonomous experimentation and the possibility of serendipitous discovery. In this position paper, we propose the paradigm of Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework that deeply integrates cognitive and embodied intelligence. ISLs unify foundation models for scientific reasoning, agent-based workflow orchestration, and embodied agents for robust physical experimentation. We argue that such systems are essential for overcoming the current limitations of scientific discovery and for realizing the full transformative potential of AI-driven science.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2401.04334.pdf' target='_blank'>https://arxiv.org/pdf/2401.04334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Huaqin Zhao, Zhengliang Liu, Haixing Dai, Lin Zhao, Bao Ge, Xiang Li, Tianming Liu, Shu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04334">Large Language Models for Robotics: Opportunities, Challenges, and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2412.19996.pdf' target='_blank'>https://arxiv.org/pdf/2412.19996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoqi Yang, Yong Chen, Jiacheng Wang, Geng Sun, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19996">Embodied AI-empowered Low Altitude Economy: Integrated Sensing, Communications, Computation, and Control (ISC3)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low altitude economy (LAE) holds immense potential to drive urban development across various sectors. However, LAE also faces challenges in data collection and processing efficiency, flight control precision, and network performance. The challenges could be solved by realizing an integration of sensing, communications, computation, and control (ISC3) for LAE. In this regard, embodied artificial intelligence (EAI), with its unique perception, planning, and decision-making capabilities, offers a promising solution to realize ISC3. Specifically, this paper investigates an application of EAI into ISC3 to support LAE, exploring potential research focuses, solutions, and case study. We begin by outlining rationales and benefits of introducing EAI into LAE, followed by reviewing research directions and solutions for EAI in ISC3. We then propose a framework of an EAI-enabled ISC3 for LAE. The framework's effectiveness is evaluated through a case study of express delivery utilizing an EAI-enabled UAV. Finally, we discuss several future research directions for advancing EAI-enabled LAE.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2509.19870.pdf' target='_blank'>https://arxiv.org/pdf/2509.19870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Wang, Jie Li, Zejia Weng, Yixu Wang, Yifeng Gao, Tianyu Pang, Chao Du, Yan Teng, Yingchun Wang, Zuxuan Wu, Xingjun Ma, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19870">FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2508.00823.pdf' target='_blank'>https://arxiv.org/pdf/2508.00823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Guo, Xiuwei Xu, Hang Yin, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00823">IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2506.05204.pdf' target='_blank'>https://arxiv.org/pdf/2506.05204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Ziyi Wang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05204">OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing semantic-aware 3D scenes from sparse views is a challenging yet essential research direction, driven by the demands of emerging applications such as virtual reality and embodied AI. Existing per-scene optimization methods require dense input views and incur high computational costs, while generalizable approaches often struggle to reconstruct regions outside the input view cone. In this paper, we propose OGGSplat, an open Gaussian growing method that expands the field-of-view in generalizable 3D reconstruction. Our key insight is that the semantic attributes of open Gaussians provide strong priors for image extrapolation, enabling both semantic consistency and visual plausibility. Specifically, once open Gaussians are initialized from sparse views, we introduce an RGB-semantic consistent inpainting module applied to selected rendered views. This module enforces bidirectional control between an image diffusion model and a semantic diffusion model. The inpainted regions are then lifted back into 3D space for efficient and progressive Gaussian parameter optimization. To evaluate our method, we establish a Gaussian Outpainting (GO) benchmark that assesses both semantic and generative quality of reconstructed open-vocabulary scenes. OGGSplat also demonstrates promising semantic-aware scene reconstruction capabilities when provided with two view images captured directly from a smartphone camera.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2506.05171.pdf' target='_blank'>https://arxiv.org/pdf/2506.05171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linxuan He, Qing-Shan Jia, Ang Li, Hongyan Sang, Ling Wang, Jiwen Lu, Tao Zhang, Jie Zhou, Yi Zhang, Yisen Wang, Peng Wei, Zhongyuan Wang, Henry X. Liu, Shuo Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05171">Towards provable probabilistic safety for scalable embodied AI systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems, comprising AI models and physical plants, are increasingly prevalent across various applications. Due to the rarity of system failures, ensuring their safety in complex operating environments remains a major challenge, which severely hinders their large-scale deployment in safety-critical domains, such as autonomous vehicles, medical devices, and robotics. While achieving provable deterministic safety--verifying system safety across all possible scenarios--remains theoretically ideal, the rarity and complexity of corner cases make this approach impractical for scalable embodied AI systems. Instead, empirical safety evaluation is employed as an alternative, but the absence of provable guarantees imposes significant limitations. To address these issues, we argue for a paradigm shift to provable probabilistic safety that integrates provable guarantees with progressive achievement toward a probabilistic safety boundary on overall system performance. The new paradigm better leverages statistical methods to enhance feasibility and scalability, and a well-defined probabilistic safety boundary enables embodied AI systems to be deployed at scale. In this Perspective, we outline a roadmap for provable probabilistic safety, along with corresponding challenges and potential solutions. By bridging the gap between theoretical safety assurance and practical deployment, this Perspective offers a pathway toward safer, large-scale adoption of embodied AI systems in safety-critical applications.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2412.18194.pdf' target='_blank'>https://arxiv.org/pdf/2412.18194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18194">VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh\&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2507.15428.pdf' target='_blank'>https://arxiv.org/pdf/2507.15428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaao Li, Kaiyuan Li, Chen Gao, Yong Li, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15428">EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egomotion videos are first-person recordings where the view changes continuously due to the agent's movement. As they serve as the primary visual input for embodied AI agents, making egomotion video reasoning more efficient is therefore essential for real-world deployment. Recent advances in vision-language models have enabled strong multimodal reasoning capabilities, but their computational cost remains prohibitive for long, redundant video inputs. Existing token pruning methods, typically designed for third-person videos, fail to leverage the spatiotemporal continuity and motion constraints inherent in egomotion settings. To address this, we propose EgoPrune, a training-free token pruning method tailored for egomotion video reasoning. EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR for temporally efficient sampling; Perspective-Aware Redundancy Filtering (PARF), which aligns visual tokens using perspective transformations and removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token selector that jointly considers visual-text relevance and intra-frame diversity. Experiments on two egomotion video benchmarks show that EgoPrune consistently outperforms prior training-free methods across various pruning ratios while significantly reducing FLOPs, memory usage, and latency. Moreover, we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB edge device, demonstrating its real-world efficiency and suitability for on-device egomotion video reasoning.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2504.09587.pdf' target='_blank'>https://arxiv.org/pdf/2504.09587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Xu, Yue Hu, Chen Gao, Zhengqiu Zhu, Yong Zhao, Yong Li, Quanjun Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09587">GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2502.11859.pdf' target='_blank'>https://arxiv.org/pdf/2502.11859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11859">Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2509.12989.pdf' target='_blank'>https://arxiv.org/pdf/2509.12989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12989">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2505.11907.pdf' target='_blank'>https://arxiv.org/pdf/2505.11907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Dongfang, Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Danda Pani Paudel, Luc Van Gool, Kailun Yang, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11907">Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 180x360 omnidirectional field of view captured by 360-degree cameras enables their use in a wide range of applications such as embodied AI and virtual reality. Although recent advances in multimodal large language models (MLLMs) have shown promise in visual-spatial reasoning, most studies focus on standard pinhole-view images, leaving omnidirectional perception largely unexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial reasoning? To investigate this, we introduce OSR-Bench, the first benchmark specifically designed for this setting. OSR-Bench includes over 153,000 diverse question-answer pairs grounded in high-fidelity panoramic indoor scene maps. It covers key reasoning types including object counting, relative distance, and direction. We also propose a negative sampling strategy that inserts non-existent objects into prompts to evaluate hallucination and grounding robustness. For fine-grained analysis, we design a two-stage evaluation framework assessing both cognitive map generation and QA accuracy using rotation-invariant matching and a combination of rule-based and LLM-based metrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5 Pro, and leading open-source models under zero-shot settings. Results show that current models struggle with spatial reasoning in panoramic contexts, highlighting the need for more perceptually grounded MLLMs. OSR-Bench and code will be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2412.01398.pdf' target='_blank'>https://arxiv.org/pdf/2412.01398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna-Maria Halacheva, Yang Miao, Jan-Nico Zaech, Xi Wang, Luc Van Gool, Danda Pani Paudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01398">Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding is a long-standing challenge in computer vision and a key component in enabling mixed reality, wearable computing, and embodied AI. Providing a solution to these applications requires a multifaceted approach that covers scene-centric, object-centric, as well as interaction-centric capabilities. While there exist numerous datasets and algorithms approaching the former two problems, the task of understanding interactable and articulated objects is underrepresented and only partly covered in the research field. In this work, we address this shortcoming by introducing: (1) Articulate3D, an expertly curated 3D dataset featuring high-quality manual annotations on 280 indoor scenes. Articulate3D provides 8 types of annotations for articulated objects, covering parts and detailed motion information, all stored in a standardized scene representation format designed for scalable 3D content creation, exchange and seamless integration into simulation environments. (2) USDNet, a novel unified framework capable of simultaneously predicting part segmentation along with a full specification of motion attributes for articulated objects. We evaluate USDNet on Articulate3D as well as two existing datasets, demonstrating the advantage of our unified dense prediction approach. Furthermore, we highlight the value of Articulate3D through cross-dataset and cross-domain evaluations and showcase its applicability in downstream tasks such as scene editing through LLM prompting and robotic policy training for articulated object manipulation. We provide open access to our dataset, benchmark, and method's source code.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2409.15250.pdf' target='_blank'>https://arxiv.org/pdf/2409.15250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15250">ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in large language models and access to large-scale robotic datasets has sparked a paradigm shift in robotics models transforming them into generalists able to adapt to various tasks, scenes, and robot modalities. A large step for the community are open Vision Language Action models which showcase strong performance in a wide variety of tasks. In this work, we study the visual generalization capabilities of three existing robotic foundation models, and propose a corresponding evaluation framework. Our study shows that the existing models do not exhibit robustness to visual out-of-domain scenarios. This is potentially caused by limited variations in the training data and/or catastrophic forgetting, leading to domain limitations in the vision foundation models. We further explore OpenVLA, which uses two pre-trained vision foundation models and is, therefore, expected to generalize to out-of-domain experiments. However, we showcase catastrophic forgetting by DINO-v2 in OpenVLA through its failure to fulfill the task of depth regression. To overcome the aforementioned issue of visual catastrophic forgetting, we propose a gradual backbone reversal approach founded on model merging. This enables OpenVLA -- which requires the adaptation of the visual backbones during initial training -- to regain its visual generalization ability. Regaining this capability enables our ReVLA model to improve over OpenVLA by a factor of 77\% and 66\% for grasping and lifting in visual OOD tasks. Comprehensive evaluations, episode rollouts and model weights are available on the ReVLA Page
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2409.16019.pdf' target='_blank'>https://arxiv.org/pdf/2409.16019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghao Qi, Shenghai Yuan, Fen Liu, Haozhi Cao, Tianchen Deng, Jianfei Yang, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16019">AIR-Embodied: An Efficient Active 3DGS-based Interaction and Reconstruction Framework with Embodied Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D reconstruction and neural rendering have enhanced the creation of high-quality digital assets, yet existing methods struggle to generalize across varying object shapes, textures, and occlusions. While Next Best View (NBV) planning and Learning-based approaches offer solutions, they are often limited by predefined criteria and fail to manage occlusions with human-like common sense. To address these problems, we present AIR-Embodied, a novel framework that integrates embodied AI agents with large-scale pretrained multi-modal language models to improve active 3DGS reconstruction. AIR-Embodied utilizes a three-stage process: understanding the current reconstruction state via multi-modal prompts, planning tasks with viewpoint selection and interactive actions, and employing closed-loop reasoning to ensure accurate execution. The agent dynamically refines its actions based on discrepancies between the planned and actual outcomes. Experimental evaluations across virtual and real-world environments demonstrate that AIR-Embodied significantly enhances reconstruction efficiency and quality, providing a robust solution to challenges in active 3D reconstruction.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2406.16258.pdf' target='_blank'>https://arxiv.org/pdf/2406.16258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Chen, Chen Tang, Jianglan Wei, Chenran Li, Ran Tian, Xiang Zhang, Wei Zhan, Peter Stone, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16258">MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce MEReQ (Maximum-Entropy Residual-Q Inverse Reinforcement Learning), designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2506.06535.pdf' target='_blank'>https://arxiv.org/pdf/2506.06535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineet Bhat, Naman Patel, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06535">MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation of unseen objects via natural language commands remains challenging. Language driven robotic grasping (LDRG) predicts stable grasp poses from natural language queries and RGB-D images. We propose MapleGrasp, a novel framework that leverages mask-guided feature pooling for efficient vision-language driven grasping. Our two-stage training first predicts segmentation masks from CLIP-based vision-language features. The second stage pools features within these masks to generate pixel-level grasp predictions, improving efficiency, and reducing computation. Incorporating mask pooling results in a 7% improvement over prior approaches on the OCID-VLG benchmark. Furthermore, we introduce RefGraspNet, an open-source dataset eight times larger than existing alternatives, significantly enhancing model generalization for open-vocabulary grasping. MapleGrasp scores a strong grasping accuracy of 89\% when compared with competing methods in the RefGraspNet benchmark. Our method achieves comparable performance to larger Vision-Language-Action models on the LIBERO benchmark, and shows significantly better generalization to unseen tasks. Real-world experiments on a Franka arm demonstrate 73% success rate with unseen objects, surpassing competitive baselines by 11%. Code is provided in our github repository.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2505.16055.pdf' target='_blank'>https://arxiv.org/pdf/2505.16055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patanjali Maithani, Aliasghar Arab, Farshad Khorrami, Prashanth Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16055">Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In collaborative human-robot environments, the unpredictable and dynamic nature of human motion can lead to situations where collisions become unavoidable. In such cases, it is essential for the robotic system to proactively mitigate potential harm through intelligent control strategies. This paper presents a hierarchical control framework based on Control Barrier Functions (CBFs) designed to ensure safe and adaptive operation of autonomous robotic manipulators during close-proximity human-robot interaction. The proposed method introduces a relaxation variable that enables real-time prioritization of safety constraints, allowing the robot to dynamically manage collision risks based on the criticality of different parts of the human body. A secondary constraint mechanism is incorporated to resolve infeasibility by increasing the priority of imminent threats. The framework is experimentally validated on a Franka Research 3 robot equipped with a ZED2i AI camera for real-time human pose and body detection. Experimental results confirm that the CBF-based controller, integrated with depth sensing, facilitates responsive and safe human-robot collaboration, while providing detailed risk analysis and maintaining robust performance in highly dynamic settings.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2505.05800.pdf' target='_blank'>https://arxiv.org/pdf/2505.05800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineet Bhat, Yu-Hsiang Lan, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05800">3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1$\%$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2505.05456.pdf' target='_blank'>https://arxiv.org/pdf/2505.05456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, Boqing Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05456">SITE: towards Spatial Intelligence Thorough Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2411.17662.pdf' target='_blank'>https://arxiv.org/pdf/2411.17662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raktim Gautam Goswami, Prashanth Krishnamurthy, Yann LeCun, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17662">RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2506.18385.pdf' target='_blank'>https://arxiv.org/pdf/2506.18385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, Tong He, Wenqi Shao, Kaipeng Zhang, Yi Wang, Botian Shi, Yanting Zhang, Jifeng Dai, Yu Qiao, Hongjie Zhang, Wenhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18385">InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent benchmarks and datasets have been proposed to improve spatial reasoning in vision-language models (VLMs), yet existing open resources remain limited in scale, visual diversity, and instruction expressiveness. In this work, we introduce InternSpatial, the largest open-source dataset for spatial reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation benchmark designed to assess spatial understanding under diverse instruction formats. InternSpatial comprises 12 million QA pairs spanning both single-view and multi-view settings, drawn from diverse visual environments and supporting 19 instruction formats that reflect varied query styles. For evaluation, we propose InternSpatial-Bench for single-view tasks and expand multi-view reasoning by introducing a novel rotation angle prediction task that has not been explored in prior work. Experimental results show that models trained on InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining strong performance on general-purpose benchmarks. We hope these resources will support the development of spatially capable VLMs in practical applications such as robotics and embodied AI.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2503.19757.pdf' target='_blank'>https://arxiv.org/pdf/2503.19757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19757">Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2410.15959.pdf' target='_blank'>https://arxiv.org/pdf/2410.15959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15959">Diffusion Transformer Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large vision-language-action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict individual discretized or continuous action by a small action head, which limits the ability in handling diverse action spaces. In contrast, we model the continuous action sequence with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head for action embedding. By leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. Extensive experiments demonstrate the effectiveness and generalization of Diffusion Transformer Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world Franka arm, achieving consistent better performance on Real-to-Sim benchmark SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo. Specifically, without bells and whistles, the proposed approach achieves state-of-the-art performance with only a single third-view camera stream in the Calvin task ABC->D, improving the average number of tasks completed in a row of 5 to 3.6, and the pretraining stage significantly facilitates the success sequence length on the Calvin by over 1.2.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2404.16006.pdf' target='_blank'>https://arxiv.org/pdf/2404.16006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16006">MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2402.16117.pdf' target='_blank'>https://arxiv.org/pdf/2402.16117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, Peize Sun, Haibao Yu, Chao Yang, Wenqi Shao, Wenhai Wang, Jifeng Dai, Yu Qiao, Mingyu Ding, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16117">RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2402.14623.pdf' target='_blank'>https://arxiv.org/pdf/2402.14623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, Yu Qiao, Huazhe Xu, Mingyu Ding, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14623">RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo. We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers. Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions. Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2305.15021.pdf' target='_blank'>https://arxiv.org/pdf/2305.15021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15021">EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering. Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2502.01376.pdf' target='_blank'>https://arxiv.org/pdf/2502.01376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Chen, Lipeng Chen, Xiangchi Chen, Haojian Lu, Yu Zheng, Jun Wu, Yue Wang, Zhengyou Zhang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01376">Compliance while resisting: a shear-thickening fluid controller for physical human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction (pHRI) is widely needed in many fields, such as industrial manipulation, home services, and medical rehabilitation, and puts higher demands on the safety of robots. Due to the uncertainty of the working environment, the pHRI may receive unexpected impact interference, which affects the safety and smoothness of the task execution. The commonly used linear admittance control (L-AC) can cope well with high-frequency small-amplitude noise, but for medium-frequency high-intensity impact, the effect is not as good. Inspired by the solid-liquid phase change nature of shear-thickening fluid, we propose a Shear-thickening Fluid Control (SFC) that can achieve both an easy human-robot collaboration and resistance to impact interference. The SFC's stability, passivity, and phase trajectory are analyzed in detail, the frequency and time domain properties are quantified, and parameter constraints in discrete control and coupled stability conditions are provided. We conducted simulations to compare the frequency and time domain characteristics of L-AC, nonlinear admittance controller (N-AC), and SFC, and validated their dynamic properties. In real-world experiments, we compared the performance of L-AC, N-AC, and SFC in both fixed and mobile manipulators. L-AC exhibits weak resistance to impact. N-AC can resist moderate impacts but not high-intensity ones, and may exhibit self-excited oscillations. In contrast, SFC demonstrated superior impact resistance and maintained stable collaboration, enhancing comfort in cooperative water delivery tasks. Additionally, a case study was conducted in a factory setting, further affirming the SFC's capability in facilitating human-robot collaborative manipulation and underscoring its potential in industrial applications.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2406.14367.pdf' target='_blank'>https://arxiv.org/pdf/2406.14367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Ma, Jing Zhang, Qiong Cao, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14367">PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation aims to accurately identify anatomical keypoints in humans and animals using monocular images, which is crucial for various applications such as human-machine interaction, embodied AI, and autonomous driving. While current models show promising results, they are typically trained and tested on clean data, potentially overlooking the corruption during real-world deployment and thus posing safety risks in practical scenarios. To address this issue, we introduce PoseBench, a comprehensive benchmark designed to evaluate the robustness of pose estimation models against real-world corruption. We evaluated 60 representative models, including top-down, bottom-up, heatmap-based, regression-based, and classification-based methods, across three datasets for human and animal pose estimation. Our evaluation involves 10 types of corruption in four categories: 1) blur and noise, 2) compression and color loss, 3) severe lighting, and 4) masks. Our findings reveal that state-of-the-art models are vulnerable to common real-world corruptions and exhibit distinct behaviors when tackling human and animal pose estimation tasks. To improve model robustness, we delve into various design considerations, including input resolution, pre-training datasets, backbone capacity, post-processing, and data augmentations. We hope that our benchmark will serve as a foundation for advancing research in robust pose estimation. The benchmark and source code will be released at https://xymsh.github.io/PoseBench
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2504.21853.pdf' target='_blank'>https://arxiv.org/pdf/2504.21853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21853">A Survey of Interactive Generative Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2505.21432.pdf' target='_blank'>https://arxiv.org/pdf/2505.21432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21432">Hume: Introducing System-2 Thinking in Visual-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2505.13888.pdf' target='_blank'>https://arxiv.org/pdf/2505.13888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Zhang, Shihan Wu, Xu Luo, Hao Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13888">InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging pretrained Vision-Language Models (VLMs) to map language instruction and visual observations to raw low-level actions, Vision-Language-Action models (VLAs) hold great promise for achieving general-purpose robotic systems. Despite their advancements, existing VLAs tend to spuriously correlate task-irrelevant visual features with actions, limiting their generalization capacity beyond the training data. To tackle this challenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet effective approach that mitigates the adverse effects of spurious correlations by boosting the spatial reasoning ability of VLAs. Specifically, InSpire redirects the VLA's attention to task-relevant factors by prepending the question "In which direction is the [object] relative to the robot?" to the language instruction and aligning the answer "right/left/up/down/front/back/grasped" and predicted actions with the ground-truth. Notably, InSpire can be used as a plugin to enhance existing autoregressive VLAs, requiring no extra training data or interaction with other large models. Extensive experimental results in both simulation and real-world environments demonstrate the effectiveness and flexibility of our approach. Our code, pretrained models and demos are publicly available at: https://Koorye.github.io/proj/Inspire.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2503.11081.pdf' target='_blank'>https://arxiv.org/pdf/2503.11081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11081">MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mobile manipulation, navigation and manipulation are often treated as separate problems, resulting in a significant gap between merely approaching an object and engaging with it effectively. Many navigation approaches primarily define success by proximity to the target, often overlooking the necessity for optimal positioning that facilitates subsequent manipulation. To address this, we introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples that provide training data for models to learn optimal final navigation positions for seamless transition to manipulation. Our dataset includes affordance-grounded floor labels collected from diverse kitchen environments, in which robotic mobile manipulators of different models attempt to grasp target objects amidst clutter. Using a fully automated pipeline, we simulate diverse real-world scenarios and generate affordance labels for optimal manipulation positions. Visual data are collected from RGB-D inputs captured by a first-person view camera mounted on the robotic arm, ensuring consistency in viewpoint during data collection. We also develop a lightweight baseline model, NavAff, for navigation affordance grounding that demonstrates promising performance on the MoMa-Kitchen benchmark. Our approach enables models to learn affordance-based final positioning that accommodates different arm types and platform heights, thereby paving the way for more robust and generalizable integration of navigation and manipulation in embodied AI. Project page: \href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2502.18041.pdf' target='_blank'>https://arxiv.org/pdf/2502.18041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18041">OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) aims to guide agents by leveraging language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising various rendering engines, a versatile toolchain, and a large-scale benchmark for aerial VLN. Firstly, we integrate diverse rendering engines and advanced techniques for environment simulation, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of our environments. Secondly, we develop a highly automated toolchain for aerial VLN data collection, streamlining point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Thirdly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. Moreover, we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key observations during flight. For benchmarking, extensive experiments and analyses are conducted, evaluating several recent VLN methods and showcasing the superiority of our OpenFly platform and agent. The toolchain, dataset, and codes will be open-sourced.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2410.18195.pdf' target='_blank'>https://arxiv.org/pdf/2410.18195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Barsellotti, Roberto Bigazzi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18195">Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2405.19586.pdf' target='_blank'>https://arxiv.org/pdf/2405.19586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Zhang, Chenjia Bai, Haoran He, Wenke Xia, Zhigang Wang, Bin Zhao, Xiu Li, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19586">SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction. Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot's end-effector. However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning. In this paper, we propose SAM-E, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning. Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios. To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency. Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2403.07076.pdf' target='_blank'>https://arxiv.org/pdf/2403.07076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Bigazzi, Lorenzo Baraldi, Shreyas Kousik, Rita Cucchiara, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07076">Mapping High-level Semantic Regions in Indoor Environments without Object Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene graph generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2301.07150.pdf' target='_blank'>https://arxiv.org/pdf/2301.07150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Bigazzi, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07150">Embodied Agents for Efficient Exploration and Smart Scene Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of embodied agents that can communicate with humans in natural language has gained increasing interest over the last years, as it facilitates the diffusion of robotic platforms in human-populated environments. As a step towards this objective, in this work, we tackle a setting for visual navigation in which an autonomous agent needs to explore and map an unseen indoor environment while portraying interesting scenes with natural language descriptions. To this end, we propose and evaluate an approach that combines recent advances in visual robotic exploration and image captioning on images generated through agent-environment interaction. Our approach can generate smart scene descriptions that maximize semantic knowledge of the environment and avoid repetitions. Further, such descriptions offer user-understandable insights into the robot's representation of the environment by highlighting the prominent objects and the correlation between them as encountered during the exploration. To quantitatively assess the performance of the proposed approach, we also devise a specific score that takes into account both exploration and description skills. The experiments carried out on both photorealistic simulated environments and real-world ones demonstrate that our approach can effectively describe the robot's point of view during exploration, improving the human-friendly interpretability of its observations.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2204.09069.pdf' target='_blank'>https://arxiv.org/pdf/2204.09069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Bigazzi, Federico Landi, Silvia Cascianelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.09069">Embodied Navigation at the Art Gallery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents, trained to explore and navigate indoor photorealistic environments, have achieved impressive results on standard datasets and benchmarks. So far, experiments and evaluations have involved domestic and working scenes like offices, flats, and houses. In this paper, we build and release a new 3D space with unique characteristics: the one of a complete art museum. We name this environment ArtGallery3D (AG3D). Compared with existing 3D scenes, the collected space is ampler, richer in visual features, and provides very sparse occupancy information. This feature is challenging for occupancy-based agents which are usually trained in crowded domestic environments with plenty of occupancy information. Additionally, we annotate the coordinates of the main points of interest inside the museum, such as paintings, statues, and other items. Thanks to this manual process, we deliver a new benchmark for PointGoal navigation inside this new space. Trajectories in this dataset are far more complex and lengthy than existing ground-truth paths for navigation in Gibson and Matterport3D. We carry on extensive experimental evaluation using our new space for evaluation and prove that existing methods hardly adapt to this scenario. As such, we believe that the availability of this 3D model will foster future research and help improve existing solutions.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2204.08502.pdf' target='_blank'>https://arxiv.org/pdf/2204.08502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Landi, Roberto Bigazzi, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.08502">Spot the Difference: A Novel Task for Embodied Agents in Changing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is a recent research area that aims at creating intelligent agents that can move and operate inside an environment. Existing approaches in this field demand the agents to act in completely new and unexplored scenes. However, this setting is far from realistic use cases that instead require executing multiple tasks in the same environment. Even if the environment changes over time, the agent could still count on its global knowledge about the scene while trying to adapt its internal representation to the current state of the environment. To make a step towards this setting, we propose Spot the Difference: a novel task for Embodied AI where the agent has access to an outdated map of the environment and needs to recover the correct layout in a fixed time budget. To this end, we collect a new dataset of occupancy maps starting from existing datasets of 3D spaces and generating a number of possible layouts for a single environment. This dataset can be employed in the popular Habitat simulator and is fully compliant with existing methods that employ reconstructed occupancy maps during navigation. Furthermore, we propose an exploration policy that can take advantage of previous knowledge of the environment and identify changes in the scene faster and more effectively than existing agents. Experimental results show that the proposed architecture outperforms existing state-of-the-art models for exploration on this new setting.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2007.07268.pdf' target='_blank'>https://arxiv.org/pdf/2007.07268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Bigazzi, Federico Landi, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.07268">Explore and Explain: Self-supervised Navigation and Recounting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI has been recently gaining attention as it aims to foster the development of autonomous and intelligent agents. In this paper, we devise a novel embodied setting in which an agent needs to explore a previously unknown environment while recounting what it sees during the path. In this context, the agent needs to navigate the environment driven by an exploration goal, select proper moments for description, and output natural language descriptions of relevant objects and scenes. Our model integrates a novel self-supervised exploration module with penalty, and a fully-attentive captioning model for explanation. Also, we investigate different policies for selecting proper moments for explanation, driven by information coming from both the environment and the navigation. Experiments are conducted on photorealistic environments from the Matterport3D dataset and investigate the navigation and explanation capabilities of the agent as well as the role of their interactions.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2508.02106.pdf' target='_blank'>https://arxiv.org/pdf/2508.02106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02106">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2410.15461.pdf' target='_blank'>https://arxiv.org/pdf/2410.15461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15461">EVA: An Embodied World Model for Future Video Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have made significant progress in simulating future states, showcasing their potential as world simulators in embodied scenarios. However, existing models often lack robust understanding, limiting their ability to perform multi-step predictions or handle Out-of-Distribution (OOD) scenarios. To address this challenge, we propose the Reflection of Generation (RoG), a set of intermediate reasoning strategies designed to enhance video prediction. It leverages the complementary strengths of pre-trained vision-language and video generation models, enabling them to function as a world model in embodied scenarios. To support RoG, we introduce Embodied Video Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates embodied world models across diverse tasks and scenarios, utilizing both in-domain and OOD datasets. Building on this foundation, we devise a world model, Embodied Video Anticipator (EVA), that follows a multistage training paradigm to generate high-fidelity video frames and apply an autoregressive strategy to enable adaptive generalization for longer video sequences. Extensive experiments demonstrate the efficacy of EVA in various downstream tasks like video generation and robotics, thereby paving the way for large-scale pre-trained models in real-world video prediction applications. The video demos are available at \hyperlink{https://sites.google.com/view/icml-eva}{https://sites.google.com/view/icml-eva}.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2507.12465.pdf' target='_blank'>https://arxiv.org/pdf/2507.12465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Cao, Zhaoxi Chen, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12465">PhysX-3D: Physical-Grounded 3D Asset Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \textbf{PhysX-3D}, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2509.20021.pdf' target='_blank'>https://arxiv.org/pdf/2509.20021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Yu-Gang Jiang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20021">Embodied AI: From LLMs to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2404.03622.pdf' target='_blank'>https://arxiv.org/pdf/2404.03622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03622">Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes at https://microsoft.github.io/visualization-of-thought
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2405.17418.pdf' target='_blank'>https://arxiv.org/pdf/2405.17418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxuan Li, Jiaming Liu, Guanqun Wang, Xiaoqi Li, Sixiang Chen, Liang Heng, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, Kaichen Zhou, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17418">A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, some studies have integrated Multimodal Large Language Models into robotic manipulation, constructing vision-language-action models (VLAs) to interpret multimodal information and predict SE(3) poses. While VLAs have shown promising progress, they may suffer from failures when faced with novel and complex tasks. To emulate human-like reasoning for more robust manipulation, we propose the self-corrected (SC-)VLA framework, which integrates fast system for directly predicting actions and slow system for reflecting on failed actions within a single VLA policy. For the fast system, we incorporate parameter-efficient fine-tuning to equip the model with pose prediction capabilities while preserving the inherent reasoning abilities of MLLMs. For the slow system, we propose a Chain-of-Thought training strategy for failure correction, designed to mimic human reflection after a manipulation failure. Specifically, our model learns to identify the causes of action failures, adaptively seek expert feedback, reflect on the current failure scenario, and iteratively generate corrective actions, step by step. Furthermore, a continuous policy learning method is designed based on successfully corrected samples, enhancing the fast system's adaptability to the current configuration. We compare SC-VLA with the previous SOTA VLA in both simulation and real-world tasks, demonstrating an efficient correction process and improved manipulation accuracy on both seen and unseen tasks.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2505.21200.pdf' target='_blank'>https://arxiv.org/pdf/2505.21200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Tan, Yaoxin Yang, Peng Ye, Jialin Zheng, Bizhe Bai, Xinyi Wang, Jia Hao, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21200">Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a powerful paradigm for general-purpose robot control through natural language instructions. However, their high inference cost-stemming from large-scale token computation and autoregressive decoding-poses significant challenges for real-time deployment and edge applications. While prior work has primarily focused on architectural optimization, we take a different perspective by identifying a dual form of redundancy in VLA models: (i) high similarity across consecutive action steps, and (ii) substantial redundancy in visual tokens. Motivated by these observations, we propose FlashVLA, the first training-free and plug-and-play acceleration framework that enables action reuse in VLA models. FlashVLA improves inference efficiency through a token-aware action reuse mechanism that avoids redundant decoding across stable action steps, and an information-guided visual token selection strategy that prunes low-contribution tokens. Extensive experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7% and latency by 36.0%, with only a 0.7% drop in task success rate. These results demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency VLA inference without retraining.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2412.09634.pdf' target='_blank'>https://arxiv.org/pdf/2412.09634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesse Atuhurra, Hidetaka Kamigaito, Hiroki Ouchi, Hiroyuki Shindo, Taro Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09634">NERsocial: Efficient Named Entity Recognition Dataset Construction for Human-Robot Interaction Utilizing RapidNER</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting named entity recognition (NER) methods to new domains poses significant challenges. We introduce RapidNER, a framework designed for the rapid deployment of NER systems through efficient dataset construction. RapidNER operates through three key steps: (1) extracting domain-specific sub-graphs and triples from a general knowledge graph, (2) collecting and leveraging texts from various sources to build the NERsocial dataset, which focuses on entities typical in human-robot interaction, and (3) implementing an annotation scheme using Elasticsearch (ES) to enhance efficiency. NERsocial, validated by human annotators, includes six entity types, 153K tokens, and 99.4K sentences, demonstrating RapidNER's capability to expedite dataset creation.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2508.21112.pdf' target='_blank'>https://arxiv.org/pdf/2508.21112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21112">EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2508.15769.pdf' target='_blank'>https://arxiv.org/pdf/2508.15769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15769">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2312.10807.pdf' target='_blank'>https://arxiv.org/pdf/2312.10807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongkuan Zhou, Xiangtong Yao, Oier Mees, Yuan Meng, Ted Xiao, Yonatan Bisk, Jean Oh, Edward Johns, Mohit Shridhar, Dhruv Shah, Jesse Thomason, Kai Huang, Joyce Chai, Zhenshan Bing, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10807">Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-conditioned robot manipulation is an emerging field aimed at enabling seamless communication and cooperation between humans and robotic agents by teaching robots to comprehend and execute instructions conveyed in natural language. This interdisciplinary area integrates scene understanding, language processing, and policy learning to bridge the gap between human instructions and robotic actions. In this comprehensive survey, we systematically explore recent advancements in language-conditioned robotic manipulation. We categorize existing methods into language-conditioned reward shaping, language-conditioned policy learning, neuro-symbolic artificial intelligence, and the utilization of foundational models (FMs) such as large language models (LLMs) and vision-language models (VLMs). Specifically, we analyze state-of-the-art techniques concerning semantic information extraction, environment and evaluation, auxiliary tasks, and task representation strategies. By conducting a comparative analysis, we highlight the strengths and limitations of current approaches in bridging language instructions with robot actions. Finally, we discuss open challenges and future research directions, focusing on potentially enhancing generalization capabilities and addressing safety issues in language-conditioned robot manipulators.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2508.17481.pdf' target='_blank'>https://arxiv.org/pdf/2508.17481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priyanka Prakash Surve, Asaf Shabtai, Yuval Elovici
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17481">SoK: Cybersecurity Assessment of Humanoid Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoids are progressing toward practical deployment across healthcare, industrial, defense, and service sectors. While typically considered cyber-physical systems (CPSs), their dependence on traditional networked software stacks (e.g., Linux operating systems), robot operating system (ROS) middleware, and over-the-air update channels, creates a distinct security profile that exposes them to vulnerabilities conventional CPS models do not fully address. Prior studies have mainly examined specific threats, such as LiDAR spoofing or adversarial machine learning (AML). This narrow focus overlooks how an attack targeting one component can cascade harm throughout the robot's interconnected systems. We address this gap through a systematization of knowledge (SoK) that takes a comprehensive approach, consolidating fragmented research from robotics, CPS, and network security domains. We introduce a seven-layer security model for humanoid robots, organizing 39 known attacks and 35 defenses across the humanoid ecosystem-from hardware to human-robot interaction. Building on this security model, we develop a quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated through Monte Carlo analysis. We demonstrate our method by evaluating three real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed varying security maturity levels, with scores ranging from 39.9% to 79.5% across the platforms. This work introduces a structured, evidence-based assessment method that enables systematic security evaluation, supports cross-platform benchmarking, and guides prioritization of security investments in humanoid robotics.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2505.20129.pdf' target='_blank'>https://arxiv.org/pdf/2505.20129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20129">Agentic 3D Scene Generation with Spatially Contextualized VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advances in multimodal content generation enabled by vision-language models (VLMs), their ability to reason about and generate structured 3D scenes remains largely underexplored. This limitation constrains their utility in spatially grounded tasks such as embodied AI, immersive simulations, and interactive 3D applications. We introduce a new paradigm that enables VLMs to generate, understand, and edit complex 3D environments by injecting a continually evolving spatial context. Constructed from multimodal input, this context consists of three components: a scene portrait that provides a high-level semantic blueprint, a semantically labeled point cloud capturing object-level geometry, and a scene hypergraph that encodes rich spatial relationships, including unary, binary, and higher-order constraints. Together, these components provide the VLM with a structured, geometry-aware working memory that integrates its inherent multimodal reasoning capabilities with structured 3D understanding for effective spatial reasoning. Building on this foundation, we develop an agentic 3D scene generation pipeline in which the VLM iteratively reads from and updates the spatial context. The pipeline features high-quality asset generation with geometric restoration, environment setup with automatic verification, and ergonomic adjustment guided by the scene hypergraph. Experiments show that our framework can handle diverse and challenging inputs, achieving a level of generalization not observed in prior work. Further results demonstrate that injecting spatial context enables VLMs to perform downstream tasks such as interactive scene editing and path planning, suggesting strong potential for spatially intelligent systems in computer graphics, 3D vision, and embodied applications. Project page: https://spatctxvlm.github.io/project_page/.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2503.13882.pdf' target='_blank'>https://arxiv.org/pdf/2503.13882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengsheng Guo, Linwei Zheng, Xinyang Chen, Xuefeng Bai, Kehai Chen, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13882">MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented Generation for Embodied AI Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While human cognition inherently retrieves information from diverse and specialized knowledge sources during decision-making processes, current Retrieval-Augmented Generation (RAG) systems typically operate through single-source knowledge retrieval, leading to a cognitive-algorithmic discrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG framework that implements a mixture of knowledge paths enhanced retrieval mechanism through functional partitioning of a large language model (LLM) corpus into distinct sections, enabling retrieval from multiple specialized knowledge paths. Applied to the generation of 3D simulated environments, our proposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into distinct sections and organizing them based on a hierarchical knowledge tree structure. Different from previous methods that only use manual evaluation, we pioneered the introduction of automated evaluation methods for 3D scenes. Both automatic and human evaluations in our experiments demonstrate that MoK-RAG3D can assist Embodied AI agents in generating diverse scenes.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2309.12188.pdf' target='_blank'>https://arxiv.org/pdf/2309.12188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12188">SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object rearrangement is pivotal in robotic-environment interactions, representing a significant capability in embodied AI. In this paper, we present SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme with a scene graph as the scene representation. Unlike previous methods that rely on either known goal priors or zero-shot large models, SG-Bot exemplifies lightweight, real-time, and user-controllable characteristics, seamlessly blending the consideration of commonsense knowledge with automatic generation capabilities. SG-Bot employs a three-fold procedure--observation, imagination, and execution--to adeptly address the task. Initially, objects are discerned and extracted from a cluttered scene during the observation. These objects are first coarsely organized and depicted within a scene graph, guided by either commonsense or user-defined criteria. Then, this scene graph subsequently informs a generative model, which forms a fine-grained goal scene considering the shape information from the initial scene and object semantics. Finally, for execution, the initial and envisioned goal scenes are matched to formulate robotic action policies. Experimental results demonstrate that SG-Bot outperforms competitors by a large margin.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2502.13451.pdf' target='_blank'>https://arxiv.org/pdf/2502.13451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13451">MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2409.13174.pdf' target='_blank'>https://arxiv.org/pdf/2409.13174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Cheng, Erjia Xiao, Yichi Wang, Chengyuan Yu, Mengshu Sun, Qiang Zhang, Yijie Guo, Kaidi Xu, Jize Zhang, Chao Shen, Philip Torr, Jindong Gu, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13174">Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, driven by advancements in Multimodal Large Language Models (MLLMs), Vision Language Action Models (VLAMs) are being proposed to achieve better performance in open-vocabulary scenarios for robotic manipulation tasks. Since manipulation tasks involve direct interaction with the physical world, ensuring robustness and safety during the execution of this task is always a very critical issue. In this paper, by synthesizing current safety research on MLLMs and the specific application scenarios of the manipulation task in the physical world, we comprehensively evaluate VLAMs in the face of potential physical threats. Specifically, we propose the Physical Vulnerability Evaluating Pipeline (PVEP) that can incorporate as many visual modal physical threats as possible for evaluating the physical robustness of VLAMs. The physical threats in PVEP specifically include Out-of-Distribution, Typography-based Visual Prompt, and Adversarial Patch Attacks. By comparing the performance fluctuations of VLAMs before and after being attacked, we provide generalizable \textbf{\textit{Analyses}} of how VLAMs respond to different physical threats.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2403.15223.pdf' target='_blank'>https://arxiv.org/pdf/2403.15223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhang, Qiang Zhang, Hao Wang, Erjia Xiao, Zixuan Jiang, Honglei Chen, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15223">TriHelper: Zero-Shot Object Navigation with Dynamic Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating toward specific objects in unknown environments without additional training, known as Zero-Shot object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in Zero-Shot object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent's navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2308.10306.pdf' target='_blank'>https://arxiv.org/pdf/2308.10306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyu Chen, Wenguan Wang, Si Liu, Hongsheng Li, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10306">Omnidirectional Information Gathering for Knowledge Transfer-based Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual navigation is an audio-targeted wayfinding task where a robot agent is entailed to travel a never-before-seen 3D environment towards the sounding source. In this article, we present ORAN, an omnidirectional audio-visual navigator based on cross-task navigation skill transfer. In particular, ORAN sharpens its two basic abilities for a such challenging task, namely wayfinding and audio-visual information gathering. First, ORAN is trained with a confidence-aware cross-task policy distillation (CCPD) strategy. CCPD transfers the fundamental, point-to-point wayfinding skill that is well trained on the large-scale PointGoal task to ORAN, so as to help ORAN to better master audio-visual navigation with far fewer training samples. To improve the efficiency of knowledge transfer and address the domain gap, CCPD is made to be adaptive to the decision confidence of the teacher policy. Second, ORAN is equipped with an omnidirectional information gathering (OIG) mechanism, i.e., gleaning visual-acoustic observations from different directions before decision-making. As a result, ORAN yields more robust navigation behaviour. Taking CCPD and OIG together, ORAN significantly outperforms previous competitors. After the model ensemble, we got 1st in Soundspaces Challenge 2022, improving SPL and SR by 53% and 35% relatively.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2307.15644.pdf' target='_blank'>https://arxiv.org/pdf/2307.15644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15644">Scaling Data Generation in Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The long-lasting generalization gap between navigating in seen and unseen environments is also reduced to less than 1% (versus 8% in the previous best method). Moreover, our paradigm also facilitates different models to achieve new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous environments.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2505.06575.pdf' target='_blank'>https://arxiv.org/pdf/2505.06575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06575">GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2405.13659.pdf' target='_blank'>https://arxiv.org/pdf/2405.13659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Yang, Wei Zhai, Chengfeng Wang, Chengjun Yu, Yang Cao, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13659">EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI. For the egocentric HOI, in addition to perceiving semantics e.g., ''what'' interaction is occurring, capturing ''where'' the interaction specifically manifests in 3D space is also crucial, which links the perception and operation. Existing methods primarily leverage observations of HOI to capture interaction regions from an exocentric view. However, incomplete observations of interacting parties in the egocentric view introduce ambiguity between visual observations and interaction contents, impairing their efficacy. From the egocentric view, humans integrate the visual cortex, cerebellum, and brain to internalize their intentions and interaction concepts of objects, allowing for the pre-formulation of interactions and making behaviors even when interaction regions are out of sight. In light of this, we propose harmonizing the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, jointly inferring 3D human contact and object affordance from egocentric videos. To achieve this, we present EgoChoir, which links object structures with interaction contexts inherent in appearance and head motion to reveal object affordance, further utilizing it to model human contact. Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios. Moreover, 3D contact and affordance are annotated for egocentric videos collected from Ego-Exo4D and GIMO to support the task. Extensive experiments on them demonstrate the effectiveness and superiority of EgoChoir.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2403.09194.pdf' target='_blank'>https://arxiv.org/pdf/2403.09194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongchen Luo, Kai Zhu, Wei Zhai, Yang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09194">Intention-driven Ego-to-Exo Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI. Benefiting from advancements in diffusion model techniques, notable progress has been achieved in video generation. However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views. To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion. Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching. Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory. Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image. Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the diffusion model, ultimately warping them into the corresponding exocentric video. We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2312.08963.pdf' target='_blank'>https://arxiv.org/pdf/2312.08963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08963">LEMON: Learning 3D Human-Object Interaction Relation from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning 3D human-object interaction relation is pivotal to embodied AI and interaction modeling. Most existing methods approach the goal by learning to predict isolated interaction elements, e.g., human contact, object affordance, and human-object spatial relation, primarily from the perspective of either the human or the object. Which underexploit certain correlations between the interaction counterparts (human and object), and struggle to address the uncertainty in interactions. Actually, objects' functionalities potentially affect humans' interaction intentions, which reveals what the interaction is. Meanwhile, the interacting humans and objects exhibit matching geometric structures, which presents how to interact. In light of this, we propose harnessing these inherent correlations between interaction counterparts to mitigate the uncertainty and jointly anticipate the above interaction elements in 3D space. To achieve this, we present LEMON (LEarning 3D huMan-Object iNteraction relation), a unified model that mines interaction intentions of the counterparts and employs curvatures to guide the extraction of geometric correlations, combining them to anticipate the interaction elements. Besides, the 3D Interaction Relation dataset (3DIR) is collected to serve as the test bed for training and evaluation. Extensive experiments demonstrate the superiority of LEMON over methods estimating each element in isolation.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2301.12416.pdf' target='_blank'>https://arxiv.org/pdf/2301.12416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaomei Zhang, Xiangyu Zhu, Ming Tang, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12416">Deep Learning for Human Parsing: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human parsing is a key topic in image processing with many applications, such as surveillance analysis, human-robot interaction, person search, and clothing category classification, among many others. Recently, due to the success of deep learning in computer vision, there are a number of works aimed at developing human parsing algorithms using deep learning models. As methods have been proposed, a comprehensive survey of this topic is of great importance. In this survey, we provide an analysis of state-of-the-art human parsing methods, covering a broad spectrum of pioneering works for semantic human parsing. We introduce five insightful categories: (1) structure-driven architectures exploit the relationship of different human parts and the inherent hierarchical structure of a human body, (2) graph-based networks capture the global information to achieve an efficient and complete human body analysis, (3) context-aware networks explore useful contexts across all pixel to characterize a pixel of the corresponding class, (4) LSTM-based methods can combine short-distance and long-distance spatial dependencies to better exploit abundant local and global contexts, and (5) combined auxiliary information approaches use related tasks or supervision to improve network performance. We also discuss the advantages/disadvantages of the methods in each category and the relationships between methods in different categories, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2507.22424.pdf' target='_blank'>https://arxiv.org/pdf/2507.22424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songsheng Wang, Rucheng Yu, Zhihang Yuan, Chao Yu, Feng Gao, Yu Wang, Derek F. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22424">Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2505.21500.pdf' target='_blank'>https://arxiv.org/pdf/2505.21500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21500">ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2505.19789.pdf' target='_blank'>https://arxiv.org/pdf/2505.19789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19789">What Can RL Bring to VLA Generalization? An Empirical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2504.11218.pdf' target='_blank'>https://arxiv.org/pdf/2504.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11218">3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2503.11117.pdf' target='_blank'>https://arxiv.org/pdf/2503.11117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixuan Jiang, Yang Liu, Weixing Chen, Jingzhou Luo, Ziliang Chen, Ling Pan, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11117">Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) is a challenging task in embodied intelligence that requires agents to dynamically explore 3D environments, actively gather visual information, and perform multi-step reasoning to answer questions. However, current EQA approaches suffer from critical limitations in exploration efficiency, dataset design, and evaluation metrics. Moreover, existing datasets often introduce biases or prior knowledge, leading to disembodied reasoning, while frontier-based exploration strategies struggle in cluttered environments and fail to ensure fine-grained exploration of task-relevant areas. To address these challenges, we construct the EXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the largest dataset designed specifically to evaluate both exploration and reasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories and 2,044 question-trajectory pairs. To improve exploration efficiency, we propose Fine-EQA, a hybrid exploration model that integrates frontier-based and goal-oriented navigation to guide agents toward task-relevant regions more effectively. Additionally, we introduce a novel evaluation metric, Exploration-Answer Consistency (EAC), which ensures faithful assessment by measuring the alignment between answer grounding and exploration reliability. Extensive experimental comparisons with state-of-the-art EQA models demonstrate the effectiveness of our EXPRESS-Bench in advancing embodied exploration and question reasoning.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2409.13573.pdf' target='_blank'>https://arxiv.org/pdf/2409.13573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Chao Yu, Yu Wang, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13573">Human-Robot Cooperative Distribution Coupling for Hamiltonian-Constrained Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating in human-filled public spaces is a critical challenge for deploying autonomous robots in real-world environments. This paper introduces NaviDIFF, a novel Hamiltonian-constrained socially-aware navigation framework designed to address the complexities of human-robot interaction and socially-aware path planning. NaviDIFF integrates a port-Hamiltonian framework to model dynamic physical interactions and a diffusion model to manage uncertainty in human-robot cooperation. The framework leverages a spatial-temporal transformer to capture social and temporal dependencies, enabling more accurate spatial-temporal environmental dynamics understanding and port-Hamiltonian physical interactive process construction. Additionally, reinforcement learning from human feedback is employed to fine-tune robot policies, ensuring adaptation to human preferences and social norms. Extensive experiments demonstrate that NaviDIFF outperforms state-of-the-art methods in social navigation tasks, offering improved stability, efficiency, and adaptability.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2502.19417.pdf' target='_blank'>https://arxiv.org/pdf/2502.19417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19417">Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don't like that one") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and incorporate situated feedback during task execution ("that's not trash"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping. Videos are available at https://www.pi.website/research/hirobot
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2501.09747.pdf' target='_blank'>https://arxiv.org/pdf/2501.09747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09747">FAST: Efficient Action Tokenization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2407.08693.pdf' target='_blank'>https://arxiv.org/pdf/2407.08693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MichaÅ Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08693">Robotic Control via Embodied Chain-of-Thought Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key limitation of learned robot control policies is their inability to generalize outside their training data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of "chain-of-thought" (CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28% across challenging generalization tasks, without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior using natural language.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2401.12963.pdf' target='_blank'>https://arxiv.org/pdf/2401.12963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12963">AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2307.15818.pdf' target='_blank'>https://arxiv.org/pdf/2307.15818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15818">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2412.09585.pdf' target='_blank'>https://arxiv.org/pdf/2412.09585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, Jianwei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09585">Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent times, the standard practice for developing MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. This approach often causes models to lean towards language comprehension and undermine the rich visual perception signals present in the data, which are critical for tasks involving spatial reasoning in the domain of embodied AI and robotics. Is it possible to optimize both at the same time? In this work, we propose VisPer-LM, the first approach that infuses visual perception knowledge from expert vision encoders into the LLM's (of an MLLM) hidden representations. We start by investigating MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Given this insight, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next (text) token prediction. Moreover, through extensive probing, we observe improved visual representation quality due to embedding optimization, underscoring the effectiveness of our probing setup. We demonstrate that our VisPer-LM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. In particular, VisPer-LM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2410.11758.pdf' target='_blank'>https://arxiv.org/pdf/2410.11758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11758">Latent Action Pretraining from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2410.09054.pdf' target='_blank'>https://arxiv.org/pdf/2410.09054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viviane Potocnik, Alfio Di Mauro, Lorenzo Lamberti, Victor Kartsch, Moritz Scherer, Francesco Conti, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09054">Circuits and Systems for Embodied AI: Exploring uJ Multi-Modal Perception for Nano-UAVs on the Kraken Shield</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied artificial intelligence (AI) requires pushing complex multi-modal models to the extreme edge for time-constrained tasks such as autonomous navigation of robots and vehicles. On small form-factor devices, e.g., nano-sized unmanned aerial vehicles (UAVs), such challenges are exacerbated by stringent constraints on energy efficiency and weight. In this paper, we explore embodied multi-modal AI-based perception for Nano-UAVs with the Kraken shield, a 7g multi-sensor (frame-based and event-based imagers) board based on Kraken, a 22 nm SoC featuring multiple acceleration engines for multi-modal event and frame-based inference based on spiking (SNN) and ternary (TNN) neural networks, respectively. Kraken can execute SNN real-time inference for depth estimation at 1.02k inf/s, 18 Î¼J/inf, TNN real-time inference for object classification at 10k inf/s, 6 Î¼J/inf, and real-time inference for obstacle avoidance at 221 frame/s, 750 Î¼J/inf.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2408.04413.pdf' target='_blank'>https://arxiv.org/pdf/2408.04413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moritz Scherer, Luka Macan, Victor Jung, Philip Wiese, Luca Bompani, Alessio Burrello, Francesco Conti, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04413">Deeploy: Enabling Energy-Efficient Deployment of Small Language Models On Heterogeneous Microcontrollers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of Embodied Foundation Models (EFMs), most notably Small Language Models (SLMs), adapting Transformers for edge applications has become a very active field of research. However, achieving end-to-end deployment of SLMs on microcontroller (MCU)-class chips without high-bandwidth off-chip main memory access is still an open challenge. In this paper, we demonstrate high-efficiency end-to-end SLM deployment on a multicore RISC-V (RV32) MCU augmented with ML instruction extensions and a hardware neural processing unit (NPU). To automate the exploration of the constrained, multi-dimensional memory vs. computation tradeoffs involved in aggressive SLM deployment on heterogeneous (multicore+NPU) resources, we introduce Deeploy, a novel Deep Neural Network (DNN) compiler, which generates highly-optimized C code requiring minimal runtime support. We demonstrate that Deeploy generates end-to-end code for executing SLMs, fully exploiting the RV32 cores' instruction extensions and the NPU: We achieve leading-edge energy and throughput of \SI{490}{\micro\joule \per Token}, at \SI{340}{Token \per \second} for an SLM trained on the TinyStories dataset, running for the first time on an MCU-class device without external memory.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2507.09876.pdf' target='_blank'>https://arxiv.org/pdf/2507.09876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongheng Zhang, Xu Liu, Ruihan Tao, Qiguang Chen, Hao Fei, Wanxiang Che, Libo Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09876">ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2505.02152.pdf' target='_blank'>https://arxiv.org/pdf/2505.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02152">Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great promise for generalist robotic manipulation in the physical world. However, existing models are restricted to robot observations and text-only instructions, lacking the flexibility of interleaved multimodal instructions enabled by recent advances in foundation models in the digital world. In this paper, we present Interleave-VLA, the first framework capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world. It offers a flexible, model-agnostic paradigm that extends state-of-the-art VLA models with minimal modifications and strong zero-shot generalization. A key challenge in realizing Interleave-VLA is the absence of large-scale interleaved embodied datasets. To bridge this gap, we develop an automatic pipeline that converts text-only instructions from real-world datasets in Open X-Embodiment into interleaved image-text instructions, resulting in the first large-scale real-world interleaved embodied dataset with 210k episodes. Through comprehensive evaluation on simulation benchmarks and real-robot experiments, we demonstrate that Interleave-VLA offers significant benefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x compared to state-of-the-art baselines, 2) supports flexible task interfaces, and 3) handles diverse user-provided image instructions in a zero-shot manner, such as hand-drawn sketches. We further analyze the factors behind Interleave-VLA's strong zero-shot performance, showing that the interleaved paradigm effectively leverages heterogeneous datasets and diverse instruction images, including those from the Internet, which demonstrates strong potential for scaling up. Our model and dataset will be open-sourced.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2501.09783.pdf' target='_blank'>https://arxiv.org/pdf/2501.09783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09783">GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GeoManip, a framework to enable generalist robots to leverage essential conditions derived from object and part relationships, as geometric constraints, for robot manipulation. For example, cutting the carrot requires adhering to a geometric constraint: the blade of the knife should be perpendicular to the carrot's direction. By interpreting these constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, enabling greater generalizability across diverse even unseen tasks, objects, and scenarios. Unlike vision-language-action models that require extensive training, operates training-free by utilizing large foundational models: a constraint generation module that predicts stage-specific geometric constraints and a geometry parser that identifies object parts involved in these constraints. A solver then optimizes trajectories to satisfy inferred constraints from task descriptions and the scene. Furthermore, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip's state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2311.17135.pdf' target='_blank'>https://arxiv.org/pdf/2311.17135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17135">TLControl: Trajectory and Language Control for Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable human motion synthesis is essential for applications in AR/VR, gaming and embodied AI. Existing methods often focus solely on either language or full trajectory control, lacking precision in synthesizing motions aligned with user-specified trajectories, especially for multi-joint control. To address these issues, we present TLControl, a novel method for realistic human motion synthesis, incorporating both low-level Trajectory and high-level Language semantics controls, through the integration of neural-based and optimization-based techniques. Specifically, we begin with training a VQ-VAE for a compact and well-structured latent motion space organized by body parts. We then propose a Masked Trajectories Transformer (MTT) for predicting a motion distribution conditioned on language and trajectory. Once trained, we use MTT to sample initial motion predictions given user-specified partial trajectories and text descriptions as conditioning. Finally, we introduce a test-time optimization to refine these coarse predictions for precise trajectory control, which offers flexibility by allowing users to specify various optimization goals and ensures high runtime efficiency. Comprehensive experiments show that TLControl significantly outperforms the state-of-the-art in trajectory accuracy and time efficiency, making it practical for interactive and high-quality animation generation.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2509.11480.pdf' target='_blank'>https://arxiv.org/pdf/2509.11480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Taherin, Juyi Lin, Arash Akbari, Arman Akbari, Pu Zhao, Weiwei Chen, David Kaeli, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11480">Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2509.10813.pdf' target='_blank'>https://arxiv.org/pdf/2509.10813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weipeng Zhong, Peizhou Cao, Yichen Jin, Li Luo, Wenzhe Cai, Jingli Lin, Hanqing Wang, Zhaoyang Lyu, Tai Wang, Bo Dai, Xudong Xu, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10813">InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2507.17520.pdf' target='_blank'>https://arxiv.org/pdf/2507.17520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17520">InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2506.19816.pdf' target='_blank'>https://arxiv.org/pdf/2506.19816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19816">CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2505.20640.pdf' target='_blank'>https://arxiv.org/pdf/2505.20640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20640">IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Embodied Question Answering (EQA) benchmarks primarily focus on household environments, often overlooking safety-critical aspects and reasoning processes pertinent to industrial settings. This drawback limits the evaluation of agent readiness for real-world industrial applications. To bridge this, we introduce IndustryEQA, the first benchmark dedicated to evaluating embodied agent capabilities within safety-critical warehouse scenarios. Built upon the NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets, dynamic human agents, and carefully designed hazardous situations inspired by real-world safety guidelines. The benchmark includes rich annotations covering six categories: equipment safety, human safety, object recognition, attribute recognition, temporal understanding, and spatial understanding. Besides, it also provides extra reasoning evaluation based on these categories. Specifically, it comprises 971 question-answer pairs generated from small warehouse and 373 pairs from large ones, incorporating scenarios with and without human. We further propose a comprehensive evaluation framework, including various baseline models, to assess their general perception and reasoning abilities in industrial environments. IndustryEQA aims to steer EQA research towards developing more robust, safety-aware, and practically applicable embodied agents for complex industrial environments. Benchmark and codes are available.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2505.11409.pdf' target='_blank'>https://arxiv.org/pdf/2505.11409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan VuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11409">Visual Planning: Let's Think Only with Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2503.03480.pdf' target='_blank'>https://arxiv.org/pdf/2503.03480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03480">SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, this exploration yields an 83.58% safety improvement compared to the current state-of-the-art method, while also maintaining task performance (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2407.09016.pdf' target='_blank'>https://arxiv.org/pdf/2407.09016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Tai Wang, Yilun Chen, Hanqing Wang, Jiangmiao Pang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09016">OVExp: Open Vocabulary Exploration for Object-Oriented Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-oriented embodied navigation aims to locate specific objects, defined by category or depicted in images. Existing methods often struggle to generalize to open vocabulary goals without extensive training data. While recent advances in Vision-Language Models (VLMs) offer a promising solution by extending object recognition beyond predefined categories, efficient goal-oriented exploration becomes more challenging in an open vocabulary setting. We introduce OVExp, a learning-based framework that integrates VLMs for Open-Vocabulary Exploration. OVExp constructs scene representations by encoding observations with VLMs and projecting them onto top-down maps for goal-conditioned exploration. Goals are encoded in the same VLM feature space, and a lightweight transformer-based decoder predicts target locations while maintaining versatile representation abilities. To address the impracticality of fusing dense pixel embeddings with full 3D scene reconstruction for training, we propose constructing maps using low-cost semantic categories and transforming them into CLIP's embedding space via the text encoder. The simple but effective design of OVExp significantly reduces computational costs and demonstrates strong generalization abilities to various navigation settings. Experiments on established benchmarks show OVExp outperforms previous zero-shot methods, can generalize to diverse scenes, and handle different goal modalities.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2310.08565.pdf' target='_blank'>https://arxiv.org/pdf/2310.08565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subash Neupane, Shaswata Mitra, Ivan A. Fernandez, Swayamjit Saha, Sudip Mittal, Jingdao Chen, Nisha Pillai, Shahram Rahimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08565">Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2509.11839.pdf' target='_blank'>https://arxiv.org/pdf/2509.11839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Liu, Pengxiang Ding, Qihang Zhou, Yuxuan Wu, Da Huang, Zimian Peng, Wei Xiao, Weinan Zhang, Lixin Yang, Cewu Lu, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11839">TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action models show potential to generalize across embodiments but struggle to quickly align with a new robot's action space when high-quality demonstrations are scarce, especially for bipedal humanoids. We present TrajBooster, a cross-embodiment framework that leverages abundant wheeled-humanoid data to boost bipedal VLA. Our key idea is to use end-effector trajectories as a morphology-agnostic interface. TrajBooster (i) extracts 6D dual-arm end-effector trajectories from real-world wheeled humanoids, (ii) retargets them in simulation to Unitree G1 with a whole-body controller trained via a heuristic-enhanced harmonized online DAgger to lift low-dimensional trajectory references into feasible high-dimensional whole-body actions, and (iii) forms heterogeneous triplets that couple source vision/language with target humanoid-compatible actions to post-pre-train a VLA, followed by only 10 minutes of teleoperation data collection on the target humanoid domain. Deployed on Unitree G1, our policy achieves beyond-tabletop household tasks, enabling squatting, cross-height manipulation, and coordinated whole-body motion with markedly improved robustness and generalization. Results show that TrajBooster allows existing wheeled-humanoid data to efficiently strengthen bipedal humanoid VLA performance, reducing reliance on costly same-embodiment data while enhancing action space understanding and zero-shot skill transfer capabilities. For more details, For more details, please refer to our \href{https://jiachengliu3.github.io/TrajBooster/}.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2507.06719.pdf' target='_blank'>https://arxiv.org/pdf/2507.06719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyang Liu, Sixiao Zheng, Siyu Chen, Cairong Zhao, Longfei Liang, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06719">A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2506.15757.pdf' target='_blank'>https://arxiv.org/pdf/2506.15757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Tong Yu, Junda Wu, Yao Liu, Julian McAuley, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15757">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2506.05287.pdf' target='_blank'>https://arxiv.org/pdf/2506.05287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqian Yuan, Ronghao Dang, Long Li, Wentong Li, Dian Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05287">EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2505.11214.pdf' target='_blank'>https://arxiv.org/pdf/2505.11214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11214">Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2503.18016.pdf' target='_blank'>https://arxiv.org/pdf/2503.18016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Lutao Jiang, Haiwei Xue, Bin Ren, Danda Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18016">Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) has emerged as a pivotal technique in artificial intelligence (AI), particularly in enhancing the capabilities of large language models (LLMs) by enabling access to external, reliable, and up-to-date knowledge sources. In the context of AI-Generated Content (AIGC), RAG has proven invaluable by augmenting model outputs with supplementary, relevant information, thus improving their quality. Recently, the potential of RAG has extended beyond natural language processing, with emerging methods integrating retrieval-augmented strategies into the computer vision (CV) domain. These approaches aim to address the limitations of relying solely on internal model knowledge by incorporating authoritative external knowledge bases, thereby improving both the understanding and generation capabilities of vision models. This survey provides a comprehensive review of the current state of retrieval-augmented techniques in CV, focusing on two main areas: (I) visual understanding and (II) visual generation. In the realm of visual understanding, we systematically review tasks ranging from basic image recognition to complex applications such as medical report generation and multimodal question answering. For visual content generation, we examine the application of RAG in tasks related to image, video, and 3D generation. Furthermore, we explore recent advancements in RAG for embodied AI, with a particular focus on applications in planning, task execution, multimodal perception, interaction, and specialized domains. Given that the integration of retrieval-augmented techniques in CV is still in its early stages, we also highlight the key limitations of current approaches and propose future research directions to drive the development of this promising area.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2503.08007.pdf' target='_blank'>https://arxiv.org/pdf/2503.08007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, Pengxiang Ding, Xuelian Cheng, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08007">MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing versatile quadruped robots that can smoothly perform various actions and tasks in real-world environments remains a significant challenge. This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL) for fine-tuning large-scale VLA models with a large amount of mixed-quality data. MoRE integrates multiple low-rank adaptation modules as distinct experts within a dense multi-modal large language model (MLLM), forming a sparse-activated mixture-of-experts model. This design enables the model to effectively adapt to a wide array of downstream tasks. Moreover, we employ a reinforcement learning-based training objective to train our model as a Q-function after deeply exploring the structural properties of our tasks. Effective learning from automatically collected mixed-quality data enhances data efficiency and model performance. Extensive experiments demonstrate that MoRE outperforms all baselines across six different skills and exhibits superior generalization capabilities in out-of-distribution scenarios. We further validate our method in real-world scenarios, confirming the practicality of our approach and laying a solid foundation for future research on multi-task learning in quadruped robots.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2502.13508.pdf' target='_blank'>https://arxiv.org/pdf/2502.13508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13508">VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have become increasingly popular in robot manipulation for their end-to-end design and remarkable performance. However, existing VLAs rely heavily on vision-language models (VLMs) that only support text-based instructions, neglecting the more natural speech modality for human-robot interaction. Traditional speech integration methods usually involves a separate speech recognition system, which complicates the model and introduces error propagation. Moreover, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which may be crucial for robots to successfully complete customized tasks. To overcome above challenges, we propose VLAS, a novel end-to-end VLA that integrates speech recognition directly into the robot policy model. VLAS allows the robot to understand spoken commands through inner speech-text alignment and produces corresponding actions to fulfill the task. We also present two new datasets, SQA and CSI, to support a three-stage tuning process for speech instructions, which empowers VLAS with the ability of multimodal interaction across text, image, speech, and robot actions. Taking a step further, a voice retrieval-augmented generation (RAG) paradigm is designed to enable our model to effectively handle tasks that require individual-specific knowledge. Our extensive experiments show that VLAS can effectively accomplish robot manipulation tasks with diverse speech commands, offering a seamless and customized interaction experience.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2410.07166.pdf' target='_blank'>https://arxiv.org/pdf/2410.07166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07166">Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2407.10062.pdf' target='_blank'>https://arxiv.org/pdf/2407.10062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyuan Zhang, Kang Chen, Shiyan Chen, Yajing Zheng, Tiejun Huang, Zhaofei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10062">SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Novel View Synthesis plays a crucial role by generating new 2D renderings from multi-view images of 3D scenes. However, capturing high-speed scenes with conventional cameras often leads to motion blur, hindering the effectiveness of 3D reconstruction. To address this challenge, high-frame-rate dense 3D reconstruction emerges as a vital technique, enabling detailed and accurate modeling of real-world objects or scenes in various fields, including Virtual Reality or embodied AI. Spike cameras, a novel type of neuromorphic sensor, continuously record scenes with an ultra-high temporal resolution, showing potential for accurate 3D reconstruction. Despite their promise, existing approaches, such as applying Neural Radiance Fields (NeRF) to spike cameras, encounter challenges due to the time-consuming rendering process. To address this issue, we make the first attempt to introduce the 3D Gaussian Splatting (3DGS) into spike cameras in high-speed capture, providing 3DGS as dense and continuous clues of views, then constructing SpikeGS. Specifically, to train SpikeGS, we establish computational equations between the rendering process of 3DGS and the processes of instantaneous imaging and exposing-like imaging of the continuous spike stream. Besides, we build a very lightweight but effective mapping process from spikes to instant images to support training. Furthermore, we introduced a new spike-based 3D rendering dataset for validation. Extensive experiments have demonstrated our method possesses the high quality of novel view rendering, proving the tremendous potential of spike cameras in modeling 3D scenes.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2405.09546.pdf' target='_blank'>https://arxiv.org/pdf/2405.09546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Ge, Yihe Tang, Jiashu Xu, Cem Gokmen, Chengshu Li, Wensi Ai, Benjamin Jose Martinez, Arman Aydin, Mona Anvari, Ayush K Chakravarthy, Hong-Xing Yu, Josiah Wong, Sanjana Srivastava, Sharon Lee, Shengxin Zha, Laurent Itti, Yunzhu Li, Roberto MartÃ­n-MartÃ­n, Miao Liu, Pengchuan Zhang, Ruohan Zhang, Li Fei-Fei, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09546">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as "filled" and "folded"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction. Project website: https://behavior-vision-suite.github.io/
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2403.18062.pdf' target='_blank'>https://arxiv.org/pdf/2403.18062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Li, Sarthak Bhagat, Joseph Campbell, Yaqi Xie, Woojun Kim, Katia Sycara, Simon Stepputtis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18062">ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2403.09227.pdf' target='_blank'>https://arxiv.org/pdf/2403.09227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto MartÃ­n-MartÃ­n, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, Li Fei-Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09227">BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2308.15962.pdf' target='_blank'>https://arxiv.org/pdf/2308.15962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15962">WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the physical robot to provide a more user-friendly interface for the instruction-guided grasping task. The further experimental results on various real-world scenarios demonstrated the feasibility and efficacy of our proposed framework. See the project website at: https://star-uu-wang.github.io/WALL-E/
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2306.13760.pdf' target='_blank'>https://arxiv.org/pdf/2306.13760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Lingelbach, Chengshu Li, Minjune Hwang, Andrey Kurenkov, Alan Lou, Roberto MartÃ­n-MartÃ­n, Ruohan Zhang, Li Fei-Fei, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13760">Task-Driven Graph Attention for Hierarchical Relational Object Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure - objects related to furniture and then to rooms - such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs as part of its input and integrates graph neural networks as its backbone, with an integrated task-driven attention mechanism, and demonstrate its better scalability and learning efficiency than state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2305.17537.pdf' target='_blank'>https://arxiv.org/pdf/2305.17537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrey Kurenkov, Michael Lingelbach, Tanmay Agarwal, Emily Jin, Chengshu Li, Ruohan Zhang, Li Fei-Fei, Jiajun Wu, Silvio Savarese, Roberto MartÃ­n-MartÃ­n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17537">Modeling Dynamic Environments with Scene Graph Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patterns typically seen at homes, and show that NEP can be trained to predict the locations of objects in a variety of environments with diverse object movement dynamics, outperforming baselines both in terms of new scene adaptability and overall accuracy. The codebase and more can be found at https://www.scenegraphmemory.com.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2509.07962.pdf' target='_blank'>https://arxiv.org/pdf/2509.07962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongzheng Zhang, Haobo Xu, Zhuo Yang, Chenghao Yue, Zehao Lin, Huan-ang Gao, Ziwei Wang, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07962">TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2407.20242.pdf' target='_blank'>https://arxiv.org/pdf/2407.20242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Changgan Yin, Minghui Li, Lulu Xue, Yichen Wang, Shengshan Hu, Aishan Liu, Peijin Guo, Leo Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20242">BadRobot: Jailbreaking Embodied LLMs in the Physical World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI represents systems where AI is integrated into physical entities. Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue remains overlooked: could these embodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot, a novel attack paradigm aiming to make embodied LLMs violate safety and ethical constraints through typical voice-based user-system interactions. Specifically, three vulnerabilities are exploited to achieve this type of attack: (i) manipulation of LLMs within robotic systems, (ii) misalignment between linguistic outputs and physical actions, and (iii) unintentional hazardous behaviors caused by world knowledge's flaws. Furthermore, we construct a benchmark of various malicious physical action queries to evaluate BadRobot's attack performance. Based on this benchmark, extensive experiments against existing prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the effectiveness of our BadRobot.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2404.17031.pdf' target='_blank'>https://arxiv.org/pdf/2404.17031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Jiayou Qin, Xiwen Chen, Ashish Bastola, John Suchanek, Zihao Gong, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17031">Motor Focus: Fast Ego-Motion Prediction for Assistive Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assistive visual navigation systems for visually impaired individuals have become increasingly popular thanks to the rise of mobile computing. Most of these devices work by translating visual information into voice commands. In complex scenarios where multiple objects are present, it is imperative to prioritize object detection and provide immediate notifications for key entities in specific directions. This brings the need for identifying the observer's motion direction (ego-motion) by merely processing visual information, which is the key contribution of this paper. Specifically, we introduce Motor Focus, a lightweight image-based framework that predicts the ego-motion - the humans (and humanoid machines) movement intentions based on their visual feeds, while filtering out camera motion without any camera calibration. To this end, we implement an optical flow-based pixel-wise temporal analysis method to compensate for the camera motion with a Gaussian aggregation to smooth out the movement prediction area. Subsequently, to evaluate the performance, we collect a dataset including 50 clips of pedestrian scenes in 5 different scenarios. We tested this framework with classical feature detectors such as SIFT and ORB to show the comparison. Our framework demonstrates its superiority in speed (> 40FPS), accuracy (MAE = 60pixels), and robustness (SNR = 23dB), confirming its potential to enhance the usability of vision-based assistive navigation tools in complex environments.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2403.12415.pdf' target='_blank'>https://arxiv.org/pdf/2403.12415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Jiayou Qin, Ashish Bastola, Xiwen Chen, John Suchanek, Zihao Gong, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12415">VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2509.20414.pdf' target='_blank'>https://arxiv.org/pdf/2509.20414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yandan Yang, Baoxiong Jia, Shujie Zhang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20414">SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2507.04789.pdf' target='_blank'>https://arxiv.org/pdf/2507.04789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Zhao, Jiale Yuan, Zhiyuan Xu, Xiaoshuai Hao, Xinyi Zhang, Kun Wu, Zhengping Che, Chi Harold Liu, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04789">Training-free Generation of Temporally Consistent Rewards from VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language models (VLMs) have significantly improved performance in embodied tasks such as goal decomposition and visual comprehension. However, providing accurate rewards for robotic manipulation without fine-tuning VLMs remains challenging due to the absence of domain-specific robotic knowledge in pre-trained datasets and high computational costs that hinder real-time applicability. To address this, we propose $\mathrm{T}^2$-VLM, a novel training-free, temporally consistent framework that generates accurate rewards through tracking the status changes in VLM-derived subgoals. Specifically, our method first queries the VLM to establish spatially aware subgoals and an initial completion estimate before each round of interaction. We then employ a Bayesian tracking algorithm to update the goal completion status dynamically, using subgoal hidden states to generate structured rewards for reinforcement learning (RL) agents. This approach enhances long-horizon decision-making and improves failure recovery capabilities with RL. Extensive experiments indicate that $\mathrm{T}^2$-VLM achieves state-of-the-art performance in two robot manipulation benchmarks, demonstrating superior reward accuracy with reduced computation consumption. We believe our approach not only advances reward generation techniques but also contributes to the broader field of embodied AI. Project website: https://t2-vlm.github.io/.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2507.04047.pdf' target='_blank'>https://arxiv.org/pdf/2507.04047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhu, Xilin Wang, Yixuan Li, Zhuofan Zhang, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Wei Liang, Qian Yu, Zhidong Deng, Siyuan Huang, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04047">Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \underline{\textbf{M}}ove \underline{\textbf{t}}o \underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework that integrates active perception with \underline{\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%, and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2506.03574.pdf' target='_blank'>https://arxiv.org/pdf/2506.03574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Li, Zhen Zhao, Zhengping Che, Fei Liao, Kun Wu, Zhiyuan Xu, Pei Ren, Zhao Jin, Ning Liu, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03574">SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in dynamic environments must be able to not only follow diverse language instructions but flexibly adapt when user intent changes mid-execution. While recent Vision-Language-Action (VLA) models have advanced multi-task learning and instruction following, they typically assume static task intent, failing to respond when new instructions arrive during ongoing execution. This limitation hinders natural and robust interaction in dynamic settings, such as retail or household environments, where real-time intent changes are common. We propose SwitchVLA, a unified, execution-aware framework that enables smooth and reactive task switching without external planners or additional switch-specific data. We model task switching as a behavior modulation problem conditioned on execution state and instruction context. Expert demonstrations are segmented into temporally grounded contact phases, allowing the policy to infer task progress and adjust its behavior accordingly. A multi-behavior conditional policy is then trained to generate flexible action chunks under varying behavior modes through conditioned trajectory modeling. Experiments in both simulation and real-world robotic manipulation demonstrate that SwitchVLA enables robust instruction adherence, fluid task switching, and strong generalization-outperforming prior VLA baselines in both task success rate and interaction naturalness.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2505.04769.pdf' target='_blank'>https://arxiv.org/pdf/2505.04769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjan Sapkota, Yang Cao, Konstantinos I. Roumeliotis, Manoj Karkee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04769">Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2505.03238.pdf' target='_blank'>https://arxiv.org/pdf/2505.03238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Boyle, Nicolas Baumann, Paviththiren Sivasothilingam, Michele Magno, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03238">RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Future robotic systems operating in real-world environments will require on-board embodied intelligence without continuous cloud connection, balancing capabilities with constraints on computational power and memory. This work presents an extension of the R1-zero approach, which enables the usage of low parameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero approach was originally developed to enable mathematical reasoning in LLMs using static datasets. We extend it to the robotics domain through integration in a closed-loop Reinforcement Learning (RL) framework. This extension enhances reasoning in Embodied Artificial Intelligence (Embodied AI) settings without relying solely on distillation of large models through Supervised Fine-Tuning (SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which enables tasks that previously required significantly larger models. In an autonomous driving setting, a performance gain of 20.2%-points over the SFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3% control adaptability score, surpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These results highlight that practical, on-board deployment of small LLMs is not only feasible but can outperform larger models if trained through environmental feedback, underscoring the importance of an interactive learning framework for robotic Embodied AI, one grounded in practical experience rather than static supervision.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2505.02388.pdf' target='_blank'>https://arxiv.org/pdf/2505.02388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02388">MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2409.02389.pdf' target='_blank'>https://arxiv.org/pdf/2409.02389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02389">Multi-modal Situated Reasoning in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2404.09465.pdf' target='_blank'>https://arxiv.org/pdf/2404.09465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yandan Yang, Baoxiong Jia, Peiyuan Zhi, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09465">PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research. Project website: http://physcene.github.io.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2402.02385.pdf' target='_blank'>https://arxiv.org/pdf/2402.02385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02385">A Survey on Robotics with Foundation Models: toward Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2401.09258.pdf' target='_blank'>https://arxiv.org/pdf/2401.09258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Zhao, Kun Wu, Tianjiao Yi, Zhiyuan Xu, Xiaozhu Ju, Zhengping Che, Chi Harold Liu, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09258">Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving generalization is one key challenge in embodied AI, where obtaining large-scale datasets across diverse scenarios is costly. Traditional weak augmentations, such as cropping and flipping, are insufficient for improving a model's performance in new environments. Existing data augmentation methods often disrupt task-relevant information in images, potentially degrading performance. To overcome these challenges, we introduce EAGLE, an efficient training framework for generalizable visuomotor policies that improves upon existing methods by (1) enhancing generalization by applying augmentation only to control-related regions identified through a self-supervised control-aware mask and (2) improving training stability and efficiency by distilling knowledge from an expert to a visuomotor student policy, which is then deployed to unseen environments without further fine-tuning. Comprehensive experiments on three domains, including the DMControl Generalization Benchmark, the enhanced Robot Manipulation Distraction Benchmark, and a long-sequential drawer-opening task, validate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2311.12871.pdf' target='_blank'>https://arxiv.org/pdf/2311.12871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12871">An Embodied Generalist Agent in 3D World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on project page.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2304.04321.pdf' target='_blank'>https://arxiv.org/pdf/2304.04321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Gong, Jiangyong Huang, Yizhou Zhao, Haoran Geng, Xiaofeng Gao, Qingyang Wu, Wensi Ai, Ziheng Zhou, Demetri Terzopoulos, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04321">ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete (e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results indicate that current models for language-conditioned manipulations continue to experience significant challenges in novel goal-state generalizations, scene generalizations, and object generalizations. These findings highlight the need to develop new algorithms that address this gap and underscore the potential for further research in this area. Project website: https://arnold-benchmark.github.io.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2508.02046.pdf' target='_blank'>https://arxiv.org/pdf/2508.02046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Luo, Wentao Yan abd Jingyu Gong, Min Wang, Zhizhong Zhang, Xuhong Wang, Yuan Xie, Xin Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02046">NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Graphical User Interface (GUI) and embodied navigation have driven significant progress, yet these domains have largely evolved in isolation, with disparate datasets and training paradigms. In this paper, we observe that both tasks can be formulated as Markov Decision Processes (MDP), suggesting a foundational principle for their unification. Hence, we present NaviMaster, the first unified agent capable of seamlessly integrating GUI navigation and embodied navigation within a single framework. Specifically, NaviMaster (i) proposes a visual-target trajectory collection pipeline that generates trajectories for both GUI and embodied tasks in one formulation. (ii) employs a unified reinforcement learning framework on the mix data for better generalization. (iii) designs a novel distance-aware reward to ensure efficient learning from the trajectories. Through extensive experiments on out-of-domain benchmarks, NaviMaster is shown to outperform state-of-the-art agents in GUI navigation, spatial affordance prediction, and embodied navigation. Ablation studies further confirm the efficacy of our unified training strategy, data mixing strategy, and reward design.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2507.21407.pdf' target='_blank'>https://arxiv.org/pdf/2507.21407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Liu, Guibin Zhang, Kun Wang, Shiyuan Li, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21407">Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2505.23450.pdf' target='_blank'>https://arxiv.org/pdf/2505.23450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhejian Yang, Yongchao Chen, Xueyang Zhou, Jiangyue Yan, Dingjie Song, Yinuo Liu, Yuting Li, Yu Zhang, Pan Zhou, Hechang Chen, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23450">Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon robotic manipulation poses significant challenges for autonomous systems, requiring extended reasoning, precise execution, and robust error recovery across complex sequential tasks. Current approaches, whether based on static planning or end-to-end visuomotor policies, suffer from error accumulation and lack effective verification mechanisms during execution, limiting their reliability in real-world scenarios. We present Agentic Robot, a brain-inspired framework that addresses these limitations through Standardized Action Procedure (SAP)--a novel coordination protocol governing component interactions throughout manipulation tasks. Drawing inspiration from Standardized Operating Procedures (SOPs) in human organizations, SAP establishes structured workflows for planning, execution, and verification phases. Our architecture comprises three specialized components: (1) a large reasoning model that decomposes high-level instructions into semantically coherent subgoals, (2) a vision-language-action executor that generates continuous control commands from real-time visual inputs, and (3) a temporal verifier that enables autonomous progression and error recovery through introspective assessment. This SAP-driven closed-loop design supports dynamic self-verification without external supervision. On the LIBERO benchmark, Agentic Robot achieves state-of-the-art performance with an average success rate of 79.6%, outperforming SpatialVLA by 6.1% and OpenVLA by 7.4% on long-horizon tasks. These results demonstrate that SAP-driven coordination between specialized components enhances both performance and interpretability in sequential manipulation, suggesting significant potential for reliable autonomous systems. Project Github: https://agentic-robot.github.io.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2505.16640.pdf' target='_blank'>https://arxiv.org/pdf/2505.16640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16640">BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have advanced robotic control by enabling end-to-end decision-making directly from multimodal inputs. However, their tightly coupled architectures expose novel security vulnerabilities. Unlike traditional adversarial perturbations, backdoor attacks represent a stealthier, persistent, and practically significant threat-particularly under the emerging Training-as-a-Service paradigm-but remain largely unexplored in the context of VLA models. To address this gap, we propose BadVLA, a backdoor attack method based on Objective-Decoupled Optimization, which for the first time exposes the backdoor vulnerabilities of VLA models. Specifically, it consists of a two-stage process: (1) explicit feature-space separation to isolate trigger representations from benign inputs, and (2) conditional control deviations that activate only in the presence of the trigger, while preserving clean-task performance. Empirical results on multiple VLA benchmarks demonstrate that BadVLA consistently achieves near-100% attack success rates with minimal impact on clean task accuracy. Further analyses confirm its robustness against common input perturbations, task transfers, and model fine-tuning, underscoring critical security vulnerabilities in current VLA deployments. Our work offers the first systematic investigation of backdoor vulnerabilities in VLA models, highlighting an urgent need for secure and trustworthy embodied model design practices. We have released the project page at https://badvla-project.github.io/.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2504.08581.pdf' target='_blank'>https://arxiv.org/pdf/2504.08581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08581">FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2403.08282.pdf' target='_blank'>https://arxiv.org/pdf/2403.08282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghan Zhao, Kewei Chen, Dongxu Guo, Wenhao Chai, Tian Ye, Yanting Zhang, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08282">Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the dynamic and unpredictable open-world setting, navigating complex environments in Minecraft poses significant challenges for multi-agent systems. Agents must interact with the environment and coordinate their actions with other agents to achieve common objectives. However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, crucial for effective multi-agent navigation. Furthermore, processing and integrating multi-modal information (such as visual, textual, and auditory data) is essential for agents to comprehend their goals and navigate the environment successfully and fully. To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete navigation tasks. In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2) an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal information platform, facilitating multi-modal perception to perform the three navigation tasks with one system. To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring. We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2509.15273.pdf' target='_blank'>https://arxiv.org/pdf/2509.15273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Ni, Min Zhang, Pengyi Li, Yifu Yuan, Lingfeng Zhang, Yuecheng Liu, Peilong Han, Longxin Kou, Shaojin Ma, Jinbin Qiao, David Gamaliel Arcos Bravo, Yuening Wang, Xiao Hu, Zhanguang Zhang, Xianze Yao, Yutong Li, Zhao Zhang, Ying Wen, Ying-Cong Chen, Xiaodan Liang, Liang Lin, Bin He, Haitham Bou-Ammar, He Wang, Huazhe Xu, Jiankang Deng, Shan Luo, Shuqiang Jiang, Wei Pan, Yang Gao, Stefanos Zafeiriou, Jan Peters, Yuzheng Zhuang, Yingxue Zhang, Yan Zheng, Hongyao Tang, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15273">Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI development significantly lags behind large foundation models due to three critical challenges: (1) lack of systematic understanding of core capabilities needed for Embodied AI, making research lack clear objectives; (2) absence of unified and standardized evaluation systems, rendering cross-benchmark evaluation infeasible; and (3) underdeveloped automated and scalable acquisition methods for embodied data, creating critical bottlenecks for model scaling. To address these obstacles, we present Embodied Arena, a comprehensive, unified, and evolving evaluation platform for Embodied AI. Our platform establishes a systematic embodied capability taxonomy spanning three levels (perception, reasoning, task execution), seven core capabilities, and 25 fine-grained dimensions, enabling unified evaluation with systematic research objectives. We introduce a standardized evaluation system built upon unified infrastructure supporting flexible integration of 22 diverse benchmarks across three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced models from 20+ worldwide institutes. Additionally, we develop a novel LLM-driven automated generation pipeline ensuring scalable embodied evaluation data with continuous evolution for diversity and comprehensiveness. Embodied Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task Planning) with dual perspectives (benchmark view and capability view), providing comprehensive overviews of advanced model capabilities. Especially, we present nine findings summarized from the evaluation results on the leaderboards of Embodied Arena. This helps to establish clear research veins and pinpoint critical research problems, thereby driving forward progress in the field of Embodied AI.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2508.13998.pdf' target='_blank'>https://arxiv.org/pdf/2508.13998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13998">Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2507.06404.pdf' target='_blank'>https://arxiv.org/pdf/2507.06404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Tiezzi, Tommaso Apicella, Carlos Cardenas-Perez, Giovanni Fregonese, Stefano Dafarra, Pietro Morerio, Daniele Pucci, Alessio Del Bue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06404">Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating and comparing the performance of autonomous Humanoid Robots is challenging, as success rate metrics are difficult to reproduce and fail to capture the complexity of robot movement trajectories, critical in Human-Robot Interaction and Collaboration (HRIC). To address these challenges, we propose a general evaluation framework that measures the quality of Imitation Learning (IL) methods by focusing on trajectory performance. We devise the Neural Meta Evaluator (NeME), a deep learning model trained to classify actions from robot joint trajectories. NeME serves as a meta-evaluator to compare the performance of robot control policies, enabling policy evaluation without requiring human involvement in the loop. We validate our framework on ergoCub, a humanoid robot, using teleoperation data and comparing IL methods tailored to the available platform. The experimental results indicate that our method is more aligned with the success rate obtained on the robot than baselines, offering a reproducible, systematic, and insightful means for comparing the performance of multimodal imitation learning approaches in complex HRI tasks.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2506.18448.pdf' target='_blank'>https://arxiv.org/pdf/2506.18448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang Nguyen, Tri Le, Huy Nguyen, Thieu Vo, Tung D. Ta, Baoru Huang, Minh N. Vu, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18448">GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach. Our project page is available at https://zquang2202.github.io/GraspMAS
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2505.13948.pdf' target='_blank'>https://arxiv.org/pdf/2505.13948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingliang Zhai, Zhi Gao, Yuwei Wu, Yunde Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13948">Memory-Centric Embodied Question Answer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) requires agents to autonomously explore and understand the environment to answer context-dependent questions. Existing frameworks typically center around the planner, which guides the stopping module, memory module, and answering module for reasoning. In this paper, we propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric EQA models where the memory module cannot fully interact with other modules, MemoryEQA flexible feeds memory information into all modules, thereby enhancing efficiency and accuracy in handling complex tasks, such as those involving multiple targets across different regions. Specifically, we establish a multi-modal hierarchical memory mechanism, which is divided into global memory that stores language-enhanced scene maps, and local memory that retains historical observations and state information. When performing EQA tasks, the multi-modal large language model is leveraged to convert memory information into the required input formats for injection into different modules. To evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset based on HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 19.8% performance gain on MT-HM3D compared to baseline model further underscores memory capability's pivotal role in resolving complex tasks.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2505.08548.pdf' target='_blank'>https://arxiv.org/pdf/2505.08548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08548">From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both "seeing" and "doing," achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2503.10070.pdf' target='_blank'>https://arxiv.org/pdf/2503.10070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiqin Cui, Yifu Yuan, Yan Zheng, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10070">AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation and manipulation in open-world environments remain unsolved challenges in the Embodied AI. The high cost of commercial mobile manipulation robots significantly limits research in real-world scenes. To address this issue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile manipulation robot system with a hardware cost of only $1,000 (excluding optional computational resources), which is less than 1/15 of the cost of popular mobile robots. The AhaRobot system consists of three components: (1) a novel low-cost hardware architecture primarily composed of off-the-shelf components, (2) an optimized control solution to enhance operational precision integrating dual-motor backlash control and static friction compensation, and (3) a simple remote teleoperation method RoboPilot. We use handles to control the dual arms and pedals for whole-body movement. The teleoperation process is low-burden and easy to operate, much like piloting. RoboPilot is designed for remote data collection in embodied scenarios. Experimental results demonstrate that RoboPilot significantly enhances data collection efficiency in complex manipulation tasks, achieving a 30% increase compared to methods using 3D mouse and leader-follower systems. It also excels at completing extremely long-horizon tasks in one go. Furthermore, AhaRobot can be used to learn end-to-end policies and autonomously perform complex manipulation tasks, such as pen insertion and cleaning up the floor. We aim to build an affordable yet powerful platform to promote the development of embodied tasks on real devices, advancing more robust and reliable embodied AI. All hardware and software systems are available at https://aha-robot.github.io.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2412.18600.pdf' target='_blank'>https://arxiv.org/pdf/2412.18600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18600">ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-scene interaction (HSI) generation is crucial for applications in embodied AI, virtual reality, and robotics. Yet, existing methods cannot synthesize interactions in unseen environments such as in-the-wild scenes or reconstructed scenes, as they rely on paired 3D scenes and captured human motion data for training, which are unavailable for unseen environments. We present ZeroHSI, a novel approach that enables zero-shot 4D human-scene interaction synthesis, eliminating the need for training on any MoCap data. Our key insight is to distill human-scene interactions from state-of-the-art video generation models, which have been trained on vast amounts of natural human movements and interactions, and use differentiable rendering to reconstruct human-scene interactions. ZeroHSI can synthesize realistic human motions in both static scenes and environments with dynamic objects, without requiring any ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different types of various indoor and outdoor scenes with different interaction prompts, demonstrating its ability to generate diverse and contextually appropriate human-scene interactions.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2407.05047.pdf' target='_blank'>https://arxiv.org/pdf/2407.05047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Zhang, Xian Fu, Jianye Hao, Peilong Han, Hao Zhang, Lei Shi, Hongyao Tang, Yan Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05047">MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation Models on Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial Intelligence (EAI) have been advancing side by side at an unprecedented pace. The integration of the two has garnered significant attention from the AI research community. In this work, we attempt to provide an in-depth and comprehensive evaluation of the performance of MFM s on embodied task planning, aiming to shed light on their capabilities and limitations in this domain. To this end, based on the characteristics of embodied task planning, we first develop a systematic evaluation framework, which encapsulates four crucial capabilities of MFMs: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. Following this, we propose a new benchmark, named MFE-ETP, characterized its complex and variable task scenarios, typical yet diverse task types, task instances of varying difficulties, and rich test case types ranging from multiple embodied question answering to embodied task reasoning. Finally, we offer a simple and easy-to-use automatic evaluation platform that enables the automated testing of multiple MFMs on the proposed benchmark. Using the benchmark and evaluation platform, we evaluated several state-of-the-art MFMs and found that they significantly lag behind human-level performance. The MFE-ETP is a high-quality, large-scale, and challenging benchmark relevant to real-world tasks.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2406.15833.pdf' target='_blank'>https://arxiv.org/pdf/2406.15833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos Cardenas-Perez, Giulio Romualdi, Mohamed Elobaid, Stefano Dafarra, Giuseppe L'Erario, Silvio Traversaro, Pietro Morerio, Alessio Del Bue, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15833">XBG: End-to-end Imitation Learning for Autonomous Behaviour in Human-Robot Interaction and Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents XBG (eXteroceptive Behaviour Generation), a multimodal end-to-end Imitation Learning (IL) system for a whole-body autonomous humanoid robot used in real-world Human-Robot Interaction (HRI) scenarios. The main contribution of this paper is an architecture for learning HRI behaviours using a data-driven approach. Through teleoperation, a diverse dataset is collected, comprising demonstrations across multiple HRI scenarios, including handshaking, handwaving, payload reception, walking, and walking with a payload. After synchronizing, filtering, and transforming the data, different Deep Neural Networks (DNN) models are trained. The final system integrates different modalities comprising exteroceptive and proprioceptive sources of information to provide the robot with an understanding of its environment and its own actions. The robot takes sequence of images (RGB and depth) and joints state information during the interactions and then reacts accordingly, demonstrating learned behaviours. By fusing multimodal signals in time, we encode new autonomous capabilities into the robotic platform, allowing the understanding of context changes over time. The models are deployed on ergoCub, a real-world humanoid robot, and their performance is measured by calculating the success rate of the robot's behaviour under the mentioned scenarios.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2401.02539.pdf' target='_blank'>https://arxiv.org/pdf/2401.02539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianye Huang, Chenguang Yang, Mingchuan Zhou, Angelos Karlas, Nassir Navab, Zhongliang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02539">Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using Virtual Fixture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots inside deep veins, which may block blood flow or even cause a life-threatening pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by pressing the target vein until its lumen is fully compressed. However, the compression exam is highly operator-dependent. To alleviate intra- and inter-variations, we present a robotic US system with a novel hybrid force motion control scheme ensuring position and force tracking accuracy, and soft landing of the probe onto the target surface. In addition, a path-based virtual fixture is proposed to realize easy human-robot interaction for repeat compression operation at the lesion location. To ensure the biometric measurements obtained in different examinations are comparable, the 6D scanning path is determined in a coarse-to-fine manner using both an external RGBD camera and US images. The RGBD camera is first used to extract a rough scanning path on the object. Then, the segmented vascular lumen from US images are used to optimize the scanning path to ensure the visibility of the target object. To generate a continuous scan path for developing virtual fixtures, an arc-length based path fitting model considering both position and orientation is proposed. Finally, the whole system is evaluated on a human-like arm phantom with an uneven surface.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2311.07226.pdf' target='_blank'>https://arxiv.org/pdf/2311.07226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07226">Large Language Models for Robotics: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2309.16299.pdf' target='_blank'>https://arxiv.org/pdf/2309.16299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Chen, Ze Ji, Shuyang Liu, Jing Huo, Yiyu Chen, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16299">CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to effectively imitate expert skills in longhorizon tasks such as locomotion, manipulation, and more, poses a long-standing challenge. Existing imitation learning (IL) approaches for robots still grapple with sub-optimal performance in complex tasks. In this paper, we consider how this challenge can be addressed within the human cognitive priors. Heuristically, we extend the usual notion of action to a dual Cognition (high-level)-Action (low-level) architecture by introducing intuitive human cognitive priors, and propose a novel skill IL framework through human-robot interaction, called Cognition-Action-based Skill Imitation Learning (CasIL), for the robotic agent to effectively cognize and imitate the critical skills from raw visual demonstrations. CasIL enables both cognition and action imitation, while high-level skill cognition explicitly guides low-level primitive actions, providing robustness and reliability to the entire skill IL process. We evaluated our method on MuJoCo and RLBench benchmarks, as well as on the obstacle avoidance and point-goal navigation tasks for quadrupedal robot locomotion. Experimental results show that our CasIL consistently achieves competitive and robust skill imitation capability compared to other counterparts in a variety of long-horizon robotic tasks.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2306.11377.pdf' target='_blank'>https://arxiv.org/pdf/2306.11377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Dinh Vuong, Toan Tien Nguyen, Minh Nhat VU, Baoru Huang, Dzung Nguyen, Huynh Thi Thanh Binh, Thieu Vo, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11377">HabiCrowd: A High Performance Simulator for Crowd-Aware Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation, a foundational aspect of Embodied AI (E-AI), has been significantly studied in the past few years. While many 3D simulators have been introduced to support visual navigation tasks, scarcely works have been directed towards combining human dynamics, creating the gap between simulation and real-world applications. Furthermore, current 3D simulators incorporating human dynamics have several limitations, particularly in terms of computational efficiency, which is a promise of E-AI simulators. To overcome these shortcomings, we introduce HabiCrowd, the first standard benchmark for crowd-aware visual navigation that integrates a crowd dynamics model with diverse human settings into photorealistic environments. Empirical evaluations demonstrate that our proposed human dynamics model achieves state-of-the-art performance in collision avoidance, while exhibiting superior computational efficiency compared to its counterparts. We leverage HabiCrowd to conduct several comprehensive studies on crowd-aware visual navigation tasks and human-robot interactions. The source code and data can be found at https://habicrowd.github.io/.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2305.01098.pdf' target='_blank'>https://arxiv.org/pdf/2305.01098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joanne Truong, April Zitkovich, Sonia Chernova, Dhruv Batra, Tingnan Zhang, Jie Tan, Wenhao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01098">IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present IndoorSim-to-OutdoorReal (I2O), an end-to-end learned visual navigation approach, trained solely in simulated short-range indoor environments, and demonstrates zero-shot sim-to-real transfer to the outdoors for long-range navigation on the Spot robot. Our method uses zero real-world experience (indoor or outdoor), and requires the simulator to model no predominantly-outdoor phenomenon (sloped grounds, sidewalks, etc). The key to I2O transfer is in providing the robot with additional context of the environment (i.e., a satellite map, a rough sketch of a map by a human, etc.) to guide the robot's navigation in the real-world. The provided context-maps do not need to be accurate or complete -- real-world obstacles (e.g., trees, bushes, pedestrians, etc.) are not drawn on the map, and openings are not aligned with where they are in the real-world. Crucially, these inaccurate context-maps provide a hint to the robot about a route to take to the goal. We find that our method that leverages Context-Maps is able to successfully navigate hundreds of meters in novel environments, avoiding novel obstacles on its path, to a distant goal without a single collision or human intervention. In comparison, policies without the additional context fail completely. Lastly, we test the robustness of the Context-Map policy by adding varying degrees of noise to the map in simulation. We find that the Context-Map policy is surprisingly robust to noise in the provided context-map. In the presence of significantly inaccurate maps (corrupted with 50% noise, or entirely blank maps), the policy gracefully regresses to the behavior of a policy with no context. Videos are available at https://www.joannetruong.com/projects/i2o.html
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2506.23351.pdf' target='_blank'>https://arxiv.org/pdf/2506.23351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxing Chen, Kaixuan Wang, Zhaohui Yang, Yuhao Zhang, Zanxin Chen, Baijun Chen, Wanxi Dong, Ziyuan Liu, Dong Chen, Tianshuo Yang, Haibao Yu, Xiaokang Yang, Yusen Qin, Zhiqiang Xie, Yao Mu, Ping Luo, Tian Nian, Weiliang Deng, Yiheng Ge, Yibin Liu, Zixuan Li, Dehui Wang, Zhixuan Liang, Haohui Xie, Rijie Zeng, Yunfei Ge, Peiqing Cong, Guannan He, Zhaoming Han, Ruocheng Yin, Jingxiang Guo, Lunkai Lin, Tianling Xu, Hongzhe Bi, Xuewu Lin, Tianwei Lin, Shujie Luo, Keyu Li, Ziyan Zhao, Ke Fan, Heyang Xu, Bo Peng, Wenlong Gao, Dongjiang Li, Feng Jin, Hui Shen, Jinming Li, Chaowei Cui, Yu Chen, Yaxin Peng, Lingdong Zeng, Wenlong Dong, Tengfei Li, Weijie Ke, Jun Chen, Erdemt Bao, Tian Lan, Tenglong Liu, Jin Yang, Huiping Zhuang, Baozhi Jia, Shuai Zhang, Zhengfeng Zou, Fangheng Guan, Tianyi Jia, Ke Zhou, Hongjiu Zhang, Yating Han, Cheng Fang, Yixian Zou, Chongyang Xu, Qinglun Zhang, Shen Cheng, Xiaohe Wang, Ping Tan, Haoqiang Fan, Shuaicheng Liu, Jiaheng Chen, Chuxuan Huang, Chengliang Lin, Kaijun Luo, Boyu Yue, Yi Liu, Jinyu Chen, Zichang Tan, Liming Deng, Shuo Xu, Zijian Cai, Shilong Yin, Hao Wang, Hongshan Liu, Tianyang Li, Long Shi, Ran Xu, Huilin Xu, Zhengquan Zhang, Congsheng Xu, Jinchang Yang, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23351">Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2506.19769.pdf' target='_blank'>https://arxiv.org/pdf/2506.19769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shulan Ruan, Rongwei Wang, Xuchen Shen, Huijie Liu, Baihui Xiao, Jun Shi, Kun Zhang, Zhenya Huang, Yu Liu, Enhong Chen, You He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19769">A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2409.12514.pdf' target='_blank'>https://arxiv.org/pdf/2409.12514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12514">TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2406.10918.pdf' target='_blank'>https://arxiv.org/pdf/2406.10918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10918">Multi-LLM QA with Embodied Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have grown in popularity due to their natural language interface and pre trained knowledge, leading to rapidly increasing success in question-answering (QA) tasks. More recently, multi-agent systems with LLM-based agents (Multi-LLM) have been utilized increasingly more for QA. In these scenarios, the models may each answer the question and reach a consensus or each model is specialized to answer different domain questions. However, most prior work dealing with Multi-LLM QA has focused on scenarios where the models are asked in a zero-shot manner or are given information sources to extract the answer. For question answering of an unknown environment, embodied exploration of the environment is first needed to answer the question. This skill is necessary for personalizing embodied AI to environments such as households. There is a lack of insight into whether a Multi-LLM system can handle question-answering based on observations from embodied exploration. In this work, we address this gap by investigating the use of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment. Multiple LLM-based agents independently explore and then answer queries about a household environment. We analyze different aggregation methods to generate a single, final answer for each query: debating, majority voting, and training a central answer module (CAM). Using CAM, we observe a $46\%$ higher accuracy compared against the other non-learning-based aggregation methods. We provide code and the query dataset for further research.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2406.01194.pdf' target='_blank'>https://arxiv.org/pdf/2406.01194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Josechu Guerrero, Giovanni Maria Farinella, Antonino Furnari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01194">AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Short-Term object-interaction Anticipation consists of detecting the location of the next-active objects, the noun and verb categories of the interaction, and the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants or human robot interaction to understand the user goals, but there is still room for improvement to perform STA in a precise and reliable way. In this work, we improve the performance of STA predictions with two contributions: 1. We propose STAformer, a novel attention-based architecture integrating frame guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair. 2. We introduce two novel modules to ground STA predictions on human behavior by modeling affordances.First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant relative Overall Top-5 mAP improvements of up to +45% on Ego4D and +42% on a novel set of curated EPIC-Kitchens STA labels. We will release the code, annotations, and pre extracted affordances on Ego4D and EPIC- Kitchens to encourage future research in this area.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2403.09905.pdf' target='_blank'>https://arxiv.org/pdf/2403.09905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09905">Right Place, Right Time! Dynamizing Topological Graphs for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Navigation tasks often involve constructing topological graphs of a scene during exploration to facilitate high-level planning and decision-making for execution in continuous environments. Prior literature makes the assumption of static graphs with stationary targets, which does not hold in many real-world environments with moving objects. To address this, we present a novel formulation generalizing navigation to dynamic environments by introducing structured object transitions to dynamize static topological graphs called Object Transition Graphs (OTGs). OTGs simulate portable targets following structured routes inspired by human habits. We apply this technique to Matterport3D (MP3D), a popular simulator for evaluating embodied tasks. On these dynamized OTGs, we establish a navigation benchmark by evaluating Oracle-based, Reinforcement Learning, and Large Language Model (LLM)-based approaches on a multi-object finding task. Further, we quantify agent adaptability, and make key inferences such as agents employing learned decision-making strategies generalize better than those relying on privileged oracle knowledge. To the best of our knowledge, ours is the first work to introduce structured temporal dynamism on topological graphs for studying generalist embodied navigation policies. The code and dataset for our OTGs will be made publicly available to foster research on embodied navigation in dynamic scenes.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2304.09448.pdf' target='_blank'>https://arxiv.org/pdf/2304.09448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Mu, Shunyu Yao, Mingyu Ding, Ping Luo, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09448">EC^2: Emergent Communication for Embodied Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, which is then used to finetune a lightweight policy network for downstream control. Through extensive experiments in Metaworld and Franka Kitchen embodied benchmarks, EC^2 is shown to consistently outperform previous contrastive learning methods for both videos and texts as task inputs. Further ablations confirm the importance of the emergent language, which is beneficial for both video and language learning, and significantly superior to using pre-trained video captions. We also present a quantitative and qualitative analysis of the emergent language and discuss future directions toward better understanding and leveraging emergent communication in embodied tasks.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2508.19236.pdf' target='_blank'>https://arxiv.org/pdf/2508.19236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19236">MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2507.05763.pdf' target='_blank'>https://arxiv.org/pdf/2507.05763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05763">DreamArt: Generating Interactable Articulated Objects from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2506.15610.pdf' target='_blank'>https://arxiv.org/pdf/2506.15610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Lan, Chenyang Zhu, Zhirui Gao, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15610">BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary 3D object detection has gained significant interest due to its critical applications in autonomous driving and embodied AI. Existing detection methods, whether offline or online, typically rely on dense point cloud reconstruction, which imposes substantial computational overhead and memory constraints, hindering real-time deployment in downstream tasks. To address this, we propose a novel reconstruction-free online framework tailored for memory-efficient and real-time 3D detection. Specifically, given streaming posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual foundation model (VFM) for single-view 3D object detection by bounding boxes, coupled with CLIP to capture open-vocabulary semantics of detected objects. To fuse all detected bounding boxes across different views into a unified one, we employ an association module for correspondences of multi-views and an optimization module to fuse the 3D bounding boxes of the same instance predicted in multi-views. The association module utilizes 3D Non-Maximum Suppression (NMS) and a box correspondence matching module, while the optimization module uses an IoU-guided efficient random optimization technique based on particle filtering to enforce multi-view consistency of the 3D bounding boxes while minimizing computational complexity. Extensive experiments on ScanNetV2 and CA-1M datasets demonstrate that our method achieves state-of-the-art performance among online methods. Benefiting from this novel reconstruction-free paradigm for 3D object detection, our method exhibits great generalization abilities in various scenarios, enabling real-time perception even in environments exceeding 1000 square meters.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2505.05108.pdf' target='_blank'>https://arxiv.org/pdf/2505.05108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaohan Feng, Ruiqi Xue, Lei Yuan, Yang Yu, Ning Ding, Meiqin Liu, Bingzhao Gao, Jian Sun, Xinhu Zheng, Gang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05108">Multi-agent Embodied AI: Advances and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2503.10480.pdf' target='_blank'>https://arxiv.org/pdf/2503.10480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10480">World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2502.11518.pdf' target='_blank'>https://arxiv.org/pdf/2502.11518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Wu, Xian Wei, Guang Chen, Hao Shen, Xiangfeng Wang, Wenhao Li, Bo Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11518">Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied multi-agent systems (EMAS) have attracted growing attention for their potential to address complex, real-world challenges in areas such as logistics and robotics. Recent advances in foundation models pave the way for generative agents capable of richer communication and adaptive problem-solving. This survey provides a systematic examination of how EMAS can benefit from these generative capabilities. We propose a taxonomy that categorizes EMAS by system architectures and embodiment modalities, emphasizing how collaboration spans both physical and virtual contexts. Central building blocks, perception, planning, communication, and feedback, are then analyzed to illustrate how generative techniques bolster system robustness and flexibility. Through concrete examples, we demonstrate the transformative effects of integrating foundation models into embodied, multi-agent frameworks. Finally, we discuss challenges and future directions, underlining the significant promise of EMAS to reshape the landscape of AI-driven collaboration.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2501.07295.pdf' target='_blank'>https://arxiv.org/pdf/2501.07295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oleg Kobzarev, Artem Lykov, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07295">GestLLM: Advanced Hand Gesture Interpretation via Large Language Models for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces GestLLM, an advanced system for human-robot interaction that enables intuitive robot control through hand gestures. Unlike conventional systems, which rely on a limited set of predefined gestures, GestLLM leverages large language models and feature extraction via MediaPipe to interpret a diverse range of gestures. This integration addresses key limitations in existing systems, such as restricted gesture flexibility and the inability to recognize complex or unconventional gestures commonly used in human communication.
  By combining state-of-the-art feature extraction and language model capabilities, GestLLM achieves performance comparable to leading vision-language models while supporting gestures underrepresented in traditional datasets. For example, this includes gestures from popular culture, such as the ``Vulcan salute" from Star Trek, without any additional pretraining, prompt engineering, etc. This flexibility enhances the naturalness and inclusivity of robot control, making interactions more intuitive and user-friendly.
  GestLLM provides a significant step forward in gesture-based interaction, enabling robots to understand and respond to a wide variety of hand gestures effectively. This paper outlines its design, implementation, and evaluation, demonstrating its potential applications in advanced human-robot collaboration, assistive robotics, and interactive entertainment.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2412.05789.pdf' target='_blank'>https://arxiv.org/pdf/2412.05789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05789">InfiniteWorld: A Unified Scalable Simulation Framework for General Visual-Language Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realizing scaling laws in embodied AI has become a focus. However, previous work has been scattered across diverse simulation platforms, with assets and models lacking unified interfaces, which has led to inefficiencies in research. To address this, we introduce InfiniteWorld, a unified and scalable simulator for general vision-language robot interaction built on Nvidia Isaac Sim. InfiniteWorld encompasses a comprehensive set of physics asset construction methods and generalized free robot interaction benchmarks. Specifically, we first built a unified and scalable simulation framework for embodied learning that integrates a series of improvements in generation-driven 3D asset construction, Real2Sim, automated annotation framework, and unified 3D asset processing. This framework provides a unified and scalable platform for robot interaction and learning. In addition, to simulate realistic robot interaction, we build four new general benchmarks, including scene graph collaborative exploration and open-world social mobile manipulation. The former is often overlooked as an important task for robots to explore the environment and build scene knowledge, while the latter simulates robot interaction tasks with different levels of knowledge agents based on the former. They can more comprehensively evaluate the embodied agent's capabilities in environmental understanding, task planning and execution, and intelligent interaction. We hope that this work can provide the community with a systematic asset interface, alleviate the dilemma of the lack of high-quality assets, and provide a more comprehensive evaluation of robot interactions.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2410.16922.pdf' target='_blank'>https://arxiv.org/pdf/2410.16922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengxin Xu, Weiwei Wan, Hesheng Wang, Kensuke Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16922">Direction-Constrained Control for Efficient Physical Human-Robot Interaction under Hierarchical Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a control method to address the physical Human-Robot Interaction (pHRI) challenge in the context of hierarchical tasks. A common approach to managing hierarchical tasks is Hierarchical Quadratic Programming (HQP), which, however, cannot be directly applied to human interaction due to its allowance of arbitrary velocity direction adjustments. To resolve this limitation, we introduce the concept of directional constraints and develop a direction-constrained optimization algorithm to handle the nonlinearities induced by these constraints. The algorithm solves two sub-problems, minimizing the error and minimizing the deviation angle, in parallel, and combines the results of the two sub-problems to produce a final optimal outcome. The mutual influence between these two sub-problems is analyzed to determine the best parameter for combination. Additionally, the velocity objective in our control framework is computed using a variable admittance controller. Traditional admittance control does not account for constraints. To address this issue, we propose a variable admittance control method to adjust control objectives dynamically. The method helps reduce the deviation between robot velocity and human intention at the constraint boundaries, thereby enhancing interaction efficiency. We evaluate the proposed method in scenarios where a human operator physically interacts with a 7-degree-of-freedom robotic arm. The results highlight the importance of incorporating directional constraints in pHRI for hierarchical tasks. Compared to existing methods, our approach generates smoother robotic trajectories during interaction while avoiding interaction delays at the constraint boundaries.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2408.02297.pdf' target='_blank'>https://arxiv.org/pdf/2408.02297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Prasanna, Daniel Honerkamp, Kshitij Sirohi, Tim Welschehold, Wolfram Burgard, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02297">Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through calibrated perception probabilities and uncertainty across aggregation and found decisions, thereby adapting the models for sequential tasks. The resulting methods can be directly integrated with pretrained models across a wide family of existing search approaches at no additional training cost. We perform extensive evaluations of aggregation methods across both different semantic perception models and policies, confirming the importance of calibrated uncertainties in both the aggregation and found decisions. We make the code and trained models available at https://semantic-search.cs.uni-freiburg.de.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2403.11796.pdf' target='_blank'>https://arxiv.org/pdf/2403.11796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Jiang, Yueming Xu, Yihan Zeng, Hang Xu, Wei Zhang, Jianfeng Feng, Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11796">OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D reconstruction has been widely used in autonomous navigation fields of mobile robotics. However, the former research can only provide the basic geometry structure without the capability of open-world scene understanding, limiting advanced tasks like human interaction and visual navigation. Moreover, traditional 3D scene understanding approaches rely on expensive labeled 3D datasets to train a model for a single task with supervision. Thus, geometric reconstruction with zero-shot scene understanding i.e. Open vocabulary 3D Understanding and Reconstruction, is crucial for the future development of mobile robots. In this paper, we propose OpenOcc, a novel framework unifying the 3D scene reconstruction and open vocabulary understanding with neural radiance fields. We model the geometric structure of the scene with occupancy representation and distill the pre-trained open vocabulary model into a 3D language field via volume rendering for zero-shot inference. Furthermore, a novel semantic-aware confidence propagation (SCP) method has been proposed to relieve the issue of language field representation degeneracy caused by inconsistent measurements in distilled features. Experimental results show that our approach achieves competitive performance in 3D scene understanding tasks, especially for small and long-tail objects.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2307.12335.pdf' target='_blank'>https://arxiv.org/pdf/2307.12335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicong Hong, Yang Zhou, Ruiyi Zhang, Franck Dernoncourt, Trung Bui, Stephen Gould, Hao Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12335">Learning Navigational Visual Representations with Semantic Map Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being able to perceive the semantics and the spatial structure of the environment is essential for visual navigation of a household robot. However, most existing works only employ visual backbones pre-trained either with independent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that humans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning transfers the compact and rich information from a map, such as objects, structure and transition, to the agent's egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2303.13186.pdf' target='_blank'>https://arxiv.org/pdf/2303.13186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Lu, Yunqiang Pei, Guoqing Wang, Yang Yang, Zheng Wang, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13186">ScanERU: Interactive 3D Visual Grounding based on Embodied Reference Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aiming to link natural language descriptions to specific regions in a 3D scene represented as 3D point clouds, 3D visual grounding is a very fundamental task for human-robot interaction. The recognition errors can significantly impact the overall accuracy and then degrade the operation of AI systems. Despite their effectiveness, existing methods suffer from the difficulty of low recognition accuracy in cases of multiple adjacent objects with similar appearances.To address this issue, this work intuitively introduces the human-robot interaction as a cue to facilitate the development of 3D visual grounding. Specifically, a new task termed Embodied Reference Understanding (ERU) is first designed for this concern. Then a new dataset called ScanERU is constructed to evaluate the effectiveness of this idea. Different from existing datasets, our ScanERU is the first to cover semi-synthetic scene integration with textual, real-world visual, and synthetic gestural information. Additionally, this paper formulates a heuristic framework based on attention mechanisms and human body movements to enlighten the research of ERU. Experimental results demonstrate the superiority of the proposed method, especially in the recognition of multiple identical objects. Our codes and dataset are ready to be available publicly.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2212.02710.pdf' target='_blank'>https://arxiv.org/pdf/2212.02710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Yuan Yao, Siqi Liu, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02710">Beyond Object Recognition: A New Benchmark towards Object Concept Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In experiments, OCRN effectively infers the object knowledge while following the causalities well. Our data and code are available at https://mvig-rhos.com/ocl.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2508.01361.pdf' target='_blank'>https://arxiv.org/pdf/2508.01361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Francisco Moreno Fuentes, Muhammad Haris Khan, Miguel Altamirano Cabrera, Valerii Serpiva, Dmitri Iarchuk, Yara Mahmoud, Issatay Tokmurziyev, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01361">VLH: Vision-Language-Haptics Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VLH, a novel Visual-Language-Haptic Foundation Model that unifies perception, language, and tactile feedback in aerial robotics and virtual reality. Unlike prior work that treats haptics as a secondary, reactive channel, VLH synthesizes mid-air force and vibration cues as a direct consequence of contextual visual understanding and natural language commands. Our platform comprises an 8-inch quadcopter equipped with dual inverse five-bar linkage arrays for localized haptic actuation, an egocentric VR camera, and an exocentric top-down view. Visual inputs and language instructions are processed by a fine-tuned OpenVLA backbone - adapted via LoRA on a bespoke dataset of 450 multimodal scenarios - to output a 7-dimensional action vector (Vx, Vy, Vz, Hx, Hy, Hz, Hv). INT8 quantization and a high-performance server ensure real-time operation at 4-5 Hz. In human-robot interaction experiments (90 flights), VLH achieved a 56.7% success rate for target acquisition (mean reach time 21.3 s, pose error 0.24 m) and 100% accuracy in texture discrimination. Generalization tests yielded 70.0% (visual), 54.4% (motion), 40.0% (physical), and 35.0% (semantic) performance on novel tasks. These results demonstrate VLH's ability to co-evolve haptic feedback with perceptual reasoning and intent, advancing expressive, immersive human-robot interactions.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2506.01174.pdf' target='_blank'>https://arxiv.org/pdf/2506.01174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Qasim Ali, Saeejith Nair, Alexander Wong, Yuchen Cui, Yuhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01174">GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structured scene representations are a core component of embodied agents, helping to consolidate raw sensory streams into readable, modular, and searchable formats. Due to their high computational overhead, many approaches build such representations in advance of the task. However, when the task specifications change, such static approaches become inadequate as they may miss key objects, spatial relations, and details. We introduce GraphPad, a modifiable structured memory that an agent can tailor to the needs of the task through API calls. It comprises a mutable scene graph representing the environment, a navigation log indexing frame-by-frame content, and a scratchpad for task-specific notes. Together, GraphPad serves as a dynamic workspace that remains complete, current, and aligned with the agent's immediate understanding of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a +3.0% increase over an image-only baseline using the same vision-language model, while operating with five times fewer input frames. These results show that allowing online, language-driven refinement of 3-D memory yields more informative representations without extra training or data collection.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2505.16517.pdf' target='_blank'>https://arxiv.org/pdf/2505.16517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, Zhongzhi Li, Rui Yan, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16517">ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated training datasets, which limits their generalization and causes them to struggle in out-of-domain (OOD) scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2501.00358.pdf' target='_blank'>https://arxiv.org/pdf/2501.00358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Fan, Xiaojian Ma, Rongpeng Su, Jun Guo, Rujie Wu, Xi Chen, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00358">Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2407.13693.pdf' target='_blank'>https://arxiv.org/pdf/2407.13693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hardik Parwana, Mitchell Black, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Danil Prokhorov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13693">Model Predictive Path Integral Methods with Reach-Avoid Tasks and Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of robotics necessitates robust tools for developing and testing safe control architectures in dynamic and uncertain environments. Ensuring safety and reliability in robotics, especially in safety-critical applications, is crucial, driving substantial industrial and academic efforts. In this context, we extend CBFkit, a Python/ROS2 toolbox, which now incorporates a planner using reach-avoid specifications as a cost function. This integration with the Model Predictive Path Integral (MPPI) controllers enables the toolbox to satisfy complex tasks while ensuring formal safety guarantees under various sources of uncertainty using Control Barrier Functions (CBFs). CBFkit is optimized for speed using JAX for automatic differentiation and jaxopt for quadratic program solving. The toolbox supports various robotic applications, including autonomous navigation, human-robot interaction, and multi-robot coordination. The toolbox also offers a comprehensive library of planner, controller, sensor, and estimator implementations. Through a series of examples, we demonstrate the enhanced capabilities of CBFkit in different robotic scenarios.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2407.07636.pdf' target='_blank'>https://arxiv.org/pdf/2407.07636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vignesh Prasad, Alap Kshirsagar, Dorothea Koert, Ruth Stock-Homburg, Jan Peters, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07636">MoVEInt: Mixture of Variational Experts for Learning Human-Robot Interactions from Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shared dynamics models are important for capturing the complexity and variability inherent in Human-Robot Interaction (HRI). Therefore, learning such shared dynamics models can enhance coordination and adaptability to enable successful reactive interactions with a human partner. In this work, we propose a novel approach for learning a shared latent space representation for HRIs from demonstrations in a Mixture of Experts fashion for reactively generating robot actions from human observations. We train a Variational Autoencoder (VAE) to learn robot motions regularized using an informative latent space prior that captures the multimodality of the human observations via a Mixture Density Network (MDN). We show how our formulation derives from a Gaussian Mixture Regression formulation that is typically used approaches for learning HRI from demonstrations such as using an HMM/GMM for learning a joint distribution over the actions of the human and the robot. We further incorporate an additional regularization to prevent "mode collapse", a common phenomenon when using latent space mixture models with VAEs. We find that our approach of using an informative MDN prior from human observations for a VAE generates more accurate robot motions compared to previous HMM-based or recurrent approaches of learning shared latent representations, which we validate on various HRI datasets involving interactions such as handshakes, fistbumps, waving, and handovers. Further experiments in a real-world human-to-robot handover scenario show the efficacy of our approach for generating successful interactions with four different human interaction partners.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2405.18515.pdf' target='_blank'>https://arxiv.org/pdf/2405.18515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunuo Chen, Tianyi Xie, Zeshun Zong, Xuan Li, Feng Gao, Yin Yang, Ying Nian Wu, Chenfanfu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18515">Atlas3D: Physically Constrained Self-Supporting Text-to-3D for Simulation and Fabrication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing diffusion-based text-to-3D generation methods primarily focus on producing visually realistic shapes and appearances, often neglecting the physical constraints necessary for downstream tasks. Generated models frequently fail to maintain balance when placed in physics-based simulations or 3D printed. This balance is crucial for satisfying user design intentions in interactive gaming, embodied AI, and robotics, where stable models are needed for reliable interaction. Additionally, stable models ensure that 3D-printed objects, such as figurines for home decoration, can stand on their own without requiring additional supports. To fill this gap, we introduce Atlas3D, an automatic and easy-to-implement method that enhances existing Score Distillation Sampling (SDS)-based text-to-3D tools. Atlas3D ensures the generation of self-supporting 3D models that adhere to physical laws of stability under gravity, contact, and friction. Our approach combines a novel differentiable simulation-based loss function with physically inspired regularization, serving as either a refinement or a post-processing module for existing frameworks. We verify Atlas3D's efficacy through extensive generation tasks and validate the resulting 3D models in both simulated and real-world environments.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2405.04899.pdf' target='_blank'>https://arxiv.org/pdf/2405.04899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Alabbas, Miguel Altamirano Cabrera, Mohamed Sayed, Oussama Alyounes, Qian Liu, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04899">MoveTouch: Robotic Motion Capturing System with Wearable Tactile Display to Achieve Safe HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The collaborative robot market is flourishing as there is a trend towards simplification, modularity, and increased flexibility on the production line. But when humans and robots are collaborating in a shared environment, the safety of humans should be a priority. We introduce a novel wearable robotic system to enhance safety during Human-Robot Interaction (HRI). The proposed wearable robot is designed to hold a fiducial marker and maintain its visibility to a motion capture system, which, in turn, localizes the user's hand with good accuracy and low latency and provides vibrotactile feedback to the user's wrist. The vibrotactile feedback guides the user's hand movement during collaborative tasks in order to increase safety and enhance collaboration efficiency. A user study was conducted to assess the recognition and discriminability of ten designed vibration patterns applied to the upper (dorsal) and the down (volar) parts of the user's wrist. The results show that the pattern recognition rate on the volar side was higher, with an average of 75.64% among all users. Four patterns with a high recognition rate were chosen to be incorporated into our system. A second experiment was carried out to evaluate users' response to the chosen patterns in real-world collaborative tasks. Results show that all participants responded to the patterns correctly, and the average response time for the patterns was between 0.24 and 2.41 seconds.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2311.16380.pdf' target='_blank'>https://arxiv.org/pdf/2311.16380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vignesh Prasad, Lea Heitlinger, Dorothea Koert, Ruth Stock-Homburg, Jan Peters, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16380">Learning Multimodal Latent Dynamics for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents a method for learning well-coordinated Human-Robot Interaction (HRI) from Human-Human Interactions (HHI). We devise a hybrid approach using Hidden Markov Models (HMMs) as the latent space priors for a Variational Autoencoder to model a joint distribution over the interacting agents. We leverage the interaction dynamics learned from HHI to learn HRI and incorporate the conditional generation of robot motions from human observations into the training, thereby predicting more accurate robot trajectories. The generated robot motions are further adapted with Inverse Kinematics to ensure the desired physical proximity with a human, combining the ease of joint space learning and accurate task space reachability. For contact-rich interactions, we modulate the robot's stiffness using HMM segmentation for a compliant interaction. We verify the effectiveness of our approach deployed on a Humanoid robot via a user study. Our method generalizes well to various humans despite being trained on data from just two humans. We find that users perceive our method as more human-like, timely, and accurate and rank our method with a higher degree of preference over other baselines. We additionally show the ability of our approach to generate successful interactions in a more complex scenario of Bimanual Robot-to-Human Handovers.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2307.08363.pdf' target='_blank'>https://arxiv.org/pdf/2307.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Alabbas, Miguel Altamirano Cabrera, Oussama Alyounes, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08363">ArUcoGlide: a Novel Wearable Robot for Position Tracking and Haptic Feedback to Increase Safety During Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The current capabilities of robotic systems make human collaboration necessary to accomplish complex tasks effectively. In this work, we are introducing a framework to ensure safety in a human-robot collaborative environment. The system is composed of a wearable 2-DOF robot, a low-cost and easy-to-install tracking system, and a collision avoidance algorithm based on the Artificial Potential Field (APF). The wearable robot is designed to hold a fiducial marker and maintain its visibility to the tracking system, which, in turn, localizes the user's hand with good accuracy and low latency and provides haptic feedback to the user. The system is designed to enhance the performance of collaborative tasks while ensuring user safety. Three experiments were carried out to evaluate the performance of the proposed system. The first one evaluated the accuracy of the tracking system. The second experiment analyzed human-robot behavior during an imminent collision. The third experiment evaluated the system in a collaborative activity in a shared working environment. The results show that the implementation of the introduced system reduces the operation time by 16% and increases the average distance between the user's hand and the robot by 5 cm.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2306.14055.pdf' target='_blank'>https://arxiv.org/pdf/2306.14055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>J. Taery Kim, Wenhao Yu, Yash Kothari, Jie Tan, Greg Turk, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14055">Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the ``Delayed Harness'' to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. We evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. We also demonstrate the integrated system on a quadrupedal robot with a rigid harness, by guiding users over $100+$~m trajectories.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2210.12418.pdf' target='_blank'>https://arxiv.org/pdf/2210.12418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vignesh Prasad, Dorothea Koert, Ruth Stock-Homburg, Jan Peters, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12418">MILD: Multimodal Interactive Latent Dynamics for Learning Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling interaction dynamics to generate robot trajectories that enable a robot to adapt and react to a human's actions and intentions is critical for efficient and effective collaborative Human-Robot Interactions (HRI). Learning from Demonstration (LfD) methods from Human-Human Interactions (HHI) have shown promising results, especially when coupled with representation learning techniques. However, such methods for learning HRI either do not scale well to high dimensional data or cannot accurately adapt to changing via-poses of the interacting partner. We propose Multimodal Interactive Latent Dynamics (MILD), a method that couples deep representation learning and probabilistic machine learning to address the problem of two-party physical HRIs. We learn the interaction dynamics from demonstrations, using Hidden Semi-Markov Models (HSMMs) to model the joint distribution of the interacting agents in the latent space of a Variational Autoencoder (VAE). Our experimental evaluations for learning HRI from HHI demonstrations show that MILD effectively captures the multimodality in the latent representations of HRI tasks, allowing us to decode the varying dynamics occurring in such tasks. Compared to related work, MILD generates more accurate trajectories for the controlled agent (robot) when conditioned on the observed agent's (human) trajectory. Notably, MILD can learn directly from camera-based pose estimations to generate trajectories, which we then map to a humanoid robot without the need for any additional training.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2210.07474.pdf' target='_blank'>https://arxiv.org/pdf/2210.07474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07474">SQA3D: Situated Question Answering in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2209.13308.pdf' target='_blank'>https://arxiv.org/pdf/2209.13308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puze Liu, Kuo Zhang, Davide Tateo, Snehal Jauhri, Zhiyuan Hu, Jan Peters, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.13308">Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety is a crucial property of every robotic platform: any control policy should always comply with actuator limits and avoid collisions with the environment and humans. In reinforcement learning, safety is even more fundamental for exploring an environment without causing any damage. While there are many proposed solutions to the safe exploration problem, only a few of them can deal with the complexity of the real world. This paper introduces a new formulation of safe exploration for reinforcement learning of various robotic tasks. Our approach applies to a wide class of robotic platforms and enforces safety even under complex collision constraints learned from data by exploring the tangent space of the constraint manifold. Our proposed approach achieves state-of-the-art performance in simulated high-dimensional and dynamic tasks while avoiding collisions with the environment. We show safe real-world deployment of our learned controller on a TIAGo++ robot, achieving remarkable performance in manipulation and human-robot interaction tasks.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2509.09560.pdf' target='_blank'>https://arxiv.org/pdf/2509.09560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shulai Zhang, Ao Xu, Quan Chen, Han Zhao, Weihao Cui, Ningxin Zheng, Haibin Lin, Xin Liu, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09560">Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary "thinking" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2509.02055.pdf' target='_blank'>https://arxiv.org/pdf/2509.02055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhang, Chenwei Wang, Ouyang Lu, Yuan Zhao, Yunfei Ge, Zhenglong Sun, Xiu Li, Chi Zhang, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02055">Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce \textbf{Align-Then-stEer (\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation framework. \texttt{ATE} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to \textbf{9.8\%} in simulation and achieves a striking \textbf{32\% success rate gain} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2508.17832.pdf' target='_blank'>https://arxiv.org/pdf/2508.17832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiping Wang, Yuxi Wang, Mengqi Zhou, Junsong Fan, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17832">HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic 3D indoor scene generation is crucial for virtual reality, interior design, embodied intelligence, and scene understanding. While existing methods have made progress in coarse-scale furniture arrangement, they struggle to capture fine-grained object placements, limiting the realism and utility of generated environments. This gap hinders immersive virtual experiences and detailed scene comprehension for embodied AI applications. To address these issues, we propose Hierarchical Layout Generation (HLG), a novel method for fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine hierarchical approach, refining scene layouts from large-scale furniture placement to intricate object arrangements. Specifically, our fine-grained layout alignment module constructs a hierarchical layout through vertical and horizontal decoupling, effectively decomposing complex 3D indoor scenes into multiple levels of granularity. Additionally, our trainable layout optimization network addresses placement issues, such as incorrect positioning, orientation errors, and object intersections, ensuring structurally coherent and physically plausible scene generation. We demonstrate the effectiveness of our approach through extensive experiments, showing superior performance in generating realistic indoor scenes compared to existing methods. This work advances the field of scene generation and opens new possibilities for applications requiring detailed 3D environments. We will release our code upon publication to encourage future research.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2507.12440.pdf' target='_blank'>https://arxiv.org/pdf/2507.12440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Xuxin Cheng, Ri-Zhao Qiu, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12440">EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: https://rchalyang.github.io/EgoVLA
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2506.19850.pdf' target='_blank'>https://arxiv.org/pdf/2506.19850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19850">Unified Vision-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2505.10415.pdf' target='_blank'>https://arxiv.org/pdf/2505.10415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuebo Ji, Zherong Pan, Xifeng Gao, Lei Yang, Xinxin Du, Kaiyun Li, Yongjin Liu, Wenping Wang, Changhe Tu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10415">Internal State Estimation in Groups via Active Information Gathering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating human internal states, such as personality traits or behavioral patterns, is critical for enhancing the effectiveness of human-robot interaction, particularly in group settings. These insights are key in applications ranging from social navigation to autism diagnosis. However, prior methods are limited by scalability and passive observation, making real-time estimation in complex, multi-human settings difficult. In this work, we propose a practical method for active human personality estimation in groups, with a focus on applications related to Autism Spectrum Disorder (ASD). Our method combines a personality-conditioned behavior model, based on the Eysenck 3-Factor theory, with an active robot information gathering policy that triggers human behaviors through a receding-horizon planner. The robot's belief about human personality is then updated via Bayesian inference. We demonstrate the effectiveness of our approach through simulations, user studies with typical adults, and preliminary experiments involving participants with ASD. Our results show that our method can scale to tens of humans and reduce personality prediction error by 29.2% and uncertainty by 79.9% in simulation. User studies with typical adults confirm the method's ability to generalize across complex personality distributions. Additionally, we explore its application in autism-related scenarios, demonstrating that the method can identify the difference between neurotypical and autistic behavior, highlighting its potential for diagnosing ASD. The results suggest that our framework could serve as a foundation for future ASD-specific interventions.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2503.22020.pdf' target='_blank'>https://arxiv.org/pdf/2503.22020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, Tsung-Yi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22020">CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Project website: https://cot-vla.github.io/
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2503.09527.pdf' target='_blank'>https://arxiv.org/pdf/2503.09527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Chen, Pi Bu, Yingyao Wang, Xinyi Wang, Ziming Wang, Jie Guo, Yingxiu Zhao, Qi Zhu, Jun Song, Siran Yang, Jiamang Wang, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09527">CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will open-source all resources, including the action tracker, dataset, benchmark, model weights, training code, and the implementation of the framework at https://combatvla.github.io/.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2410.05191.pdf' target='_blank'>https://arxiv.org/pdf/2410.05191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05191">LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building on the advancements of Large Language Models (LLMs) and Vision Language Models (VLMs), recent research has introduced Vision-Language-Action (VLA) models as an integrated solution for robotic manipulation tasks. These models take camera images and natural language task instructions as input and directly generate control actions for robots to perform specified tasks, greatly improving both decision-making capabilities and interaction with human users. However, the data-driven nature of VLA models, combined with their lack of interpretability, makes the assurance of their effectiveness and robustness a challenging task. This highlights the need for a reliable testing and evaluation platform. For this purpose, in this work, we propose LADEV, a comprehensive and efficient platform specifically designed for evaluating VLA models. We first present a language-driven approach that automatically generates simulation environments from natural language inputs, mitigating the need for manual adjustments and significantly improving testing efficiency. Then, to further assess the influence of language input on the VLA models, we implement a paraphrase mechanism that produces diverse natural language task instructions for testing. Finally, to expedite the evaluation process, we introduce a batch-style method for conducting large-scale testing of VLA models. Using LADEV, we conducted experiments on several state-of-the-art VLA models, demonstrating its effectiveness as a tool for evaluating these models. Our results showed that LADEV not only enhances testing efficiency but also establishes a solid baseline for evaluating VLA models, paving the way for the development of more intelligent and advanced robotic systems.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2409.12894.pdf' target='_blank'>https://arxiv.org/pdf/2409.12894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12894">VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative AI and multi-modal foundation models has shown significant potential in advancing robotic manipulation. Vision-language-action (VLA) models, in particular, have emerged as a promising approach for visuomotor control by leveraging large-scale vision-language data and robot demonstrations. However, current VLA models are typically evaluated using a limited set of hand-crafted scenes, leaving their general performance and robustness in diverse scenarios largely unexplored. To address this gap, we present VLATest, a fuzzing framework designed to generate robotic manipulation scenes for testing VLA models. Based on VLATest, we conducted an empirical study to assess the performance of seven representative VLA models. Our study results revealed that current VLA models lack the robustness necessary for practical deployment. Additionally, we investigated the impact of various factors, including the number of confounding objects, lighting conditions, camera poses, unseen objects, and task instruction mutations, on the VLA model's performance. Our findings highlight the limitations of existing VLA models, emphasizing the need for further research to develop reliable and trustworthy VLA applications.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2312.07062.pdf' target='_blank'>https://arxiv.org/pdf/2312.07062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanxing Lu, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07062">ThinkBot: Embodied Instruction Following with Thought Chain Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Instruction Following (EIF) requires agents to complete human instruction by interacting objects in complicated surrounding environments. Conventional methods directly consider the sparse human instruction to generate action plans for agents, which usually fail to achieve human goals because of the instruction incoherence in action descriptions. On the contrary, we propose ThinkBot that reasons the thought chain in human instruction to recover the missing action descriptions, so that the agent can successfully complete human goals by following the coherent instruction. Specifically, we first design an instruction completer based on large language models to recover the missing actions with interacted objects between consecutive human instruction, where the perceived surrounding environments and the completed sub-goals are considered for instruction completion. Based on the partially observed scene semantic maps, we present an object localizer to infer the position of interacted objects for agents to achieve complex human goals. Extensive experiments in the simulated environment show that our ThinkBot outperforms the state-of-the-art EIF methods by a sizable margin in both success rate and execution efficiency.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2312.05822.pdf' target='_blank'>https://arxiv.org/pdf/2312.05822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>William Wei Wang, Dongqi Han, Xufang Luo, Yifei Shen, Charles Ling, Boyu Wang, Dongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05822">Toward Open-ended Embodied Tasks Solving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empowering embodied agents, such as robots, with Artificial Intelligence (AI) has become increasingly important in recent years. A major challenge is task open-endedness. In practice, robots often need to perform tasks with novel goals that are multifaceted, dynamic, lack a definitive "end-state", and were not encountered during training. To tackle this problem, this paper introduces \textit{Diffusion for Open-ended Goals} (DOG), a novel framework designed to enable embodied AI to plan and act flexibly and dynamically for open-ended task goals. DOG synergizes the generative prowess of diffusion models with state-of-the-art, training-free guidance techniques to adaptively perform online planning and control. Our evaluations demonstrate that DOG can handle various kinds of novel task goals not seen during training, in both maze navigation and robot control problems. Our work sheds light on enhancing embodied AI's adaptability and competency in tackling open-ended goals.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2304.03767.pdf' target='_blank'>https://arxiv.org/pdf/2304.03767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Ding, Yan Xu, Zhenfang Chen, David Daniel Cox, Ping Luo, Joshua B. Tenenbaum, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03767">Embodied Concept Learner: Self-supervised Learning of Concepts and Mapping through Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans, even at a very early age, can learn visual concepts and understand geometry and layout through active interaction with the environment, and generalize their compositions to complete tasks described by natural languages in novel scenes. To mimic such capability, we propose Embodied Concept Learner (ECL) in an interactive 3D environment. Specifically, a robot agent can ground visual concepts, build semantic maps and plan actions to complete tasks by learning purely from human demonstrations and language instructions, without access to ground-truth semantic and depth supervisions from simulations. ECL consists of: (i) an instruction parser that translates the natural languages into executable programs; (ii) an embodied concept learner that grounds visual concepts based on language descriptions; (iii) a map constructor that estimates depth and constructs semantic maps by leveraging the learned concepts; and (iv) a program executor with deterministic policies to execute each program. ECL has several appealing benefits thanks to its modularized design. Firstly, it enables the robotic agent to learn semantics and depth unsupervisedly acting like babies, e.g., ground concepts through active interaction and perceive depth by disparities when moving forward. Secondly, ECL is fully transparent and step-by-step interpretable in long-term planning. Thirdly, ECL could be beneficial for the embodied instruction following (EIF), outperforming previous works on the ALFRED benchmark when the semantic label is not provided. Also, the learned concept can be reused for other downstream tasks, such as reasoning of object states. Project page: http://ecl.csail.mit.edu/
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2508.09071.pdf' target='_blank'>https://arxiv.org/pdf/2508.09071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09071">GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2508.01184.pdf' target='_blank'>https://arxiv.org/pdf/2508.01184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhang Wan, Dongqiang Gou, Xinwang Liu, En Zhu, Xuming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01184">Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A core problem of Embodied AI is to learn object manipulation from observation, as humans do. To achieve this, it is important to localize 3D object affordance areas through observation such as images (3D affordance grounding) and understand their functionalities (affordance classification). Previous attempts usually tackle these two tasks separately, leading to inconsistent predictions due to lacking proper modeling of their dependency. In addition, these methods typically only ground the incomplete affordance areas depicted in images, failing to predict the full potential affordance areas, and operate at a fixed scale, resulting in difficulty in coping with affordances significantly varying in scale with respect to the whole object. To address these issues, we propose a novel approach that learns an affordance-aware 3D representation and employs a stage-wise inference strategy leveraging the dependency between grounding and classification tasks. Specifically, we first develop a cross-modal 3D representation through efficient fusion and multi-scale geometric feature propagation, enabling inference of full potential affordance areas at a suitable regional scale. Moreover, we adopt a simple two-stage prediction mechanism, effectively coupling grounding and classification for better affordance understanding. Experiments demonstrate the effectiveness of our method, showing improved performance in both affordance grounding and classification.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2506.21117.pdf' target='_blank'>https://arxiv.org/pdf/2506.21117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Ackermann, Jonas Kulhanek, Shengqu Cai, Haofei Xu, Marc Pollefeys, Gordon Wetzstein, Leonidas Guibas, Songyou Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21117">CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2505.12278.pdf' target='_blank'>https://arxiv.org/pdf/2505.12278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Luo, Chen Tessler, Toru Lin, Ye Yuan, Tairan He, Wenli Xiao, Yunrong Guo, Gal Chechik, Kris Kitani, Linxi Fan, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12278">Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human behavior is fundamentally shaped by visual perception -- our ability to interact with the world depends on actively gathering relevant information and adapting our movements accordingly. Behaviors like searching for objects, reaching, and hand-eye coordination naturally emerge from the structure of our sensory system. Inspired by these principles, we introduce Perceptive Dexterous Control (PDC), a framework for vision-driven dexterous whole-body control with simulated humanoids. PDC operates solely on egocentric vision for task specification, enabling object search, target placement, and skill selection through visual cues, without relying on privileged state information (e.g., 3D object positions and geometries). This perception-as-interface paradigm enables learning a single policy to perform multiple household tasks, including reaching, grasping, placing, and articulated object manipulation. We also show that training from scratch with reinforcement learning can produce emergent behaviors such as active search. These results demonstrate how vision-driven control and complex tasks induce human-like behaviors and can serve as the key ingredients in closing the perception-action loop for animation, robotics, and embodied AI.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2504.01252.pdf' target='_blank'>https://arxiv.org/pdf/2504.01252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuhiro Sasabuchi, Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Katsushi Ikeuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01252">Plan-and-Act using Large Language Models for Interactive Agreement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large language models (LLMs) are capable of planning robot actions. In this paper, we explore how LLMs can be used for planning actions with tasks involving situational human-robot interaction (HRI). A key problem of applying LLMs in situational HRI is balancing between "respecting the current human's activity" and "prioritizing the robot's task," as well as understanding the timing of when to use the LLM to generate an action plan. In this paper, we propose a necessary plan-and-act skill design to solve the above problems. We show that a critical factor for enabling a robot to switch between passive / active interaction behavior is to provide the LLM with an action text about the current robot's action. We also show that a second-stage question to the LLM (about the next timing to call the LLM) is necessary for planning actions at an appropriate timing. The skill design is applied to an Engage skill and is tested on four distinct interaction scenarios. We show that by using the skill design, LLMs can be leveraged to easily scale to different HRI scenarios with a reasonable success rate reaching 90% on the test scenarios.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2503.15491.pdf' target='_blank'>https://arxiv.org/pdf/2503.15491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuhiro Sasabuchi, Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Katsushi Ikeuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15491">Agreeing to Interact in Human-Robot Interaction using Large Language Models and Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction (HRI), the beginning of an interaction is often complex. Whether the robot should communicate with the human is dependent on several situational factors (e.g., the current human's activity, urgency of the interaction, etc.). We test whether large language models (LLM) and vision language models (VLM) can provide solutions to this problem. We compare four different system-design patterns using LLMs and VLMs, and test on a test set containing 84 human-robot situations. The test set mixes several publicly available datasets and also includes situations where the appropriate action to take is open-ended. Our results using the GPT-4o and Phi-3 Vision model indicate that LLMs and VLMs are capable of handling interaction beginnings when the desired actions are clear, however, challenge remains in the open-ended situations where the model must balance between the human and robot situation.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2411.00785.pdf' target='_blank'>https://arxiv.org/pdf/2411.00785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00785">IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2410.01273.pdf' target='_blank'>https://arxiv.org/pdf/2410.01273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhwan Choi, Yongjun Cho, Minchan Kim, Jaeyoon Jung, Myunchul Joe, Yubeen Park, Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, Jiwan Chung, Youngjae Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01273">CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2409.04965.pdf' target='_blank'>https://arxiv.org/pdf/2409.04965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congcong Wen, Yifan Liu, Geeta Chandra Raju Bethala, Shuaihang Yuan, Hao Huang, Yu Hao, Mengyu Wang, Yu-Shen Liu, Anthony Tzes, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04965">Socially-Aware Robot Navigation Enhanced by Bidirectional Natural Language Conversations Using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot navigation is crucial across various domains, yet traditional methods focus on efficiency and obstacle avoidance, often overlooking human behavior in shared spaces. With the rise of service robots, socially aware navigation has gained prominence. However, existing approaches primarily predict pedestrian movements or issue alerts, lacking true human-robot interaction. We introduce Hybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), a novel framework for socially aware navigation. By integrating deep reinforcement learning with large language models, HSAC-LLM enables bidirectional natural language interactions, predicting both continuous and discrete navigation actions. When potential collisions arise, the robot proactively communicates with pedestrians to determine avoidance strategies. Experiments in 2D simulation, Gazebo, and real-world environments demonstrate that HSAC-LLM outperforms state-of-the-art DRL methods in interaction, navigation, and obstacle avoidance. This paradigm advances effective human-robot interactions in dynamic settings. Videos are available at https://hsacllm.github.io/.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2406.11818.pdf' target='_blank'>https://arxiv.org/pdf/2406.11818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Hang Yin, Yinan Liang, Angyuan Ma, Jiwen Lu, Haibin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11818">Embodied Instruction Following in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling embodied agents to complete complex human instructions from natural language is crucial to autonomous systems in household services. Conventional methods can only accomplish human instructions in the known environment where all interactive objects are provided to the embodied agent, and directly deploying the existing approaches for the unknown environment usually generates infeasible plans that manipulate non-existing objects. On the contrary, we propose an embodied instruction following (EIF) method for complex tasks in the unknown environment, where the agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. Specifically, we build a hierarchical embodied instruction following framework including the high-level task planner and the low-level exploration controller with multimodal large language models. We then construct a semantic representation map of the scene with dynamic region attention to demonstrate the known visual clues, where the goal of task planning and scene exploration is aligned for human instruction. For the task planner, we generate the feasible step-by-step plans for human goal accomplishment according to the task completion process and the known visual clues. For the exploration controller, the optimal navigation or object interaction policy is predicted based on the generated step-wise plans and the known visual clues. The experimental results demonstrate that our method can achieve 45.09% success rate in 204 complex human instructions such as making breakfast and tidying rooms in large house-level scenes. Code and supplementary are available at https://gary3410.github.io/eif_unknown.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2405.11286.pdf' target='_blank'>https://arxiv.org/pdf/2405.11286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11286">Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2307.01848.pdf' target='_blank'>https://arxiv.org/pdf/2307.01848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01848">Embodied Task Planning with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2305.17066.pdf' target='_blank'>https://arxiv.org/pdf/2305.17066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, RÃ³bert CsordÃ¡s, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li, Shuming Liu, Jinjie Mai, Piotr PiÄkos, Aditya Ramesh, Imanol Schlag, Weimin Shi, Aleksandar StaniÄ, Wenyi Wang, Yuhui Wang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, JÃ¼rgen Schmidhuber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17066">Mindstorms in Natural Language-Based Societies of Mind</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Both Minsky's "society of mind" and Schmidhuber's "learning to think" inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a "mindstorm." Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs), new agents -- all communicating through the same universal symbolic language -- are easily added in a modular fashion. To demonstrate the power of NLSOMs, we assemble and experiment with several of them (having up to 129 members), leveraging mindstorms in them to solve some practical AI tasks: visual question answering, image captioning, text-to-image synthesis, 3D generation, egocentric retrieval, embodied AI, and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents-some of which may be humans. And with this emergence of great societies of heterogeneous minds, many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM? What would be the (dis)advantages of having a monarchical rather than a democratic structure? How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM? In this work, we identify, discuss, and try to answer some of these questions.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2305.00970.pdf' target='_blank'>https://arxiv.org/pdf/2305.00970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuyuan Huang, Jae Sung Park, Abhinav Gupta, Paul Bennett, Ran Gong, Subhojit Som, Baolin Peng, Owais Khan Mohammed, Chris Pal, Yejin Choi, Jianfeng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00970">ArK: Augmented Reality with Knowledge Interactive Emergent Ability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the growing adoption of mixed reality and interactive AI agents, it remains challenging for these systems to generate high quality 2D/3D scenes in unseen environments. The common practice requires deploying an AI agent to collect large amounts of data for model training for every new task. This process is costly, or even impossible, for many domains. In this study, we develop an infinite agent that learns to transfer knowledge memory from general foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene understanding and generation in the physical or virtual world. The heart of our approach is an emerging mechanism, dubbed Augmented Reality with Knowledge Inference Interaction (ArK), which leverages knowledge-memory to generate scenes in unseen physical world and virtual reality environments. The knowledge interactive emergent ability (Figure 1) is demonstrated as the observation learns i) micro-action of cross-modality: in multi-modality models to collect a large amount of relevant knowledge memory data for each interaction task (e.g., unseen scene understanding) from the physical reality; and ii) macro-behavior of reality-agnostic: in mix-reality environments to improve interactions that tailor to different characterized roles, target variables, collaborative information, and so on. We validate the effectiveness of ArK on the scene generation and editing tasks. We show that our ArK approach, combined with large foundation models, significantly improves the quality of generated 2D/3D scenes, compared to baselines, demonstrating the potential benefit of incorporating ArK in generative AI for applications such as metaverse and gaming simulation.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2509.08126.pdf' target='_blank'>https://arxiv.org/pdf/2509.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08126">Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: https://z.umn.edu/ogrg
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2507.13468.pdf' target='_blank'>https://arxiv.org/pdf/2507.13468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiye Cao, Maia Stiber, Amama Mahmood, Maria Teresa Parreira, Wendy Ju, Micol Spitale, Hatice Gunes, Chien-Ming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13468">ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2507.10894.pdf' target='_blank'>https://arxiv.org/pdf/2507.10894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongtao He, Liuyi Wang, Lu Chen, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10894">NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2507.05198.pdf' target='_blank'>https://arxiv.org/pdf/2507.05198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05198">EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91\%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2505.23705.pdf' target='_blank'>https://arxiv.org/pdf/2505.23705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23705">Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2503.12974.pdf' target='_blank'>https://arxiv.org/pdf/2503.12974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueying Jiang, Wenhao Li, Xiaoqin Zhang, Ling Shao, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12974">Exploring 3D Reasoning-Driven Planning: From Implicit Human Intentions to Route-Aware Activity Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D task planning has attracted increasing attention in human-robot interaction and embodied AI thanks to the recent advances in multimodal learning. However, most existing studies are facing two common challenges: 1) heavy reliance on explicit instructions with little reasoning on implicit user intention; 2) negligence of inter-step route planning on robot moves. We address the above challenges by proposing 3D Reasoning-Driven Planning, a novel 3D task that reasons the intended activities from implicit instructions and decomposes them into steps with inter-step routes and planning under the guidance of fine-grained 3D object shapes and locations from scene segmentation. We tackle the new 3D task from two perspectives. First, we construct ReasonPlan3D, a large-scale benchmark that covers diverse 3D scenes with rich implicit instructions and detailed annotations for multi-step task planning, inter-step route planning, and fine-grained segmentation. Second, we design a novel framework that introduces progressive plan generation with contextual consistency across multiple steps, as well as a scene graph that is updated dynamically for capturing critical objects and their spatial relations. Extensive experiments demonstrate the effectiveness of our benchmark and framework in reasoning activities from implicit human instructions, producing accurate stepwise task plans and seamlessly integrating route planning for multi-step moves. The dataset and code will be released.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2503.11684.pdf' target='_blank'>https://arxiv.org/pdf/2503.11684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Micol Spitale, Srikar Babu, Serhan Cakmak, Jiaee Cheong, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11684">Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the primary goals of Human-Robot Interaction (HRI) research is to develop robots that can interpret human behavior and adapt their responses accordingly. Adaptive learning models, such as continual and reinforcement learning, play a crucial role in improving robots' ability to interact effectively in real-world settings. However, these models face significant challenges due to the limited availability of real-world data, particularly in sensitive domains like healthcare and well-being. This data scarcity can hinder a robot's ability to adapt to new situations. To address these challenges, causality provides a structured framework for understanding and modeling the underlying relationships between actions, events, and outcomes. By moving beyond mere pattern recognition, causality enables robots to make more explainable and generalizable decisions. This paper presents an exploratory causality-based analysis through a case study of an adaptive robotic coach delivering positive psychology exercises over four weeks in a workplace setting. The robotic coach autonomously adapts to multimodal human behaviors, such as facial valence and speech duration. By conducting both macro- and micro-level causal analyses, this study aims to gain deeper insights into how adaptability can enhance well-being during interactions. Ultimately, this research seeks to advance our understanding of how causality can help overcome challenges in HRI, particularly in real-world applications.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2503.08367.pdf' target='_blank'>https://arxiv.org/pdf/2503.08367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runling Long, Yunlong Wang, Jia Wan, Xiang Deng, Xinting Zhu, Weili Guan, Antoni B. Chan, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08367">Embodied Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occlusion is one of the fundamental challenges in crowd counting. In the community, various data-driven approaches have been developed to address this issue, yet their effectiveness is limited. This is mainly because most existing crowd counting datasets on which the methods are trained are based on passive cameras, restricting their ability to fully sense the environment. Recently, embodied navigation methods have shown significant potential in precise object detection in interactive scenes. These methods incorporate active camera settings, holding promise in addressing the fundamental issues in crowd counting. However, most existing methods are designed for indoor navigation, showing unknown performance in analyzing complex object distribution in large scale scenes, such as crowds. Besides, most existing embodied navigation datasets are indoor scenes with limited scale and object quantity, preventing them from being introduced into dense crowd analysis. Based on this, a novel task, Embodied Crowd Counting (ECC), is proposed. We first build up an interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables large scale scenes and large object quantity. A prior probability distribution that approximates realistic crowd distribution is introduced to generate crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method contains a MLLM driven coarse-to-fine navigation mechanism, enabling active Z-axis exploration, and a normal-line-based crowd distribution analysis method for fine counting. Experimental results against baselines show that the proposed method achieves the best trade-off between counting accuracy and navigation cost.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2503.06241.pdf' target='_blank'>https://arxiv.org/pdf/2503.06241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koji Inoue, Yuki Okafuji, Jun Baba, Yoshiki Ohira, Katsuya Hyodo, Tatsuya Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06241">A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2502.11918.pdf' target='_blank'>https://arxiv.org/pdf/2502.11918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runze Liu, Chenjia Bai, Jiafei Lyu, Shengjie Sun, Yali Du, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11918">VLP: Vision-Language Preference Learning for Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward engineering is one of the key challenges in Reinforcement Learning (RL). Preference-based RL effectively addresses this issue by learning from human feedback. However, it is both time-consuming and expensive to collect human preference labels. In this paper, we propose a novel \textbf{V}ision-\textbf{L}anguage \textbf{P}reference learning framework, named \textbf{VLP}, which learns a vision-language preference model to provide preference feedback for embodied manipulation tasks. To achieve this, we define three types of language-conditioned preferences and construct a vision-language preference dataset, which contains versatile implicit preference orders without human annotations. The preference model learns to extract language-related features, and then serves as a preference annotator in various downstream tasks. The policy can be learned according to the annotated preferences via reward learning or direct policy optimization. Extensive empirical results on simulated embodied manipulation tasks demonstrate that our method provides accurate preferences and generalizes to unseen tasks and unseen language instructions, outperforming the baselines by a large margin.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2412.09867.pdf' target='_blank'>https://arxiv.org/pdf/2412.09867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Haur Pang, Yahui Fu, Divesh Lala, Mikey Elmers, Koji Inoue, Tatsuya Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09867">Human-Like Embodied AI Interviewer: Employing Android ERICA in Real International Conference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the human-like embodied AI interviewer which integrates android robots equipped with advanced conversational capabilities, including attentive listening, conversational repairs, and user fluency adaptation. Moreover, it can analyze and present results post-interview. We conducted a real-world case study at SIGDIAL 2024 with 42 participants, of whom 69% reported positive experiences. This study demonstrated the system's effectiveness in conducting interviews just like a human and marked the first employment of such a system at an international conference. The demonstration video is available at https://youtu.be/jCuw9g99KuE.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2409.17114.pdf' target='_blank'>https://arxiv.org/pdf/2409.17114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Wolniakowski, Kanstantsin Miatliuk, Jose J. Quintana, Miguel A. Ferrer, Moises Diaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17114">Towards human-like kinematics in industrial robotic arms: a case study on a UR3 robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety in industrial robotic environments is a hot research topic in the area of human-robot interaction (HRI). Up to now, a robotic arm on an assembly line interacts with other machines away from human workers. Nowadays, robotic arm manufactures are aimed to their robots could increasingly perform tasks collaborating with humans. One of the ways to improve this collaboration is by making the movement of robots more humanlike. This way, it would be easier for a human to foresee the movement of the robot and approach it without fear of contact. The main difference between the movement of a human and of a robotic arm is that the former has a bell-shaped speed profile while the latter has a uniform speed one. To generate this speed profile, the kinematic theory of rapid human movements and its Sigma-Lognormal model has been used. This model is widely used to explain most of the basic phenomena related to the control of human movements. Both human-like and robotic-like movements are transferred to the UR3 robot. In this paper we detail the how the UR3 robot was programmed to produce both kinds of movement. The dissimilarities result between the input motion and output motion to the robot confirm the possibility to develop human-like velocities in the UR3 robot.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2407.06094.pdf' target='_blank'>https://arxiv.org/pdf/2407.06094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Micol Spitale, Maria Teresa Parreira, Maia Stiber, Minja Axelsson, Neval Kara, Garima Kankariya, Chien-Ming Huang, Malte Jung, Wendy Ju, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06094">ERR@HRI 2024 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent advancements in robotics and machine learning (ML), the deployment of autonomous robots in our everyday lives is still an open challenge. This is due to multiple reasons among which are their frequent mistakes, such as interrupting people or having delayed responses, as well as their limited ability to understand human speech, i.e., failure in tasks like transcribing speech to text. These mistakes may disrupt interactions and negatively influence human perception of these robots. To address this problem, robots need to have the ability to detect human-robot interaction (HRI) failures. The ERR@HRI 2024 challenge tackles this by offering a benchmark multimodal dataset of robot failures during human-robot interactions (HRI), encouraging researchers to develop and benchmark multimodal machine learning models to detect these failures. We created a dataset featuring multimodal non-verbal interaction data, including facial, speech, and pose features from video clips of interactions with a robotic coach, annotated with labels indicating the presence or absence of robot mistakes, user awkwardness, and interaction ruptures, allowing for the training and evaluation of predictive models. Challenge participants have been invited to submit their multimodal ML models for detection of robot errors and to be evaluated against various performance metrics such as accuracy, precision, recall, F1 score, with and without a margin of error reflecting the time-sensitivity of these metrics. The results of this challenge will help the research field in better understanding the robot failures in human-robot interactions and designing autonomous robots that can mitigate their own errors after successfully detecting them.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2407.02957.pdf' target='_blank'>https://arxiv.org/pdf/2407.02957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Micol Spitale, Minja Axelsson, Sooyeon Jeong, Paige TuttosÄ±, Caitlin A. Stamatis, Guy Laban, Angelica Lim, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02957">Past, Present, and Future: A Survey of The Evolution of Affective Robotics For Well-being</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in affective robots has recognized their potential in supporting human well-being. Due to rapidly developing affective and artificial intelligence technologies, this field of research has undergone explosive expansion and advancement in recent years. In order to develop a deeper understanding of recent advancements, we present a systematic review of the past 10 years of research in affective robotics for wellbeing. In this review, we identify the domains of well-being that have been studied, the methods used to investigate affective robots for well-being, and how these have evolved over time. We also examine the evolution of the multifaceted research topic from three lenses: technical, design, and ethical. Finally, we discuss future opportunities for research based on the gaps we have identified in our review -- proposing pathways to take affective robotics from the past and present to the future. The results of our review are of interest to human-robot interaction and affective computing researchers, as well as clinicians and well-being professionals who may wish to examine and incorporate affective robotics in their practices.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2407.01562.pdf' target='_blank'>https://arxiv.org/pdf/2407.01562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaee Cheong, Micol Spitale, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01562">Small but Fair! Fairness for Multimodal Human-Human and Robot-Human Mental Wellbeing Coaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the affective computing (AC) and human-robot interaction (HRI) research communities have put fairness at the centre of their research agenda. However, none of the existing work has addressed the problem of machine learning (ML) bias in HRI settings. In addition, many of the current datasets for AC and HRI are "small", making ML bias and debias analysis challenging. This paper presents the first work to explore ML bias analysis and mitigation of three small multimodal datasets collected within both a human-human and robot-human wellbeing coaching settings. The contributions of this work includes: i) being the first to explore the problem of ML bias and fairness within HRI settings; and ii) providing a multimodal analysis evaluated via modelling performance and fairness metrics across both high and low-level features and proposing a simple and effective data augmentation strategy (MixFeat) to debias the small datasets presented within this paper; and iii) conducting extensive experimentation and analyses to reveal ML fairness insights unique to AC and HRI research in order to distill a set of recommendations to aid AC and HRI researchers to be more engaged with fairness-aware ML-based research.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2406.19236.pdf' target='_blank'>https://arxiv.org/pdf/2406.19236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Li, Minghan Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19236">Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2406.09039.pdf' target='_blank'>https://arxiv.org/pdf/2406.09039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huy Hoang Nguyen, Minh Nhat Vu, Florian Beck, Gerald Ebmer, Anh Nguyen, Andreas Kugi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09039">Language-Driven Closed-Loop Grasping with Model-Predictive Trajectory Replanning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining a vision module inside a closed-loop control system for a \emph{seamless movement} of a robot in a manipulation task is challenging due to the inconsistent update rates between utilized modules. This task is even more difficult in a dynamic environment, e.g., objects are moving. This paper presents a \emph{modular} zero-shot framework for language-driven manipulation of (dynamic) objects through a closed-loop control system with real-time trajectory replanning and an online 6D object pose localization. We segment an object within $\SI{0.5}{\second}$ by leveraging a vision language model via language commands. Then, guided by natural language commands, a closed-loop system, including a unified pose estimation and tracking and online trajectory planning, is utilized to continuously track this object and compute the optimal trajectory in real-time. Our proposed zero-shot framework provides a smooth trajectory that avoids jerky movements and ensures the robot can grasp a non-stationary object. Experiment results exhibit the real-time capability of the proposed zero-shot modular framework for the trajectory optimization module to accurately and efficiently grasp moving objects, i.e., up to \SI{30}{\hertz} update rates for the online 6D pose localization module and \SI{10}{\hertz} update rates for the receding-horizon trajectory optimization. These advantages highlight the modular framework's potential applications in robotics and human-robot interaction; see the video in https://www.acin.tuwien.ac.at/en/6e64/.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2305.13681.pdf' target='_blank'>https://arxiv.org/pdf/2305.13681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiye Zhao, Yifan Sun, Feihan Li, Rui Chen, Ruixuan Liu, Tianhao Wei, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13681">GUARD: A Safe Reinforcement Learning Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state-of-the-art safe RL algorithms in various task settings using GUARD and establish baselines that future work can build on.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2304.01902.pdf' target='_blank'>https://arxiv.org/pdf/2304.01902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Micol Spitale, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01902">Affective Robotics For Wellbeing: A Scoping Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affective robotics research aims to better understand human social and emotional signals to improve human-robot interaction (HRI), and has been widely used during the last decade in multiple application fields. Past works have demonstrated, indeed, the potential of using affective robots (i.e., that can recognize, or interpret, or process, or simulate human affects) for healthcare applications, especially wellbeing. This paper systematically review the last decade (January 2013 - May 2022) of HRI literature to identify the main features of affective robotics for wellbeing. Specifically, we focused on the types of wellbeing goals affective robots addressed, their platforms, their shapes, their affective capabilities, and their autonomy in the surveyed studies. Based on this analysis, we list a set of recommendations that emerged, and we also present a research agenda to provide future directions to researchers in the field of affective robotics for wellbeing.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2212.14128.pdf' target='_blank'>https://arxiv.org/pdf/2212.14128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubin Kim, Huili Chen, Sharifa Alghowinem, Cynthia Breazeal, Hae Won Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14128">Joint Engagement Classification using Video Augmentation Techniques for Multi-person Human-robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affect understanding capability is essential for social robots to autonomously interact with a group of users in an intuitive and reciprocal way. However, the challenge of multi-person affect understanding comes from not only the accurate perception of each user's affective state (e.g., engagement) but also the recognition of the affect interplay between the members (e.g., joint engagement) that presents as complex, but subtle, nonverbal exchanges between them. Here we present a novel hybrid framework for identifying a parent-child dyad's joint engagement by combining a deep learning framework with various video augmentation techniques. Using a dataset of parent-child dyads reading storybooks together with a social robot at home, we first train RGB frame- and skeleton-based joint engagement recognition models with four video augmentation techniques (General Aug, DeepFake, CutOut, and Mixed) applied datasets to improve joint engagement classification performance. Second, we demonstrate experimental results on the use of trained models in the robot-parent-child interaction context. Third, we introduce a behavior-based metric for evaluating the learned representation of the models to investigate the model interpretability when recognizing joint engagement. This work serves as the first step toward fully unlocking the potential of end-to-end video understanding models pre-trained on large public datasets and augmented with data augmentation and visualization techniques for affect recognition in the multi-person human-robot interaction in the wild.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2209.07753.pdf' target='_blank'>https://arxiv.org/pdf/2209.07753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07753">Code as Policies: Language Model Programs for Embodied Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2508.16292.pdf' target='_blank'>https://arxiv.org/pdf/2508.16292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Han Hsieh, Elvis Hsieh, Dantong Niu, Trevor Darrell, Roei Herzig, David M. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16292">Do What? Teaching Vision-Language-Action Models to Reject the Impossible</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2508.12948.pdf' target='_blank'>https://arxiv.org/pdf/2508.12948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wei, Shaojie Zhang, Yonghao Dang, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12948">MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action recognition is a crucial task for intelligent robotics, particularly within the context of human-robot collaboration research. In self-supervised skeleton-based action recognition, the mask-based reconstruction paradigm learns the spatial structure and motion patterns of the skeleton by masking joints and reconstructing the target from unlabeled data. However, existing methods focus on a limited set of joints and low-order motion patterns, limiting the model's ability to understand complex motion patterns. To address this issue, we introduce MaskSem, a novel semantic-guided masking method for learning 3D hybrid high-order motion representations. This novel framework leverages Grad-CAM based on relative motion to guide the masking of joints, which can be represented as the most semantically rich temporal orgions. The semantic-guided masking process can encourage the model to explore more discriminative features. Furthermore, we propose using hybrid high-order motion as the reconstruction target, enabling the model to learn multi-order motion patterns. Specifically, low-order motion velocity and high-order motion acceleration are used together as the reconstruction target. This approach offers a more comprehensive description of the dynamic motion process, enhancing the model's understanding of motion patterns. Experiments on the NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla transformer, improves skeleton-based action recognition, making it more suitable for applications in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2507.04452.pdf' target='_blank'>https://arxiv.org/pdf/2507.04452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingdong Wu, Lehong Wu, Yizhuo Wu, Weiyao Huang, Hongwei Fan, Zheyuan Hu, Haoran Geng, Jinzhou Li, Jiahe Ying, Long Yang, Yuanpei Chen, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04452">SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous learning of dexterous, long-horizon robotic skills has been a longstanding pursuit of embodied AI. Recent advances in robotic reinforcement learning (RL) have demonstrated remarkable performance and robustness in real-world visuomotor control tasks. However, applying RL in the real world faces challenges such as low sample efficiency, slow exploration, and significant reliance on human intervention. In contrast, simulators offer a safe and efficient environment for extensive exploration and data collection, while the visual sim-to-real gap, often a limiting factor, can be mitigated using real-to-sim techniques. Building on these, we propose SimLauncher, a novel framework that combines the strengths of real-world RL and real-to-sim-to-real approaches to overcome these challenges. Specifically, we first pre-train a visuomotor policy in the digital twin simulation environment, which then benefits real-world RL in two ways: (1) bootstrapping target values using extensive simulated demonstrations and real-world demonstrations derived from pre-trained policy rollouts, and (2) Incorporating action proposals from the pre-trained policy for better exploration. We conduct comprehensive experiments across multi-stage, contact-rich, and dexterous hand manipulation tasks. Compared to prior real-world RL approaches, SimLauncher significantly improves sample efficiency and achieves near-perfect success rates. We hope this work serves as a proof of concept and inspires further research on leveraging large-scale simulation pre-training to benefit real-world robotic RL.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2506.16211.pdf' target='_blank'>https://arxiv.org/pdf/2506.16211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puhao Li, Yingying Wu, Ziheng Xi, Wanlin Li, Yuzhe Huang, Zhiyuan Zhang, Yinghan Chen, Jianan Wang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16211">ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies. In real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7% success rate while requiring only 10-20 demonstrations -- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. Additional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2506.05341.pdf' target='_blank'>https://arxiv.org/pdf/2506.05341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Ran, Yixuan Li, Linning Xu, Mulin Yu, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05341">Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2505.22050.pdf' target='_blank'>https://arxiv.org/pdf/2505.22050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, Bo Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22050">Reinforced Reasoning for Embodied Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2503.21860.pdf' target='_blank'>https://arxiv.org/pdf/2503.21860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailin Li, Puhao Li, Tengyu Liu, Yuyang Li, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21860">ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2410.03488.pdf' target='_blank'>https://arxiv.org/pdf/2410.03488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongcheng Wang, Peiqi Liu, Wenzhe Cai, Mingdong Wu, Zhengyu Qian, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03488">MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as ``I am thirsty.'' The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2409.18800.pdf' target='_blank'>https://arxiv.org/pdf/2409.18800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyou Zhu, Yanyuan Qiao, Siqi Zhang, Xingjian He, Qi Wu, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18800">MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced rapidly, yet the increasing size of models conflicts with the limited computational capabilities of Embodied AI platforms. To address this challenge, we aim to achieve both high model performance and practical deployability. Specifically, we focus on Vision-and-Language Navigation (VLN), a core task in Embodied AI. This paper introduces a two-stage knowledge distillation framework, producing a student model, MiniVLN, and showcasing the significant potential of distillation techniques in developing lightweight models. The proposed method aims to capture fine-grained knowledge during the pretraining phase and navigation-specific knowledge during the fine-tuning phase. Our findings indicate that the two-stage distillation approach is more effective in narrowing the performance gap between the teacher model and the student model compared to single-stage distillation. On the public R2R and REVERIE benchmarks, MiniVLN achieves performance on par with the teacher model while having only about 12% of the teacher model's parameter count.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2408.10899.pdf' target='_blank'>https://arxiv.org/pdf/2408.10899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Wang, Hao Zheng, Yunshuang Nie, Wenjun Xu, Qingwei Wang, Hua Ye, Zhe Li, Kaidong Zhang, Xuewen Cheng, Wanxi Dong, Chang Cai, Liang Lin, Feng Zheng, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10899">All Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is transforming how AI systems interact with the physical world, yet existing datasets are inadequate for developing versatile, general-purpose agents. These limitations include a lack of standardized formats, insufficient data diversity, and inadequate data volume. To address these issues, we introduce ARIO (All Robots In One), a new data standard that enhances existing datasets by offering a unified data format, comprehensive sensory modalities, and a combination of real-world and simulated data. ARIO aims to improve the training of embodied AI agents, increasing their robustness and adaptability across various tasks and environments. Building upon the proposed new standard, we present a large-scale unified ARIO dataset, comprising approximately 3 million episodes collected from 258 series and 321,064 tasks. The ARIO standard and dataset represent a significant step towards bridging the gaps of existing data resources. By providing a cohesive framework for data collection and representation, ARIO paves the way for the development of more powerful and versatile embodied AI agents, capable of navigating and interacting with the physical world in increasingly complex and diverse ways. The project is available on https://imaei.github.io/project_pages/ario/
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2404.07110.pdf' target='_blank'>https://arxiv.org/pdf/2404.07110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MatÃ­as Mattamala, Jonas Frey, Piotr Libera, Nived Chebrolu, Georg Martius, Cesar Cadena, Marco Hutter, Maurice Fallon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07110">Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN), an online self-supervised learning system for visual traversability estimation. The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing. One of the key ideas to achieve this is the use of high-dimensional features from pre-trained self-supervised models, which implicitly encode semantic information that massively simplifies the learning task. Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild. We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains. Code: https://bit.ly/498b0CV - Project page:https://bit.ly/3M6nMHH
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2402.02431.pdf' target='_blank'>https://arxiv.org/pdf/2402.02431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyuan Liu, Chen Chen, Songtao Wu, Fanyang Meng, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02431">Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities. Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations. Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2306.10531.pdf' target='_blank'>https://arxiv.org/pdf/2306.10531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyao Zhang, Mingdong Wu, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10531">GenPose: Generative Category-level Object Pose Estimation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object pose estimation plays a vital role in embodied AI and computer vision, enabling intelligent agents to comprehend and interact with their surroundings. Despite the practicality of category-level pose estimation, current approaches encounter challenges with partially observed point clouds, known as the multihypothesis issue. In this study, we propose a novel solution by reframing categorylevel object pose estimation as conditional generative modeling, departing from traditional point-to-point regression. Leveraging score-based diffusion models, we estimate object poses by sampling candidates from the diffusion model and aggregating them through a two-step process: filtering out outliers via likelihood estimation and subsequently mean-pooling the remaining candidates. To avoid the costly integration process when estimating the likelihood, we introduce an alternative method that trains an energy-based model from the original score-based model, enabling end-to-end likelihood estimation. Our approach achieves state-of-the-art performance on the REAL275 dataset, surpassing 50% and 60% on strict 5d2cm and 5d5cm metrics, respectively. Furthermore, our method demonstrates strong generalizability to novel categories sharing similar symmetric properties without fine-tuning and can readily adapt to object pose tracking tasks, yielding comparable results to the current state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2305.08510.pdf' target='_blank'>https://arxiv.org/pdf/2305.08510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Frey, Matias Mattamala, Nived Chebrolu, Cesar Cadena, Maurice Fallon, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08510">Fast Traversability Estimation for Wild Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our experiments were executed with a quadruped robot, ANYmal, the approach presented can generalize to any ground robot.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2509.18953.pdf' target='_blank'>https://arxiv.org/pdf/2509.18953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang, Tingsong Jiang, Weien Zhou, Wen Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18953">Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2509.09090.pdf' target='_blank'>https://arxiv.org/pdf/2509.09090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengyu Fang, Yijiang Liu, Yuan Du, Li Du, Huanrui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09090">SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct quantization or token pruning in an ad-hoc manner but fail to enable both for a holistic efficiency improvement due to an observed incompatibility. This work introduces SQAP-VLA, the first structured, training-free VLA inference acceleration framework that simultaneously enables state-of-the-art quantization and token pruning. We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on an aggressively quantized model while improving the quantizer design to enhance pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields significant gains in computational efficiency and inference speed while successfully preserving core model performance, achieving a $\times$1.93 speedup and up to a 4.5\% average success rate enhancement compared to the original model.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2508.13446.pdf' target='_blank'>https://arxiv.org/pdf/2508.13446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13446">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2505.11350.pdf' target='_blank'>https://arxiv.org/pdf/2505.11350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Ming Siang Tan, Shailesh, Boyang Liu, Alok Raj, Qi Xuan Ang, Weiheng Dai, Tanishq Duhan, Jimmy Chiun, Yuhong Cao, Florian Shkurti, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11350">Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To perform outdoor autonomous visual navigation and search, a robot may leverage satellite imagery as a prior map. This can help inform high-level search and exploration strategies, even when such images lack sufficient resolution to allow for visual recognition of targets. However, there are limited training datasets of satellite images with annotated targets that are not directly visible. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework with a flexible plug-and-play interface compatible with various input modalities (e.g. image, text, sound) and planning methods. First, we pretrain a satellite image encoder to align with CLIP's visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP's predictions during search using a test-time adaptation mechanism. Through a novel feedback loop inspired by Spatial Poisson Point Processes, uncertainty-weighted gradient updates are used to correct potentially inaccurate predictions and improve search performance. To train and evaluate Search-TTA, we curate AVS-Bench, a visual search dataset based on internet-scale ecological data that contains up to 380k training and 8k validation images (in- and out-domain). We find that Search-TTA improves planner performance by up to 30.0%, particularly in cases with poor initial CLIP predictions due to limited training data. It also performs comparably with significantly larger VLMs, and achieves zero-shot generalization to unseen modalities. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2505.08243.pdf' target='_blank'>https://arxiv.org/pdf/2505.08243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08243">Training Strategies for Efficient Embodied Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2505.05592.pdf' target='_blank'>https://arxiv.org/pdf/2505.05592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noriaki Hirose, Lydia Ignatova, Kyle Stachowicz, Catherine Glossop, Sergey Levine, Dhruv Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05592">Learning to Drive Anywhere with Model-Based Reannotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2505.05495.pdf' target='_blank'>https://arxiv.org/pdf/2505.05495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Zhou, Yilun Du, Yuncong Yang, Lei Han, Peihao Chen, Dit-Yan Yeung, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05495">Learning 3D Persistent Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2504.20995.pdf' target='_blank'>https://arxiv.org/pdf/2504.20995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20995">TesserAct: Learning 4D Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2504.03602.pdf' target='_blank'>https://arxiv.org/pdf/2504.03602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Lascheit, Daniel Barath, Marc Pollefeys, Leonidas Guibas, Francis Engelmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03602">Robust Human Registration with Body Part Segmentation on Noisy Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Registering human meshes to 3D point clouds is essential for applications such as augmented reality and human-robot interaction but often yields imprecise results due to noise and background clutter in real-world data. We introduce a hybrid approach that incorporates body-part segmentation into the mesh fitting process, enhancing both human pose estimation and segmentation accuracy. Our method first assigns body part labels to individual points, which then guide a two-step SMPL-X fitting: initial pose and orientation estimation using body part centroids, followed by global refinement of the point cloud alignment. Additionally, we demonstrate that the fitted human mesh can refine body part labels, leading to improved segmentation. Evaluations on the cluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that our approach significantly outperforms prior methods in both pose estimation and segmentation accuracy. Code and results are available on our project website: https://segfit.github.io
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2502.16976.pdf' target='_blank'>https://arxiv.org/pdf/2502.16976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An-Lan Wang, Nuo Chen, Kun-Yu Lin, Li Yuan-Ming, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16976">Task-Oriented 6-DoF Grasp Pose Detection in Clutters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In general, humans would grasp an object differently for different tasks, e.g., "grasping the handle of a knife to cut" vs. "grasping the blade to hand over". In the field of robotic grasp pose detection research, some existing works consider this task-oriented grasping and made some progress, but they are generally constrained by low-DoF gripper type or non-cluttered setting, which is not applicable for human assistance in real life. With an aim to get more general and practical grasp models, in this paper, we investigate the problem named Task-Oriented 6-DoF Grasp Pose Detection in Clutters (TO6DGC), which extends the task-oriented problem to a more general 6-DOF Grasp Pose Detection in Cluttered (multi-object) scenario. To this end, we construct a large-scale 6-DoF task-oriented grasping dataset, 6-DoF Task Grasp (6DTG), which features 4391 cluttered scenes with over 2 million 6-DoF grasp poses. Each grasp is annotated with a specific task, involving 6 tasks and 198 objects in total. Moreover, we propose One-Stage TaskGrasp (OSTG), a strong baseline to address the TO6DGC problem. Our OSTG adopts a task-oriented point selection strategy to detect where to grasp, and a task-oriented grasp generation module to decide how to grasp given a specific task. To evaluate the effectiveness of OSTG, extensive experiments are conducted on 6DTG. The results show that our method outperforms various baselines on multiple metrics. Real robot experiments also verify that our OSTG has a better perception of the task-oriented grasp points and 6-DoF grasp poses.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2502.09680.pdf' target='_blank'>https://arxiv.org/pdf/2502.09680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Albina Klepach, Alexander Nikulin, Ilya Zisman, Denis Tarasov, Alexander Derevyagin, Andrei Polubarov, Nikita Lyubaykin, Vladislav Kurenkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09680">Object-Centric Latent Action Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging vast amounts of unlabeled internet video data for embodied AI is currently bottlenecked by the lack of action labels and the presence of action-correlated visual distractors. Although recent latent action policy optimization (LAPO) has shown promise in inferring proxy-action labels from visual observations, its performance degrades significantly when distractors are present. To address this limitation, we propose a novel object-centric latent action learning framework that centers on objects rather than pixels. We leverage self-supervised object-centric pretraining to disentangle action-related and distracting dynamics. This allows LAPO to focus on task-relevant interactions, resulting in more robust proxy-action labels, enabling better imitation learning and efficient adaptation of the agent with just a few action-labeled trajectories. We evaluated our method in eight visually complex tasks across the Distracting Control Suite (DCS) and Distracting MetaWorld (DMW). Our results show that object-centric pretraining mitigates the negative effects of distractors by 50%, as measured by downstream task performance: average return (DCS) and success rate (DMW).
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2412.08591.pdf' target='_blank'>https://arxiv.org/pdf/2412.08591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfei Han, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08591">RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators. To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes $\sim$100K open-ended description-enriched trajectories with $\sim$200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2411.17735.pdf' target='_blank'>https://arxiv.org/pdf/2411.17735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17735">3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over extended periods. Existing representations, such as object-centric 3D scene graphs, oversimplify spatial relationships by modeling scenes as isolated objects with restrictive textual relationships, making it difficult to address queries requiring nuanced spatial understanding. Moreover, these representations lack natural mechanisms for active exploration and memory management, hindering their application to lifelong autonomy. In this work, we propose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem employs informative multi-view images, termed Memory Snapshots, to represent the scene and capture rich visual information of explored regions. It further integrates frontier-based exploration by introducing Frontier Snapshots-glimpses of unexplored areas-enabling agents to make informed decisions by considering both known and potential new information. To support lifelong memory in active exploration settings, we present an incremental construction pipeline for 3D-Mem, as well as a memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that 3D-Mem significantly enhances agents' exploration and reasoning capabilities in 3D environments, highlighting its potential for advancing applications in embodied AI.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2411.15590.pdf' target='_blank'>https://arxiv.org/pdf/2411.15590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixiang Yan, Dragan GaÅ¡eviÄ, Linxuan Zhao, Vanessa Echeverria, Yueqiao Jin, Roberto Martinez-Maldonado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15590">From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Learning Analytics (MMLA) leverages advanced sensing technologies and artificial intelligence to capture complex learning processes, but integrating diverse data sources into cohesive insights remains challenging. This study introduces a novel methodology for integrating latent class analysis (LCA) within MMLA to map monomodal behavioural indicators into parsimonious multimodal ones. Using a high-fidelity healthcare simulation context, we collected positional, audio, and physiological data, deriving 17 monomodal indicators. LCA identified four distinct latent classes: Collaborative Communication, Embodied Collaboration, Distant Interaction, and Solitary Engagement, each capturing unique monomodal patterns. Epistemic network analysis compared these multimodal indicators with the original monomodal indicators and found that the multimodal approach was more parsimonious while offering higher explanatory power regarding students' task and collaboration performances. The findings highlight the potential of LCA in simplifying the analysis of complex multimodal data while capturing nuanced, cross-modality behaviours, offering actionable insights for educators and enhancing the design of collaborative learning interventions. This study proposes a pathway for advancing MMLA, making it more parsimonious and manageable, and aligning with the principles of learner-centred education.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2410.11110.pdf' target='_blank'>https://arxiv.org/pdf/2410.11110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Soler Garcia, Petar Lukovic, Lucie Reynaud, Andrea Sgobbi, Federica Bruni, Martin Brun, Marc ZÃ¼nd, Riccardo Bollati, Marc Pollefeys, Hermann Blum, Zuria Bauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11110">HoloSpot: Intuitive Object Manipulation via Mixed Reality Drag-and-Drop</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction through mixed reality (MR) technologies enables novel, intuitive interfaces to control robots in remote operations. Such interfaces facilitate operations in hazardous environments, where human presence is risky, yet human oversight remains crucial. Potential environments include disaster response scenarios and areas with high radiation or toxic chemicals. In this paper we present an interface system projecting a 3D representation of a scanned room as a scaled-down 'dollhouse' hologram, allowing users to select and manipulate objects using a straightforward drag-and-drop interface. We then translate these drag-and-drop user commands into real-time robot actions based on the recent Spot-Compose framework. The Unity-based application provides an interactive tutorial and a user-friendly experience, ensuring ease of use. Through comprehensive end-to-end testing, we validate the system's capability in executing pick-and-place tasks and a complementary user study affirms the interface's intuitive controls. Our findings highlight the advantages of this interface in improving user experience and operational efficiency. This work lays the groundwork for a robust framework that advances the potential for seamless human-robot collaboration in diverse applications. Paper website: https://holospot.github.io/
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2409.17621.pdf' target='_blank'>https://arxiv.org/pdf/2409.17621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangshan Liu, Wenlong Dong, Jiankun Wang, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17621">Leveraging Semantic and Geometric Information for Zero-Shot Robot-to-Human Handover</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) encompasses a wide range of collaborative tasks, with handover being one of the most fundamental. As robots become more integrated into human environments, the potential for service robots to assist in handing objects to humans is increasingly promising. In robot-to-human (R2H) handover, selecting the optimal grasp is crucial for success, as it requires avoiding interference with the humans preferred grasp region and minimizing intrusion into their workspace. Existing methods either inadequately consider geometric information or rely on data-driven approaches, which often struggle to generalize across diverse objects. To address these limitations, we propose a novel zero-shot system that combines semantic and geometric information to generate optimal handover grasps. Our method first identifies grasp regions using semantic knowledge from vision-language models (VLMs) and, by incorporating customized visual prompts, achieves finer granularity in region grounding. A grasp is then selected based on grasp distance and approach angle to maximize human ease and avoid interference. We validate our approach through ablation studies and real-world comparison experiments. Results demonstrate that our system improves handover success rates and provides a more user-preferred interaction experience. Videos, appendixes and more are available at https://sites.google.com/view/vlm-handover/.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2409.03966.pdf' target='_blank'>https://arxiv.org/pdf/2409.03966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Chen, Yunchao Yao, Ruixuan Liu, Changliu Liu, Jeffrey Ichnowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03966">Automating Robot Failure Recovery Using Vision-Language Models With Optimized Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current robot autonomy struggles to operate beyond the assumed Operational Design Domain (ODD), the specific set of conditions and environments in which the system is designed to function, while the real-world is rife with uncertainties that may lead to failures. Automating recovery remains a significant challenge. Traditional methods often rely on human intervention to manually address failures or require exhaustive enumeration of failure cases and the design of specific recovery policies for each scenario, both of which are labor-intensive. Foundational Vision-Language Models (VLMs), which demonstrate remarkable common-sense generalization and reasoning capabilities, have broader, potentially unbounded ODDs. However, limitations in spatial reasoning continue to be a common challenge for many VLMs when applied to robot control and motion-level error recovery. In this paper, we investigate how optimizing visual and text prompts can enhance the spatial reasoning of VLMs, enabling them to function effectively as black-box controllers for both motion-level position correction and task-level recovery from unknown failures. Specifically, the optimizations include identifying key visual elements in visual prompts, highlighting these elements in text prompts for querying, and decomposing the reasoning process for failure detection and control generation. In experiments, prompt optimizations significantly outperform pre-trained Vision-Language-Action Models in correcting motion-level position errors and improve accuracy by 65.78% compared to VLMs with unoptimized prompts. Additionally, for task-level failures, optimized prompts enhanced the success rate by 5.8%, 5.8%, and 7.5% in VLMs' abilities to detect failures, analyze issues, and generate recovery plans, respectively, across a wide range of unknown errors in Lego assembly.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2406.07544.pdf' target='_blank'>https://arxiv.org/pdf/2406.07544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunze Man, Liang-Yan Gui, Yu-Xiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07544">Situational Awareness Matters in 3D Vision Language Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human-centered embodied AI. In this work, we demonstrate that a critical and distinct challenge in 3D vision language reasoning is situational awareness, which incorporates two key components: (1) The autonomous agent grounds its self-location based on a language prompt. (2) The agent answers open-ended questions from the perspective of its calculated position. To address this challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D vision language reasoning. We tokenize the 3D scene into sparse voxel representation and propose a language-grounded situation estimator, followed by a situated question answering module. Experiments on the SQA3D and ScanQA datasets show that SIG3D outperforms state-of-the-art models in situation estimation and question answering by a large margin (e.g., an enhancement of over 30% on situation estimation accuracy). Subsequent analysis corroborates our architectural design choices, explores the distinct functions of visual and textual tokens, and highlights the importance of situational awareness in the domain of 3D question answering.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2404.14565.pdf' target='_blank'>https://arxiv.org/pdf/2404.14565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Daniel Barath, Iro Armeni, Marc Pollefeys, Hermann Blum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14565">"Where am I?" Scene Retrieval with Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural language interfaces to embodied AI are becoming more ubiquitous in our daily lives. This opens up further opportunities for language-based interaction with embodied agents, such as a user verbally instructing an agent to execute some task in a specific location. For example, "put the bowls back in the cupboard next to the fridge" or "meet me at the intersection under the red sign." As such, we need methods that interface between natural language and map representations of the environment. To this end, we explore the question of whether we can use an open-set natural language query to identify a scene represented by a 3D scene graph. We define this task as "language-based scene-retrieval" and it is closely related to "coarse-localization," but we are instead searching for a match from a collection of disjoint scenes and not necessarily a large-scale continuous map. We present Text2SceneGraphMatcher, a "scene-retrieval" pipeline that learns joint embeddings between text descriptions and scene graphs to determine if they are a match. The code, trained models, and datasets will be made public.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2403.09631.pdf' target='_blank'>https://arxiv.org/pdf/2403.09631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09631">3D-VLA: A 3D Vision-Language-Action Generative World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2403.05313.pdf' target='_blank'>https://arxiv.org/pdf/2403.05313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05313">RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2403.05046.pdf' target='_blank'>https://arxiv.org/pdf/2403.05046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irving Fang, Yuzhong Chen, Yifan Wang, Jianghan Zhang, Qiushi Zhang, Jiali Xu, Xibo He, Weibo Gao, Hao Su, Yiming Li, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05046">EgoPAT3Dv2: Predicting 3D Action Target from 2D Egocentric Vision for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A robot's ability to anticipate the 3D action target location of a hand's movement from egocentric videos can greatly improve safety and efficiency in human-robot interaction (HRI). While previous research predominantly focused on semantic action classification or 2D target region prediction, we argue that predicting the action target's 3D coordinate could pave the way for more versatile downstream robotics tasks, especially given the increasing prevalence of headset devices. This study expands EgoPAT3D, the sole dataset dedicated to egocentric 3D action target prediction. We augment both its size and diversity, enhancing its potential for generalization. Moreover, we substantially enhance the baseline algorithm by introducing a large pre-trained model and human prior knowledge. Remarkably, our novel algorithm can now achieve superior prediction outcomes using solely RGB images, eliminating the previous need for 3D point clouds and IMU input. Furthermore, we deploy our enhanced baseline algorithm on a real-world robotic platform to illustrate its practical utility in straightforward HRI tasks. The demonstrations showcase the real-world applicability of our advancements and may inspire more HRI use cases involving egocentric vision. All code and data are open-sourced and can be found on the project website.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2401.14673.pdf' target='_blank'>https://arxiv.org/pdf/2401.14673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthik Mahadevan, Jonathan Chien, Noah Brown, Zhuo Xu, Carolina Parada, Fei Xia, Andy Zeng, Leila Takayama, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14673">Generative Expressive Robot Behaviors using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying "excuse me" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2311.11898.pdf' target='_blank'>https://arxiv.org/pdf/2311.11898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Pandya, Tianhao Wei, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11898">Multimodal Safe Control for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating safe behaviors for autonomous systems is important as they continue to be deployed in the real world, especially around people. In this work, we focus on developing a novel safe controller for systems where there are multiple sources of uncertainty. We formulate a novel multimodal safe control method, called the Multimodal Safe Set Algorithm (MMSSA) for the case where the agent has uncertainty over which discrete mode the system is in, and each mode itself contains additional uncertainty. To our knowledge, this is the first energy-function-based safe control method applied to systems with multimodal uncertainty. We apply our controller to a simulated human-robot interaction where the robot is uncertain of the human's true intention and each potential intention has its own additional uncertainty associated with it, since the human is not a perfectly rational actor. We compare our proposed safe controller to existing safe control methods and find that it does not impede the system performance (i.e. efficiency) while also improving the safety of the system.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2311.05997.pdf' target='_blank'>https://arxiv.org/pdf/2311.05997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05997">JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks, e.g., "chopping trees" to long-horizon tasks, e.g., "obtaining a diamond pickaxe". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. In the classic long-term task of $\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. The project page is available at https://craftjarvis.org/JARVIS-1
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2309.14893.pdf' target='_blank'>https://arxiv.org/pdf/2309.14893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Beber, Edoardo Lamon, Davide Nardi, Daniele Fontanelli, Matteo Saveriano, Luigi Palopoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14893">A Passive Variable Impedance Control Strategy with Viscoelastic Parameters Estimation of Soft Tissues for Safe Ultrasonography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of telehealth, robotic approaches have proven a valuable solution to in-person visits in remote areas, with decreased costs for patients and infection risks. In particular, in ultrasonography, robots have the potential to reproduce the skills required to acquire high-quality images while reducing the sonographer's physical efforts. In this paper, we address the control of the interaction of the probe with the patient's body, a critical aspect of ensuring safe and effective ultrasonography. We introduce a novel approach based on variable impedance control, allowing real-time optimisation of a compliant controller parameters during ultrasound procedures. This optimisation is formulated as a quadratic programming problem and incorporates physical constraints derived from viscoelastic parameter estimations. Safety and passivity constraints, including an energy tank, are also integrated to minimise potential risks during human-robot interaction. The proposed method's efficacy is demonstrated through experiments on a patient dummy torso, highlighting its potential for achieving safe behaviour and accurate force control during ultrasound procedures, even in cases of contact loss.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2306.14846.pdf' target='_blank'>https://arxiv.org/pdf/2306.14846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14846">ViNT: A Foundation Model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose pre-trained models ("foundation models") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page at https://visualnav-transformer.github.io.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2306.01874.pdf' target='_blank'>https://arxiv.org/pdf/2306.01874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noriaki Hirose, Dhruv Shah, Ajay Sridhar, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01874">SACSoN: Scalable Autonomous Control for Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. In this paper, our goal is to develop methods for training policies for socially unobtrusive navigation, such that robots can navigate among humans in ways that don't disturb human behavior. We introduce a definition for such behavior based on the counterfactual perturbation of the human: if the robot had not intruded into the space, would the human have acted in the same way? By minimizing this counterfactual perturbation, we can induce robots to behave in ways that do not alter the natural behavior of humans in the shared space. Instantiating this principle requires training policies to minimize their effect on human behavior, and this in turn requires data that allows us to model the behavior of humans in the presence of robots. Therefore, our approach is based on two key contributions. First, we collect a large dataset where an indoor mobile robot interacts with human bystanders. Second, we utilize this dataset to train policies that minimize counterfactual perturbation. We provide supplementary videos and make publicly available the largest-of-its-kind visual navigation dataset on our project page.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2304.14880.pdf' target='_blank'>https://arxiv.org/pdf/2304.14880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14880">SGAligner : 3D Scene Alignment with Scene Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building 3D scene graphs has recently emerged as a topic in scene representation for several embodied AI applications to represent the world in a structured and rich manner. With their increased use in solving downstream tasks (eg, navigation and room rearrangement), can we leverage and recycle them for creating 3D maps of environments, a pivotal step in agent operation? We focus on the fundamental problem of aligning pairs of 3D scene graphs whose overlap can range from zero to partial and can contain arbitrary changes. We propose SGAligner, the first method for aligning pairs of 3D scene graphs that is robust to in-the-wild scenarios (ie, unknown overlap -- if any -- and changes in the environment). We get inspired by multi-modality knowledge graphs and use contrastive learning to learn a joint, multi-modal embedding space. We evaluate on the 3RScan dataset and further showcase that our method can be used for estimating the transformation between pairs of 3D scenes. Since benchmarks for these tasks are missing, we create them on this dataset. The code, benchmark, and trained models are available on the project website.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2303.09192.pdf' target='_blank'>https://arxiv.org/pdf/2303.09192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang He, Irving Fang, Yiming Li, Rushi Bhavesh Shah, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09192">Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal. The two planners are jointly trained via deeply-supervised imitation learning from expert demonstrations. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. The resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. The source code is available at \url{https://ai4ce.github.io/DeepExplorer/}.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2303.03504.pdf' target='_blank'>https://arxiv.org/pdf/2303.03504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan K. Cosner, Yuxiao Chen, Karen Leung, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03504">Learning Responsibility Allocations for Safe Human-Robot Interaction with Applications to Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drivers have a responsibility to exercise reasonable care to avoid collision with other road users. This assumed responsibility allows interacting agents to maintain safety without explicit coordination. Thus to enable safe autonomous vehicle (AV) interactions, AVs must understand what their responsibilities are to maintain safety and how they affect the safety of nearby agents. In this work we seek to understand how responsibility is shared in multi-agent settings where an autonomous agent is interacting with human counterparts. We introduce Responsibility-Aware Control Barrier Functions (RA-CBFs) and present a method to learn responsibility allocations from data. By combining safety-critical control and learning-based techniques, RA-CBFs allow us to account for scene-dependent responsibility allocations and synthesize safe and efficient driving behaviors without making worst-case assumptions that typically result in overly-conservative behaviors. We test our framework using real-world driving data and demonstrate its efficacy as a tool for both safe control and forensic analysis of unsafe driving.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2509.19041.pdf' target='_blank'>https://arxiv.org/pdf/2509.19041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuning Zhang, Hong Jia, Simin Li, Ting Dang, Yongquan `Owen' Hu, Xin Yi, Hewu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19041">Position: Human-Robot Interaction in Embodied Intelligence Demands a Shift From Static Privacy Controls to Dynamic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reasoning capabilities of embodied agents introduce a critical, under-explored inferential privacy challenge, where the risk of an agent generate sensitive conclusions from ambient data. This capability creates a fundamental tension between an agent's utility and user privacy, rendering traditional static controls ineffective. To address this, this position paper proposes a framework that reframes privacy as a dynamic learning problem grounded in theory of Contextual Integrity (CI). Our approach enables agents to proactively learn and adapt to individual privacy norms through interaction, outlining a research agenda to develop embodied agents that are both capable and function as trustworthy safeguards of user privacy.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2509.12594.pdf' target='_blank'>https://arxiv.org/pdf/2509.12594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan, Peng Jia, Yahui Liu, Sheng Sun, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12594">The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2509.09332.pdf' target='_blank'>https://arxiv.org/pdf/2509.09332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09332">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2507.23682.pdf' target='_blank'>https://arxiv.org/pdf/2507.23682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, Jianyu Chen, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23682">villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2507.17220.pdf' target='_blank'>https://arxiv.org/pdf/2507.17220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiansong Wan, Chengming Zhou, Jinkua Liu, Xiangge Huang, Xiaoyu Chen, Xiaohan Yi, Qisen Yang, Baiting Zhu, Xin-Qiang Cai, Lixing Liu, Rushuai Yang, Chuheng Zhang, Sherif Abdelfattah, Hayong Shin, Pushi Zhang, Li Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17220">PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have explored pretrained (foundation) models for vision-based robotic navigation, aiming to achieve generalizable navigation and positive transfer across diverse environments while enhancing zero-shot performance in unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal Navigation), a new approach that further investigates pretraining strategies for vision-based navigation models and contributes in two key areas. Model-wise, we identify two critical design choices that consistently improve the performance of pretrained navigation models: (1) integrating an early-fusion network structure to combine visual observations and goal images via appropriately pretrained Vision Transformer (ViT) image encoder, and (2) introducing suitable auxiliary tasks to enhance global navigation representation learning, thus further improving navigation performance. Dataset-wise, we propose a novel data preprocessing pipeline for efficiently labeling large-scale game video datasets for navigation model training. We demonstrate that augmenting existing open navigation datasets with diverse gameplay videos improves model performance. Our model achieves an average improvement of 22.6% in zero-shot settings and a 37.5% improvement in fine-tuning settings over existing visual navigation foundation models in two complex simulated environments and one real-world environment. These results advance the state-of-the-art in pretrained image-goal navigation models. Notably, our model maintains competitive performance while requiring significantly less fine-tuning data, highlighting its potential for real-world deployment with minimal labeled supervision.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2507.12026.pdf' target='_blank'>https://arxiv.org/pdf/2507.12026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongtao Xu, Han Gao, Mingming Yu, Dong An, Shunpeng Chen, Changwei Wang, Li Guo, Xiaodan Liang, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12026">3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the growing need for diverse and scalable data in indoor scene tasks, such as question answering and dense captioning, we propose 3D-MoRe, a novel paradigm designed to generate large-scale 3D-language datasets by leveraging the strengths of foundational models. The framework integrates key components, including multi-modal embedding, cross-modal interaction, and a language model decoder, to process natural language instructions and 3D scene data. This approach facilitates enhanced reasoning and response generation in complex 3D environments. Using the ScanNet 3D scene dataset, along with text annotations from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs and 73,000 object descriptions across 1,513 scenes. We also employ various data augmentation techniques and implement semantic filtering to ensure high-quality data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms state-of-the-art baselines, with the CIDEr score improving by 2.15\%. Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5 by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated datasets will be publicly released to benefit the community, and both can be accessed on the https://3D-MoRe.github.io.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2506.13456.pdf' target='_blank'>https://arxiv.org/pdf/2506.13456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangye Ji, Yuan Meng, Hanyun Cui, Ye Li, Shengjia Hua, Lei Chen, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13456">Block-wise Adaptive Caching for Accelerating Diffusion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Policy has demonstrated strong visuomotor modeling capabilities, but its high computational cost renders it impractical for real-time robotic control. Despite huge redundancy across repetitive denoising steps, existing diffusion acceleration techniques fail to generalize to Diffusion Policy due to fundamental architectural and data divergences. In this paper, we propose Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by caching intermediate action features. BAC achieves lossless action generation acceleration by adaptively updating and reusing cached features at the block level, based on a key observation that feature similarities vary non-uniformly across timesteps and locks. To operationalize this insight, we first propose the Adaptive Caching Scheduler, designed to identify optimal update timesteps by maximizing the global feature similarities between cached and skipped features. However, applying this scheduler for each block leads to signiffcant error surges due to the inter-block propagation of caching errors, particularly within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop the Bubbling Union Algorithm, which truncates these errors by updating the upstream blocks with signiffcant caching errors before downstream FFNs. As a training-free plugin, BAC is readily integrable with existing transformer-based Diffusion Policy and vision-language-action models. Extensive experiments on multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference speedup for free.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2505.16278.pdf' target='_blank'>https://arxiv.org/pdf/2505.16278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16278">DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $Ï_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$Ï_0$. Specifically, we add Vision MoE to Drive-$Ï_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$Ï_0$.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2505.10105.pdf' target='_blank'>https://arxiv.org/pdf/2505.10105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10105">EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EmbodiedMAE, a unified 3D multi-modal representation for robot manipulation. Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information. To overcome these limitations, we enhance the DROID dataset with high-quality depth maps and point clouds, constructing DROID-3D as a valuable supplement for 3D embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion. Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms. The model exhibits strong scaling behavior with size and promotes effective policy learning from 3D inputs. Experimental results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly in precise tabletop manipulation settings where spatial perception is critical.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2505.09601.pdf' target='_blank'>https://arxiv.org/pdf/2505.09601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09601">Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2504.04573.pdf' target='_blank'>https://arxiv.org/pdf/2504.04573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieyi Zhang, Wenqiang Xu, Zhenjun Yu, Pengfei Xie, Tutian Tang, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04573">DexTOG: Learning Task-Oriented Dexterous Grasp with Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a novel language-guided diffusion-based learning framework, DexTOG, aimed at advancing the field of task-oriented grasping (TOG) with dexterous hands. Unlike existing methods that mainly focus on 2-finger grippers, this research addresses the complexities of dexterous manipulation, where the system must identify non-unique optimal grasp poses under specific task constraints, cater to multiple valid grasps, and search in a high degree-of-freedom configuration space in grasp planning. The proposed DexTOG includes a diffusion-based grasp pose generation model, DexDiffu, and a data engine to support the DexDiffu. By leveraging DexTOG, we also proposed a new dataset, DexTOG-80K, which was developed using a shadow robot hand to perform various tasks on 80 objects from 5 categories, showcasing the dexterity and multi-tasking capabilities of the robotic hand. This research not only presents a significant leap in dexterous TOG but also provides a comprehensive dataset and simulation validation, setting a new benchmark in robotic manipulation research.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2503.20220.pdf' target='_blank'>https://arxiv.org/pdf/2503.20220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20220">DINeMo: Learning Neural Mesh Models with no 3D Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2503.19901.pdf' target='_blank'>https://arxiv.org/pdf/2503.19901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, Jingbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19901">TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2503.19516.pdf' target='_blank'>https://arxiv.org/pdf/2503.19516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liming Zheng, Feng Yan, Fanfan Liu, Chengjian Feng, Yufeng Zhong, Lin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19516">Boosting Robotic Manipulation Generalization with Minimal Costly Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing adoption of Vision-Language-Action (VLA) models in embodied AI intensifies the demand for diverse manipulation demonstrations. However, high costs associated with data collection often result in insufficient data coverage across all scenarios, which limits the performance of the models. It is observed that the spatial reasoning phase (SRP) in large workspace dominates the failure cases. Fortunately, this data can be collected with low cost, underscoring the potential of leveraging inexpensive data to improve model performance. In this paper, we introduce the RoboTron-Craft, a stage-divided and cost-effective pipeline for realistic manipulation generation. Base on this, the RoboTron-Platter method is introduced, a framework that decouples training trajectories into distinct task stages and leverages abundant easily collectible SRP data to enhance VLA model's generalization. Through analysis we demonstrate that sub-task-specific training with additional SRP data with proper proportion can act as a performance catalyst for robot manipulation, maximizing the utilization of costly physical interaction phase (PIP) data. Experiments show that through introducing large proportion of cost-effective SRP trajectories into a limited set of PIP data, we can achieve a maximum improvement of 41\% on success rate in zero-shot scenes, while with the ability to transfer manipulation skill to novel targets. Project available at https://github.com/ notFoundThisPerson/RoboTron-Craft.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2503.18525.pdf' target='_blank'>https://arxiv.org/pdf/2503.18525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufeng Zhong, Chengjian Feng, Feng Yan, Fanfan Liu, Liming Zheng, Lin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18525">RoboTron-Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In language-guided visual navigation, agents locate target objects in unseen environments using natural language instructions. For reliable navigation in unfamiliar scenes, agents should possess strong perception, planning, and prediction capabilities. Additionally, when agents revisit previously explored areas during long-term navigation, they may retain irrelevant and redundant historical perceptions, leading to suboptimal results. In this work, we propose RoboTron-Nav, a unified framework that integrates perception, planning, and prediction capabilities through multitask collaborations on navigation and embodied question answering tasks, thereby enhancing navigation performances. Furthermore, RoboTron-Nav employs an adaptive 3D-aware history sampling strategy to effectively and efficiently utilize historical observations. By leveraging large language model, RoboTron-Nav comprehends diverse commands and complex visual scenes, resulting in appropriate navigation actions. RoboTron-Nav achieves an 81.1% success rate in object goal navigation on the $\mathrm{CHORES}$-$\mathbb{S}$ benchmark, setting a new state-of-the-art performance. Project page: https://yvfengzhong.github.io/RoboTron-Nav
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2502.14254.pdf' target='_blank'>https://arxiv.org/pdf/2502.14254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, Haoping Xu, Guowei Huang, Zhanpeng Zhang, Tongtong Cao, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang, Yingxue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14254">Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2502.10177.pdf' target='_blank'>https://arxiv.org/pdf/2502.10177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingcong Lei, Yiming Zhao, Ge Wang, Zhixin Mai, Shuguang Cui, Yatong Han, Jinke Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10177">STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2501.10074.pdf' target='_blank'>https://arxiv.org/pdf/2501.10074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Helong Huang, Guangjian Tian, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10074">SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning is an essential problem in embodied AI research. Efforts to enhance spatial reasoning abilities through supplementary spatial data and fine-tuning have proven limited and ineffective when addressing complex embodied tasks, largely due to their dependence on language-based outputs. While some approaches have introduced a point-based action space to mitigate this issue, they fall short in managing more intricate tasks within complex environments. This deficiency arises from their failure to fully exploit the inherent thinking and reasoning capabilities that are fundamental strengths of Vision-Language Models (VLMs). To address these limitations, we propose a novel approach named SpatialCoT, specifically designed to bolster the spatial reasoning capabilities of VLMs. Our approach comprises two stages: spatial coordinate bi-directional alignment, which aligns vision-language inputs with spatial coordinates, and chain-of-thought spatial grounding, which harnesses the reasoning capabilities of language models for advanced spatial reasoning. We evaluate SpatialCoT on challenging navigation and manipulation tasks, both in simulation and real-world settings. Experimental results demonstrate that our method significantly outperforms previous state-of-the-art approaches in both tasks.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2411.13927.pdf' target='_blank'>https://arxiv.org/pdf/2411.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueying Jiang, Lewei Lu, Ling Shao, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13927">Multimodal 3D Reasoning Segmentation with Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent development in multimodal learning has greatly advanced the research in 3D scene understanding in various real-world tasks such as embodied AI. However, most existing studies are facing two common challenges: 1) they are short of reasoning ability for interaction and interpretation of human intentions and 2) they focus on scenarios with single-category objects and over-simplified textual descriptions and neglect multi-object scenarios with complicated spatial relations among objects. We address the above challenges by proposing a 3D reasoning segmentation task for reasoning segmentation with multiple objects in scenes. The task allows producing 3D segmentation masks and detailed textual explanations as enriched by 3D spatial relations among objects. To this end, we create ReasonSeg3D, a large-scale and high-quality benchmark that integrates 3D segmentation masks and 3D spatial relations with generated question-answer pairs. In addition, we design MORE3D, a novel 3D reasoning network that works with queries of multiple objects and is tailored for 3D scene understanding. MORE3D learns detailed explanations on 3D relations and employs them to capture spatial information of objects and reason textual outputs. Extensive experiments show that MORE3D excels in reasoning and segmenting complex multi-object 3D scenes. In addition, the created ReasonSeg3D offers a valuable platform for future exploration of 3D reasoning segmentation. The data and code will be released.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2410.14682.pdf' target='_blank'>https://arxiv.org/pdf/2410.14682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhang, Yuening Wang, Hongjian Gu, Atia Hamidizadeh, Zhanguang Zhang, Yuecheng Liu, Yutong Wang, David Gamaliel Arcos Bravo, Junyi Dong, Shunbo Zhou, Tongtong Cao, Xingyue Quan, Yuzheng Zhuang, Yingxue Zhang, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14682">ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards Spatial-Temporal Cognition with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) have spurred numerous attempts to apply these technologies to embodied tasks, particularly focusing on high-level task planning and task decomposition. To further explore this area, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which specifically targets embodied task planning using LLMs. It features a controllable and diverse set of embodied tasks varying in different levels of difficulties and complexities, and is designed to evaluate two critical dimensions of LLMs' application in embodied task understanding: spatial (relation constraint, occlusion for target objects) and temporal & causal understanding of the sequence of actions in the environment. By using multi-source simulators as the backend simulator, it can provide immediate environment feedback to LLMs, which enables LLMs to interact dynamically with the environment and re-plan as necessary. We evaluated the state-of-the-art open source and closed source foundation models, including GPT-4, LLAMA and Mistral on our proposed benchmark. While they perform adequately well on simple navigation tasks, their performance can significantly deteriorate when faced with tasks that require a deeper understanding of spatial, temporal, and causal relationships. Thus, our benchmark distinguishes itself as a large-scale, quantifiable, highly automated, and fine-grained diagnostic framework that presents a significant challenge to the latest foundation models. We hope it can spark and drive further research in embodied task planning using foundation models.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2410.13691.pdf' target='_blank'>https://arxiv.org/pdf/2410.13691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13691">Jailbreaking LLM-Controlled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles. When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails. To assess the risks of deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from LLM-controlled robots, a phenomenon we experimentally demonstrate in three scenarios: (i) a white-box setting, wherein the attacker has full access to the NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker has partial access to a Clearpath Robotics Jackal UGV robot equipped with a GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each scenario and across three new datasets of harmful robotic actions, we demonstrate that RoboPAIR, as well as several static baselines, finds jailbreaks quickly and effectively, often achieving 100% attack success rates. Our results reveal, for the first time, that the risks of jailbroken LLMs extend far beyond text generation, given the distinct possibility that jailbroken robots could cause physical damage in the real world. Indeed, our results on the Unitree Go2 represent the first successful jailbreak of a deployed commercial robotic system. Addressing this emerging vulnerability is critical for ensuring the safe deployment of LLMs in robotics. Additional media is available at: https://robopair.org
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2410.13002.pdf' target='_blank'>https://arxiv.org/pdf/2410.13002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Makram Chahine, Alex Quach, Alaa Maalouf, Tsun-Hsuan Wang, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13002">Flex: End-to-End Text-Instructed Visual Navigation from Foundation Model Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end learning directly maps sensory inputs to actions, creating highly integrated and efficient policies for complex robotics tasks. However, such models often struggle to generalize beyond their training scenarios, limiting adaptability to new environments, tasks, and concepts. In this work, we investigate the minimal data requirements and architectural adaptations necessary to achieve robust closed-loop performance with vision-based control policies under unseen text instructions and visual distribution shifts. Our findings are synthesized in Flex (Fly lexically), a framework that uses pre-trained Vision Language Models (VLMs) as frozen patch-wise feature extractors, generating spatially aware embeddings that integrate semantic and visual information. We demonstrate the effectiveness of this approach on a quadrotor fly-to-target task, where agents trained via behavior cloning on a small simulated dataset successfully generalize to real-world scenes with diverse novel goals and command formulations.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2410.08901.pdf' target='_blank'>https://arxiv.org/pdf/2410.08901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosheng Li, Weixin Mao, Weipeng Deng, Chenyu Meng, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Hongan Wang, Xiaoming Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08901">SegGrasp: Zero-Shot Task-Oriented Grasping via Semantic and Geometric Guided Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping, which involves grasping specific parts of objects based on their functions, is crucial for developing advanced robotic systems capable of performing complex tasks in dynamic environments. In this paper, we propose a training-free framework that incorporates both semantic and geometric priors for zero-shot task-oriented grasp generation. The proposed framework, SegGrasp, first leverages the vision-language models like GLIP for coarse segmentation. It then uses detailed geometric information from convex decomposition to improve segmentation quality through a fusion policy named GeoFusion. An effective grasp pose can be generated by a grasping network with improved segmentation. We conducted the experiments on both segmentation benchmark and real-world robot grasping. The experimental results show that SegGrasp surpasses the baseline by more than 15\% in grasp and segmentation performance.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2409.13822.pdf' target='_blank'>https://arxiv.org/pdf/2409.13822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Wang, Dezhong Zhao, Dayoon Suh, Ziqin Yuan, Guohua Chen, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13822">Personalization in Human-Robot Interaction through Preference-based Action Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preference-based reinforcement learning (PbRL) has shown significant promise for personalization in human-robot interaction (HRI) by explicitly integrating human preferences into the robot learning process. However, existing practices often require training a personalized robot policy from scratch, resulting in inefficient use of human feedback. In this paper, we propose preference-based action representation learning (PbARL), an efficient fine-tuning method that decouples common task structure from preference by leveraging pre-trained robot policies. Instead of directly fine-tuning the pre-trained policy with human preference, PbARL uses it as a reference for an action representation learning task that maximizes the mutual information between the pre-trained source domain and the target user preference-aligned domain. This approach allows the robot to personalize its behaviors while preserving original task performance and eliminates the need for extensive prior information from the source domain, thereby enhancing efficiency and practicality in real-world HRI scenarios. Empirical results on the Assistive Gym benchmark and a real-world user study (N=8) demonstrate the benefits of our method compared to state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2408.14997.pdf' target='_blank'>https://arxiv.org/pdf/2408.14997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Yu, Haixin Yu, Shoujie Li, Huang Yan, Ziwu Song, Wenbo Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14997">Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot Handover</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transparent objects are common in daily life, while their optical properties pose challenges for RGB-D cameras to capture accurate depth information. This issue is further amplified when these objects are hand-held, as hand occlusions further complicate depth estimation. For assistant robots, however, accurately perceiving hand-held transparent objects is critical to effective human-robot interaction. This paper presents a Hand-Aware Depth Restoration (HADR) method based on creating an implicit neural representation function from a single RGB-D image. The proposed method utilizes hand posture as an important guidance to leverage semantic and geometric information of hand-object interaction. To train and evaluate the proposed method, we create a high-fidelity synthetic dataset named TransHand-14K with a real-to-sim data generation scheme. Experiments show that our method has better performance and generalization ability compared with existing methods. We further develop a real-world human-to-robot handover system based on HADR, demonstrating its potential in human-robot interaction applications.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2407.08213.pdf' target='_blank'>https://arxiv.org/pdf/2407.08213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Ike Obi, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08213">PrefCLM: Enhancing Preference-based Reinforcement Learning with Crowdsourced Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preference-based reinforcement learning (PbRL) is emerging as a promising approach to teaching robots through human comparative feedback, sidestepping the need for complex reward engineering. However, the substantial volume of feedback required in existing PbRL methods often lead to reliance on synthetic feedback generated by scripted teachers. This approach necessitates intricate reward engineering again and struggles to adapt to the nuanced preferences particular to human-robot interaction (HRI) scenarios, where users may have unique expectations toward the same task. To address these challenges, we introduce PrefCLM, a novel framework that utilizes crowdsourced large language models (LLMs) as simulated teachers in PbRL. We utilize Dempster-Shafer Theory to fuse individual preferences from multiple LLM agents at the score level, efficiently leveraging their diversity and collective intelligence. We also introduce a human-in-the-loop pipeline that facilitates collective refinements based on user interactive feedback. Experimental results across various general RL tasks show that PrefCLM achieves competitive performance compared to traditional scripted teachers and excels in facilitating more more natural and efficient behaviors. A real-world user study (N=10) further demonstrates its capability to tailor robot behaviors to individual user preferences, significantly enhancing user satisfaction in HRI scenarios.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2402.05725.pdf' target='_blank'>https://arxiv.org/pdf/2402.05725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilong Mu, Runze Zhao, Zenan Lin, Yan Huang, Shoujie Li, Chenchang Li, Xiao-Ping Zhang, Wenbo Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05725">Dual-modal Tactile E-skin: Enabling Bidirectional Human-Robot Interaction via Integrated Tactile Perception and Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To foster an immersive and natural human-robot interaction, the implementation of tactile perception and feedback becomes imperative, effectively bridging the conventional sensory gap. In this paper, we propose a dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing and vibration feedback for enhanced human-robot interaction. The dual-modal tactile e-skin offers multi-functional tactile sensing and programmable haptic feedback, underpinned by a layered structure comprised of flexible magnetic films, soft silicone, a Hall sensor and actuator array, and a microcontroller unit. The e-skin captures the magnetic field changes caused by subtle deformations through Hall sensors, employing deep learning for accurate tactile perception. Simultaneously, the actuator array generates mechanical vibrations to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably, the dual-modal e-skin is capable of transmitting tactile information bidirectionally, enabling object recognition and fine-weighing operations. This bidirectional tactile interaction framework will enhance the immersion and efficiency of interactions between humans and robots.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2304.09349.pdf' target='_blank'>https://arxiv.org/pdf/2304.09349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinjie Mai, Jun Chen, Bing Li, Guocheng Qian, Mohamed Elhoseiny, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09349">LLM as A Robotic Brain: Unifying Egocentric Memory and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2303.18240.pdf' target='_blank'>https://arxiv.org/pdf/2303.18240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, Dhruv Batra, Yixin Lin, Oleksandr Maksymets, Aravind Rajeswaran, Franziska Meier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18240">Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2202.00199.pdf' target='_blank'>https://arxiv.org/pdf/2202.00199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyuan Fu, Wenqiang Xu, Ruolin Ye, Han Xue, Zhenjun Yu, Tutian Tang, Yutong Li, Wenxin Du, Jieyi Zhang, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.00199">RFUniverse: A Multiphysics Simulation Platform for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiphysics phenomena, the coupling effects involving different aspects of physics laws, are pervasive in the real world and can often be encountered when performing everyday household tasks. Intelligent agents which seek to assist or replace human laborers will need to learn to cope with such phenomena in household task settings. To equip the agents with such kind of abilities, the research community needs a simulation environment, which will have the capability to serve as the testbed for the training process of these intelligent agents, to have the ability to support multiphysics coupling effects. Though many mature simulation software for multiphysics simulation have been adopted in industrial production, such techniques have not been applied to robot learning or embodied AI research. To bridge the gap, we propose a novel simulation environment named RFUniverse. This simulator can not only compute rigid and multi-body dynamics, but also multiphysics coupling effects commonly observed in daily life, such as air-solid interaction, fluid-solid interaction, and heat transfer. Because of the unique multiphysics capacities of this simulator, we can benchmark tasks that involve complex dynamics due to multiphysics coupling effects in a simulation environment before deploying to the real world. RFUniverse provides multiple interfaces to let the users interact with the virtual world in various ways, which is helpful and essential for learning, planning, and control. We benchmark three tasks with reinforcement learning, including food cutting, water pushing, and towel catching. We also evaluate butter pushing with a classic planning-control paradigm. This simulator offers an enhancement of physics simulation in terms of the computation of multiphysics coupling effects.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2508.15201.pdf' target='_blank'>https://arxiv.org/pdf/2508.15201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15201">Survey of Vision-Language-Action Models for Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2506.14317.pdf' target='_blank'>https://arxiv.org/pdf/2506.14317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Chen, Qiyang Yan, Yuanpei Chen, Tianhao Wu, Jiyao Zhang, Zihan Ding, Jinzhou Li, Yaodong Yang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14317">ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a geometry and spatially-embedded scene representation and a novel comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2506.09049.pdf' target='_blank'>https://arxiv.org/pdf/2506.09049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09049">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2505.14235.pdf' target='_blank'>https://arxiv.org/pdf/2505.14235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yequan Wang, Aixin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14235">Toward Embodied AGI: A Review of Embodied AI and the Road Ahead</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial General Intelligence (AGI) is often envisioned as inherently embodied. With recent advances in robotics and foundational AI models, we stand at the threshold of a new era-one marked by increasingly generalized embodied AI systems. This paper contributes to the discourse by introducing a systematic taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing research and challenges at the foundational stages (L1-L2) and outline the key components required to achieve higher-level capabilities (L3-L5). Building on these insights and existing technologies, we propose a conceptual framework for an L3+ robotic brain, offering both a technical outlook and a foundation for future exploration.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2505.08664.pdf' target='_blank'>https://arxiv.org/pdf/2505.08664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerio Belcamino, Alessandro CarfÃ¬, Valeria Seidita, Fulvio Mastrogiovanni, Antonio Chella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08664">A Social Robot with Inner Speech for Dietary Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the use of inner speech as a mechanism to enhance transparency and trust in social robots for dietary advice. In humans, inner speech structures thought processes and decision-making; in robotics, it improves explainability by making reasoning explicit. This is crucial in healthcare scenarios, where trust in robotic assistants depends on both accurate recommendations and human-like dialogue, which make interactions more natural and engaging. Building on this, we developed a social robot that provides dietary advice, and we provided the architecture with inner speech capabilities to validate user input, refine reasoning, and generate clear justifications. The system integrates large language models for natural language understanding and a knowledge graph for structured dietary information. By making decisions more transparent, our approach strengthens trust and improves human-robot interaction in healthcare. We validated this by measuring the computational efficiency of our architecture and conducting a small user study, which assessed the reliability of inner speech in explaining the robot's behavior.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2503.16408.pdf' target='_blank'>https://arxiv.org/pdf/2503.16408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Qin, Li Kang, Xiufeng Song, Zhenfei Yin, Xiaohong Liu, Xihui Liu, Ruimao Zhang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16408">RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2503.13446.pdf' target='_blank'>https://arxiv.org/pdf/2503.13446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wu, Yuheng Zhou, Xiuwei Xu, Ziwei Wang, Haibin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13446">MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and environments because of the lack of large-scale training. In contrast, recent advances in vision-language-action (VLA) models have shown impressive generalization capabilities, but these foundation models are developed for fixed-base manipulation tasks. Therefore, we propose an efficient policy adaptation framework named MoManipVLA to transfer pre-trained VLA models of fix-base manipulation to mobile manipulation, so that high generalization ability across tasks and environments can be achieved in mobile manipulation policy. Specifically, we utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability. We design motion planning objectives for the mobile base and the robot arm, which aim at maximizing the physical feasibility of the trajectory. Finally, we present an efficient bi-level objective optimization framework for trajectory generation, where the upper-level optimization predicts waypoints for base movement to enhance the manipulator policy space, and the lower-level optimization selects the optimal end-effector trajectory to complete the manipulation task. In this way, MoManipVLA can adjust the position of the robot base in a zero-shot manner, thus making the waypoints predicted from the fixed-base VLA models feasible. Extensive experimental results on OVMM and the real world demonstrate that MoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile manipulation, and only requires 50 training cost for real world deployment due to the strong generalization ability in the pre-trained VLA models.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2503.04163.pdf' target='_blank'>https://arxiv.org/pdf/2503.04163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04163">VLA Model-Expert Collaboration for Bi-directional Manipulation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. Although these models have achieved significant improvements, their generalization in multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces expert workload relative to manual operation while simultaneously improving the reliability and generalization of VLA models. Furthermore, manipulation data collected during collaboration can further refine the VLA model, while human participants concurrently enhance their skills. This bi-directional learning loop boosts the overall performance of the collaboration system. Experimental results across various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and learning, as evidenced by improved success rates across tasks. Additionally, validation using a brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of low-speed action systems by involving VLA model during manipulation. These promising results pave the way for advancing human-robot interaction in the era of foundation models for robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2502.17894.pdf' target='_blank'>https://arxiv.org/pdf/2502.17894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiheng Liu, Yuxuan Wan, Jilong Wang, Yuxuan Kuang, Wenbo Cui, Xuesong Shi, Haoran Li, Dongbin Zhao, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17894">FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable object fetching in cluttered scenes remains a fundamental and application-critical challenge in embodied AI. Closely packed objects cause inevitable occlusions, making safe action generation particularly difficult. Under such partial observability, effective policies must not only generalize across diverse objects and layouts but also reason about occlusion to avoid collisions. However, collecting large-scale real-world data for this task remains prohibitively expensive, leaving this problem largely unsolved. In this paper, we introduce FetchBot, a sim-to-real framework for this challenge. We first curate a large-scale synthetic dataset featuring 1M diverse scenes and 500k representative demonstrations. Based on this dataset, FetchBot employs a depth-conditioned method for action generation, which leverages structural cues to enable robust obstacle-aware action planning. However, depth is perfect in simulation but noisy in real-world environments. To address this sim-to-real gap, FetchBot predicts depth from RGB inputs using a foundation model and integrates local occupancy prediction as a pre-training task, providing a generalizable latent representation for sim-to-real transfer. Extensive experiments in simulation and real-world environments demonstrate the strong zero-shot sim-to-real transfer, effective clutter handling, and adaptability to novel scenarios. In cluttered environments, it achieves an average real-world success rate of 89.95%, significantly outperforming prior methods. Moreover, FetchBot demonstrates excellent robustness in challenging cases, such as fetching transparent, reflective, and irregular objects, highlighting its practical value.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2501.03841.pdf' target='_blank'>https://arxiv.org/pdf/2501.03841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03841">OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2411.16310.pdf' target='_blank'>https://arxiv.org/pdf/2411.16310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaime Corsetti, Francesco Giuliari, Alice Fasoli, Davide Boscaini, Fabio Poiesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16310">Functionality understanding and segmentation in 3D scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like 'turn on the ceiling light', an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description. To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a vision and language model. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Project page: https://tev-fbk.github.io/fun3du/
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2411.03487.pdf' target='_blank'>https://arxiv.org/pdf/2411.03487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Wang, Qiming Liu, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03487">Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of visual navigation in unknown scenes, both "exploration" and "exploitation" are equally crucial. Robots must first establish environmental cognition through exploration and then utilize the cognitive information to accomplish target searches. However, most existing methods for image-goal navigation prioritize target search over the generation of exploratory behavior. To address this, we propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, which uses an implicit and compact scene representation, NeRF, as a cognitive structure. We estimate the uncertainty of NeRF and augment the exploratory ability by the uncertainty to in turn facilitate the construction of implicit representation. Simultaneously, we extract memory information from NeRF to enhance the robot's reasoning ability for determining the location of the target. Ultimately, we seamlessly combine the two generated abilities to produce navigational actions. Our pipeline is end-to-end, with the environmental cognitive structure being constructed online. Extensive experimental results on image-goal navigation demonstrate the capability of our pipeline to enhance exploratory behaviors, while also enabling a natural transition from the exploration to exploitation phase. This enables our model to outperform existing memory-based cognitive navigation structures in terms of navigation performance.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2406.17898.pdf' target='_blank'>https://arxiv.org/pdf/2406.17898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoqun Xu, Yang Liu, Xiaoqi Li, Jiyao Zhang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17898">Human-centered In-building Embodied Delivery Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the concept of embodied intelligence has been widely accepted and popularized, leading people to naturally consider the potential for commercialization in this field. In this work, we propose a specific commercial scenario simulation, human-centered in-building embodied delivery. Furthermore, for this scenario, we have developed a brand-new virtual environment system from scratch, constructing a multi-level connected building space modeled after a polar research station. This environment also includes autonomous human characters and robots with grasping and mobility capabilities, as well as a large number of interactive items. Based on this environment, we have built a delivery dataset containing 13k language instructions to guide robots in providing services. We simulate human behavior through human characters and sample their various needs in daily life. Finally, we proposed a method centered around a large multimodal model to serve as the baseline system for this dataset. Compared to past embodied data work, our work focuses on a virtual environment centered around human-robot interaction for commercial scenarios. We believe this will bring new perspectives and exploration angles to the embodied community.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2404.15366.pdf' target='_blank'>https://arxiv.org/pdf/2404.15366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao-Yin Liu, Guotao Li, Xiao-Hu Zhou, Xu Liang, Zeng-Guang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15366">A Weight-aware-based Multi-source Unsupervised Domain Adaptation Method for Human Motion Intention Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recognition of human motion intention (HMI) is beneficial for exoskeleton robots to improve the wearing comfort level and achieve natural human-robot interaction. A classifier trained on labeled source subjects (domains) performs poorly on unlabeled target subject since the difference in individual motor characteristics. The unsupervised domain adaptation (UDA) method has become an effective way to this problem. However, the labeled data are collected from multiple source subjects that might be different not only from the target subject but also from each other. The current UDA methods for HMI recognition ignore the difference between each source subject, which reduces the classification accuracy. Therefore, this paper considers the differences between source subjects and develops a novel theory and algorithm for UDA to recognize HMI, where the margin disparity discrepancy (MDD) is extended to multi-source UDA theory and a novel weight-aware-based multi-source UDA algorithm (WMDD) is proposed. The source domain weight, which can be adjusted adaptively by the MDD between each source subject and target subject, is incorporated into UDA to measure the differences between source subjects. The developed multi-source UDA theory is theoretical and the generalization error on target subject is guaranteed. The theory can be transformed into an optimization problem for UDA, successfully bridging the gap between theory and algorithm. Moreover, a lightweight network is employed to guarantee the real-time of classification and the adversarial learning between feature generator and ensemble classifiers is utilized to further improve the generalization ability. The extensive experiments verify theoretical analysis and show that WMDD outperforms previous UDA methods on HMI recognition tasks.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2403.02274.pdf' target='_blank'>https://arxiv.org/pdf/2403.02274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia Fermuller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02274">NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in multimodal Human-Robot Interaction (HRI) datasets have highlighted the fusion of speech and gesture, expanding robots' capabilities to absorb explicit and implicit HRI insights. However, existing speech-gesture HRI datasets often focus on elementary tasks, like object pointing and pushing, revealing limitations in scaling to intricate domains and prioritizing human command data over robot behavior records. To bridge these gaps, we introduce NatSGD, a multimodal HRI dataset encompassing human commands through speech and gestures that are natural, synchronized with robot behavior demonstrations. NatSGD serves as a foundational resource at the intersection of machine learning and HRI research, and we demonstrate its effectiveness in training robots to understand tasks through multimodal human commands, emphasizing the significance of jointly considering speech and gestures. We have released our dataset, simulator, and code to facilitate future research in human-robot interaction system learning; access these resources at https://www.snehesh.com/natsgd/
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2311.05599.pdf' target='_blank'>https://arxiv.org/pdf/2311.05599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05599">SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. Project page: https://eth-ait.github.io/synthetic-handovers/
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2310.00465.pdf' target='_blank'>https://arxiv.org/pdf/2310.00465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linda Lastrico, Nuno Ferreira Duarte, Alessandro CarfÃ¬, Francesco Rea, Alessandra Sciutti, Fulvio Mastrogiovanni, JosÃ© Santos-Victor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00465">Expressing and Inferring Action Carefulness in Human-to-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Implicit communication plays such a crucial role during social exchanges that it must be considered for a good experience in human-robot interaction. This work addresses implicit communication associated with the detection of physical properties, transport, and manipulation of objects. We propose an ecological approach to infer object characteristics from subtle modulations of the natural kinematics occurring during human object manipulation. Similarly, we take inspiration from human strategies to shape robot movements to be communicative of the object properties while pursuing the action goals. In a realistic HRI scenario, participants handed over cups - filled with water or empty - to a robotic manipulator that sorted them. We implemented an online classifier to differentiate careful/not careful human movements, associated with the cups' content. We compared our proposed "expressive" controller, which modulates the movements according to the cup filling, against a neutral motion controller. Results show that human kinematics is adjusted during the task, as a function of the cup content, even in reach-to-grasp motion. Moreover, the carefulness during the handover of full cups can be reliably inferred online, well before action completion. Finally, although questionnaires did not reveal explicit preferences from participants, the expressive robot condition improved task efficiency.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2306.09273.pdf' target='_blank'>https://arxiv.org/pdf/2306.09273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Li, Wenhao Ding, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09273">Privacy Risks in Reinforcement Learning for Household Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advances in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly concerning reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the training process of the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervisory signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers. Nevertheless, these gradients contain sufficient information to potentially expose private data. To validate our approach, we conducted experiments on the AI2THOR simulator and evaluated our algorithm on active perception, a prevalent task in embodied AI. The experimental results demonstrate the effectiveness of our method in successfully reconstructing all information from the data in 120 room layouts. Check our website for videos.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2304.09413.pdf' target='_blank'>https://arxiv.org/pdf/2304.09413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehesh Shrestha, Ge Gao, Cornelia Fermuller, Yiannis Aloimonos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09413">Considerations for Minimizing Data Collection Biases for Eliciting Natural Behavior in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many of us researchers take extra measures to control for known-unknowns. However, unknown-unknowns can, at best, be negligible, but otherwise, they could produce unreliable data that might have dire consequences in real-life downstream applications. Human-Robot Interaction standards informed by empirical data could save us time and effort and provide us with the path toward the robots of the future. To this end, we share some of our pilot studies, lessons learned, and how they affected the outcome of our experiments. While these aspects might not be publishable in themselves, we hope our work might save time and effort for other researchers towards their research and serve as additional considerations for discussion at the workshop.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2304.00213.pdf' target='_blank'>https://arxiv.org/pdf/2304.00213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Botao Zhao, Mojtaba Esfandiari, David E. Usevitch, Peter Gehlbach, Iulian Iordachita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00213">Human-Robot Interaction in Retinal Surgery: A Comparative Study of Serial and Parallel Cooperative Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative robots for intraocular surgery allow surgeons to perform vitreoretinal surgery with high precision and stability. Several robot structural designs have shown capabilities to perform these surgeries. This research investigates the comparative performance of a serial and parallel cooperative-controlled robot in completing a retinal vessel-following task, with a focus on human-robot interaction performance and user experience. Our results indicate that despite differences in robot structure and interaction forces and torques, the two robots exhibited similar levels of performance in terms of general robot-to-patient interaction and average operating time. These findings have implications for the development and implementation of surgical robotics, suggesting that both serial and parallel cooperative-controlled robots can be effective for vitreoretinal surgery tasks.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2301.05821.pdf' target='_blank'>https://arxiv.org/pdf/2301.05821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangxin Liu, Zeyu Zhang, Ziyuan Jiao, Zhenliang Zhang, Minchen Li, Chenfanfu Jiang, Yixin Zhu, Song-Chun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05821">A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object's physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system's three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2509.20109.pdf' target='_blank'>https://arxiv.org/pdf/2509.20109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yinan Zheng, Yue Wang, Huimin Wang, Hang Zhao, Jingjing Liu, Xianyuan Zhan, Kun Zhan, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20109">Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2507.15729.pdf' target='_blank'>https://arxiv.org/pdf/2507.15729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jens V. RÃ¼ppel, Andrey Rudenko, Tim Schreiter, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15729">Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of Large Language Models (LLMs) creates an exciting potential for flexible, general knowledge-driven Human-Robot Interaction (HRI) systems for assistive robots. Existing HRI systems demonstrate great progress in interpreting and following user instructions, action generation, and robot task solving. On the other hand, bi-directional, multi-modal, and context-aware support of the user in collaborative tasks still remains an open challenge. In this paper, we present a gaze- and speech-informed interface to the assistive robot, which is able to perceive the working environment from multiple vision inputs and support the dynamic user in their tasks. Our system is designed to be modular and transferable to adapt to diverse tasks and robots, and it is capable of real-time use of language-based interaction state representation and fast on board perception modules. Its development was supported by multiple public dissemination events, contributing important considerations for improved robustness and user experience. Furthermore, in two lab studies, we compare the performance and user ratings of our system with those of a traditional scripted HRI pipeline. Our findings indicate that an LLM-based approach enhances adaptability and marginally improves user engagement and task execution metrics but may produce redundant output, while a scripted pipeline is well suited for more straightforward tasks.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2507.06484.pdf' target='_blank'>https://arxiv.org/pdf/2507.06484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan-Yun Sun, Shengguang Wu, Christian Jacobsen, Thomas Yim, Haoming Zou, Alex Zook, Shangru Li, Yu-Hsin Chou, Ethem Can, Xunlei Wu, Clemens Eppner, Valts Blukis, Jonathan Tremblay, Jiajun Wu, Stan Birchfield, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06484">3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite large-scale pretraining endowing models with language and vision reasoning capabilities, improving their spatial reasoning capability remains challenging due to the lack of data grounded in the 3D world. While it is possible for humans to manually create immersive and interactive worlds through 3D graphics, as seen in applications such as VR, gaming, and robotics, this process remains highly labor-intensive. In this paper, we propose a scalable method for generating high-quality 3D environments that can serve as training data for foundation models. We recast 3D environment building as a sequential decision-making problem, employing Vision-Language-Models (VLMs) as policies that output actions to jointly craft a 3D environment's layout, materials, lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to generate more prompt-aligned 3D environments via self-improvement fine-tuning. We demonstrate the effectiveness of 3D-Generalist and the proposed training strategy in generating simulation-ready 3D environments. Furthermore, we demonstrate its quality and scalability in synthetic data generation by pretraining a vision foundation model on the generated data. After fine-tuning the pre-trained model on downstream tasks, we show that it surpasses models pre-trained on meticulously human-crafted synthetic data and approaches results achieved with real data orders of magnitude larger.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2505.15197.pdf' target='_blank'>https://arxiv.org/pdf/2505.15197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxin Liu, Haiyang Liu, Luchuan Song, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15197">Intentional Gesture: Deliver Your Intentions with Gestures for Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When humans speak, gestures help convey communicative intentions, such as adding emphasis or describing concepts. However, current co-speech gesture generation methods rely solely on superficial linguistic cues (\textit{e.g.} speech audio or text transcripts), neglecting to understand and leverage the communicative intention that underpins human gestures. This results in outputs that are rhythmically synchronized with speech but are semantically shallow. To address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework that casts gesture generation as an intention-reasoning task grounded in high-level communicative functions. % First, we curate the \textbf{InG} dataset by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text sentences summarizing intentions), which are automatically annotated using large vision-language models. Next, we introduce the \textbf{Intentional Gesture Motion Tokenizer} to leverage these intention annotations. It injects high-level communicative functions (\textit{e.g.}, intentions) into tokenized motion representations to enable intention-aware gesture synthesis that are both temporally aligned and semantically meaningful, achieving new state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a modular foundation for expressive gesture generation in digital humans and embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2503.02247.pdf' target='_blank'>https://arxiv.org/pdf/2503.02247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02247">WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2502.17971.pdf' target='_blank'>https://arxiv.org/pdf/2502.17971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Andrey Rudenko, Jens V. RÃ¼ppel, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17971">Multimodal Interaction and Intention Communication for Industrial Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful adoption of industrial robots will strongly depend on their ability to safely and efficiently operate in human environments, engage in natural communication, understand their users, and express intentions intuitively while avoiding unnecessary distractions. To achieve this advanced level of Human-Robot Interaction (HRI), robots need to acquire and incorporate knowledge of their users' tasks and environment and adopt multimodal communication approaches with expressive cues that combine speech, movement, gazes, and other modalities. This paper presents several methods to design, enhance, and evaluate expressive HRI systems for non-humanoid industrial robots. We present the concept of a small anthropomorphic robot communicating as a proxy for its non-humanoid host, such as a forklift. We developed a multimodal and LLM-enhanced communication framework for this robot and evaluated it in several lab experiments, using gaze tracking and motion capture to quantify how users perceive the robot and measure the task progress.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2502.13175.pdf' target='_blank'>https://arxiv.org/pdf/2502.13175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenpeng Xing, Minghao Li, Mohan Li, Meng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13175">Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2502.10040.pdf' target='_blank'>https://arxiv.org/pdf/2502.10040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shichao Fan, Quantao Yang, Yajie Liu, Kun Wu, Zhengping Che, Qingjie Liu, Min Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10040">Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vision-Language-Action models (VLA) have advanced robot imitation learning, but high data collection costs and limited demonstrations hinder generalization and current imitation learning methods struggle in out-of-distribution scenarios, especially for long-horizon tasks. A key challenge is how to mitigate compounding errors in imitation learning, which lead to cascading failures over extended trajectories. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates 2D trajectories through a diffusion model to guide policy learning for long-horizon tasks. By leveraging task-relevant trajectories, DTP provides trajectory-level guidance to reduce error accumulation. Our two-stage approach first trains a generative vision-language model to create diffusion-based trajectories, then refines the imitation policy using them. Experiments on the CALVIN benchmark show that DTP outperforms state-of-the-art baselines by 25% in success rate, starting from scratch without external pretraining. Moreover, DTP significantly improves real-world robot performance.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2502.09142.pdf' target='_blank'>https://arxiv.org/pdf/2502.09142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchong Zhang, Bastian Orthmann, Michael C. Welle, Jonne Van Haastregt, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09142">LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded Robot Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of robotics and augmented reality (AR) presents transformative opportunities for advancing human-robot interaction (HRI) by improving usability, intuitiveness, and accessibility. This work introduces a controller-free, LLM-driven voice-commanded AR puppeteering system, enabling users to teleoperate a robot by manipulating its virtual counterpart in real time. By leveraging natural language processing (NLP) and AR technologies, our system -- prototyped using Meta Quest 3 -- eliminates the need for physical controllers, enhancing ease of use while minimizing potential safety risks associated with direct robot operation. A preliminary user demonstration successfully validated the system's functionality, demonstrating its potential for safer, more intuitive, and immersive robotic control.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2412.09624.pdf' target='_blank'>https://arxiv.org/pdf/2412.09624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09624">GenEx: Generating an Explorable World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2411.17347.pdf' target='_blank'>https://arxiv.org/pdf/2411.17347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Ansalone, Flavio Maiorana, Daniele Affinita, Flavio Volpi, Eugenio Bugli, Francesco Petri, Michele Brienza, Valerio Spagnoli, Vincenzo Suriani, Daniele Nardi, Domenico D. Bloisi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17347">Real-Time Multimodal Signal Processing for HRI in RoboCup: Understanding a Human Referee</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancing human-robot communication is crucial for autonomous systems operating in dynamic environments, where accurate real-time interpretation of human signals is essential. RoboCup provides a compelling scenario for testing these capabilities, requiring robots to understand referee gestures and whistle with minimal network reliance. Using the NAO robot platform, this study implements a two-stage pipeline for gesture recognition through keypoint extraction and classification, alongside continuous convolutional neural networks (CCNNs) for efficient whistle detection. The proposed approach enhances real-time human-robot interaction in a competitive setting like RoboCup, offering some tools to advance the development of autonomous systems capable of cooperating with humans.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2411.11844.pdf' target='_blank'>https://arxiv.org/pdf/2411.11844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11844">Generative World Explorer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can $\textit{imagine}$ unseen parts of the world through a mental exploration and $\textit{revise}$ their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the $\textit{Generative World Explorer (Genex)}$, an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) $\textit{Genex}$ can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2410.00425.pdf' target='_blank'>https://arxiv.org/pdf/2410.00425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Viswesh Nagaswamy Rajesh, Yong Woo Choi, Yen-Ru Chen, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00425">ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2408.05478.pdf' target='_blank'>https://arxiv.org/pdf/2408.05478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michele Brienza, Francesco Argenziano, Vincenzo Suriani, Domenico D. Bloisi, Daniele Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05478">Multi-Agent Planning Using Visual Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) and Visual Language Models (VLMs) are attracting increasing interest due to their improving performance and applications across various domains and tasks. However, LLMs and VLMs can produce erroneous results, especially when a deep understanding of the problem domain is required. For instance, when planning and perception are needed simultaneously, these models often struggle because of difficulties in merging multi-modal information. To address this issue, fine-tuned models are typically employed and trained on specialized data structures representing the environment. This approach has limited effectiveness, as it can overly complicate the context for processing. In this paper, we propose a multi-agent architecture for embodied task planning that operates without the need for specific data structures as input. Instead, it uses a single image of the environment, handling free-form domains by leveraging commonsense knowledge. We also introduce a novel, fully automatic evaluation procedure, PG2S, designed to better assess the quality of a plan. We validated our approach using the widely recognized ALFRED dataset, comparing PG2S to the existing KAS metric to further evaluate the quality of the generated plans.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2406.17333.pdf' target='_blank'>https://arxiv.org/pdf/2406.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mike Allenspach, Michael Pantic, Rik Girod, Lionel Ott, Roland Siegwart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17333">Task Adaptation in Industrial Human-Robot Interaction: Leveraging Riemannian Motion Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world industrial environments, modern robots often rely on human operators for crucial decision-making and mission synthesis from individual tasks. Effective and safe collaboration between humans and robots requires systems that can adjust their motion based on human intentions, enabling dynamic task planning and adaptation. Addressing the needs of industrial applications, we propose a motion control framework that (i) removes the need for manual control of the robot's movement; (ii) facilitates the formulation and combination of complex tasks; and (iii) allows the seamless integration of human intent recognition and robot motion planning. For this purpose, we leverage a modular and purely reactive approach for task parametrization and motion generation, embodied by Riemannian Motion Policies. The effectiveness of our method is demonstrated, evaluated, and compared to \remove{state-of-the-art approaches}\add{a representative state-of-the-art approach} in experimental scenarios inspired by realistic industrial Human-Robot Interaction settings.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2405.08711.pdf' target='_blank'>https://arxiv.org/pdf/2405.08711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Tesfazgi, Markus KeÃler, Emilio Trigili, Armin Lederer, Sandra Hirche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08711">Data-driven Force Observer for Human-Robot Interaction with Series Elastic Actuators using Gaussian Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction. Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components. While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments. In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression. By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate. Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications. We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2404.09613.pdf' target='_blank'>https://arxiv.org/pdf/2404.09613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yu, Shaocong Wang, Woyu Zhang, Xinyuan Zhang, Xiuzhe Wu, Yangu He, Jichang Yang, Yue Zhang, Ning Lin, Bo Wang, Xi Chen, Songqi Wang, Xumeng Zhang, Xiaojuan Qi, Zhongrui Wang, Dashan Shang, Qi Liu, Kwang-Ting Cheng, Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09613">Efficient and accurate neural field reconstruction using resistive memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human beings construct perception of space by integrating sparse observations into massively interconnected synapses and neurons, offering a superior parallelism and efficiency. Replicating this capability in AI finds wide applications in medical imaging, AR/VR, and embodied AI, where input data is often sparse and computing resources are limited. However, traditional signal reconstruction methods on digital computers face both software and hardware challenges. On the software front, difficulties arise from storage inefficiencies in conventional explicit signal representation. Hardware obstacles include the von Neumann bottleneck, which limits data transfer between the CPU and memory, and the limitations of CMOS circuits in supporting parallel processing. We propose a systematic approach with software-hardware co-optimizations for signal reconstruction from sparse inputs. Software-wise, we employ neural field to implicitly represent signals via neural networks, which is further compressed using low-rank decomposition and structured pruning. Hardware-wise, we design a resistive memory-based computing-in-memory (CIM) platform, featuring a Gaussian Encoder (GE) and an MLP Processing Engine (PE). The GE harnesses the intrinsic stochasticity of resistive memory for efficient input encoding, while the PE achieves precise weight mapping through a Hardware-Aware Quantization (HAQ) circuit. We demonstrate the system's efficacy on a 40nm 256Kb resistive memory-based in-memory computing macro, achieving huge energy efficiency and parallelism improvements without compromising reconstruction quality in tasks like 3D CT sparse reconstruction, novel view synthesis, and novel view synthesis for dynamic scenes. This work advances the AI-driven signal restoration technology and paves the way for future efficient and robust medical AI and 3D vision applications.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2403.13640.pdf' target='_blank'>https://arxiv.org/pdf/2403.13640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13640">LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2403.09285.pdf' target='_blank'>https://arxiv.org/pdf/2403.09285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Luigi Palmieri, Tomasz P. Kucner, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09285">THÃR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new large dataset of indoor human and robot navigation and interaction, called THÃR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. THÃR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, THÃR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. THÃR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2402.10729.pdf' target='_blank'>https://arxiv.org/pdf/2402.10729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viswa Narayanan Sankaranarayanan, Akshit Saradagi, Sumeet Satpute, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10729">A CBF-Adaptive Control Architecture for Visual Navigation for UAV in the Presence of Uncertainties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances. The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner. The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which is tracked by an adaptive controller to overcome modeling uncertainties and external disturbances. The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2307.06125.pdf' target='_blank'>https://arxiv.org/pdf/2307.06125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06125">Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real world that demonstrate that, with accurate perception, the decision making of HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics. These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2307.00841.pdf' target='_blank'>https://arxiv.org/pdf/2307.00841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Lucas Morillo-Mendez, Ravi T. Chadalavada, Andrey Rudenko, Erik Billing, Martin Magnusson, Kai O. Arras, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00841">Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly used in shared environments with humans, making effective communication a necessity for successful human-robot interaction. In our work, we study a crucial component: active communication of robot intent. Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an "Anthropomorphic Robotic Mock Driver" (ARMoD). We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instructions to collaborate on a joint task. The experiments feature two interaction styles of the ARMoD: a verbal-only mode using only speech and a multimodal mode, additionally including robotic gaze and pointing gestures to support communication and register intent in space. Our results show that the multimodal interaction style, including head movements and eye gaze as well as pointing gestures, leads to more natural fixation behavior. Participants naturally identified and fixated longer on the areas relevant for intent communication, and reacted faster to instructions in collaborative tasks. Our research further indicates that the ARMoD intent communication improves engagement and social interaction with mobile robots in workplace settings.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2303.13483.pdf' target='_blank'>https://arxiv.org/pdf/2303.13483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joy Hsu, Jiayuan Mao, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13483">NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grounding object properties and relations in 3D scenes is a prerequisite for a wide range of artificial intelligence tasks, such as visually grounded dialogues and embodied manipulation. However, the variability of the 3D domain induces two fundamental challenges: 1) the expense of labeling and 2) the complexity of 3D grounded language. Hence, essential desiderata for models are to be data-efficient, generalize to different data distributions and tasks with unseen semantic forms, as well as ground complex language semantics (e.g., view-point anchoring and multi-object reference). To address these challenges, we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates language into programs with hierarchical structures by leveraging large language-to-code models. Different functional modules in the programs are implemented as neural networks. Notably, NS3D extends prior neuro-symbolic visual reasoning methods by introducing functional modules that effectively reason about high-arity relations (i.e., relations among more than two objects), key in disambiguating objects in complex 3D scenes. Modular and compositional architecture enables NS3D to achieve state-of-the-art results on the ReferIt3D view-dependence task, a 3D referring expression comprehension benchmark. Importantly, NS3D shows significantly improved performance on settings of data-efficiency and generalization, and demonstrate zero-shot transfer to an unseen 3D question-answering task.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2303.12492.pdf' target='_blank'>https://arxiv.org/pdf/2303.12492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincenzo Suriani, Daniele Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12492">Preserving HRI Capabilities: Physical, Remote and Simulated Modalities in the SciRoc 2021 Competition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last years, robots are moving out of research laboratories to enter everyday life. Competitions aiming at benchmarking the capabilities of a robot in everyday scenarios are useful to make a step forward in this path. In fact, they foster the development of robust architectures capable of solving issues that might occur during human-robot coexistence in human-shaped scenarios. One of those competitions is SciRoc that, in its second edition, proposed new benchmarking environments. In particular, Episode 1 of SciRoc 2 proposed three different modalities of participation while preserving the Human-Robot Interaction (HRI), being a fundamental benchmarking functionality. The Coffee Shop environment, used to challenge the participating teams, represented an excellent testbed enabling for the benchmarking of different robotics functionalities, but also an exceptional opportunity for proposing novel solutions to guarantee real human-robot interaction procedures despite the Covid-19 pandemic restrictions. The developed software is publicly released.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2302.04659.pdf' target='_blank'>https://arxiv.org/pdf/2302.04659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04659">ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2212.00478.pdf' target='_blank'>https://arxiv.org/pdf/2212.00478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armin Lederer, Azra BegzadiÄ, Neha Das, Sandra Hirche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.00478">Safe Learning-Based Control of Elastic Joint Robots via Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety is of paramount importance in physical human-robot interaction applications. This requires both adherence to safety constraints defined on the system state, as well as guaranteeing compliant behavior of the robot. If the underlying dynamical system is known exactly, the former can be addressed with the help of control barrier functions. The incorporation of elastic actuators in the robot's mechanical design can address the latter requirement. However, this elasticity can increase the complexity of the resulting system, leading to unmodeled dynamics, such that control barrier functions cannot directly ensure safety. In this paper, we mitigate this issue by learning the unknown dynamics using Gaussian process regression. By employing the model in a feedback linearizing control law, the safety conditions resulting from control barrier functions can be robustified to take into account model errors, while remaining feasible. In order to enforce them on-line, we formulate the derived safety conditions in the form of a second-order cone program. We demonstrate our proposed approach with simulations on a two-degree-of-freedom planar robot with elastic joints.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2208.14925.pdf' target='_blank'>https://arxiv.org/pdf/2208.14925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Tomasz P. Kucner, Oscar Martinez Mozos, Martin Magnusson, Luigi Palmieri, Kai O. Arras, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.14925">The Magni Human Motion Dataset: Accurate, Complex, Multi-Modal, Natural, Semantically-Rich and Contextualized</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid development of social robots stimulates active research in human motion modeling, interpretation and prediction, proactive collision avoidance, human-robot interaction and co-habitation in shared spaces. Modern approaches to this end require high quality datasets for training and evaluation. However, the majority of available datasets suffers from either inaccurate tracking data or unnatural, scripted behavior of the tracked people. This paper attempts to fill this gap by providing high quality tracking information from motion capture, eye-gaze trackers and on-board robot sensors in a semantically-rich environment. To induce natural behavior of the recorded participants, we utilise loosely scripted task assignment, which induces the participants navigate through the dynamic laboratory environment in a natural and purposeful way. The motion dataset, presented in this paper, sets a high quality standard, as the realistic and accurate data is enhanced with semantic information, enabling development of new algorithms which rely not only on the tracking information but also on contextual cues of the moving agents, static and dynamic environment.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2111.14843.pdf' target='_blank'>https://arxiv.org/pdf/2111.14843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Younes, Daniel Honerkamp, Tim Welschehold, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.14843">Catch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped Environments with Moving Sounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual navigation combines sight and hearing to navigate to a sound-emitting source in an unmapped environment. While recent approaches have demonstrated the benefits of audio input to detect and find the goal, they focus on clean and static sound sources and struggle to generalize to unheard sounds. In this work, we propose the novel dynamic audio-visual navigation benchmark which requires catching a moving sound source in an environment with noisy and distracting sounds, posing a range of new challenges. We introduce a reinforcement learning approach that learns a robust navigation policy for these complex settings. To achieve this, we propose an architecture that fuses audio-visual information in the spatial feature space to learn correlations of geometric information inherent in both local maps and audio signals. We demonstrate that our approach consistently outperforms the current state-of-the-art by a large margin across all tasks of moving sounds, unheard sounds, and noisy environments, on two challenging 3D scanned real-world environments, namely Matterport3D and Replica. The benchmark is available at http://dav-nav.cs.uni-freiburg.de.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2509.15155.pdf' target='_blank'>https://arxiv.org/pdf/2509.15155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15155">Self-Improving Embodied Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at https://self-improving-efms.github.io .
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2509.14967.pdf' target='_blank'>https://arxiv.org/pdf/2509.14967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Davila, Jacinto Colan, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14967">Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot collaboration in surgery is affected by the inherent ambiguity of verbal communication. This paper presents a framework for a robotic surgical assistant that interprets and disambiguates verbal instructions from a surgeon by grounding them in the visual context of the operating field. The system employs a two-level affordance-based reasoning process that first analyzes the surgical scene using a multimodal vision-language model and then reasons about the instruction using a knowledge base of tool capabilities. To ensure patient safety, a dual-set conformal prediction method is used to provide a statistically rigorous confidence measure for robot decisions, allowing it to identify and flag ambiguous commands. We evaluated our framework on a curated dataset of ambiguous surgical requests from cholecystectomy videos, demonstrating a general disambiguation rate of 60% and presenting a method for safer human-robot interaction in the operating room.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2509.12129.pdf' target='_blank'>https://arxiv.org/pdf/2509.12129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12129">Embodied Navigation Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2508.20840.pdf' target='_blank'>https://arxiv.org/pdf/2508.20840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20840">Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2507.11525.pdf' target='_blank'>https://arxiv.org/pdf/2507.11525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Davila, Jacinto Colan, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11525">LLM-based ambiguity detection in natural language instructions for collaborative surgical robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues. Individual evaluator assessments are synthesized through conformal prediction, which yields non-conformity scores based on comparison to a labeled calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed classification accuracy exceeding 60% in differentiating ambiguous from unambiguous surgical instructions. Our approach improves the safety and reliability of human-robot collaboration in surgery by offering a mechanism to identify potentially ambiguous instructions before robot action.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2507.01925.pdf' target='_blank'>https://arxiv.org/pdf/2507.01925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01925">A Survey on Vision-Language-Action Models: An Action Tokenization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2507.01016.pdf' target='_blank'>https://arxiv.org/pdf/2507.01016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01016">VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2506.23127.pdf' target='_blank'>https://arxiv.org/pdf/2506.23127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoye Fei, Li Ji, Siyin Wang, Junhao Shi, Jingjing Gong, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23127">Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they face significant challenges in embodied task planning scenarios that require continuous environmental understanding and action generation. Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments. We introduce Embodied Planner-R1, a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. Our framework incorporates three key innovations: (1) Without human annotations, we employ pure reinforcement learning with group rollout, incorporating in-environment interaction through parallel exploration; (2) completion-driven sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories. Across two challenging text-based Embodied planning benchmarks, Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments, evidencing strong generalization.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2506.13751.pdf' target='_blank'>https://arxiv.org/pdf/2506.13751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoru Xue, Xiaoyu Huang, Dantong Niu, Qiayuan Liao, Thomas Kragerud, Jan Tommy Gravdahl, Xue Bin Peng, Guanya Shi, Trevor Darrell, Koushil Sreenath, Shankar Sastry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13751">LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action "vocabulary" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2506.09839.pdf' target='_blank'>https://arxiv.org/pdf/2506.09839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Gao, Liankai Jin, Xingyu Peng, Jiazhao Zhang, Yue Deng, Annan Li, He Wang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09839">OctoNav: Towards Generalist Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2505.23189.pdf' target='_blank'>https://arxiv.org/pdf/2505.23189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23189">TrackVLA: Embodied Visual Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2505.13441.pdf' target='_blank'>https://arxiv.org/pdf/2505.13441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhay Deshpande, Yuquan Deng, Arijit Ray, Jordi Salvador, Winson Han, Jiafei Duan, Kuo-Hao Zeng, Yuke Zhu, Ranjay Krishna, Rose Hendrix
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13441">GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given "pour me some tea", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from PRISM, a novel large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. GraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot. We release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation, which, along with videos, are available at https://abhaybd.github.io/GraspMolmo/.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2505.11191.pdf' target='_blank'>https://arxiv.org/pdf/2505.11191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11191">Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: multi-modal multi-task foundation models (M3T-FMs) provide a pathway toward generalization across tasks and modalities, whereas federated learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied AI environments. In this vision paper, we introduce multi-modal multi-task federated foundation models (M3T-FFMs) for embodied AI, a new paradigm that unifies the strengths of M3T-FMs with the privacy-preserving distributed training nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of M3T-FFMs in embodied AI ecosystems under a unified framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying M3T-FFMs in embodied AI systems, along with the associated trade-offs. Finally, we present a prototype implementation of M3T-FFMs and evaluate their energy and latency performance.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2505.02836.pdf' target='_blank'>https://arxiv.org/pdf/2505.02836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, Zhaoshuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02836">Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2504.20041.pdf' target='_blank'>https://arxiv.org/pdf/2504.20041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20041">Learning Streaming Video Representation via Multitask Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding continuous video streams plays a fundamental role in real-time applications including embodied AI and autonomous driving. Unlike offline video understanding, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions. To address these challenges, our main contributions are three-fold. (i) We develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability. (ii) To train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) We conduct extensive experiments on online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive results while maintaining efficiency, demonstrating its potential for real-time applications.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2504.00697.pdf' target='_blank'>https://arxiv.org/pdf/2504.00697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marlene Wessels, Jorge de Heuvel, Leon MÃ¼ller, Anna Luisa Maier, Maren Bennewitz, Johannes Kraus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00697">Auditory Localization and Assessment of Consequential Robot Sounds: A Multi-Method Study in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile robots increasingly operate alongside humans but are often out of sight, so that humans need to rely on the sounds of the robots to recognize their presence. For successful human-robot interaction (HRI), it is therefore crucial to understand how humans perceive robots by their consequential sounds, i.e., operating noise. Prior research suggests that the sound of a quadruped Go1 is more detectable than that of a wheeled Turtlebot. This study builds on this and examines the human ability to localize consequential sounds of three robots (quadruped Go1, wheeled Turtlebot 2i, wheeled HSR) in Virtual Reality. In a within-subjects design, we assessed participants' localization performance for the robots with and without an acoustic vehicle alerting system (AVAS) for two velocities (0.3, 0.8 m/s) and two trajectories (head-on, radial). In each trial, participants were presented with the sound of a moving robot for 3~s and were tasked to point at its final position (localization task). Localization errors were measured as the absolute angular difference between the participants' estimated and the actual robot position. Results showed that the robot type significantly influenced the localization accuracy and precision, with the sound of the wheeled HSR (especially without AVAS) performing worst under all experimental conditions. Surprisingly, participants rated the HSR sound as more positive, less annoying, and more trustworthy than the Turtlebot and Go1 sound. This reveals a tension between subjective evaluation and objective auditory localization performance. Our findings highlight consequential robot sounds as a critical factor for designing intuitive and effective HRI, with implications for human-centered robot design and social navigation.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2504.00682.pdf' target='_blank'>https://arxiv.org/pdf/2504.00682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge de Heuvel, Sebastian MÃ¼ller, Marlene Wessels, Aftab Akhtar, Christian Bauckhage, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00682">Immersive Explainability: Visualizing Robot Navigation Decisions through XAI Semantic Scene Projections in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end robot policies achieve high performance through neural networks trained via reinforcement learning (RL). Yet, their black box nature and abstract reasoning pose challenges for human-robot interaction (HRI), because humans may experience difficulty in understanding and predicting the robot's navigation decisions, hindering trust development. We present a virtual reality (VR) interface that visualizes explainable AI (XAI) outputs and the robot's lidar perception to support intuitive interpretation of RL-based navigation behavior. By visually highlighting objects based on their attribution scores, the interface grounds abstract policy explanations in the scene context. This XAI visualization bridges the gap between obscure numerical XAI attribution scores and a human-centric semantic level of explanation. A within-subjects study with 24 participants evaluated the effectiveness of our interface for four visualization conditions combining XAI and lidar. Participants ranked scene objects across navigation scenarios based on their importance to the robot, followed by a questionnaire assessing subjective understanding and predictability. Results show that semantic projection of attributions significantly enhances non-expert users' objective understanding and subjective awareness of robot behavior. In addition, lidar visualization further improves perceived predictability, underscoring the value of integrating XAI and sensor for transparent, trustworthy HRI.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2503.22869.pdf' target='_blank'>https://arxiv.org/pdf/2503.22869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexey Gavryushin, Alexandros Delitzas, Luc Van Gool, Marc Pollefeys, Kaichun Mo, Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22869">SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D Hand-Object Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When humans grasp an object, they naturally form trajectories in their minds to manipulate it for specific tasks. Modeling hand-object interaction priors holds significant potential to advance robotic and embodied AI systems in learning to operate effectively within the physical world. We introduce SIGHT, a novel task focused on generating realistic and physically plausible 3D hand-object interaction trajectories from a single image and a brief language-based task description. Prior work on hand-object trajectory generation typically relies on textual input that lacks explicit grounding to the target object, or assumes access to 3D object meshes, which are often considerably more difficult to obtain than 2D images. We propose SIGHT-Fusion, a novel diffusion-based image-text conditioned generative model that tackles this task by retrieving the most similar 3D object mesh from a database and enforcing geometric hand-object interaction constraints via a novel inference-time diffusion guidance. We benchmark our model on the HOI4D and H2O datasets, adapting relevant baselines for this novel task. Experiments demonstrate our superior performance in the diversity and quality of generated trajectories, as well as in hand-object interaction geometry metrics.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2503.14501.pdf' target='_blank'>https://arxiv.org/pdf/2503.14501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaowei Miao, Kehan Li, Jinsheng Quan, Zhiyuan Min, Shaojie Ma, Yichao Xu, Yi Yang, Ping Liu, Yawei Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14501">Advances in 4D Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative artificial intelligence has recently progressed from static image and video synthesis to 3D content generation, culminating in the emergence of 4D generation-the task of synthesizing temporally coherent dynamic 3D assets guided by user input. As a burgeoning research frontier, 4D generation enables richer interactive and immersive experiences, with applications ranging from digital humans to autonomous driving. Despite rapid progress, the field lacks a unified understanding of 4D representations, generative frameworks, basic paradigms, and the core technical challenges it faces. This survey provides a systematic and in-depth review of the 4D generation landscape. To comprehensively characterize 4D generation, we first categorize fundamental 4D representations and outline associated techniques for 4D generation. We then present an in-depth analysis of representative generative pipelines based on conditions and representation methods. Subsequently, we discuss how motion and geometry priors are integrated into 4D outputs to ensure spatio-temporal consistency under various control schemes. From an application perspective, this paper summarizes 4D generation tasks in areas such as dynamic object/scene generation, digital human synthesis, editable 4D content, and embodied AI. Furthermore, we summarize and multi-dimensionally compare four basic paradigms for 4D generation: End-to-End, Generated-Data-Based, Implicit-Distillation-Based, and Explicit-Supervision-Based. Concluding our analysis, we highlight five key challenges-consistency, controllability, diversity, efficiency, and fidelity-and contextualize these with current approaches.By distilling recent advances and outlining open problems, this work offers a comprehensive and forward-looking perspective to guide future research in 4D generation.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2503.07771.pdf' target='_blank'>https://arxiv.org/pdf/2503.07771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Wu, Yide Shentu, Qiayuan Liao, Ding Jin, Menglong Guo, Koushil Sreenath, Xingyu Lin, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07771">RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from human demonstration is an effective approach for learning complex manipulation skills. However, existing approaches heavily focus on learning from passive human demonstration data for its simplicity in data collection. Interactive human teaching has appealing theoretical and practical properties, but they are not well supported by existing human-robot interfaces. This paper proposes a novel system that enables seamless control switching between human and an autonomous policy for bi-manual manipulation tasks, enabling more efficient learning of new tasks. This is achieved through a compliant, bilateral teleoperation system. Through simulation and hardware experiments, we demonstrate the value of our system in an interactive human teaching for learning complex bi-manual manipulation skills.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2503.07511.pdf' target='_blank'>https://arxiv.org/pdf/2503.07511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07511">PointVLA: Injecting the 3D World into Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.
  Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2502.14420.pdf' target='_blank'>https://arxiv.org/pdf/2502.14420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14420">ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2501.09167.pdf' target='_blank'>https://arxiv.org/pdf/2501.09167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhen Wang, Chenda Duan, Zhenghao Peng, Yuxin Liu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09167">Embodied Scene Understanding for Vision Language Models via MetaVQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present MetaVQA: a comprehensive benchmark designed to assess and enhance VLMs' understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations. MetaVQA leverages Set-of-Mark prompting and top-down view ground-truth annotations from nuScenes and Waymo datasets to automatically generate extensive question-answer pairs based on diverse real-world traffic scenarios, ensuring object-centric and context-rich instructions. Our experiments show that fine-tuning VLMs with the MetaVQA dataset significantly improves their spatial reasoning and embodied scene comprehension in safety-critical simulations, evident not only in improved VQA accuracies but also in emerging safety-aware driving maneuvers. In addition, the learning demonstrates strong transferability from simulation to real-world observation. Code and data will be publicly available at https://metadriverse.github.io/metavqa .
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2501.06693.pdf' target='_blank'>https://arxiv.org/pdf/2501.06693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06693">Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim-to-real gap has long posed a significant challenge for robot learning in simulation, preventing the deployment of learned models in the real world. Previous work has primarily focused on domain randomization and system identification to mitigate this gap. However, these methods are often limited by the inherent constraints of the simulation and graphics engines. In this work, we propose Vid2Sim, a novel framework that effectively bridges the sim2real gap through a scalable and cost-efficient real2sim pipeline for neural 3D scene reconstruction and simulation. Given a monocular video as input, Vid2Sim can generate photorealistic and physically interactable 3D simulation environments to enable the reinforcement learning of visual navigation agents in complex urban environments. Extensive experiments demonstrate that Vid2Sim significantly improves the performance of urban navigation in the digital twins and real world by 31.2% and 68.3% in success rate compared with agents trained with prior simulation methods.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2412.20977.pdf' target='_blank'>https://arxiv.org/pdf/2412.20977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangwei Zhong, Kui Wu, Churan Wang, Hao Chen, Hai Ci, Zhoujun Li, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20977">UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects. UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2412.20451.pdf' target='_blank'>https://arxiv.org/pdf/2412.20451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinming Li, Yichen Zhu, Zhibin Tang, Junjie Wen, Minjie Zhu, Xiaoyu Liu, Chengmeng Li, Ran Cheng, Yaxin Peng, Yan Peng, Feifei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20451">CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot foundation models, particularly Vision-Language-Action (VLA) models, have garnered significant attention for their ability to enhance robot policy learning, greatly improving robot's generalization and robustness. OpenAI's recent model, O1, showcased impressive capabilities in solving complex problems by utilizing extensive reasoning chains. This prompts an important question: can robot models achieve better performance in multi-task , complex environments by reviewing prior observations and then providing task-specific reasoning to guide action prediction? In this paper, we introduce Chain-of-Affordance (CoA-VLA) , a novel approach to scaling robot models by incorporating reasoning in the format of sequential robot affordances to facilitate task completion. Specifically, we prompt the model to consider the following four types of affordances before taking action: (1) object affordance - what object to manipulate and where it is ; (2) grasp affordance - the specific object part to grasp ; (3) spatial affordance - the optimal space to place the object ; and (4) movement affordance-the collision - free path for movement. We further transform each affordance into two prompting formats: visual affordance and textual affordance. We introduce a novel vision-language co-injection module that integrates this knowledge into the policy network. This allows the robot to leverage essential contextual information during action inference, resulting in improved precision and robustness. Our experiments demonstrate that CoA-VLA outperforms state-of-the-art robot foundation models, including OpenVLA and Octo, on a variety of tasks. Furthermore, CoA-VLA exhibits strong generalization capabilities, including recognizing unseen object poses, identifying free space, and avoiding obstacles in novel environments.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2412.19562.pdf' target='_blank'>https://arxiv.org/pdf/2412.19562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Yang, Shenao Zhang, Zhihan Liu, Huaxiu Yao, Zhaoran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19562">Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on building a task planner for Embodied Instruction Following (EIF) using Large Language Models (LLMs). Previous works typically train a planner to imitate expert trajectories, treating this as a supervised task. While these methods achieve competitive performance, they often lack sufficient robustness. When a suboptimal action is taken, the planner may encounter an out-of-distribution state, which can lead to task failure. In contrast, we frame the task as a Partially Observable Markov Decision Process (POMDP) and aim to develop a robust planner under a few-shot assumption. Thus, we propose a closed-loop planner with an adaptation module and a novel hindsight method, aiming to use as much information as possible to assist the planner. Our experiments on the ALFRED dataset indicate that our planner achieves competitive performance under a few-shot assumption. For the first time, our few-shot agent's performance approaches and even surpasses that of the full-shot supervised agent.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2412.06877.pdf' target='_blank'>https://arxiv.org/pdf/2412.06877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Pouplin, Katarzyna Kobalczyk, Hao Sun, Mihaela van der Schaar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06877">The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce TEDUO, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, TEDUO operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that TEDUO achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2412.06224.pdf' target='_blank'>https://arxiv.org/pdf/2412.06224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06224">Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments. Uni-NaVid achieves this by harmonizing the input and output data configurations for all commonly used embodied navigation tasks and thereby integrating all tasks in one model. For training Uni-NaVid, we collect 3.6 million navigation data samples in total from four essential navigation sub-tasks and foster synergy in learning across them. Extensive experiments on comprehensive navigation benchmarks clearly demonstrate the advantages of unification modeling in Uni-NaVid and show it achieves state-of-the-art performance. Additionally, real-world experiments confirm the model's effectiveness and efficiency, shedding light on its strong generalizability.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2412.01292.pdf' target='_blank'>https://arxiv.org/pdf/2412.01292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyan Zhi, Peihao Chen, Junyan Li, Shuailei Ma, Xinyu Sun, Tianhang Xiang, Yinjie Lei, Mingkui Tan, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01292">LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2410.13825.pdf' target='_blank'>https://arxiv.org/pdf/2410.13825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13825">AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent's observation/action representation and the pre-training data of the LLM it's based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2410.08208.pdf' target='_blank'>https://arxiv.org/pdf/2410.08208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyi Zhu, Honghui Yang, Yating Wang, Jiange Yang, Limin Wang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08208">SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present the most comprehensive evaluation of embodied representation learning to date, covering 268 tasks across 8 simulators with diverse policies in both single-task and language-conditioned multi-task scenarios. The results are compelling: SPA consistently outperforms more than 10 state-of-the-art representation methods, including those specifically designed for embodied AI, vision-centric tasks, and multi-modal applications, while using less training data. Furthermore, we conduct a series of real-world experiments to confirm its effectiveness in practical scenarios. These results highlight the critical role of 3D spatial awareness for embodied representation learning. Our strongest model takes more than 6000 GPU hours to train and we are committed to open-sourcing all code and model weights to foster future research in embodied representation learning. Project Page: https://haoyizhu.github.io/spa/.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2410.01481.pdf' target='_blank'>https://arxiv.org/pdf/2410.01481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Li, Wendi Sang, Chang Zeng, Runxuan Yang, Guo Chen, Xiaolin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01481">SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systematic evaluation of speech separation and enhancement models under moving sound source conditions requires extensive and diverse data. However, real-world datasets often lack sufficient data for training and evaluation, and synthetic datasets, while larger, lack acoustic realism. Consequently, neither effectively meets practical needs. To address this issue, we introduce SonicSim, a synthetic toolkit based on the embodied AI simulation platform Habitat-sim, designed to generate highly customizable data for moving sound sources. SonicSim supports multi-level adjustments, including scene-level, microphone-level, and source-level adjustments, enabling the creation of more diverse synthetic data. Leveraging SonicSim, we constructed a benchmark dataset called SonicSet, utilizing LibriSpeech, Freesound Dataset 50k (FSD50K), Free Music Archive (FMA), and 90 scenes from Matterport3D to evaluate speech separation and enhancement models. Additionally, to investigate the differences between synthetic and real-world data, we selected 5 hours of raw, non-reverberant data from the SonicSet validation set and recorded a real-world speech separation dataset, providing a reference for comparing SonicSet with other synthetic datasets. For speech enhancement, we utilized the real-world dataset RealMAN to validate the acoustic gap between SonicSet and existing synthetic datasets. The results indicate that models trained on SonicSet generalize better to real-world scenarios compared to other synthetic datasets. The code is publicly available at https://cslikai.cn/SonicSim/.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2405.20774.pdf' target='_blank'>https://arxiv.org/pdf/2405.20774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20774">Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-based Decision-Making Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2405.14868.pdf' target='_blank'>https://arxiv.org/pdf/2405.14868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14868">Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision. Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications. In this paper, we propose $\textbf{GCD}$, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters. Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently. Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments. We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2404.03570.pdf' target='_blank'>https://arxiv.org/pdf/2404.03570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jake Varley, Sumeet Singh, Deepali Jain, Krzysztof Choromanski, Andy Zeng, Somnath Basu Roy Chowdhury, Avinava Dubey, Vikas Sindhwani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03570">Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace. Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies. Please see https://sites.google.com/corp/view/safe-robots .
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2403.06186.pdf' target='_blank'>https://arxiv.org/pdf/2403.06186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchong Zhang, Nona Rajabi, Farzaneh Taleb, Andrii Matviienko, Yong Ma, MÃ¥rten BjÃ¶rkman, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06186">Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain-robot interaction (BRI) empowers individuals to control (semi-)automated machines through their brain activity, either passively or actively. In the past decade, BRI systems have achieved remarkable success, predominantly harnessing electroencephalogram (EEG) signals as the central component. This paper offers an up-to-date and exhaustive examination of 87 curated studies published during the last five years (2018-2023), focusing on identifying the research landscape of EEG-based BRI systems. This review aims to consolidate and underscore methodologies, interaction modes, application contexts, system evaluation, existing challenges, and potential avenues for future investigations in this domain. Based on our analysis, we present a BRI system model with three entities: Brain, Robot, and Interaction, depicting the internal relationships of a BRI system. We especially investigate the essence and principles on interaction modes between human brains and robots, a domain that has not yet been identified anywhere. We then discuss these entities with different dimensions encompassed. Within this model, we scrutinize and classify current research, reveal insights, specify challenges, and provide recommendations for future research trajectories in this field. Meanwhile, we envision our findings offer a design space for future human-robot interaction (HRI) research, informing the creation of efficient BRI frameworks.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2402.17511.pdf' target='_blank'>https://arxiv.org/pdf/2402.17511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxun Ju, Chao Yang, Hongbo Wang, Yu Qiao, Fuchun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17511">Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning. To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions. Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works. Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2402.15852.pdf' target='_blank'>https://arxiv.org/pdf/2402.15852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15852">NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2401.09077.pdf' target='_blank'>https://arxiv.org/pdf/2401.09077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Pascher, Alia Saad, Jonathan Liebers, Roman Heger, Jens Gerken, Stefan Schneegass, Uwe Gruene
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09077">Hands-On Robotics: Enabling Communication Through Direct Gesture Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective Human-Robot Interaction (HRI) is fundamental to seamlessly integrating robotic systems into our daily lives. However, current communication modes require additional technological interfaces, which can be cumbersome and indirect. This paper presents a novel approach, using direct motion-based communication by moving a robot's end effector. Our strategy enables users to communicate with a robot by using four distinct gestures -- two handshakes ('formal' and 'informal') and two letters ('W' and 'S'). As a proof-of-concept, we conducted a user study with 16 participants, capturing subjective experience ratings and objective data for training machine learning classifiers. Our findings show that the four different gestures performed by moving the robot's end effector can be distinguished with close to 100% accuracy. Our research offers implications for the design of future HRI interfaces, suggesting that motion-based interaction can empower human operators to communicate directly with robots, removing the necessity for additional hardware.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2312.09067.pdf' target='_blank'>https://arxiv.org/pdf/2312.09067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09067">Holodeck: Language Guided Generation of 3D Embodied AI Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as "apartment for a researcher with a cat" and "office of a professor who is a fan of Star Wars". Holodeck leverages a large language model (i.e., GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2312.01990.pdf' target='_blank'>https://arxiv.org/pdf/2312.01990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isabel Leal, Krzysztof Choromanski, Deepali Jain, Avinava Dubey, Jake Varley, Michael Ryoo, Yao Lu, Frederick Liu, Vikas Sindhwani, Quan Vuong, Tamas Sarlos, Ken Oslund, Karol Hausman, Kanishka Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01990">SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2311.04193.pdf' target='_blank'>https://arxiv.org/pdf/2311.04193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainaz Eftekhar, Kuo-Hao Zeng, Jiafei Duan, Ali Farhadi, Ani Kembhavi, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04193">Selective Visual Representations Improve Convergence and Generalization for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. Code and pretrained models are available at our project website: https://embodied-codebook.github.io.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2310.15887.pdf' target='_blank'>https://arxiv.org/pdf/2310.15887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Pascher, Felix Ferdinand Goldau, Kirill Kronhardt, Udo Frese, Jens Gerken
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15887">AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) researchers to rapidly design and test novel interaction methods, intervention strategies, and multi-modal feedback techniques, without requiring an actual physical robotic arm during the early phases of ideation, prototyping, and evaluation. Also, a Robot Operating System (ROS) integration enables the controlling of a real robotic arm in a PhysicalTwin approach without any simulation-reality gap. Here, we review the capabilities and limitations of AdaptiX in detail and present three bodies of research based on the framework. AdaptiX can be accessed at https://adaptix.robot-research.de.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2309.07970.pdf' target='_blank'>https://arxiv.org/pdf/2309.07970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Rashid, Satvik Sharma, Chung Min Kim, Justin Kerr, Lawrence Chen, Angjoo Kanazawa, Ken Goldberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07970">Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grasping objects by a specific part is often crucial for safety and for executing downstream tasks. Yet, learning-based grasp planners lack this behavior unless they are trained on specific object part data, making it a significant challenge to scale object diversity. Instead, we propose LERF-TOGO, Language Embedded Radiance Fields for Task-Oriented Grasping of Objects, which uses vision-language models zero-shot to output a grasp distribution over an object given a natural language query. To accomplish this, we first reconstruct a LERF of the scene, which distills CLIP embeddings into a multi-scale 3D language field queryable with text. However, LERF has no sense of objectness, meaning its relevancy outputs often return incomplete activations over an object which are insufficient for subsequent part queries. LERF-TOGO mitigates this lack of spatial grouping by extracting a 3D object mask via DINO features and then conditionally querying LERF on this mask to obtain a semantic distribution over the object with which to rank grasps from an off-the-shelf grasp planner. We evaluate LERF-TOGO's ability to grasp task-oriented object parts on 31 different physical objects, and find it selects grasps on the correct part in 81% of all trials and grasps successfully in 69%. See the project website at: lerftogo.github.io
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2308.08538.pdf' target='_blank'>https://arxiv.org/pdf/2308.08538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobo Liu, Xudong Han, Wei Hong, Fang Wan, Chaoyang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08538">Proprioceptive Learning with Soft Polyhedral Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proprioception is the "sixth sense" that detects limb postures with motor neurons. It requires a natural integration between the musculoskeletal systems and sensory receptors, which is challenging among modern robots that aim for lightweight, adaptive, and sensitive designs at a low cost. Here, we present the Soft Polyhedral Network with an embedded vision for physical interactions, capable of adaptive kinesthesia and viscoelastic proprioception by learning kinetic features. This design enables passive adaptations to omni-directional interactions, visually captured by a miniature high-speed motion tracking system embedded inside for proprioceptive learning. The results show that the soft network can infer real-time 6D forces and torques with accuracies of 0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We also incorporate viscoelasticity in proprioception during static adaptation by adding a creep and relaxation modifier to refine the predicted results. The proposed soft network combines simplicity in design, omni-adaptation, and proprioceptive sensing with high accuracy, making it a versatile solution for robotics at a low cost with more than 1 million use cycles for tasks such as sensitive and competitive grasping, and touch-based geometry reconstruction. This study offers new insights into vision-based proprioception for soft robots in adaptive grasping, soft manipulation, and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2304.05008.pdf' target='_blank'>https://arxiv.org/pdf/2304.05008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongqi Han, Kenji Doya, Dongsheng Li, Jun Tani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05008">Habits and goals in synergy: a variational Bayesian framework for behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to behave efficiently and flexibly is a central problem for understanding biological agents and creating intelligent embodied AI. It has been well known that behavior can be classified as two types: reward-maximizing habitual behavior, which is fast while inflexible; and goal-directed behavior, which is flexible while slow. Conventionally, habitual and goal-directed behaviors are considered handled by two distinct systems in the brain. Here, we propose to bridge the gap between the two behaviors, drawing on the principles of variational Bayesian theory. We incorporate both behaviors in one framework by introducing a Bayesian latent variable called "intention". The habitual behavior is generated by using prior distribution of intention, which is goal-less; and the goal-directed behavior is generated by the posterior distribution of intention, which is conditioned on the goal. Building on this idea, we present a novel Bayesian framework for modeling behaviors. Our proposed framework enables skill sharing between the two kinds of behaviors, and by leveraging the idea of predictive coding, it enables an agent to seamlessly generalize from habitual to goal-directed behavior without requiring additional training. The proposed framework suggests a fresh perspective for cognitive science and embodied AI, highlighting the potential for greater integration between habitual and goal-directed behaviors.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2303.00362.pdf' target='_blank'>https://arxiv.org/pdf/2303.00362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Pascher, Uwe Gruenefeld, Stefan Schneegass, Jens Gerken
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00362">How to Communicate Robot Motion Intent: A Scoping Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are becoming increasingly omnipresent in our daily lives, supporting us and carrying out autonomous tasks. In Human-Robot Interaction, human actors benefit from understanding the robot's motion intent to avoid task failures and foster collaboration. Finding effective ways to communicate this intent to users has recently received increased research interest. However, no common language has been established to systematize robot motion intent. This work presents a scoping review aimed at unifying existing knowledge. Based on our analysis, we present an intent communication model that depicts the relationship between robot and human through different intent dimensions (intent type, intent information, intent location). We discuss these different intent dimensions and their interrelationships with different kinds of robots and human roles. Throughout our analysis, we classify the existing research literature along our intent communication model, allowing us to identify key patterns and possible directions for future research.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2212.00338.pdf' target='_blank'>https://arxiv.org/pdf/2212.00338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Zhang, Liu Dai, Fanpeng Meng, Qingnan Fan, Xuelin Chen, Kai Xu, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.00338">3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal navigation (ObjectNav) in unseen environments is a fundamental task for Embodied AI. Agents in existing works learn ObjectNav policies based on 2D maps, scene graphs, or image sequences. Considering this task happens in 3D space, a 3D-aware agent can advance its ObjectNav capability via learning from fine-grained spatial information. However, leveraging 3D scene representation can be prohibitively unpractical for policy learning in this floor-level task, due to low sample efficiency and expensive computational cost. In this work, we propose a framework for the challenging 3D-aware ObjectNav based on two straightforward sub-policies. The two sub-polices, namely corner-guided exploration policy and category-aware identification policy, simultaneously perform by utilizing online fused 3D points as observation. Through extensive experiments, we show that this framework can dramatically improve the performance in ObjectNav through learning from 3D scene representation. Our framework achieves the best performance among all modular-based methods on the Matterport3D and Gibson datasets, while requiring (up to 30x) less computational cost for training.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2210.01683.pdf' target='_blank'>https://arxiv.org/pdf/2210.01683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge de Heuvel, Nathan Corral, Benedikt Kreis, Jacobus Conradi, Anne Driemel, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.01683">Learning Depth Vision-Based Personalized Robot Navigation From Dynamic Demonstrations in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For the best human-robot interaction experience, the robot's navigation policy should take into account personal preferences of the user. In this paper, we present a learning framework complemented by a perception pipeline to train a depth vision-based, personalized navigation controller from user demonstrations. Our virtual reality interface enables the demonstration of robot navigation trajectories under motion of the user for dynamic interaction scenarios. The novel perception pipeline enrolls a variational autoencoder in combination with a motion predictor. It compresses the perceived depth images to a latent state representation to enable efficient reasoning of the learning agent about the robot's dynamic environment. In a detailed analysis and ablation study, we evaluate different configurations of the perception pipeline. To further quantify the navigation controller's quality of personalization, we develop and apply a novel metric to measure preference reflection based on the FrÃ©chet Distance. We discuss the robot's navigation performance in various virtual scenes and demonstrate the first personalized robot navigation controller that solely relies on depth images. A supplemental video highlighting our approach is available online.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2509.06932.pdf' target='_blank'>https://arxiv.org/pdf/2509.06932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, Xiaoyan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06932">LLaDA-VLA: Vision Language Diffusion Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2508.17753.pdf' target='_blank'>https://arxiv.org/pdf/2508.17753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresa Pekarek Rosin, Julia Gachot, Henri-Leon Kordt, Matthias Kerzel, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17753">Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Speech Recognition (ASR) systems in real-world settings need to handle imperfect audio, often degraded by hardware limitations or environmental noise, while accommodating diverse user groups. In human-robot interaction (HRI), these challenges intersect to create a uniquely challenging recognition environment. We evaluate four state-of-the-art ASR systems on eight publicly available datasets that capture six dimensions of difficulty: domain-specific, accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis demonstrates significant variations in performance, hallucination tendencies, and inherent biases, despite similar scores on standard benchmarks. These limitations have serious implications for HRI, where recognition errors can interfere with task performance, user trust, and safety.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2508.08896.pdf' target='_blank'>https://arxiv.org/pdf/2508.08896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08896">Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2507.16815.pdf' target='_blank'>https://arxiv.org/pdf/2507.16815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16815">ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2507.10672.pdf' target='_blank'>https://arxiv.org/pdf/2507.10672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Waseem Akram, Lyes Saad Saoud, Jan Rosell, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10672">Vision Language Action Models in Robotic Manipulation: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework. This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models. These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems. Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape. Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks. Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies. This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2507.07299.pdf' target='_blank'>https://arxiv.org/pdf/2507.07299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonia Raychaudhuri, Enrico Cancelli, Tommaso Campari, Lamberto Ballan, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07299">LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Despite these advances, we still lack a clear, language-focused benchmark for testing how well such agents ground the words in their instructions. We address this gap with LangNav, an open-set dataset specifically created to test an agent's ability to locate objects described at different levels of detail, from broad category names to fine attributes and object-object relations. Every description in LangNav was manually checked, yielding a lower error rate than existing lifelong- and semantic-navigation datasets. On top of LangNav we build LangNavBench, a benchmark that measures how well current semantic-navigation methods understand and act on these descriptions while moving toward their targets. LangNavBench allows us to systematically compare models on their handling of attributes, spatial and relational cues, and category hierarchies, offering the first thorough, language-centric evaluation of embodied navigation systems. We also present Multi-Layered Feature Map (MLFM), a method that builds a queryable multi-layered semantic map, particularly effective when dealing with small objects or instructions involving spatial relations. MLFM outperforms state-of-the-art mapping-based navigation baselines on the LangNav dataset.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2506.01031.pdf' target='_blank'>https://arxiv.org/pdf/2506.01031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanyuan Qiao, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie, Xinyu Wang, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01031">NavBench: Probing Multimodal Large Language Models for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2505.11917.pdf' target='_blank'>https://arxiv.org/pdf/2505.11917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11917">OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose robots capable of performing diverse tasks require synergistic reasoning and acting capabilities. However, recent dual-system approaches, which separate high-level reasoning from low-level acting, often suffer from challenges such as limited mutual understanding of capabilities between systems and latency issues. This paper introduces OneTwoVLA, a single unified vision-language-action model that can perform both acting (System One) and reasoning (System Two). Crucially, OneTwoVLA adaptively switches between two modes: explicitly reasoning at critical moments during task execution, and generating actions based on the most recent reasoning at other times. To further unlock OneTwoVLA's reasoning and generalization capabilities, we design a scalable pipeline for synthesizing embodied reasoning-centric vision-language data, used for co-training with robot data. We validate OneTwoVLA's effectiveness through extensive experiments, highlighting its superior performance across four key capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding, enabling the model to perform long-horizon, highly dexterous manipulation tasks such as making hotpot or mixing cocktails.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2504.18945.pdf' target='_blank'>https://arxiv.org/pdf/2504.18945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zishen Wan, Jiayi Qian, Yuhang Du, Jason Jabbour, Yilun Du, Yang Katie Zhao, Arijit Raychowdhury, Tushar Krishna, Vijay Janapa Reddi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18945">Generative AI in Embodied Systems: System-Level Analysis of Performance, Efficiency and Scalability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied systems, where generative autonomous agents engage with the physical world through integrated perception, cognition, action, and advanced reasoning powered by large language models (LLMs), hold immense potential for addressing complex, long-horizon, multi-objective tasks in real-world environments. However, deploying these systems remains challenging due to prolonged runtime latency, limited scalability, and heightened sensitivity, leading to significant system inefficiencies.
  In this paper, we aim to understand the workload characteristics of embodied agent systems and explore optimization solutions. We systematically categorize these systems into four paradigms and conduct benchmarking studies to evaluate their task performance and system efficiency across various modules, agent scales, and embodied tasks. Our benchmarking studies uncover critical challenges, such as prolonged planning and communication latency, redundant agent interactions, complex low-level control mechanisms, memory inconsistencies, exploding prompt lengths, sensitivity to self-correction and execution, sharp declines in success rates, and reduced collaboration efficiency as agent numbers increase. Leveraging these profiling insights, we suggest system optimization strategies to improve the performance, efficiency, and scalability of embodied agents across different paradigms. This paper presents the first system-level analysis of embodied AI agents, and explores opportunities for advancing future embodied system design.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2504.09778.pdf' target='_blank'>https://arxiv.org/pdf/2504.09778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Farias, Pablo Moraes, Igor Nunes, Juan Deniz, Sebastian Barcelona, Hiago Sodre, William Moraes, Monica Rodriguez, Ahilen Mazondo, Vincent Sandin, Gabriel da Silva, Victoria Saravia, Vinicio Melgar, Santiago Fernandez, Ricardo Grando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09778">RoboCup Rescue 2025 Team Description Paper UruBots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes the approach used by Team UruBots for participation in the 2025 RoboCup Rescue Robot League competition. Our team aims to participate for the first time in this competition at RoboCup, using experience learned from previous competitions and research. We present our vehicle and our approach to tackle the task of detecting and finding victims in search and rescue environments. Our approach contains known topics in robotics, such as ROS, SLAM, Human Robot Interaction and segmentation and perception. Our proposed approach is open source, available to the RoboCup Rescue community, where we aim to learn and contribute to the league.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2502.16718.pdf' target='_blank'>https://arxiv.org/pdf/2502.16718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia FermÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16718">NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans' multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at https://www.snehesh.com/natsgld/ to support future HRI research.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2502.09379.pdf' target='_blank'>https://arxiv.org/pdf/2502.09379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorgen Cani, Panagiotis Koletsis, Konstantinos Foteinos, Ioannis Kefaloukos, Lampros Argyriou, Manolis Falelakis, IvÃ¡n Del Pino, Angel Santamaria-Navarro, Martin Äech, OndÅej Severa, Alessandro Umbrico, Francesca Fracasso, AndreA Orlandini, Dimitrios Drakoulis, Evangelos Markakis, Iraklis Varlamis, Georgios Th. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09379">TRIFFID: Autonomous Robotic Aid For Increasing First Responders Efficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing complexity of natural disaster incidents demands innovative technological solutions to support first responders in their efforts. This paper introduces the TRIFFID system, a comprehensive technical framework that integrates unmanned ground and aerial vehicles with advanced artificial intelligence functionalities to enhance disaster response capabilities across wildfires, urban floods, and post-earthquake search and rescue missions. By leveraging state-of-the-art autonomous navigation, semantic perception, and human-robot interaction technologies, TRIFFID provides a sophisticated system composed of the following key components: hybrid robotic platform, centralized ground station, custom communication infrastructure, and smartphone application. The defined research and development activities demonstrate how deep neural networks, knowledge graphs, and multimodal information fusion can enable robots to autonomously navigate and analyze disaster environments, reducing personnel risks and accelerating response times. The proposed system enhances emergency response teams by providing advanced mission planning, safety monitoring, and adaptive task execution capabilities. Moreover, it ensures real-time situational awareness and operational support in complex and risky situations, facilitating rapid and precise information collection and coordinated actions.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2502.07645.pdf' target='_blank'>https://arxiv.org/pdf/2502.07645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoting Li, Rodrigo PÃ©rez-Dattari, Robert Babuska, Cosimo Della Santina, Jens Kober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07645">Beyond Behavior Cloning: Robustness through Interactive Imitation and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Behavior cloning (BC) traditionally relies on demonstration data, assuming the demonstrated actions are optimal. This can lead to overfitting under noisy data, particularly when expressive models are used (e.g., the energy-based model in Implicit BC). To address this, we extend behavior cloning into an iterative process of optimal action estimation within the Interactive Imitation Learning framework. Specifically, we introduce Contrastive policy Learning from Interactive Corrections (CLIC). CLIC leverages human corrections to estimate a set of desired actions and optimizes the policy to select actions from this set. We provide theoretical guarantees for the convergence of the desired action set to optimal actions in both single and multiple optimal action cases. Extensive simulation and real-robot experiments validate CLIC's advantages over existing state-of-the-art methods, including stable training of energy-based models, robustness to feedback noise, and adaptability to diverse feedback types beyond demonstrations. Our code will be publicly available soon.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2411.05342.pdf' target='_blank'>https://arxiv.org/pdf/2411.05342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh Nguyen Canh, Ba Phuong Nguyen, Hong Quan Tran, Xiem HoangVan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05342">Development of a Human-Robot Interaction Platform for Dual-Arm Robots Based on ROS and Multimodal Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose the development of an interactive platform between humans and a dual-arm robotic system based on the Robot Operating System (ROS) and a multimodal artificial intelligence model. Our proposed platform consists of two main components: a dual-arm robotic hardware system and software that includes image processing tasks and natural language processing using a 3D camera and embedded computing. First, we designed and developed a dual-arm robotic system with a positional accuracy of less than 2 cm, capable of operating independently, performing industrial and service tasks while simultaneously simulating and modeling the robot in the ROS environment. Second, artificial intelligence models for image processing are integrated to execute object picking and classification tasks with an accuracy of over 90%. Finally, we developed remote control software using voice commands through a natural language processing model. Experimental results demonstrate the accuracy of the multimodal artificial intelligence model and the flexibility of the dual-arm robotic system in interactive human environments.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2410.03907.pdf' target='_blank'>https://arxiv.org/pdf/2410.03907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03907">ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models~(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models~(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model's reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2409.18896.pdf' target='_blank'>https://arxiv.org/pdf/2409.18896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Iliash, Hanxiao Jiang, Yiming Zhang, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18896">S2O: Static to Openable Enhancement for Articulated 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress in large 3D datasets there are currently few interactive 3D object datasets, and their scale is limited due to the manual effort required in their construction. We introduce the static to openable (S2O) task which creates interactive articulated 3D objects from static counterparts through openable part detection, motion prediction, and interior geometry completion. We formulate a unified framework to tackle this task, and curate a challenging dataset of openable 3D objects that serves as a test bed for systematic evaluation. Our experiments benchmark methods from prior work, extended and improved methods, and simple yet effective heuristics for the S2O task. We find that turning static 3D objects into interactively openable counterparts is possible but that all methods struggle to generalize to realistic settings of the task, and we highlight promising future work directions. Our work enables efficient creation of interactive 3D objects for robotic manipulation and embodied AI tasks.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2409.12192.pdf' target='_blank'>https://arxiv.org/pdf/2409.12192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Jeff Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Haldar, Lerrel Pinto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12192">DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2408.03168.pdf' target='_blank'>https://arxiv.org/pdf/2408.03168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elia Cereda, Alessandro Giusti, Daniele Palossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03168">Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training labels aboard our CPS, we also employ a self-supervised method based on ego-motion consistency. Albeit our work builds on top of a specific real-world vision-based human pose estimation task, it is widely applicable for many embedded TinyML use cases. Our 512-image on-device training procedure is fully deployed aboard an ultra-low power GWT GAP9 System-on-Chip and requires only 1MB of memory while consuming as low as 19mW or running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our on-device learning approach by field-testing our closed-loop CPS, showing a reduction in horizontal position error of up to 26% vs. a non-fine-tuned state-of-the-art baseline. In the most challenging never-seen-before environment, our on-device learning procedure makes the difference between succeeding or failing the mission.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2404.10446.pdf' target='_blank'>https://arxiv.org/pdf/2404.10446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Gadd, Daniele De Martini, Luke Pitt, Wayne Tubby, Matthew Towlson, Chris Prahacs, Oliver Bartlett, John Jackson, Man Qi, Paul Newman, Andrew Hector, Roberto Salguero-GÃ³mez, Nick Hawes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10446">Watching Grass Grow: Long-term Visual Navigation and Mission Planning for Autonomous Biodiversity Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe a challenging robotics deployment in a complex ecosystem to monitor a rich plant community. The study site is dominated by dynamic grassland vegetation and is thus visually ambiguous and liable to drastic appearance change over the course of a day and especially through the growing season. This dynamism and complexity in appearance seriously impact the stability of the robotics platform, as localisation is a foundational part of that control loop, and so routes must be carefully taught and retaught until autonomy is robust and repeatable. Our system is demonstrated over a 6-week period monitoring the response of grass species to experimental climate change manipulations. We also discuss the applicability of our pipeline to monitor biodiversity in other complex natural settings.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2403.08248.pdf' target='_blank'>https://arxiv.org/pdf/2403.08248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08248">CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2402.19007.pdf' target='_blank'>https://arxiv.org/pdf/2402.19007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Ma, Hongming Dai, Yao Mu, Pengying Wu, Hao Wang, Xiaowei Chi, Yang Fei, Shanghang Zhang, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19007">DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancies from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables the evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset can be found at https://DOZE-Dataset.github.io/.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2312.08991.pdf' target='_blank'>https://arxiv.org/pdf/2312.08991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Lamberti, Elia Cereda, Gabriele Abbate, Lorenzo Bellone, Victor Javier Kartsch Morinigo, MichaÅ Barcis, Agata Barcis, Alessandro Giusti, Francesco Conti, Daniele Palossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08991">A Sim-to-Real Deep Learning-based Framework for Autonomous Nano-drone Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous drone racing competitions are a proxy to improve unmanned aerial vehicles' perception, planning, and control skills. The recent emergence of autonomous nano-sized drone racing imposes new challenges, as their ~10cm form factor heavily restricts the resources available onboard, including memory, computation, and sensors. This paper describes the methodology and technical implementation of the system winning the first autonomous nano-drone racing international competition: the IMAV 2022 Nanocopter AI Challenge. We developed a fully onboard deep learning approach for visual navigation trained only on simulation images to achieve this goal. Our approach includes a convolutional neural network for obstacle avoidance, a sim-to-real dataset collection procedure, and a navigation policy that we selected, characterized, and adapted through simulation and actual in-field experiments. Our system ranked 1st among seven competing teams at the competition. In our best attempt, we scored 115m of traveled distance in the allotted 5-minute flight, never crashing while dodging static and dynamic obstacles. Sharing our knowledge with the research community, we aim to provide a solid groundwork to foster future development in this field.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2310.13622.pdf' target='_blank'>https://arxiv.org/pdf/2310.13622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Gadd, Benjamin Ramtoula, Daniele De Martini, Paul Newman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13622">What you see is what you get: Experience ranking with deep neural dataset-to-dataset similarity for topological localisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recalling the most relevant visual memories for localisation or understanding a priori the likely outcome of localisation effort against a particular visual memory is useful for efficient and robust visual navigation. Solutions to this problem should be divorced from performance appraisal against ground truth - as this is not available at run-time - and should ideally be based on generalisable environmental observations. For this, we propose applying the recently developed Visual DNA as a highly scalable tool for comparing datasets of images - in this work, sequences of map and live experiences. In the case of localisation, important dataset differences impacting performance are modes of appearance change, including weather, lighting, and season. Specifically, for any deep architecture which is used for place recognition by matching feature volumes at a particular layer, we use distribution measures to compare neuron-wise activation statistics between live images and multiple previously recorded past experiences, with a potentially large seasonal (winter/summer) or time of day (day/night) shift. We find that differences in these statistics correlate to performance when localising using a past experience with the same appearance gap. We validate our approach over the Nordland cross-season dataset as well as data from Oxford's University Parks with lighting and mild seasonal change, showing excellent ability of our system to rank actual localisation performance across candidate experiences.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2309.13144.pdf' target='_blank'>https://arxiv.org/pdf/2309.13144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingrid Navarro, Jay Patrikar, Joao P. A. Dantas, Rohan Baijal, Ian Higgins, Sebastian Scherer, Jean Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13144">SoRTS: Learned Tree Search for Long Horizon Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fast-growing demand for fully autonomous robots in shared spaces calls for the development of trustworthy agents that can safely and seamlessly navigate in crowded environments. Recent models for motion prediction show promise in characterizing social interactions in such environments. Still, adapting them for navigation is challenging as they often suffer from generalization failures. Prompted by this, we propose Social Robot Tree Search (SoRTS), an algorithm for safe robot navigation in social domains. SoRTS aims to augment existing socially aware motion prediction models for long-horizon navigation using Monte Carlo Tree Search.
  We use social navigation in general aviation as a case study to evaluate our approach and further the research in full-scale aerial autonomy. In doing so, we introduce XPlaneROS, a high-fidelity aerial simulator that enables human-robot interaction. We use XPlaneROS to conduct a first-of-its-kind user study where 26 FAA-certified pilots interact with a human pilot, our algorithm, and its ablation. Our results, supported by statistical evidence, show that SoRTS exhibits a comparable performance to competent human pilots, significantly outperforming its ablation. Finally, we complement these results with a broad set of self-play experiments to showcase our algorithm's performance in scenarios with increasing complexity.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2306.16917.pdf' target='_blank'>https://arxiv.org/pdf/2306.16917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Recasens, Martin R. Oswald, Marc Pollefeys, Javier Civera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16917">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2306.07933.pdf' target='_blank'>https://arxiv.org/pdf/2306.07933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07933">Understanding Telecom Language Through Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2304.03696.pdf' target='_blank'>https://arxiv.org/pdf/2304.03696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonia Raychaudhuri, Tommaso Campari, Unnat Jain, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03696">MOPA: Modular Object Navigation with PointGoal Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a simple but effective modular approach MOPA (Modular ObjectNav with PointGoal agents) to systematically investigate the inherent modularity of the object navigation task in Embodied AI. MOPA consists of four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore the environment, and (d) a navigation module to move to identified target objects. We show that we can effectively reuse a pretrained PointGoal agent as the navigation model instead of learning to navigate from scratch, thus saving time and compute. We also compare various exploration strategies for MOPA and find that a simple uniform strategy significantly outperforms more advanced exploration methods.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2303.07595.pdf' target='_blank'>https://arxiv.org/pdf/2303.07595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lishuang Zhan, Yancheng Cao, Qitai Chen, Haole Guo, Jiasi Gao, Yiyue Luo, Shihui Guo, Guyue Zhou, Jiangtao Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07595">Enable Natural Tactile Interaction for Robot Dog based on Large-format Distributed Flexible Pressure Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Touch is an important channel for human-robot interaction, while it is challenging for robots to recognize human touch accurately and make appropriate responses. In this paper, we design and implement a set of large-format distributed flexible pressure sensors on a robot dog to enable natural human-robot tactile interaction. Through a heuristic study, we sorted out 81 tactile gestures commonly used when humans interact with real dogs and 44 dog reactions. A gesture classification algorithm based on ResNet is proposed to recognize these 81 human gestures, and the classification accuracy reaches 98.7%. In addition, an action prediction algorithm based on Transformer is proposed to predict dog actions from human gestures, reaching a 1-gram BLEU score of 0.87. Finally, we compare the tactile interaction with the voice interaction during a freedom human-robot-dog interactive playing study. The results show that tactile interaction plays a more significant role in alleviating user anxiety, stimulating user excitement and improving the acceptability of robot dogs.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2207.03739.pdf' target='_blank'>https://arxiv.org/pdf/2207.03739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta Lagomarsino, Marta Lorenzini, Elena De Momi, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.03739">Robot Trajectory Adaptation to Optimise the Trade-off between Human Cognitive Ergonomics and Workplace Productivity in Collaborative Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In hybrid industrial environments, workers' comfort and positive perception of safety are essential requirements for successful acceptance and usage of collaborative robots. This paper proposes a novel human-robot interaction framework in which the robot behaviour is adapted online according to the operator's cognitive workload and stress. The method exploits the generation of B-spline trajectories in the joint space and formulation of a multi-objective optimisation problem to online adjust the total execution time and smoothness of the robot trajectories. The former ensures human efficiency and productivity of the workplace, while the latter contributes to safeguarding the user's comfort and cognitive ergonomics. The performance of the proposed framework was evaluated in a typical industrial task. Results demonstrated its capability to enhance the productivity of the human-robot dyad while mitigating the cognitive workload induced in the worker.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2206.03674.pdf' target='_blank'>https://arxiv.org/pdf/2206.03674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L. S. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.03674">Integrating Symmetry into Differentiable Planning with Steerable Convolutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study how group symmetry helps improve data efficiency and generalization for end-to-end differentiable planning algorithms when symmetry appears in decision-making tasks. Motivated by equivariant convolution networks, we treat the path planning problem as \textit{signals} over grids. We show that value iteration in this case is a linear equivariant operator, which is a (steerable) convolution. This extends Value Iteration Networks (VINs) on using convolutional networks for path planning with additional rotation and reflection symmetry. Our implementation is based on VINs and uses steerable convolution networks to incorporate symmetry. The experiments are performed on four tasks: 2D navigation, visual navigation, and 2 degrees of freedom (2DOFs) configuration space and workspace manipulation. Our symmetric planning algorithms improve training efficiency and generalization by large margins compared to non-equivariant counterparts, VIN and GPPN.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2201.06365.pdf' target='_blank'>https://arxiv.org/pdf/2201.06365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Giammarino, Juan M. Gandarias, Pietro Balatti, Mattia Leonori, Marta Lorenzini, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.06365">SUPER-MAN: SUPERnumerary Robotic Bodies for Physical Assistance in HuMAN-Robot Conjoined Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a mobile supernumerary robotic approach to physical assistance in human-robot conjoined actions. The study starts with a description of the SUPER-MAN concept. The idea is to develop and utilize mobile collaborative systems that can follow human loco-manipulation commands to perform industrial tasks through three main components: i) an admittance-type interface, ii) a human-robot interaction controller, and iii) a supernumerary robotic body. Next, we present two possible implementations within the framework from theoretical and hardware perspectives. The first system is called MOCA-MAN and comprises a redundant torque-controlled robotic arm and an omnidirectional mobile platform. The second one is called Kairos-MAN, formed by a high-payload 6-DoF velocity-controlled robotic arm and an omnidirectional mobile platform. The systems share the same admittance interface, through which user wrenches are translated to loco-manipulation commands generated by whole-body controllers of each system. Besides, a thorough user study with multiple and cross-gender subjects is presented to reveal the quantitative performance of the two systems in effort-demanding and dexterous tasks. Moreover, we provide qualitative results from the NASA-TLX questionnaire to demonstrate the SUPER-MAN approach's potential and its acceptability from the users' viewpoint.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2509.11417.pdf' target='_blank'>https://arxiv.org/pdf/2509.11417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shresth Grover, Akshay Gopalkrishnan, Bo Ai, Henrik I. Christensen, Hao Su, Xuanlin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11417">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2509.08222.pdf' target='_blank'>https://arxiv.org/pdf/2509.08222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjong Yoo, Jinwoo Jang, Wei-jin Park, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08222">Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents an Exploratory Retrieval-Augmented Planning (ExRAP) framework, designed to tackle continual instruction following tasks of embodied agents in dynamic, non-stationary environments. The framework enhances Large Language Models' (LLMs) embodied reasoning capabilities by efficiently exploring the physical environment and establishing the environmental context memory, thereby effectively grounding the task planning process in time-varying environment contexts. In ExRAP, given multiple continual instruction following tasks, each instruction is decomposed into queries on the environmental context memory and task executions conditioned on the query results. To efficiently handle these multiple tasks that are performed continuously and simultaneously, we implement an exploration-integrated task planning scheme by incorporating the {information-based exploration} into the LLM-based planning process. Combined with memory-augmented query evaluation, this integrated scheme not only allows for a better balance between the validity of the environmental context memory and the load of environment exploration, but also improves overall task performance. Furthermore, we devise a {temporal consistency refinement} scheme for query evaluation to address the inherent decay of knowledge in the memory. Through experiments with VirtualHome, ALFRED, and CARLA, our approach demonstrates robustness against a variety of embodied instruction following scenarios involving different instruction scales and types, and non-stationarity degrees, and it consistently outperforms other state-of-the-art LLM-based task planning approaches in terms of both goal success rate and execution efficiency.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2509.03956.pdf' target='_blank'>https://arxiv.org/pdf/2509.03956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjong Yoo, Jinwoo Jang, Sihyung Yoon, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03956">World Model Implanting for Test-time Adaptation of Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2506.10100.pdf' target='_blank'>https://arxiv.org/pdf/2506.10100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10100">EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2506.07339.pdf' target='_blank'>https://arxiv.org/pdf/2506.07339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Black, Manuel Y. Galliker, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07339">Real-Time Execution of Action Chunking Flow Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, "freezing" actions guaranteed to execute and "inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2506.05095.pdf' target='_blank'>https://arxiv.org/pdf/2506.05095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaee Cheong, Yang Liu, Harold Soh, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05095">FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing prevalence and deployment of Emotion AI-powered facial affect analysis (FAA) tools, concerns about the trustworthiness of these systems have become more prominent. This first workshop on "Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)" aims to bring together researchers who are investigating different challenges in relation to trustworthiness-such as interpretability, uncertainty, biases, and privacy-across various facial affect analysis tasks, including macro/ micro-expression recognition, facial action unit detection, other corresponding applications such as pain and depression detection, as well as human-robot interaction and collaboration. In alignment with FG2025's emphasis on ethics, as demonstrated by the inclusion of an Ethical Impact Statement requirement for this year's submissions, this workshop supports FG2025's efforts by encouraging research, discussion and dialogue on trustworthy FAA.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2505.01458.pdf' target='_blank'>https://arxiv.org/pdf/2505.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01458">A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2501.16698.pdf' target='_blank'>https://arxiv.org/pdf/2501.16698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueen Ma, Yuzheng Zhuang, Jianye Hao, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16698">3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose Diffusion via Rectified Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D vision and spatial reasoning have long been recognized as preferable for accurately perceiving our three-dimensional world, especially when compared with traditional visual reasoning based on 2D images. Due to the difficulties in collecting high-quality 3D data, research in this area has only recently gained momentum. With the advent of powerful large language models (LLMs), multi-modal LLMs for 3D vision have been developed over the past few years. However, most of these models focus primarily on the vision encoder for 3D data. In this paper, we propose converting existing densely activated LLMs into mixture-of-experts (MoE) models, which have proven effective for multi-modal data processing. In addition to leveraging these models' instruction-following capabilities, we further enable embodied task planning by attaching a diffusion head, Pose-DiT, that employs a novel rectified flow diffusion scheduler. Experimental results on 3D question answering and task-planning tasks demonstrate that our 3D-MoE framework achieves improved performance with fewer activated parameters.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2412.04728.pdf' target='_blank'>https://arxiv.org/pdf/2412.04728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyan Yu, Yiyuan Wang, Tram Thi Minh Tran, Yi Zhao, Julie Stephany Berrio Perez, Marius Hoggenmuller, Justine Humphry, Lian Loke, Lynn Masuda, Callum Parker, Martin Tomitsch, Stewart Worrall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04728">Robots in the Wild: Contextually-Adaptive Human-Robot Interactions in Urban Public Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing transition of human-robot interaction (HRI) context from controlled settings to dynamic, real-world public environments calls for enhanced adaptability in robotic systems. This can go beyond algorithmic navigation or traditional HRI strategies in structured settings, requiring the ability to navigate complex public urban systems containing multifaceted dynamics and various socio-technical needs. Therefore, our proposed workshop seeks to extend the boundaries of adaptive HRI research beyond predictable, semi-structured contexts and highlight opportunities for adaptable robot interactions in urban public environments. This half-day workshop aims to explore design opportunities and challenges in creating contextually-adaptive HRI within these spaces and establish a network of interested parties within the OzCHI research community. By fostering ongoing discussions, sharing of insights, and collaborations, we aim to catalyse future research that empowers robots to navigate the inherent uncertainties and complexities of real-world public interactions.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2411.09823.pdf' target='_blank'>https://arxiv.org/pdf/2411.09823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yian Wang, Xiaowen Qiu, Jiageng Liu, Zhehuan Chen, Jiting Cai, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09823">Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating large-scale interactive 3D environments is essential for the development of Robotics and Embodied AI research. Current methods, including manual design, procedural generation, diffusion-based scene generation, and large language model (LLM) guided scene design, are hindered by limitations such as excessive human effort, reliance on predefined rules or training datasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image generative models better capture scene and object configuration than LLMs, we address these challenges by introducing Architect, a generative framework that creates complex and realistic 3D embodied environments leveraging diffusion-based 2D image inpainting. In detail, we utilize foundation visual perception models to obtain each generated object from the image and leverage pre-trained depth estimation models to lift the generated 2D image to 3D space. Our pipeline is further extended to a hierarchical and iterative inpainting process to continuously generate placement of large furniture and small objects to enrich the scene. This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2408.01147.pdf' target='_blank'>https://arxiv.org/pdf/2408.01147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueen Ma, Dafeng Chi, Shiguang Wu, Yuecheng Liu, Yuzheng Zhuang, Jianye Hao, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01147">Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models have gained significant attention for their ability to model trajectories in robot learning. However, most existing models rely on Transformer models with vanilla causal attention, which we find suboptimal for processing segmented multi-modal sequences. Additionally, the autoregressive generation approach falls short in generating multi-dimensional actions. In this paper, we introduce Actra, an optimized Transformer architecture featuring trajectory attention and learnable action queries, designed for effective encoding and decoding of segmented vision-language-action trajectories in robot imitation learning. Furthermore, we devise a multi-modal contrastive learning objective to explicitly align different modalities, complementing the primary behavior cloning objective. Through extensive experiments conducted across various environments, Actra exhibits substantial performance improvement when compared to state-of-the-art models in terms of generalizability, dexterity, and precision.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2407.04292.pdf' target='_blank'>https://arxiv.org/pdf/2407.04292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Huang, Yuhui Hao, Bo Yu, Feng Yan, Yuxin Yang, Feng Min, Yinhe Han, Lin Ma, Shaoshan Liu, Qiang Liu, Yiming Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04292">DaDu-Corki: Algorithm-Architecture Co-Design for Embodied AI-powered Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI robots have the potential to fundamentally improve the way human beings live and manufacture. Continued progress in the burgeoning field of using large language models to control robots depends critically on an efficient computing substrate, and this trend is strongly evident in manipulation tasks. In particular, today's computing systems for embodied AI robots for manipulation tasks are designed purely based on the interest of algorithm developers, where robot actions are divided into a discrete frame basis. Such an execution pipeline creates high latency and energy consumption. This paper proposes \textsc{Corki}\xspace, an algorithm-architecture co-design framework for real-time embodied AI-powered robotic manipulation applications. We aim to decouple LLM inference, robotic control, and data communication in the embodied AI robots' compute pipeline. Instead of predicting action for one single frame, \textsc{Corki}\xspace predicts the trajectory for the near future to reduce the frequency of LLM inference. The algorithm is coupled with a hardware that accelerates transforming trajectory into actual torque signals used to control robots and an execution pipeline that parallels data communication with computation. \textsc{Corki}\xspace largely reduces LLM inference frequency by up to $5.1\times$, resulting in up to $5.9\times$ speed up. The success rate improvement can be up to 13.9\%.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2406.11128.pdf' target='_blank'>https://arxiv.org/pdf/2406.11128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyun Song, Minjong Yoo, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11128">Model Adaptation for Time Constrained Embodied Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When adopting a deep learning model for embodied agents, it is required that the model structure be optimized for specific tasks and operational conditions. Such optimization can be static such as model compression or dynamic such as adaptive inference. Yet, these techniques have not been fully investigated for embodied control systems subject to time constraints, which necessitate sequential decision-making for multiple tasks, each with distinct inference latency limitations. In this paper, we present MoDeC, a time constraint-aware embodied control framework using the modular model adaptation. We formulate model adaptation to varying operational conditions on resource and time restrictions as dynamic routing on a modular network, incorporating these conditions as part of multi-task objectives. Our evaluation across several vision-based embodied environments demonstrates the robustness of MoDeC, showing that it outperforms other model adaptation methods in both performance and adherence to time constraints in robotic manipulation and autonomous driving applications
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2401.12076.pdf' target='_blank'>https://arxiv.org/pdf/2401.12076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Fu, Fares Abawi, Philipp Allgeuer, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12076">Human Impression of Humanoid Robots Mirroring Social Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people's perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people's perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. We discovered that the iCub robot was perceived as more humanlike than the Pepper robot when mirroring affect. A vision-based controlled iCub outperformed the IMU-based controlled one in the movement mirroring task. Our findings suggest that different robotic platforms impact people's perception of robots' mirroring during HRI. The control method also contributes to the robot's mirroring performance. Our work sheds light on the design and application of different humanoid robots in the real world.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2401.05302.pdf' target='_blank'>https://arxiv.org/pdf/2401.05302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mudit Verma, Siddhant Bhambri, Subbarao Kambhampati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05302">Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example "Given a robot's behavior X, would the human observer find it explicable?". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2311.12996.pdf' target='_blank'>https://arxiv.org/pdf/2311.12996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12996">RLIF: Interactive Imitation Learning as Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although reinforcement learning methods offer a powerful framework for automatic skill acquisition, for practical learning-based control problems in domains such as robotics, imitation learning often provides a more convenient and accessible alternative. In particular, an interactive imitation learning method such as DAgger, which queries a near-optimal expert to intervene online to collect correction data for addressing the distributional shift challenges that afflict naÃ¯ve behavioral cloning, can enjoy good performance both in theory and practice without requiring manually specified reward functions and other components of full reinforcement learning methods. In this paper, we explore how off-policy reinforcement learning can enable improved performance under assumptions that are similar but potentially even more practical than those of interactive imitation learning. Our proposed method uses reinforcement learning with user intervention signals themselves as rewards. This relaxes the assumption that intervening experts in interactive imitation learning should be near-optimal and enables the algorithm to learn behaviors that improve over the potential suboptimal human expert. We also provide a unified framework to analyze our RL method and DAgger; for which we present the asymptotic analysis of the suboptimal gap for both methods as well as the non-asymptotic sample complexity bound of our method. We then evaluate our method on challenging high-dimensional continuous control simulation benchmarks as well as real-world robotic vision-based manipulation tasks. The results show that it strongly outperforms DAgger-like approaches across the different tasks, especially when the intervening experts are suboptimal. Code and videos can be found on the project website: https://rlif-page.github.io
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2311.01378.pdf' target='_blank'>https://arxiv.org/pdf/2311.01378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01378">Vision-Language Foundation Models as Effective Robot Imitators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2309.16237.pdf' target='_blank'>https://arxiv.org/pdf/2309.16237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaman Li, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16237">Object Motion Guided Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human behaviors in contextual environments has a wide range of applications in character animation, embodied AI, VR/AR, and robotics. In real-world scenarios, humans frequently interact with the environment and manipulate various objects to complete daily tasks. In this work, we study the problem of full-body human motion synthesis for the manipulation of large-sized objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a conditional diffusion framework that can generate full-body manipulation behaviors from only the object motion. Since naively applying diffusion models fails to precisely enforce contact constraints between the hands and the object, OMOMO learns two separate denoising processes to first predict hand positions from object motion and subsequently synthesize full-body poses based on the predicted hand positions. By employing the hand positions as an intermediate representation between the two denoising processes, we can explicitly enforce contact constraints, resulting in more physically plausible manipulation motions. With the learned model, we develop a novel system that captures full-body human manipulation motions by simply attaching a smartphone to the object being manipulated. Through extensive experiments, we demonstrate the effectiveness of our proposed pipeline and its ability to generalize to unseen objects. Additionally, as high-quality human-object interaction datasets are scarce, we collect a large-scale dataset consisting of 3D object geometry, object motion, and human motion. Our dataset contains human-object interaction motion for 15 objects, with a total duration of approximately 10 hours.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2307.03610.pdf' target='_blank'>https://arxiv.org/pdf/2307.03610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kareem A. Eltouny, Wansong Liu, Sibo Tian, Minghui Zheng, Xiao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03610">DE-TGN: Uncertainty-Aware Human Motion Forecasting using Deep Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring the safety of human workers in a collaborative environment with robots is of utmost importance. Although accurate pose prediction models can help prevent collisions between human workers and robots, they are still susceptible to critical errors. In this study, we propose a novel approach called deep ensembles of temporal graph neural networks (DE-TGN) that not only accurately forecast human motion but also provide a measure of prediction uncertainty. By leveraging deep ensembles and employing stochastic Monte-Carlo dropout sampling, we construct a volumetric field representing a range of potential future human poses based on covariance ellipsoids. To validate our framework, we conducted experiments using three motion capture datasets including Human3.6M, and two human-robot interaction scenarios, achieving state-of-the-art prediction error. Moreover, we discovered that deep ensembles not only enable us to quantify uncertainty but also improve the accuracy of our predictions.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2303.00638.pdf' target='_blank'>https://arxiv.org/pdf/2303.00638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiatao Sun, Shuo Yang, Mingyan Zhou, Kunpeng Liu, Rahul Mangharam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00638">MEGA-DAgger: Imitation Learning with Multiple Imperfect Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has been widely applied to various autonomous systems thanks to recent development in interactive algorithms that address covariate shift and compounding errors induced by traditional approaches like behavior cloning. However, existing interactive imitation learning methods assume access to one perfect expert. Whereas in reality, it is more likely to have multiple imperfect experts instead. In this paper, we propose MEGA-DAgger, a new DAgger variant that is suitable for interactive learning with multiple imperfect experts. First, unsafe demonstrations are filtered while aggregating the training data, so the imperfect demonstrations have little influence when training the novice policy. Next, experts are evaluated and compared on scenarios-specific metrics to resolve the conflicted labels among experts. Through experiments in autonomous racing scenarios, we demonstrate that policy learned using MEGA-DAgger can outperform both experts and policies learned using the state-of-the-art interactive imitation learning algorithms such as Human-Gated DAgger. The supplementary video can be found at \url{https://youtu.be/wPCht31MHrw}.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2210.05894.pdf' target='_blank'>https://arxiv.org/pdf/2210.05894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanrui Li, Xinyang Liu, Giuseppe Loianno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05894">Human-Aware Physical Human-Robot Collaborative Transportation and Manipulation with Multiple Aerial Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction will play an essential role in various industries and daily tasks, enabling robots to effectively collaborate with humans and reduce their physical workload. Most of the existing approaches for physical human-robot interaction focus on collaboration between a human and a single ground or aerial robot. In recent years, very little progress has been made in this research area when considering multiple aerial robots, which offer increased versatility and mobility. This paper proposes a novel approach for physical human-robot collaborative transportation and manipulation of a cable-suspended payload with multiple aerial robots. The proposed method enables smooth and intuitive interaction between the transported objects and a human worker. In the same time, we consider distance constraints during the operations by exploiting the internal redundancy of the multi-robot transportation system. The key elements of our approach are (a) a collaborative payload external wrench estimator that does not rely on any force sensor; (b) a 6D admittance controller for human-aerial-robot collaborative transportation and manipulation; (c) a human-aware force distribution that exploits the internal system redundancy to guarantee the execution of additional tasks such inter-human-robot separation without affecting the payload trajectory tracking or quality of interaction. We validate the approach through extensive simulation and real-world experiments. These include scenarios where the robot team assists the human in transporting and manipulating a load, or where the human helps the robot team navigate the environment. We experimentally demonstrate for the first time, to the best of our knowledge, that our approach enables a quadrotor team to physically collaborate with a human in manipulating a payload in all 6 DoF in collaborative human-robot transportation and manipulation tasks.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2209.15073.pdf' target='_blank'>https://arxiv.org/pdf/2209.15073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiatao Sun, Mingyan Zhou, Zhijun Zhuang, Shuo Yang, Johannes Betz, Rahul Mangharam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.15073">A Benchmark Comparison of Imitation Learning-based Control Policies for Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous racing with scaled race cars has gained increasing attention as an effective approach for developing perception, planning and control algorithms for safe autonomous driving at the limits of the vehicle's handling. To train agile control policies for autonomous racing, learning-based approaches largely utilize reinforcement learning, albeit with mixed results. In this study, we benchmark a variety of imitation learning policies for racing vehicles that are applied directly or for bootstrapping reinforcement learning both in simulation and on scaled real-world environments. We show that interactive imitation learning techniques outperform traditional imitation learning methods and can greatly improve the performance of reinforcement learning policies by bootstrapping thanks to its better sample efficiency. Our benchmarks provide a foundation for future research on autonomous racing using Imitation Learning and Reinforcement Learning.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2202.13401.pdf' target='_blank'>https://arxiv.org/pdf/2202.13401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Leonori, Juan M. Gandarias, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13401">MOCA-S: A Sensitive Mobile Collaborative Robotic Assistant exploiting Low-Cost Capacitive Tactile Cover and Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety is one of the most fundamental aspects of robotics, especially when it comes to collaborative robots (cobots) that are expected to physically interact with humans. Although a large body of literature has focused on safety-related aspects for fixed-based cobots, a low effort has been put into developing collaborative mobile manipulators. In response to this need, this work presents MOCA-S, i.e., Sensitive Mobile Collaborative Robotic Assistant, that integrates a low-cost, capacitive tactile cover to measure interaction forces applied to the robot base. The tactile cover comprises a set of 11 capacitive large-area tactile sensors distributed as a 1-D tactile array around the base. Characterization of the tactile sensors with different materials is included. Moreover, two expanded whole-body controllers that exploit the platform's tactile cover and the loco-manipulation features are proposed. These controllers are tested in two experiments, demonstrating the potential of MOCA-S for safe physical Human-Robot Interaction (pHRI). Finally, an experiment is carried out in which an undesired collision occurs between MOCA-S and a human during a loco-manipulation task. The results demonstrate the intrinsic safety of MOCA-S and the proposed controllers, suggesting a new step towards creating safe mobile manipulators.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2105.01220.pdf' target='_blank'>https://arxiv.org/pdf/2105.01220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zahra Zahedi, Mudit Verma, Sarath Sreedharan, Subbarao Kambhampati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.01220">Trust-Aware Planning: Modeling Trust Evolution in Iterated Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trust between team members is an essential requirement for any successful cooperation. Thus, engendering and maintaining the fellow team members' trust becomes a central responsibility for any member trying to not only successfully participate in the task but to ensure the team achieves its goals. The problem of trust management is particularly challenging in mixed human-robot teams where the human and the robot may have different models about the task at hand and thus may have different expectations regarding the current course of action, thereby forcing the robot to focus on the costly explicable behavior. We propose a computational model for capturing and modulating trust in such iterated human-robot interaction settings, where the human adopts a supervisory role. In our model, the robot integrates human's trust and their expectations about the robot into its planning process to build and maintain trust over the interaction horizon. By establishing the required level of trust, the robot can focus on maximizing the team goal by eschewing explicit explanatory or explicable behavior without worrying about the human supervisor monitoring and intervening to stop behaviors they may not necessarily understand. We model this reasoning about trust levels as a meta reasoning process over individual planning tasks. We additionally validate our model through a human subject experiment.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2509.20739.pdf' target='_blank'>https://arxiv.org/pdf/2509.20739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoyang Zhao, Yudong Li, Weiqing Qi, Kai Zhang, Bonan Liu, Kai Chen, Haoang Li, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20739">SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration. To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations. A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference. And a semantic-probabilistic topological map supports coarse-to-fine planning: LLM-based global reasoning for subgoal selection and vision-based local planning for obstacle avoidance. Integrated with reinforcement-learning locomotion controllers, the framework is deployable across diverse legged robot platforms. Experiments in simulation and real-world settings demonstrate consistent improvements in semantic accuracy, planning quality, and navigation success, while ablation studies further showcase the necessity of both hierarchical perception and fine local planning. This work introduces a new paradigm for SLAM-free, vision-language-driven navigation, shifting robotic exploration from geometry-centric mapping to semantics-driven decision making.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2509.16924.pdf' target='_blank'>https://arxiv.org/pdf/2509.16924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Li, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16924">Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention \textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2508.20812.pdf' target='_blank'>https://arxiv.org/pdf/2508.20812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20812">Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2508.15663.pdf' target='_blank'>https://arxiv.org/pdf/2508.15663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15663">Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks are crucial for evaluating progress in robotics and embodied AI. However, a significant gap exists between benchmarks designed for high-level language instruction following, which often assume perfect low-level execution, and those for low-level robot control, which rely on simple, one-step commands. This disconnect prevents a comprehensive evaluation of integrated systems where both task planning and physical execution are critical. To address this, we propose Kitchen-R, a novel benchmark that unifies the evaluation of task planning and low-level control within a simulated kitchen environment. Built as a digital twin using the Isaac Sim simulator and featuring more than 500 complex language instructions, Kitchen-R supports a mobile manipulator robot. We provide baseline methods for our benchmark, including a task-planning strategy based on a vision-language model and a low-level control policy based on diffusion policy. We also provide a trajectory collection system. Our benchmark offers a flexible framework for three evaluation modes: independent assessment of the planning module, independent assessment of the control policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R bridges a key gap in embodied AI research, enabling more holistic and realistic benchmarking of language-guided robotic agents.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2508.09032.pdf' target='_blank'>https://arxiv.org/pdf/2508.09032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxim A. Patratskiy, Alexey K. Kovalev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09032">Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at https://ampiromax.github.io/ST-VLA.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2508.07611.pdf' target='_blank'>https://arxiv.org/pdf/2508.07611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifan Wang, Xun Yang, Jianzhuang Zhao, Jiaming Zhou, Teli Ma, Ziyao Gao, Arash Ajoudani, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07611">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of humanoid robots in unstructured, human-centric environments requires navigation capabilities that extend beyond simple locomotion to include robust perception, provable safety, and socially aware behavior. Current reinforcement learning approaches are often limited by blind controllers that lack environmental awareness or by vision-based systems that fail to perceive complex 3D obstacles. In this work, we present an end-to-end locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to motor commands, enabling robust navigation in cluttered dynamic scenes. We formulate the control problem as a Constrained Markov Decision Process (CMDP) to formally separate safety from task objectives. Our key contribution is a novel methodology that translates the principles of Control Barrier Functions (CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal Policy Optimization (P3O) to enforce safety constraints during training. Furthermore, we introduce a set of comfort-oriented rewards, grounded in human-robot interaction research, to promote motions that are smooth, predictable, and less intrusive. We demonstrate the efficacy of our framework through a successful sim-to-real transfer to a physical humanoid robot, which exhibits agile and safe navigation around both static and dynamic 3D obstacles.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2508.04598.pdf' target='_blank'>https://arxiv.org/pdf/2508.04598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhang, Xiaoshuai Hao, Yingbo Tang, Haoxiang Fu, Xinyu Zheng, Pengwei Wang, Zhongyuan Wang, Wenbo Ding, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04598">$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation is a fundamental capability of embodied intelligence, enabling robots to move and interact within physical environments. However, existing navigation tasks primarily focus on predefined object navigation or instruction following, which significantly differs from human needs in real-world scenarios involving complex, open-ended scenes. To bridge this gap, we introduce a challenging long-horizon navigation task that requires understanding high-level human instructions and performing spatial-aware object navigation in real-world environments. Existing embodied navigation methods struggle with such tasks due to their limitations in comprehending high-level human instructions and localizing objects with an open vocabulary. In this paper, we propose $NavA^3$, a hierarchical framework divided into two stages: global and local policies. In the global policy, we leverage the reasoning capabilities of Reasoning-VLM to parse high-level human instructions and integrate them with global 3D scene views. This allows us to reason and navigate to regions most likely to contain the goal object. In the local policy, we have collected a dataset of 1.0 million samples of spatial-aware object affordances to train the NaviAfford model (PointingVLM), which provides robust open-vocabulary object localization and spatial awareness for precise goal identification and navigation in complex environments. Extensive experiments demonstrate that $NavA^3$ achieves SOTA results in navigation performance and can successfully complete longhorizon navigation tasks across different robot embodiments in real-world settings, paving the way for universal embodied navigation. The dataset and code will be made available. Project website: https://NavigationA3.github.io/.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2507.02029.pdf' target='_blank'>https://arxiv.org/pdf/2507.02029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Mengfei Du, Cheng Chi, Mengdi Zhao, Xiaoshuai Hao, Junkai Zhao, Xiaojie Zhang, Shanyu Rong, Huaihai Lyu, Zhengliang Cai, Yankai Fu, Ning Chen, Bolun Zhang, Lingfeng Zhang, Shuyi Zhang, Dong Liu, Xi Feng, Songjing Wang, Xiaodan Liu, Yance Jiao, Mengsi Lyu, Zhuo Chen, Chenrui He, Yulong Ao, Xue Sun, Zheqi He, Jingshu Zheng, Xi Yang, Donghai Shi, Kunchang Xie, Bochao Zhang, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02029">RoboBrain 2.0 Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2506.15377.pdf' target='_blank'>https://arxiv.org/pdf/2506.15377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Xinshu Li, Chen Wang, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15377">Efficient and Generalizable Environmental Understanding for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2506.10600.pdf' target='_blank'>https://arxiv.org/pdf/2506.10600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinjie Wang, Liu Liu, Yu Cao, Ruiqi Wu, Wenkang Qin, Dehui Wang, Wei Sui, Zhizhong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10600">EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2506.08334.pdf' target='_blank'>https://arxiv.org/pdf/2506.08334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weikun Peng, Jun Lv, Cewu Lu, Manolis Savva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08334">Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Articulated objects are prevalent in daily life. Understanding their kinematic structure and reconstructing them have numerous applications in embodied AI and robotics. However, current methods require carefully captured data for training or inference, preventing practical, scalable, and generalizable reconstruction of articulated objects. We focus on reconstruction of an articulated object from a casually captured RGBD video shot with a hand-held camera. A casually captured video of an interaction with an articulated object is easy to acquire at scale using smartphones. However, this setting is quite challenging, as the object and camera move simultaneously and there are significant occlusions as the person interacts with the object. To tackle these challenges, we introduce a coarse-to-fine framework that infers joint parameters and segments movable parts of the object from a dynamic RGBD video. To evaluate our method under this new setting, we build a 20$\times$ larger synthetic dataset of 784 videos containing 284 objects across 11 categories. We compare our approach with existing methods that also take video as input. Experiments show that our method can reconstruct synthetic and real articulated objects across different categories from dynamic RGBD videos, outperforming existing methods significantly.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2505.18078.pdf' target='_blank'>https://arxiv.org/pdf/2505.18078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Chen, Mingjin Chen, Jianjin Xu, Xiang Li, Junting Dong, Mingze Sun, Puhua Jiang, Hongxiang Li, Yuhang Yang, Hao Zhao, Xiaoxiao Long, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18078">DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds "who" and "how" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2505.16663.pdf' target='_blank'>https://arxiv.org/pdf/2505.16663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haihong Hao, Mingfei Han, Changlin Li, Zhihui Li, Xiaojun Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16663">CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation demands comprehensive scene understanding and precise spatial reasoning. While image-text models excel at interpreting pixel-level color and lighting cues, 3D-text models capture volumetric structure and spatial relationships. However, unified fusion approaches that jointly fuse 2D images, 3D point clouds, and textual instructions face challenges in limited availability of triple-modality data and difficulty resolving conflicting beliefs among modalities. In this work, we introduce CoNav, a collaborative cross-modal reasoning framework where a pretrained 3D-text model explicitly guides an image-text navigation agent by providing structured spatial-semantic knowledge to resolve ambiguities during navigation. Specifically, we introduce Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance by simply sharing textual hypotheses from the 3D-text model to the navigation agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the navigation agent learns to integrate visual cues with spatial-semantic knowledge derived from the 3D-text model, enabling effective reasoning in embodied navigation. CoNav achieves significant improvements on four standard embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success Rate, CoNav often generates shorter paths compared to other methods (as measured by SPL), showcasing the potential and challenges of fusing data from different modalities in embodied navigation. Project Page: https://oceanhao.github.io/CoNav/
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2505.11528.pdf' target='_blank'>https://arxiv.org/pdf/2505.11528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Huang, Jiazhao Zhang, Shilong Zou, Xinwang Liu, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11528">LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2505.07446.pdf' target='_blank'>https://arxiv.org/pdf/2505.07446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanjing Ye, Yu Zhan, Weixi Situ, Guangcheng Chen, Jingwen Yu, Ziqi Zhao, Kuanqi Cai, Arash Ajoudani, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07446">TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking a target person from robot-egocentric views is crucial for developing autonomous robots that provide continuous personalized assistance or collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most existing target person tracking (TPT) benchmarks are limited to controlled laboratory environments with few distractions, clean backgrounds, and short-term occlusions. In this paper, we introduce a large-scale dataset designed for TPT in crowded and unstructured environments, demonstrated through a robot-person following task. The dataset is collected by a human pushing a sensor-equipped cart while following a target person, capturing human-like following behavior and emphasizing long-term tracking challenges, including frequent occlusions and the need for re-identification from numerous pedestrians. It includes multi-modal data streams, including odometry, 3D LiDAR, IMU, panoramic images, and RGB-D images, along with exhaustively annotated 2D bounding boxes of the target person across 48 sequences, both indoors and outdoors. Using this dataset and visual annotations, we perform extensive experiments with existing SOTA TPT methods, offering a thorough analysis of their limitations and suggesting future research directions.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2505.03233.pdf' target='_blank'>https://arxiv.org/pdf/2505.03233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, Heming Cui, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03233">GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2503.23765.pdf' target='_blank'>https://arxiv.org/pdf/2503.23765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Li, Yiming Zhang, Tao Lin, Xiangrui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23765">STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we introduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2503.21056.pdf' target='_blank'>https://arxiv.org/pdf/2503.21056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqing Shen, Bohan Liu, Chenjia Li, Lalithkumar Seenivasan, Mathias Unberath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21056">Online Reasoning Video Segmentation with Just-in-Time Digital Twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- a LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as "just-in-time" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2503.15764.pdf' target='_blank'>https://arxiv.org/pdf/2503.15764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Xiao, Guangming Shi, Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15764">Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promising potential of AI and network convergence in improving networking performance and enabling new service capabilities has recently attracted significant interest. Existing network AI solutions, while powerful, are mainly built based on the close-loop and passive learning framework, resulting in major limitations in autonomous solution finding and dynamic environmental adaptation. Agentic AI has recently been introduced as a promising solution to address the above limitations and pave the way for true generally intelligent and beneficial AI systems. The key idea is to create a networking ecosystem to support a diverse range of autonomous and embodied AI agents in fulfilling their goals. In this paper, we focus on the novel challenges and requirements of agentic AI networking. We propose AgentNet, a novel framework for supporting interaction, collaborative learning, and knowledge transfer among AI agents. We introduce a general architectural framework of AgentNet and then propose a generative foundation model (GFM)-based implementation in which multiple GFM-as-agents have been created as an interactive knowledge-base to bootstrap the development of embodied AI agents according to different task requirements and environmental features. We consider two application scenarios, digital-twin-based industrial automation and metaverse-based infotainment system, to describe how to apply AgentNet for supporting efficient task-driven collaboration and interaction among AI agents.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2503.05231.pdf' target='_blank'>https://arxiv.org/pdf/2503.05231.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Jiang, Haonan Li, Ruochen Ren, Yanmin Zhou, Zhipeng Wang, Bin He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05231">Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cutting-edge robot learning techniques including foundation models and imitation learning from humans all pose huge demands on large-scale and high-quality datasets which constitute one of the bottleneck in the general intelligent robot fields. This paper presents the Kaiwu multimodal dataset to address the missing real-world synchronized multimodal data problems in the sophisticated assembling scenario,especially with dynamics information and its fine-grained labelling. The dataset first provides an integration of human,environment and robot data collection framework with 20 subjects and 30 interaction objects resulting in totally 11,664 instances of integrated actions. For each of the demonstration,hand motions,operation pressures,sounds of the assembling process,multi-view videos, high-precision motion capture information,eye gaze with first-person videos,electromyography signals are all recorded. Fine-grained multi-level annotation based on absolute timestamp,and semantic segmentation labelling are performed. Kaiwu dataset aims to facilitate robot learning,dexterous manipulation,human intention investigation and human-robot collaboration research.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2503.02916.pdf' target='_blank'>https://arxiv.org/pdf/2503.02916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhan, Hanjing Ye, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02916">Monocular Person Localization under Camera Ego-motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localizing a person from a moving monocular camera is critical for Human-Robot Interaction (HRI). To estimate the 3D human position from a 2D image, existing methods either depend on the geometric assumption of a fixed camera or use a position regression model trained on datasets containing little camera ego-motion. These methods are vulnerable to fierce camera ego-motion, resulting in inaccurate person localization. We consider person localization as a part of a pose estimation problem. By representing a human with a four-point model, our method jointly estimates the 2D camera attitude and the person's 3D location through optimization. Evaluations on both public datasets and real robot experiments demonstrate our method outperforms baselines in person localization accuracy. Our method is further implemented into a person-following system and deployed on an agile quadruped robot.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2503.01238.pdf' target='_blank'>https://arxiv.org/pdf/2503.01238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jensen Gao, Suneel Belkhale, Sudeep Dasari, Ashwin Balakrishna, Dhruv Shah, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01238">A Taxonomy for Evaluating Generalist Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning for robotics promises to unlock generalization to novel tasks and environments. Guided by this promise, many recent works have focused on scaling up robot data collection and developing larger, more expressive policies to achieve this. But how do we measure progress towards this goal of policy generalization in practice? Evaluating and quantifying generalization is the Wild West of modern robotics, with each work proposing and measuring different types of generalization in their own, often difficult to reproduce, settings. In this work, our goal is (1) to outline the forms of generalization we believe are important in robot manipulation in a comprehensive and fine-grained manner, and (2) to provide reproducible guidelines for measuring these notions of generalization. We first propose STAR-Gen, a taxonomy of generalization for robot manipulation structured around visual, semantic, and behavioral generalization. We discuss how our taxonomy encompasses most prior notions of generalization in robotics. Next, we instantiate STAR-Gen with a concrete real-world benchmark based on the widely-used Bridge V2 dataset. We evaluate a variety of state-of-the-art models on this benchmark to demonstrate the utility of our taxonomy in practice. Our taxonomy of generalization can yield many interesting insights into existing models: for example, we observe that current vision-language-action models struggle with various types of semantic generalization, despite the promise of pre-training on internet-scale language datasets. We believe STAR-Gen and our guidelines can improve the dissemination and evaluation of progress towards generalization in robotics, which we hope will guide model design and future data collection efforts. We provide videos and demos at our website stargen-taxonomy.github.io.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2503.00778.pdf' target='_blank'>https://arxiv.org/pdf/2503.00778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingbo Tang, Shuaike Zhang, Xiaoshuai Hao, Pengwei Wang, Jianlong Wu, Zhongyuan Wang, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00778">AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring the affordance of an object and grasping it in a task-oriented manner is crucial for robots to successfully complete manipulation tasks. Affordance indicates where and how to grasp an object by taking its functionality into account, serving as the foundation for effective task-oriented grasping. However, current task-oriented methods often depend on extensive training data that is confined to specific tasks and objects, making it difficult to generalize to novel objects and complex scenes. In this paper, we introduce AffordGrasp, a novel open-vocabulary grasping framework that leverages the reasoning capabilities of vision-language models (VLMs) for in-context affordance reasoning. Unlike existing methods that rely on explicit task and object specifications, our approach infers tasks directly from implicit user instructions, enabling more intuitive and seamless human-robot interaction in everyday scenarios. Building on the reasoning outcomes, our framework identifies task-relevant objects and grounds their part-level affordances using a visual grounding module. This allows us to generate task-oriented grasp poses precisely within the affordance regions of the object, ensuring both functional and context-aware robotic manipulation. Extensive experiments demonstrate that AffordGrasp achieves state-of-the-art performance in both simulation and real-world scenarios, highlighting the effectiveness of our method. We believe our approach advances robotic manipulation techniques and contributes to the broader field of embodied AI. Project website: https://eqcy.github.io/affordgrasp/.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2503.00508.pdf' target='_blank'>https://arxiv.org/pdf/2503.00508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dehao Huang, Wenlong Dong, Chao Tang, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00508">HGDiffuser: Efficient Task-Oriented Grasp Generation via Human-Guided Grasp Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) is essential for robots to perform manipulation tasks, requiring grasps that are both stable and compliant with task-specific constraints. Humans naturally grasp objects in a task-oriented manner to facilitate subsequent manipulation tasks. By leveraging human grasp demonstrations, current methods can generate high-quality robotic parallel-jaw task-oriented grasps for diverse objects and tasks. However, they still encounter challenges in maintaining grasp stability and sampling efficiency. These methods typically rely on a two-stage process: first performing exhaustive task-agnostic grasp sampling in the 6-DoF space, then applying demonstration-induced constraints (e.g., contact regions and wrist orientations) to filter candidates. This leads to inefficiency and potential failure due to the vast sampling space. To address this, we propose the Human-guided Grasp Diffuser (HGDiffuser), a diffusion-based framework that integrates these constraints into a guided sampling process. Through this approach, HGDiffuser directly generates 6-DoF task-oriented grasps in a single stage, eliminating exhaustive task-agnostic sampling. Furthermore, by incorporating Diffusion Transformer (DiT) blocks as the feature backbone, HGDiffuser improves grasp generation quality compared to MLP-based methods. Experimental results demonstrate that our approach significantly improves the efficiency of task-oriented grasp generation, enabling more effective transfer of human grasping strategies to robotic systems. To access the source code and supplementary videos, visit https://sites.google.com/view/hgdiffuser.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2412.02075.pdf' target='_blank'>https://arxiv.org/pdf/2412.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Liu, Xinjie Wang, Jiaxiong Qiu, Tianwei Lin, Xiaolin Zhou, Zhizhong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02075">Gaussian Object Carver: Object-Compositional Gaussian Splatting with surfaces completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene reconstruction is a foundational problem in computer vision. Despite recent advancements in Neural Implicit Representations (NIR), existing methods often lack editability and compositional flexibility, limiting their use in scenarios requiring high interactivity and object-level manipulation. In this paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and scalable framework for object-compositional 3D scene reconstruction. GOC leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors and multi-view geometry regularization, to achieve high-quality and flexible reconstruction. Furthermore, we propose a zero-shot Object Surface Completion (OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved surfaces, ensuring object completeness even in occluded areas. Experimental results demonstrate that GOC improves reconstruction efficiency and geometric fidelity. It holds promise for advancing the practical application of digital twins in embodied AI, AR/VR, and interactive simulation environments.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2411.12286.pdf' target='_blank'>https://arxiv.org/pdf/2411.12286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teli Ma, Zifan Wang, Jiaming Zhou, Mengmeng Wang, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12286">GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict the visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 table-top real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. We also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2411.08579.pdf' target='_blank'>https://arxiv.org/pdf/2411.08579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youzhi Liu, Fanglong Yao, Yuanchang Yue, Guangluan Xu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08579">NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2409.16033.pdf' target='_blank'>https://arxiv.org/pdf/2409.16033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Dong, Dehao Huang, Jiangshan Liu, Chao Tang, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16033">RTAGrasp: Learning Task-Oriented Grasping from Human Videos via Retrieval, Transfer, and Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) is crucial for robots to accomplish manipulation tasks, requiring the determination of TOG positions and directions. Existing methods either rely on costly manual TOG annotations or only extract coarse grasping positions or regions from human demonstrations, limiting their practicality in real-world applications. To address these limitations, we introduce RTAGrasp, a Retrieval, Transfer, and Alignment framework inspired by human grasping strategies. Specifically, our approach first effortlessly constructs a robot memory from human grasping demonstration videos, extracting both TOG position and direction constraints. Then, given a task instruction and a visual observation of the target object, RTAGrasp retrieves the most similar human grasping experience from its memory and leverages semantic matching capabilities of vision foundation models to transfer the TOG constraints to the target object in a training-free manner. Finally, RTAGrasp aligns the transferred TOG constraints with the robot's action for execution. Evaluations on the public TOG benchmark, TaskGrasp dataset, show the competitive performance of RTAGrasp on both seen and unseen object categories compared to existing baseline methods. Real-world experiments further validate its effectiveness on a robotic arm. Our code, appendix, and video are available at \url{https://sites.google.com/view/rtagrasp/home}.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2409.02669.pdf' target='_blank'>https://arxiv.org/pdf/2409.02669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Yao Liu, Yuanjiang Cao, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02669">Causality-Aware Transformer Networks for Robotic Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current research in Visual Navigation reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Navigation tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Navigation. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2409.01581.pdf' target='_blank'>https://arxiv.org/pdf/2409.01581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Guo, Yifan Xie, Weijing Xie, Peng Huang, Fei Ma, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01581">GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2408.15717.pdf' target='_blank'>https://arxiv.org/pdf/2408.15717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salma Salimi, Sahar Salimpour, Jorge PeÃ±a Queralta, Wallace Moreira Bessa, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15717">Benchmarking ML Approaches to UWB-Based Range-Only Posture Recognition for Human Robot-Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose estimation involves detecting and tracking the positions of various body parts using input data from sources such as images, videos, or motion and inertial sensors. This paper presents a novel approach to human pose estimation using machine learning algorithms to predict human posture and translate them into robot motion commands using ultra-wideband (UWB) nodes, as an alternative to motion sensors. The study utilizes five UWB sensors implemented on the human body to enable the classification of still poses and more robust posture recognition. This approach ensures effective posture recognition across a variety of subjects. These range measurements serve as input features for posture prediction models, which are implemented and compared for accuracy. For this purpose, machine learning algorithms including K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and deep Multi-Layer Perceptron (MLP) neural network are employed and compared in predicting corresponding postures. We demonstrate the proposed approach for real-time control of different mobile/aerial robots with inference implemented in a ROS 2 node. Experimental results demonstrate the efficacy of the approach, showcasing successful prediction of human posture and corresponding robot movements with high accuracy.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2408.15511.pdf' target='_blank'>https://arxiv.org/pdf/2408.15511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanglong Yao, Yuanchang Yue, Youzhi Liu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15511">AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerospace embodied intelligence aims to empower unmanned aerial vehicles (UAVs) and other aerospace platforms to achieve autonomous perception, cognition, and action, as well as egocentric active interaction with humans and the environment. The aerospace embodied world model serves as an effective means to realize the autonomous intelligence of UAVs and represents a necessary pathway toward aerospace embodied intelligence. However, existing embodied world models primarily focus on ground-level intelligent agents in indoor scenarios, while research on UAV intelligent agents remains unexplored. To address this gap, we construct the first large-scale real-world image-text pre-training dataset, AerialAgent-Ego10k, featuring urban drones from a first-person perspective. We also create a virtual image-text-pose alignment dataset, CyberAgent Ego500k, to facilitate the pre-training of the aerospace embodied world model. For the first time, we clearly define 5 downstream tasks, i.e., aerospace embodied scene awareness, spatial reasoning, navigational exploration, task planning, and motion decision, and construct corresponding instruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k and SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace embodiment world model. Simultaneously, we develop SkyAgentEval, the downstream task evaluation metrics based on GPT-4, to comprehensively, flexibly, and objectively assess the results, revealing the potential and limitations of 2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over 10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning datasets, more than 10 evaluation metrics, and a simulator into the benchmark suite, i.e., AeroVerse, which will be released to the community to promote exploration and development of aerospace embodied intelligence.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2405.05852.pdf' target='_blank'>https://arxiv.org/pdf/2405.05852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05852">Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2405.02821.pdf' target='_blank'>https://arxiv.org/pdf/2405.02821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changan Chen, Jordi Ramos, Anshul Tomar, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02821">Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim2real transfer has received increasing attention lately due to the success of learning robotic tasks in simulation end-to-end. While there has been a lot of progress in transferring vision-based navigation policies, the existing sim2real strategy for audio-visual navigation performs data augmentation empirically without measuring the acoustic gap. The sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real. We propose the first treatment of sim2real for audio-visual navigation by disentangling it into acoustic field prediction (AFP) and waypoint navigation. We first validate our design choice in the SoundSpaces simulator and show improvement on the Continuous AudioGoal navigation benchmark. We then collect real-world data to measure the spectral difference between the simulation and the real world by training AFP models that only take a specific frequency subband as input. We further propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured spectral difference and the energy distribution of the received audio, which improves the performance on the real data. Lastly, we build a real robot platform and show that the transferred policy can successfully navigate to sounding objects. This work demonstrates the potential of building intelligent agents that can see, hear, and act entirely from simulation, and transferring them to the real world.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2405.01472.pdf' target='_blank'>https://arxiv.org/pdf/2405.01472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Hoque, Ajay Mandlekar, Caelan Garrett, Ken Goldberg, Dieter Fox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01472">IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning is a promising paradigm for training robot control policies, but these policies can suffer from distribution shift, where the conditions at evaluation time differ from those in the training data. A popular approach for increasing policy robustness to distribution shift is interactive imitation learning (i.e., DAgger and variants), where a human operator provides corrective interventions during policy rollouts. However, collecting a sufficient amount of interventions to cover the distribution of policy mistakes can be burdensome for human operators. We propose IntervenGen (I-Gen), a novel data generation system that can autonomously produce a large set of corrective interventions with rich coverage of the state space from a small number of human interventions. We apply I-Gen to 4 simulated environments and 1 physical environment with object pose estimation error and show that it can increase policy robustness by up to 39x with only 10 human interventions. Videos and more results are available at https://sites.google.com/view/intervengen2024.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2404.10399.pdf' target='_blank'>https://arxiv.org/pdf/2404.10399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Tang, Dehao Huang, Wenlong Dong, Ruinian Xu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10399">FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG), which refers to synthesizing grasps on an object that are configurationally compatible with the downstream manipulation task, is the first milestone towards tool manipulation. Analogous to the activation of two brain regions responsible for semantic and geometric reasoning during cognitive processes, modeling the intricate relationship between objects, tasks, and grasps necessitates rich semantic and geometric prior knowledge about these elements. Existing methods typically restrict the prior knowledge to a closed-set scope, limiting their generalization to novel objects and tasks out of the training set. To address such a limitation, we propose FoundationGrasp, a foundation model-based TOG framework that leverages the open-ended knowledge from foundation models to learn generalizable TOG skills. Extensive experiments are conducted on the contributed Language and Vision Augmented TaskGrasp (LaViA-TaskGrasp) dataset, demonstrating the superiority of FoundationGrasp over existing methods when generalizing to novel object instances, object classes, and tasks out of the training set. Furthermore, the effectiveness of FoundationGrasp is validated in real-robot grasping and manipulation experiments on a 7-DoF robotic arm. Our code, data, appendix, and video are publicly available at https://sites.google.com/view/foundationgrasp.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2404.06609.pdf' target='_blank'>https://arxiv.org/pdf/2404.06609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06609">GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images. However, these navigation models often handle only a single input modality as the target. With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots. To facilitate this goal, we propose GOAT-Bench, a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion. We benchmark monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2402.18380.pdf' target='_blank'>https://arxiv.org/pdf/2402.18380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ines Sorrentino, Giulio Romualdi, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18380">UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multimodal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the underlying robot sensor suit. To validate the approach, we show how the proposed sensor fusion can be integrated into a twolevel torque control architecture aiming at task-space torquecontrol. The performances of the proposed approach are shown through extensive tests on the new humanoid robot ergoCub, currently being developed at Istituto Italiano di Tecnologia. We also compare our strategy with the existing state-of-theart approach based on the recursive Newton-Euler algorithm. Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2310.14925.pdf' target='_blank'>https://arxiv.org/pdf/2310.14925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Castri, Sariah Mghames, Nicola Bellotto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14925">Efficient Causal Discovery for Robotics Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using robots for automating tasks in environments shared with humans, such as warehouses, shopping centres, or hospitals, requires these robots to comprehend the fundamental physical interactions among nearby agents and objects. Specifically, creating models to represent cause-and-effect relationships among these elements can aid in predicting unforeseen human behaviours and anticipate the outcome of particular robot actions. To be suitable for robots, causal analysis must be both fast and accurate, meeting real-time demands and the limited computational resources typical in most robotics applications. In this paper, we present a practical demonstration of our approach for fast and accurate causal analysis, known as Filtered PCMCI (F-PCMCI), along with a real-world robotics application. The provided application illustrates how our F-PCMCI can accurately and promptly reconstruct the causal model of a human-robot interaction scenario, which can then be leveraged to enhance the quality of the interaction.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2310.13724.pdf' target='_blank'>https://arxiv.org/pdf/2310.13724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, VladimÃ­r VondruÅ¡, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, Roozbeh Mottaghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13724">Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2310.06964.pdf' target='_blank'>https://arxiv.org/pdf/2310.06964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viet-Anh Le, Vaishnav Tadiparthi, Behdad Chalaki, Hossein Nourkhiz Mahjoub, Jovin D'sa, Ehsan Moradi-Pari, Andreas A. Malikopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06964">Multi-Robot Cooperative Navigation in Crowds: A Game-Theoretic Learning-Based Model Predictive Control Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we develop a control framework for the coordination of multiple robots as they navigate through crowded environments. Our framework comprises of a local model predictive control (MPC) for each robot and a social long short-term memory model that forecasts pedestrians' trajectories. We formulate the local MPC formulation for each individual robot that includes both individual and shared objectives, in which the latter encourages the emergence of coordination among robots. Next, we consider the multi-robot navigation and human-robot interaction, respectively, as a potential game and a two-player game, then employ an iterative best response approach to solve the resulting optimization problems in a centralized and distributed fashion. Finally, we demonstrate the effectiveness of coordination among robots in simulated crowd navigation.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2309.15302.pdf' target='_blank'>https://arxiv.org/pdf/2309.15302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haresh Karnan, Elvin Yang, Daniel Farkash, Garrett Warnell, Joydeep Biswas, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15302">STERLING: Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments in off-road environments, we evaluate STERLING features on the task of preference-aligned visual navigation and find that STERLING features perform on par with fully supervised approaches and outperform other state-of-the-art methods with respect to preference alignment. Additionally, we perform a large-scale experiment of autonomously hiking a 3-mile long trail which STERLING completes successfully with only two manual interventions, demonstrating its robustness to real-world off-road conditions.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2309.11727.pdf' target='_blank'>https://arxiv.org/pdf/2309.11727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanjing Ye, Jieting Zhao, Yu Zhan, Weinan Chen, Li He, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11727">Person Re-Identification for Robot Person Following with Online Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot person following (RPF) is a crucial capability in human-robot interaction (HRI) applications, allowing a robot to persistently follow a designated person. In practical RPF scenarios, the person can often be occluded by other objects or people. Consequently, it is necessary to re-identify the person when he/she reappears within the robot's field of view. Previous person re-identification (ReID) approaches to person following rely on a fixed feature extractor. Such an approach often fails to generalize to different viewpoints and lighting conditions in practical RPF environments. In other words, it suffers from the so-called domain shift problem where it cannot re-identify the person when his re-appearance is out of the domain modeled by the fixed feature extractor. To mitigate this problem, we propose a ReID framework for RPF where we use a feature extractor that is optimized online with both short-term and long-term experiences (i.e., recently and previously observed samples during RPF) using the online continual learning (OCL) framework. The long-term experiences are maintained by a memory manager to enable OCL to update the feature extractor. Our experiments demonstrate that even in the presence of severe appearance changes and distractions from visually similar people, the proposed method can still re-identify the person more accurately than the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2309.09912.pdf' target='_blank'>https://arxiv.org/pdf/2309.09912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haresh Karnan, Elvin Yang, Garrett Warnell, Joydeep Biswas, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09912">Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference Aligned Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous mobility tasks such as lastmile delivery require reasoning about operator indicated preferences over terrains on which the robot should navigate to ensure both robot safety and mission success. However, coping with out of distribution data from novel terrains or appearance changes due to lighting variations remains a fundamental problem in visual terrain adaptive navigation. Existing solutions either require labor intensive manual data recollection and labeling or use handcoded reward functions that may not align with operator preferences. In this work, we posit that operator preferences for visually novel terrains, which the robot should adhere to, can often be extrapolated from established terrain references within the inertial, proprioceptive, and tactile domain. Leveraging this insight, we introduce Preference extrApolation for Terrain awarE Robot Navigation, PATERN, a novel framework for extrapolating operator terrain preferences for visual navigation. PATERN learns to map inertial, proprioceptive, tactile measurements from the robots observations to a representation space and performs nearest neighbor search in this space to estimate operator preferences over novel terrains. Through physical robot experiments in outdoor environments, we assess PATERNs capability to extrapolate preferences and generalize to novel terrains and challenging lighting conditions. Compared to baseline approaches, our findings indicate that PATERN robustly generalizes to diverse terrains and varied lighting conditions, while navigating in a preference aligned manner.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2309.04596.pdf' target='_blank'>https://arxiv.org/pdf/2309.04596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Chen, Ye-Ji Mun, Zhe Huang, Yilong Niu, Yiqing Xie, D. Livingston McPherson, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04596">Learning Task Skills and Goals Simultaneously from Physical Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world human-robot systems, it is essential for a robot to comprehend human objectives and respond accordingly while performing an extended series of motor actions. Although human objective alignment has recently emerged as a promising paradigm in the realm of physical human-robot interaction, its application is typically confined to generating simple motions due to inherent theoretical limitations. In this work, our goal is to develop a general formulation to learn manipulation functional modules and long-term task goals simultaneously from physical human-robot interaction. We show the feasibility of our framework in enabling robots to align their behaviors with the long-term task objectives inferred from human interactions.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2308.08854.pdf' target='_blank'>https://arxiv.org/pdf/2308.08854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Taioli, Federico Cunico, Federico Girella, Riccardo Bologna, Alessandro Farinelli, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08854">Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field maps with natural language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Le-RNR-Map, a Language-enhanced Renderable Neural Radiance map for Visual Navigation with natural language query prompts. The recently proposed RNR-Map employs a grid structure comprising latent codes positioned at each pixel. These latent codes, which are derived from image observation, enable: i) image rendering given a camera pose, since they are converted to Neural Radiance Field; ii) image navigation and localization with astonishing accuracy. On top of this, we enhance RNR-Map with CLIP-based embedding latent codes, allowing natural language search without additional label data. We evaluate the effectiveness of this map in single and multi-object searches. We also investigate its compatibility with a Large Language Model as an "affordance query resolver". Code and videos are available at https://intelligolabs.github.io/Le-RNR-Map/
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2308.03222.pdf' target='_blank'>https://arxiv.org/pdf/2308.03222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Huang, Ye-Ji Mun, Haonan Chen, Yiqing Xie, Yilong Niu, Xiang Li, Ninghan Zhong, Haoyuan You, D. Livingston McPherson, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03222">Towards Safe Multi-Level Human-Robot Interaction in Industrial Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple levels of safety measures are required by multiple interaction modes which collaborative robots need to perform industrial tasks with human co-workers. We develop three independent modules to account for safety in different types of human-robot interaction: vision-based safety monitoring pauses robot when human is present in a shared space; contact-based safety monitoring pauses robot when unexpected contact happens between human and robot; hierarchical intention tracking keeps robot in a safe distance from human when human and robot work independently, and switches robot to compliant mode when human intends to guide robot. We discuss the prospect of future research in development and integration of multi-level safety modules. We focus on how to provide safety guarantees for collaborative robot solutions with human behavior modeling.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2308.00096.pdf' target='_blank'>https://arxiv.org/pdf/2308.00096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viktor Rakhmatulin, Denis Grankin, Mikhail Konenkov, Sergei Davidenko, Daria Trinitatova, Oleg Sautenkov, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00096">AirTouch: Towards Safe Human-Robot Interaction Using Air Pressure Feedback and IR Mocap System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing use of robots in urban environments has raised concerns about potential safety hazards, especially in public spaces where humans and robots may interact. In this paper, we present a system for safe human-robot interaction that combines an infrared (IR) camera with a wearable marker and airflow potential field. IR cameras enable real-time detection and tracking of humans in challenging environments, while controlled airflow creates a physical barrier that guides humans away from dangerous proximity to robots without the need for wearable devices. A preliminary experiment was conducted to measure the accuracy of the perception of safety barriers rendered by controlled air pressure. In a second experiment, we evaluated our approach in an imitation scenario of an interaction between an inattentive person and an autonomous robotic system. Experimental results show that the proposed system significantly improves a participant's ability to maintain a safe distance from the operating robot compared to trials without the system.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2307.13204.pdf' target='_blank'>https://arxiv.org/pdf/2307.13204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Tang, Dehao Huang, Wenqi Ge, Weiyu Liu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13204">GraspGPT: Leveraging Semantic Knowledge from a Large Language Model for Task-Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) refers to the problem of predicting grasps on an object that enable subsequent manipulation tasks. To model the complex relationships between objects, tasks, and grasps, existing methods incorporate semantic knowledge as priors into TOG pipelines. However, the existing semantic knowledge is typically constructed based on closed-world concept sets, restraining the generalization to novel concepts out of the pre-defined sets. To address this issue, we propose GraspGPT, a large language model (LLM) based TOG framework that leverages the open-end semantic knowledge from an LLM to achieve zero-shot generalization to novel concepts. We conduct experiments on Language Augmented TaskGrasp (LA-TaskGrasp) dataset and demonstrate that GraspGPT outperforms existing TOG methods on different held-out settings when generalizing to novel concepts out of the training set. The effectiveness of GraspGPT is further validated in real-robot experiments. Our code, data, appendix, and video are publicly available at https://sites.google.com/view/graspgpt/.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2307.01886.pdf' target='_blank'>https://arxiv.org/pdf/2307.01886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye-Ji Mun, Zhe Huang, Haonan Chen, Yilong Niu, Haoyuan You, D. Livingston McPherson, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01886">User-Friendly Safety Monitoring System for Manufacturing Cobots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots are being increasingly utilized in industrial production lines due to their efficiency and accuracy. However, the close proximity between humans and robots can pose safety risks due to the robot's high-speed movements and powerful forces. To address this, we developed a vision-based safety monitoring system that creates a 3D reconstruction of the collaborative scene. Our system records the human-robot interaction data in real-time and reproduce their virtual replicas in a simulator for offline analysis. The objective is to provide workers with a user-friendly visualization tool for reviewing performance and diagnosing failures, thereby enhancing safety in manufacturing settings.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2306.06531.pdf' target='_blank'>https://arxiv.org/pdf/2306.06531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06531">AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2305.14260.pdf' target='_blank'>https://arxiv.org/pdf/2305.14260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Fan, Jing Gu, Kaizhi Zheng, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14260">R2H: Building Multimodal Navigation Helpers that Respond to Help Requests</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent's ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations. Project website: https://sites.google.com/view/response2helprequests/home.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2304.02738.pdf' target='_blank'>https://arxiv.org/pdf/2304.02738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopeng Lu, Ingrid Navarro, Jean Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02738">Core Challenges in Embodied Vision-Language Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in the areas of Multimodal Machine Learning and Artificial Intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Robotics. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly leverage computer vision and natural language for interaction in physical environments. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the current and new algorithmic approaches, metrics, simulators, and datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalisability and furthers real-world deployment.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2303.07798.pdf' target='_blank'>https://arxiv.org/pdf/2303.07798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira, Oleksandr Maksymets, Dhruv Batra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07798">OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a single neural network architecture composed of task-agnostic components (ViTs, convolutions, and LSTMs) that achieves state-of-art results on both the ImageNav ("go to location in <this picture>") and ObjectNav ("find a chair") tasks without any task-specific modules like object detection, segmentation, mapping, or planning modules. Such general-purpose methods offer advantages of simplicity in design, positive scaling with available compute, and versatile applicability to multiple tasks. Our work builds upon the recent success of self-supervised learning (SSL) for pre-training vision transformers (ViT). However, while the training recipes for convolutional networks are mature and robust, the recipes for ViTs are contingent and brittle, and in the case of ViTs for visual navigation, yet to be fully discovered. Specifically, we find that vanilla ViTs do not outperform ResNets on visual navigation. We propose the use of a compression layer operating over ViT patch representations to preserve spatial information along with policy training improvements. These improvements allow us to demonstrate positive scaling laws for the first time in visual navigation tasks. Consequently, our model advances state-of-the-art performance on ImageNav from 54.2% to 82.0% success and performs competitively against concurrent state-of-art on ObjectNav with success rate of 64.0% vs. 65.0%. Overall, this work does not present a fundamentally new approach, but rather recommendations for training a general-purpose architecture that achieves state-of-art performance today and could serve as a strong baseline for future methods.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2302.02121.pdf' target='_blank'>https://arxiv.org/pdf/2302.02121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanjing Ye, Jieting Zhao, Yaling Pan, Weinan Chen, Li He, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02121">Robot Person Following Under Partial Occlusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot person following (RPF) is a capability that supports many useful human-robot-interaction (HRI) applications. However, existing solutions to person following often assume full observation of the tracked person. As a consequence, they cannot track the person reliably under partial occlusion where the assumption of full observation is not satisfied. In this paper, we focus on the problem of robot person following under partial occlusion caused by a limited field of view of a monocular camera. Based on the key insight that it is possible to locate the target person when one or more of his/her joints are visible, we propose a method in which each visible joint contributes a location estimate of the followed person. Experiments on a public person-following dataset show that, even under partial occlusion, the proposed method can still locate the person more reliably than the existing SOTA methods. As well, the application of our method is demonstrated in real experiments on a mobile robot.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2206.08312.pdf' target='_blank'>https://arxiv.org/pdf/2206.08312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.08312">SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks -- embodied navigation and far-field automatic speech recognition -- and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2509.06266.pdf' target='_blank'>https://arxiv.org/pdf/2509.06266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Yong Zhang, Mohammad Akbari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06266">Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2508.07839.pdf' target='_blank'>https://arxiv.org/pdf/2508.07839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07839">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affective tactile interaction constitutes a fundamental component of human communication. In natural human-human encounters, touch is seldom experienced in isolation; rather, it is inherently multisensory. Individuals not only perceive the physical sensation of touch but also register the accompanying auditory cues generated through contact. The integration of haptic and auditory information forms a rich and nuanced channel for emotional expression. While extensive research has examined how robots convey emotions through facial expressions and speech, their capacity to communicate social gestures and emotions via touch remains largely underexplored. To address this gap, we developed a multimodal interaction system incorporating a 5*5 grid of 25 vibration motors synchronized with audio playback, enabling robots to deliver combined haptic-audio stimuli. In an experiment involving 32 Chinese participants, ten emotions and six social gestures were presented through vibration, sound, or their combination. Participants rated each stimulus on arousal and valence scales. The results revealed that (1) the combined haptic-audio modality significantly enhanced decoding accuracy compared to single modalities; (2) each individual channel-vibration or sound-effectively supported certain emotions recognition, with distinct advantages depending on the emotional expression; and (3) gestures alone were generally insufficient for conveying clearly distinguishable emotions. These findings underscore the importance of multisensory integration in affective human-robot interaction and highlight the complementary roles of haptic and auditory cues in enhancing emotional communication.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2508.07770.pdf' target='_blank'>https://arxiv.org/pdf/2508.07770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizheng Zhang, Zhenjun Yu, Jiaxin Lai, Cewu Lu, Lei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07770">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment. The code, datasets will be available at https://yizhengzhang1.github.io/agent_world/
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2508.03053.pdf' target='_blank'>https://arxiv.org/pdf/2508.03053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojun Xu, Jiaqi Xiang, Wu Wei, Jinyu Chen, Linqing Zhong, Linjiang Huang, Hongyu Yang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03053">SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A typical human strategy for giving navigation guidance is to sketch route maps based on the environmental layout. Inspired by this, we introduce Sketch map-based visual Navigation (SkeNa), an embodied navigation task in which an agent must reach a goal in an unseen environment using only a hand-drawn sketch map as guidance. To support research for SkeNa, we present a large-scale dataset named SoR, comprising 54k trajectory and sketch map pairs across 71 indoor scenes. In SoR, we introduce two navigation validation sets with varying levels of abstraction in hand-drawn sketches, categorized based on their preservation of spatial scales in the environment, to facilitate future research. To construct SoR, we develop an automated sketch-generation pipeline that efficiently converts floor plans into hand-drawn representations. To solve SkeNa, we propose SkeNavigator, a navigation framework that aligns visual observations with hand-drawn maps to estimate navigation targets. It employs a Ray-based Map Descriptor (RMD) to enhance sketch map valid feature representation using equidistant sampling points and boundary distances. To improve alignment with visual observations, a Dual-Map Aligned Goal Predictor (DAGP) leverages the correspondence between sketch map features and on-site constructed exploration map features to predict goal position and guide navigation. SkeNavigator outperforms prior floor plan navigation methods by a large margin, improving SPL on the high-abstract validation set by 105% relatively. Our code and dataset will be released.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2508.02062.pdf' target='_blank'>https://arxiv.org/pdf/2508.02062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02062">RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $Ï_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$Ï_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2507.22905.pdf' target='_blank'>https://arxiv.org/pdf/2507.22905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22905">Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) become increasingly integrated into robotic systems, their potential to generate socially and culturally appropriate affective touch remains largely unexplored. This study investigates whether LLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive tactile behaviours to convey emotions in human-robot interaction. We produced text based touch descriptions for 12 distinct emotions across three cultural contexts (Chinese, Belgian, and unspecified), and examined their interpretability in both robot-to-human and human-to-robot scenarios. A total of 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified) evaluated these LLM-generated tactile behaviours for emotional decoding and perceived appropriateness. Results reveal that: (1) under matched cultural conditions, participants successfully decoded six out of twelve emotions-mainly socially oriented emotions such as love and Ekman emotions such as anger, however, self-focused emotions like pride and embarrassment were more difficult to interpret; (2) tactile behaviours were perceived as more appropriate when directed from human to robot than from robot to human, revealing an asymmetry in social expectations based on interaction roles; (3) behaviours interpreted as aggressive (e.g., anger), overly intimate (e.g., love), or emotionally ambiguous (i.e., not clearly decodable) were significantly more likely to be rated as inappropriate; and (4) cultural mismatches reduced decoding accuracy and increased the likelihood of behaviours being judged as inappropriate.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2507.21496.pdf' target='_blank'>https://arxiv.org/pdf/2507.21496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Terajima, Katsuma Inoue, Kohei Nakajima, Yasuo Kuniyoshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21496">Multifunctional physical reservoir computing in soft tensegrity robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have demonstrated that the dynamics of physical systems can be utilized for the desired information processing under the framework of physical reservoir computing (PRC). Robots with soft bodies are examples of such physical systems, and their nonlinear body-environment dynamics can be used to compute and generate the motor signals necessary for the control of their own behavior. In this simulation study, we extend this approach to control and embed not only one but also multiple behaviors into a type of soft robot called a tensegrity robot. The resulting system, consisting of the robot and the environment, is a multistable dynamical system that converges to different attractors from varying initial conditions. Furthermore, attractor analysis reveals that there exist "untrained attractors" in the state space of the system outside the training data. These untrained attractors reflect the intrinsic properties and structures of the tensegrity robot and its interactions with the environment. The impacts of these recent findings in PRC remain unexplored in embodied AI research. We here illustrate their potential to understand various features of embodied cognition that have not been fully addressed to date.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2507.17376.pdf' target='_blank'>https://arxiv.org/pdf/2507.17376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianshu Ruan, Aniketh Ramesh, Rustam Stolkin, Manolis Chiou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17376">An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the impact of high-level semantics (evaluation of the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction (HRI) in the context of mobile robot deployments. Although semantics has been widely researched in AI, how high-level semantics can benefit the HRT paradigm is underexplored, often fuzzy, and intractable. We applied a semantics-based framework that could reveal different indicators of the environment (i.e. how much semantic information exists) in a mock-up disaster response mission. In such missions, semantics are crucial as the HRT should handle complex situations and respond quickly with correct decisions, where humans might have a high workload and stress. Especially when human operators need to shift their attention between robots and other tasks, they will struggle to build Situational Awareness (SA) quickly. The experiment suggests that the presented semantics: 1) alleviate the perceived workload of human operators; 2) increase the operator's trust in the SA; and 3) help to reduce the reaction time in switching the level of autonomy when needed. Additionally, we find that participants with higher trust in the system are encouraged by high-level semantics to use teleoperation mode more.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2506.22355.pdf' target='_blank'>https://arxiv.org/pdf/2506.22355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, ThÃ©o Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22355">Embodied AI Agents: Modeling the World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2506.19179.pdf' target='_blank'>https://arxiv.org/pdf/2506.19179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19179">Situated Haptic Interaction: Exploring the Role of Context in Affective Perception of Robotic Touch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affective interaction is not merely about recognizing emotions; it is an embodied, situated process shaped by context and co-created through interaction. In affective computing, the role of haptic feedback within dynamic emotional exchanges remains underexplored. This study investigates how situational emotional cues influence the perception and interpretation of haptic signals given by a robot. In a controlled experiment, 32 participants watched video scenarios in which a robot experienced either positive actions (such as being kissed), negative actions (such as being slapped) or neutral actions. After each video, the robot conveyed its emotional response through haptic communication, delivered via a wearable vibration sleeve worn by the participant. Participants rated the robot's emotional state-its valence (positive or negative) and arousal (intensity)-based on the video, the haptic feedback, and the combination of the two. The study reveals a dynamic interplay between visual context and touch. Participants' interpretation of haptic feedback was strongly shaped by the emotional context of the video, with visual context often overriding the perceived valence of the haptic signal. Negative haptic cues amplified the perceived valence of the interaction, while positive cues softened it. Furthermore, haptics override the participants' perception of arousal of the video. Together, these results offer insights into how situated haptic feedback can enrich affective human-robot interaction, pointing toward more nuanced and embodied approaches to emotional communication with machines.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2506.17811.pdf' target='_blank'>https://arxiv.org/pdf/2506.17811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacky Kwok, Christopher Agia, Rohan Sinha, Matt Foutter, Shulu Li, Ion Stoica, Azalia Mirhoseini, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17811">RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in visuomotor control, yet ensuring their robustness in unstructured real-world environments remains a persistent challenge. In this paper, we investigate test-time scaling through the lens of sampling and verification as means to enhance the robustness and generalization of VLAs. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on these insights, we introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbation and majority voting to construct an action proposal distribution, and then uses a Vision Language Model (VLM)-based verifier to select the optimal action. We propose a synthetic data generation pipeline for training such VLM-based action verifiers, and demonstrate that scaling the synthetic dataset consistently improves verification and downstream accuracy. Through extensive simulated and hardware experiments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25% absolute improvement on out-of-distribution tasks and 9% on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7% performance increase compared to fine-tuning VLAs alone.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2506.07570.pdf' target='_blank'>https://arxiv.org/pdf/2506.07570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Yang, Zhen Luo, Tongsheng Ding, Junru Lu, Mingqi Gao, Jinyu Yang, Victor Sanchez, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07570">OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2506.03834.pdf' target='_blank'>https://arxiv.org/pdf/2506.03834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joonkyung Kim, Joonyeol Sim, Woojun Kim, Katia Sycara, Changjoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03834">CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose CARE (Collision Avoidance via Repulsive Estimation) to improve the robustness of learning-based visual navigation methods. Recently, visual navigation models, particularly foundation models, have demonstrated promising performance by generating viable trajectories using only RGB images. However, these policies can generalize poorly to environments containing out-of-distribution (OOD) scenes characterized by unseen objects or different camera setups (e.g., variations in field of view, camera pose, or focal length). Without fine-tuning, such models could produce trajectories that lead to collisions, necessitating substantial efforts in data collection and additional training. To address this limitation, we introduce CARE, an attachable module that enhances the safety of visual navigation without requiring additional range sensors or fine-tuning of pretrained models. CARE can be integrated seamlessly into any RGB-based navigation model that generates local robot trajectories. It dynamically adjusts trajectories produced by a pretrained model using repulsive force vectors computed from depth images estimated directly from RGB inputs. We evaluate CARE by integrating it with state-of-the-art visual navigation models across diverse robot platforms. Real-world experiments show that CARE significantly reduces collisions (up to 100%) without compromising navigation performance in goal-conditioned navigation, and further improves collision-free travel distance (up to 10.7x) in exploration tasks. Project page: https://airlab-sogang.github.io/CARE/
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2506.01196.pdf' target='_blank'>https://arxiv.org/pdf/2506.01196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ishika Singh, Ankit Goyal, Stan Birchfield, Dieter Fox, Animesh Garg, Valts Blukis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01196">OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and multi-view RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA projects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2505.07634.pdf' target='_blank'>https://arxiv.org/pdf/2505.07634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Liu, Xiongtao Shi, Thai Duy Nguyen, Haitian Zhang, Tianxiang Zhang, Wei Sun, Yanjie Li, Athanasios V. Vasilakos, Giovanni Iacca, Arshad Ali Khan, Arvind Kumar, Jae Won Cho, Ajmal Mian, Lihua Xie, Erik Cambria, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07634">Neural Brain: A Neuroscience-inspired Framework for Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2504.06167.pdf' target='_blank'>https://arxiv.org/pdf/2504.06167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanfang Lyu, Xiaoyu Wang, Nandi Zhang, Shuai Ma, Qian Zhu, Yuhan Luo, Fugee Tsung, Xiaojuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06167">Signaling Human Intentions to Service Robots: Understanding the Use of Social Cues during In-Person Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As social service robots become commonplace, it is essential for them to effectively interpret human signals, such as verbal, gesture, and eye gaze, when people need to focus on their primary tasks to minimize interruptions and distractions. Toward such a socially acceptable Human-Robot Interaction, we conducted a study ($N=24$) in an AR-simulated context of a coffee chat. Participants elicited social cues to signal intentions to an anthropomorphic, zoomorphic, grounded technical, or aerial technical robot waiter when they were speakers or listeners. Our findings reveal common patterns of social cues over intentions, the effects of robot morphology on social cue position and conversational role on social cue complexity, and users' rationale in choosing social cues. We offer insights into understanding social cues concerning perceptions of robots, cognitive load, and social context. Additionally, we discuss design considerations on approaching, social cue recognition, and response strategies for future service robots.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2504.03800.pdf' target='_blank'>https://arxiv.org/pdf/2504.03800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Huang, Qinying Gu, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03800">Decision SpikeFormer: Spike-Driven Transformer for Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) enables policy training solely on pre-collected data, avoiding direct environment interaction - a crucial benefit for energy-constrained embodied AI applications. Although Artificial Neural Networks (ANN)-based methods perform well in offline RL, their high computational and energy demands motivate exploration of more efficient alternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given their low power consumption. In this work, we introduce DSFormer, the first spike-driven transformer model designed to tackle offline RL via sequence modeling. Unlike existing SNN transformers focused on spatial dimensions for vision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional Spiking Self-Attention (PSSA) in DSFormer to capture the temporal and positional dependencies essential for sequence modeling in RL. Additionally, we propose Progressive Threshold-dependent Batch Normalization (PTBN), which combines the benefits of LayerNorm and BatchNorm to preserve temporal dependencies while maintaining the spiking nature of SNNs. Comprehensive results in the D4RL benchmark show DSFormer's superiority over both SNN and ANN counterparts, achieving 78.4% energy savings, highlighting DSFormer's advantages not only in energy efficiency but also in competitive performance. Code and models are public at https://wei-nijuan.github.io/DecisionSpikeFormer.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2503.19317.pdf' target='_blank'>https://arxiv.org/pdf/2503.19317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoting Peng, Haonan Chen, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19317">Towards Uncertainty Unification: A Case Study for Preference Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning human preferences is essential for human-robot interaction, as it enables robots to adapt their behaviors to align with human expectations and goals. However, the inherent uncertainties in both human behavior and robotic systems make preference learning a challenging task. While probabilistic robotics algorithms offer uncertainty quantification, the integration of human preference uncertainty remains underexplored. To bridge this gap, we introduce uncertainty unification and propose a novel framework, uncertainty-unified preference learning (UUPL), which enhances Gaussian Process (GP)-based preference learning by unifying human and robot uncertainties. Specifically, UUPL includes a human preference uncertainty model that improves GP posterior mean estimation, and an uncertainty-weighted Gaussian Mixture Model (GMM) that enhances GP predictive variance accuracy. Additionally, we design a user-specific calibration process to align uncertainty representations across users, ensuring consistency and reliability in the model performance. Comprehensive experiments and user studies demonstrate that UUPL achieves state-of-the-art performance in both prediction accuracy and user rating. An ablation study further validates the effectiveness of human uncertainty model and uncertainty-weighted GMM of UUPL.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2503.08306.pdf' target='_blank'>https://arxiv.org/pdf/2503.08306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steeven Janny, HervÃ© Poirier, Leonid Antsfeld, Guillaume Bono, Gianluca Monaci, Boris Chidlovskii, Francesco Giuliari, Alessio Del Bue, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08306">Reasoning in visual navigation of end-to-end trained agents: a dynamical systems approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at europe.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2502.18012.pdf' target='_blank'>https://arxiv.org/pdf/2502.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunkun Liang, Dongcai Tan, Banglei Guan, Zhang Li, Guangcheng Dai, Nianpeng Pan, Liang Shen, Yang Shang, Qifeng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18012">High-precision visual navigation device calibration method based on collimator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation devices require precise calibration to achieve high-precision localization and navigation, which includes camera and attitude calibration. To address the limitations of time-consuming camera calibration and complex attitude adjustment processes, this study presents a collimator-based calibration method and system. Based on the optical characteristics of the collimator, a single-image camera calibration algorithm is introduced. In addition, integrated with the precision adjustment mechanism of the calibration frame, a rotation transfer model between coordinate systems enables efficient attitude calibration. Experimental results demonstrate that the proposed method achieves accuracy and stability comparable to traditional multi-image calibration techniques. Specifically, the re-projection errors are less than 0.1463 pixels, and average attitude angle errors are less than 0.0586 degrees with a standard deviation less than 0.0257 degrees, demonstrating high precision and robustness.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2502.14185.pdf' target='_blank'>https://arxiv.org/pdf/2502.14185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Khanna, Andreas Naoum, Elmira Yadollahi, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14185">REFLEX Dataset: A Multimodal Dataset of Human Reactions to Robot Failures and Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents REFLEX: Robotic Explanations to FaiLures and Human EXpressions, a comprehensive multimodal dataset capturing human reactions to robot failures and subsequent explanations in collaborative settings. It aims to facilitate research into human-robot interaction dynamics, addressing the need to study reactions to both initial failures and explanations, as well as the evolution of these reactions in long-term interactions. By providing rich, annotated data on human responses to different types of failures, explanation levels, and explanation varying strategies, the dataset contributes to the development of more robust, adaptive, and satisfying robotic systems capable of maintaining positive relationships with human collaborators, even during challenges like repeated failures.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2502.07358.pdf' target='_blank'>https://arxiv.org/pdf/2502.07358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Chen, Yiteng Xu, Yiming Ren, Yaoqin Ye, Xinran Li, Ning Ding, Yuxuan Wu, Yaoze Liu, Peishan Cong, Ziyi Wang, Bushi Liu, Yuhan Chen, Zhiyang Dou, Xiaokun Leng, Manyi Li, Yuexin Ma, Changhe Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07358">SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for Adaptive Human-Robot Symbiosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of intelligent robots seeks to seamlessly integrate them into the human world, providing assistance and companionship in daily life and work, with the ultimate goal of achieving human-robot symbiosis. This requires robots with intelligent interaction abilities to work naturally and effectively with humans. However, current robotic simulators fail to support real human participation, limiting their ability to provide authentic interaction experiences and gather valuable human feedback essential for enhancing robotic capabilities. In this paper, we introduce SymBridge, the first human-in-the-loop cyber-physical interactive system designed to enable the safe and efficient development, evaluation, and optimization of human-robot interaction methods. Specifically, we employ augmented reality technology to enable real humans to interact with virtual robots in physical environments, creating an authentic interactive experience. Building on this, we propose a novel robotic interaction model that generates responsive, precise robot actions in real time through continuous human behavior observation. The model incorporates multi-resolution human motion features and environmental affordances, ensuring contextually adaptive robotic responses. Additionally, SymBridge enables continuous robot learning by collecting human feedback and dynamically adapting the robotic interaction model. By leveraging a carefully designed system architecture and modules, SymBridge builds a bridge between humans and robots, as well as between cyber and physical spaces, providing a natural and realistic online interaction experience while facilitating the continuous evolution of robotic intelligence. Extensive experiments, user studies, and real robot testing demonstrate the promising performance of the system and highlight its potential to significantly advance research on human-robot symbiosis.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2502.03814.pdf' target='_blank'>https://arxiv.org/pdf/2502.03814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Zijian An, Shams Abrar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03814">Large Language Models for Multi-Robot Systems: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source GitHub repository.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2501.18867.pdf' target='_blank'>https://arxiv.org/pdf/2501.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18867">UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2501.16411.pdf' target='_blank'>https://arxiv.org/pdf/2501.16411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16411">PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world -- likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2501.07224.pdf' target='_blank'>https://arxiv.org/pdf/2501.07224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoqiao Ren, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07224">Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Touch is a fundamental aspect of emotion-rich communication, playing a vital role in human interaction and offering significant potential in human-robot interaction. Previous research has demonstrated that a sparse representation of human touch can effectively convey social tactile signals. However, advances in human-robot tactile interaction remain limited, as many humanoid robots possess simplistic capabilities, such as only opening and closing their hands, restricting nuanced tactile expressions. In this study, we explore how a robot can use sparse representations of tactile vibrations to convey emotions to a person. To achieve this, we developed a wearable sleeve integrated with a 5x5 grid of vibration motors, enabling the robot to communicate diverse tactile emotions and gestures. Using chain prompts within a Large Language Model (LLM), we generated distinct 10-second vibration patterns corresponding to 10 emotions (e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap). Participants (N = 32) then rated each vibration stimulus based on perceived valence and arousal. People are accurate at recognising intended emotions, a result which aligns with earlier findings. These results highlight the LLM's ability to generate emotional haptic data and effectively convey emotions through tactile signals. By translating complex emotional and tactile expressions into vibratory patterns, this research demonstrates how LLMs can enhance physical interaction between humans and robots.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2501.02127.pdf' target='_blank'>https://arxiv.org/pdf/2501.02127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Khanna, Elmira Yadollahi, Iolanda Leite, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02127">How do Humans take an Object from a Robot: Behavior changes observed in a User Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate human-robot interaction and gain human trust, a robot should recognize and adapt to changes in human behavior. This work documents different human behaviors observed while taking objects from an interactive robot in an experimental study, categorized across two dimensions: pull force applied and handedness. We also present the changes observed in human behavior upon repeated interaction with the robot to take various objects.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2412.17282.pdf' target='_blank'>https://arxiv.org/pdf/2412.17282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riku Uemura, Kanji Tanaka, Kenta Tsukahara, Daiki Iwata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17282">LMD-PGN: Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point goal navigation (PGN) is a mapless navigation approach that trains robots to visually navigate to goal points without relying on pre-built maps. Despite significant progress in handling complex environments using deep reinforcement learning, current PGN methods are designed for single-robot systems, limiting their generalizability to multi-robot scenarios with diverse platforms. This paper addresses this limitation by proposing a knowledge transfer framework for PGN, allowing a teacher robot to transfer its learned navigation model to student robots, including those with unknown or black-box platforms. We introduce a novel knowledge distillation (KD) framework that transfers first-person-view (FPV) representations (view images, turning/forward actions) to universally applicable third-person-view (TPV) representations (local maps, subgoals). The state is redefined as reconstructed local maps using SLAM, while actions are mapped to subgoals on a predefined grid. To enhance training efficiency, we propose a sampling-efficient KD approach that aligns training episodes via a noise-robust local map descriptor (LMD). Although validated on 2D wheeled robots, this method can be extended to 3D action spaces, such as drones. Experiments conducted in Habitat-Sim demonstrate the feasibility of the proposed framework, requiring minimal implementation effort. This study highlights the potential for scalable and cross-platform PGN solutions, expanding the applicability of embodied AI systems in multi-robot scenarios.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2412.11523.pdf' target='_blank'>https://arxiv.org/pdf/2412.11523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daiki Iwata, Kanji Tanaka, Shoya Miyazaki, Kouki Terashima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11523">ON as ALC: Active Loop Closing Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In simultaneous localization and mapping, active loop closing (ALC) is an active vision problem that aims to visually guide a robot to maximize the chances of revisiting previously visited points, thereby resetting the drift errors accumulated in the incrementally built map during travel. However, current mainstream navigation strategies that leverage such incomplete maps as workspace prior knowledge often fail in modern long-term autonomy long-distance travel scenarios where map accumulation errors become significant. To address these limitations of map-based navigation, this paper is the first to explore mapless navigation in the embodied AI field, in particular, to utilize object-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques that efficiently explore target objects without using such a prior map. Specifically, in this work, we start from an off-the-shelf mapless ON planner, extend it to utilize a prior map, and further show that the performance in long-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss" and ``ON loss". This study highlights a simple and effective approach, called ALC-ON (ALCON), to accelerate the progress of challenging long-distance ALC technology by leveraging the growing frontier-guided, data-driven, and LLM-guided ON technologies.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2412.10439.pdf' target='_blank'>https://arxiv.org/pdf/2412.10439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou, Bo Du, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10439">CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2410.19374.pdf' target='_blank'>https://arxiv.org/pdf/2410.19374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Lombardi, Elisa Maiettini, Agnieszka Wykowska, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19374">Gaze estimation learning architecture as support to affective, social and cognitive studies in natural human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze is a crucial social cue in any interacting scenario and drives many mechanisms of social cognition (joint and shared attention, predicting human intention, coordination tasks). Gaze direction is an indication of social and emotional functions affecting the way the emotions are perceived. Evidence shows that embodied humanoid robots endowing social abilities can be seen as sophisticated stimuli to unravel many mechanisms of human social cognition while increasing engagement and ecological validity. In this context, building a robotic perception system to automatically estimate the human gaze only relying on robot's sensors is still demanding. Main goal of the paper is to propose a learning robotic architecture estimating the human gaze direction in table-top scenarios without any external hardware. Table-top tasks are largely used in many studies in experimental psychology because they are suitable to implement numerous scenarios allowing agents to collaborate while maintaining a face-to-face interaction. Such an architecture can provide a valuable support in studies where external hardware might represent an obstacle to spontaneous human behaviour, especially in environments less controlled than the laboratory (e.g., in clinical settings). A novel dataset was also collected with the humanoid robot iCub, including images annotated from 24 participants in different gaze conditions.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2410.11623.pdf' target='_blank'>https://arxiv.org/pdf/2410.11623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijie Cheng, Kechen Fang, Yangyang Yu, Sicheng Zhou, Bohao Li, Ye Tian, Tingguang Li, Lei Han, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11623">VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2410.11402.pdf' target='_blank'>https://arxiv.org/pdf/2410.11402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixu Yan, Zeyu Zhang, Muzhi Han, Zaijin Wang, Qi Xie, Zhitian Li, Zhehan Li, Hangxin Liu, Xinggang Wang, Song-Chun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11402">M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2410.01971.pdf' target='_blank'>https://arxiv.org/pdf/2410.01971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asher J. Hancock, Allen Z. Ren, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01971">Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models trained on large-scale internet data and robot demonstrations have the potential to serve as generalist robot policies. However, despite their large-scale training, VLAs are often brittle to task-irrelevant visual details such as distractor objects or background colors. We introduce Bring Your Own VLA (BYOVLA): a run-time intervention scheme that (1) dynamically identifies regions of the input image that the model is sensitive to, and (2) minimally alters task-irrelevant regions to reduce the model's sensitivity using automated image editing tools. Our approach is compatible with any off the shelf VLA without model fine-tuning or access to the model's weights. Hardware experiments on language-instructed manipulation tasks demonstrate that BYOVLA enables state-of-the-art VLA models to nearly retain their nominal performance in the presence of distractor objects and backgrounds, which otherwise degrade task success rates by up to 40%. Website with additional information, videos, and code: https://aasherh.github.io/byovla/ .
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2408.06105.pdf' target='_blank'>https://arxiv.org/pdf/2408.06105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakob Thumm, Christopher Agia, Marco Pavone, Matthias Althoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06105">Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adjusting robot behavior to human preferences can require intensive human feedback, preventing quick adaptation to new users and changing circumstances. Moreover, current approaches typically treat user preferences as a reward, which requires a manual balance between task success and user satisfaction. To integrate new user preferences in a zero-shot manner, our proposed Text2Interaction framework invokes large language models to generate a task plan, motion preferences as Python code, and parameters of a safety controller. By maximizing the combined probability of task completion and user satisfaction instead of a weighted sum of rewards, we can reliably find plans that fulfill both requirements. We find that 83 % of users working with Text2Interaction agree that it integrates their preferences into the plan of the robot, and 94 % prefer Text2Interaction over the baseline. Our ablation study shows that Text2Interaction aligns better with unseen preferences than other baselines while maintaining a high success rate. Real-world demonstrations and code are made available at sites.google.com/view/text2interaction.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2408.00343.pdf' target='_blank'>https://arxiv.org/pdf/2408.00343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Schoch, Fan Yang, Yuntao Ma, Stefan Leutenegger, Marco Hutter, Quentin Leboutet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00343">IN-Sight: Interactive Navigation through Sight</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic map, facilitating long-range path planning in complex, maze-like environments. To precisely navigate around obstacles, IN-Sight employs a local planner, trained imperatively on a differentiable costmap using representation learning techniques. The entire framework undergoes end-to-end training within the state-of-the-art photorealistic Intel SPEAR Simulator. We validate the effectiveness of IN-Sight through extensive benchmarking in a variety of simulated scenarios and ablation studies. Moreover, we demonstrate the system's real-world applicability with zero-shot sim-to-real transfer, deploying our planner on the legged robot platform ANYmal, showcasing its practical potential for interactive navigation in real environments.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2407.12061.pdf' target='_blank'>https://arxiv.org/pdf/2407.12061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>So Yeon Min, Xavi Puig, Devendra Singh Chaplot, Tsung-Yen Yang, Akshara Rai, Priyam Parashar, Ruslan Salakhutdinov, Yonatan Bisk, Roozbeh Mottaghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12061">Situated Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language is never spoken in a vacuum. It is expressed, comprehended, and contextualized within the holistic backdrop of the speaker's history, actions, and environment. Since humans are used to communicating efficiently with situated language, the practicality of robotic assistants hinge on their ability to understand and act upon implicit and situated instructions. In traditional instruction following paradigms, the agent acts alone in an empty house, leading to language use that is both simplified and artificially "complete." In contrast, we propose situated instruction following, which embraces the inherent underspecification and ambiguity of real-world communication with the physical presence of a human speaker. The meaning of situated instructions naturally unfold through the past actions and the expected future behaviors of the human involved. Specifically, within our settings we have instructions that (1) are ambiguously specified, (2) have temporally evolving intent, (3) can be interpreted more precisely with the agent's dynamic actions. Our experiments indicate that state-of-the-art Embodied Instruction Following (EIF) models lack holistic understanding of situated human intention.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2406.17420.pdf' target='_blank'>https://arxiv.org/pdf/2406.17420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>H. P. Madushanka, Rafaela Scaciota, Sumudu Samarakoon, Mehdi Bennis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17420">Real-Time Remote Control via VR over Limited Wireless Connectivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces a solution to enhance human-robot interaction over limited wireless connectivity. The goal is toenable remote control of a robot through a virtual reality (VR)interface, ensuring a smooth transition to autonomous mode in the event of connectivity loss. The VR interface provides accessto a dynamic 3D virtual map that undergoes continuous updatesusing real-time sensor data collected and transmitted by therobot. Furthermore, the robot monitors wireless connectivity and automatically switches to a autonomous mode in scenarios with limited connectivity. By integrating four key functionalities: real-time mapping, remote control through glasses VR, continuous monitoring of wireless connectivity, and autonomous navigation during limited connectivity, we achieve seamless end-to-end operation.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2405.19531.pdf' target='_blank'>https://arxiv.org/pdf/2405.19531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingqi Yuan, Huijiang Wang, Kai-Fung Chu, Fumiya Iida, Bo Li, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19531">Hierarchical Procedural Framework for Low-latency Robot-Assisted Hand-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in robotics have been driving the development of human-robot interaction (HRI) technologies. However, accurately perceiving human actions and achieving adaptive control remains a challenge in facilitating seamless coordination between human and robotic movements. In this paper, we propose a hierarchical procedural framework to enable dynamic robot-assisted hand-object interaction (HOI). An open-loop hierarchy leverages the RGB-based 3D reconstruction of the human hand, based on which motion primitives have been designed to translate hand motions into robotic actions. The low-level coordination hierarchy fine-tunes the robot's action by using the continuously updated 3D hand models. Experimental validation demonstrates the effectiveness of the hierarchical control architecture. The adaptive coordination between human and robot behavior has achieved a delay of $\leq 0.3$ seconds in the tele-interaction scenario. A case study of ring-wearing tasks indicates the potential application of this work in assistive technologies such as healthcare and manufacturing.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2404.17394.pdf' target='_blank'>https://arxiv.org/pdf/2404.17394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruben Janssens, Eva Verhelst, Giulio Antonio Abbo, Qiaoqiao Ren, Maria Jose Pinto Bernal, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17394">Child Speech Recognition in Human-Robot Interaction: Problem Solved?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated Speech Recognition shows superhuman performance for adult English speech on a range of benchmarks, but disappoints when fed children's speech. This has long sat in the way of child-robot interaction. Recent evolutions in data-driven speech recognition, including the availability of Transformer architectures and unprecedented volumes of training data, might mean a breakthrough for child speech recognition and social robot applications aimed at children. We revisit a study on child speech recognition from 2017 and show that indeed performance has increased, with newcomer OpenAI Whisper doing markedly better than leading commercial cloud services. Performance improves even more in highly structured interactions when priming models with specific phrases. While transcription is not perfect yet, the best model recognises 60.3% of sentences correctly barring small grammatical differences, with sub-second transcription time running on a local GPU, showing potential for usable autonomous child-robot speech interactions.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2404.01686.pdf' target='_blank'>https://arxiv.org/pdf/2404.01686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy-Tho Le, Chenhui Gou, Stavya Datta, Hengcan Shi, Ian Reid, Jianfei Cai, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01686">JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2403.15941.pdf' target='_blank'>https://arxiv.org/pdf/2403.15941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15941">Explore until Confident: Efficient Exploration for Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: https://explore-eqa.github.io/
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2403.11487.pdf' target='_blank'>https://arxiv.org/pdf/2403.11487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11487">Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data. We finally discuss the applicability of our approach in enabling a generalizable evaluation of embodied navigation policies. To the best of our knowledge, ours is the first LLM-driven approach capable of generating "human-like" instructions in a platform-agnostic manner, without training.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2403.03017.pdf' target='_blank'>https://arxiv.org/pdf/2403.03017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Bang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03017">OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2401.14349.pdf' target='_blank'>https://arxiv.org/pdf/2401.14349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillaume Bono, HervÃ© Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14349">Learning to navigate efficiently and precisely in real environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2312.09337.pdf' target='_blank'>https://arxiv.org/pdf/2312.09337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Hwang, Luca Weihs, Chanwoo Park, Kimin Lee, Aniruddha Kembhavi, Kiana Ehsani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09337">Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Customizing robotic behaviors to be aligned with diverse human preferences is an underexplored challenge in the field of embodied AI. In this paper, we present Promptable Behaviors, a novel framework that facilitates efficient personalization of robotic agents to diverse human preferences in complex environments. We use multi-objective reinforcement learning to train a single policy adaptable to a broad spectrum of preferences. We introduce three distinct methods to infer human preferences by leveraging different types of interactions: (1) human demonstrations, (2) preference feedback on trajectory comparisons, and (3) language instructions. We evaluate the proposed method in personalized object-goal navigation and flee navigation tasks in ProcTHOR and RoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human preferences in various scenarios. Project page: https://promptable-behaviors.github.io
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2311.08244.pdf' target='_blank'>https://arxiv.org/pdf/2311.08244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08244">Language and Sketching: An LLM-driven Interactive Multimodal Multitask Robot Navigation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Extensive experiments are conducted in both simulation and the real world demonstrating that LIM2N has superior user needs understanding, alongside an enhanced interactive experience.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2311.06855.pdf' target='_blank'>https://arxiv.org/pdf/2311.06855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanta Kaneda, Ryosuke Korekata, Yuiga Wada, Shunya Nagashima, Motonari Kambara, Yui Iioka, Haruka Matsuo, Yuto Imai, Takayuki Nishimura, Komei Sugiura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06855">DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on the DialFRED task, which is the task of embodied instruction following in a setting where an agent can actively ask questions about the task. To address this task, we propose DialMAT. DialMAT introduces Moment-based Adversarial Training, which incorporates adversarial perturbations into the latent space of language, image, and action. Additionally, it introduces a crossmodal parallel feature extraction mechanism that applies foundation models to both language and image. We evaluated our model using a dataset constructed from the DialFRED dataset and demonstrated superior performance compared to the baseline method in terms of success rate and path weighted success rate. The model secured the top position in the DialFRED Challenge, which took place at the CVPR 2023 Embodied AI workshop.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2310.09676.pdf' target='_blank'>https://arxiv.org/pdf/2310.09676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiachen Li, Qiaozi Gao, Michael Johnston, Xiaofeng Gao, Xuehai He, Suhaila Shakiah, Hangjie Shi, Reza Ghanadan, William Yang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09676">Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models' tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving vision signals with text descriptions. This type of task poses a major challenge to robots' capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability. Project page: \url{https://midas-icml.github.io/}.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2309.16634.pdf' target='_blank'>https://arxiv.org/pdf/2309.16634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillaume Bono, Leonid Antsfeld, Boris Chidlovskii, Philippe Weinzaepfel, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16634">End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most recent work in goal oriented visual navigation resorts to large-scale machine learning in simulated environments. The main challenge lies in learning compact representations generalizable to unseen environments and in learning high-capacity perception modules capable of reasoning on high-dimensional input. The latter is particularly difficult when the goal is not given as a category ("ObjectNav") but as an exemplar image ("ImageNav"), as the perception module needs to learn a comparison strategy requiring to solve an underlying visual correspondence problem. This has been shown to be difficult from reward alone or with standard auxiliary tasks. We address this problem through a sequence of two pretext tasks, which serve as a prior for what we argue is one of the main bottleneck in perception, extremely wide-baseline relative pose estimation and visibility prediction in complex scenes. The first pretext task, cross-view completion is a proxy for the underlying visual correspondence problem, while the second task addresses goal detection and finding directly. We propose a new dual encoder with a large-capacity binocular ViT model and show that correspondence solutions naturally emerge from the training signals. Experiments show significant improvements and SOTA performance on the two benchmarks, ImageNav and the Instance-ImageNav variant, where camera intrinsics and height differ between observation and goal.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2308.13318.pdf' target='_blank'>https://arxiv.org/pdf/2308.13318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiva Hanifi, Elisa Maiettini, Maria Lombardi, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13318">iCub Detecting Gazed Objects: A Pipeline Estimating Human Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research report explores the role of eye gaze in human-robot interactions and proposes a learning system for detecting objects gazed at by humans using solely visual feedback. The system leverages face detection, human attention prediction, and online object detection, and it allows the robot to perceive and interpret human gaze accurately, paving the way for establishing joint attention with human partners. Additionally, a novel dataset collected with the humanoid robot iCub is introduced, comprising over 22,000 images from ten participants gazing at different annotated objects. This dataset serves as a benchmark for the field of human gaze estimation in table-top human-robot interaction (HRI) contexts. In this work, we use it to evaluate the performance of the proposed pipeline and examine the performance of each component. Furthermore, the developed system is deployed on the iCub, and a supplementary video showcases its functionality. The results demonstrate the potential of the proposed approach as a first step to enhance social awareness and responsiveness in social robotics, as well as improve assistance and support in collaborative scenarios, promoting efficient human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2307.07166.pdf' target='_blank'>https://arxiv.org/pdf/2307.07166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryosuke Korekata, Motonari Kambara, Yu Yoshida, Shintaro Ishikawa, Yosuke Kawasaki, Masaki Takahashi, Komei Sugiura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07166">Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as "Move the bottle on the left side of the plate to the empty chair," the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a newly-built dataset consisting of object manipulation instructions and semi photo-realistic images captured in a standard Embodied AI simulator. The results show that our method outperforms the baseline method in terms of language comprehension accuracy. Furthermore, we conduct physical experiments in which a DSR delivers standardized everyday objects in a standardized domestic environment as requested by instructions with referring expressions. The experimental results show that the object grasping and placing actions are achieved with success rates of more than 90%.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2305.06141.pdf' target='_blank'>https://arxiv.org/pdf/2305.06141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mitsuki Yoshida, Kanji Tanaka, Ryogo Yamamoto, Daiki Iwata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06141">Active Semantic Localization with Graph Neural Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic localization, i.e., robot self-localization with semantic image modality, is critical in recently emerging embodied AI applications (e.g., point-goal navigation, object-goal navigation, vision language navigation) and topological mapping applications (e.g., graph neural SLAM, ego-centric topological map). However, most existing works on semantic localization focus on passive vision tasks without viewpoint planning, or rely on additional rich modalities (e.g., depth measurements). Thus, the problem is largely unsolved. In this work, we explore a lightweight, entirely CPU-based, domain-adaptive semantic localization framework, called graph neural localizer. Our approach is inspired by two recently emerging technologies: (1) Scene graph, which combines the viewpoint- and appearance- invariance of local and global features; (2) Graph neural network, which enables direct learning/recognition of graph data (i.e., non-vector data). Specifically, a graph convolutional neural network is first trained as a scene graph classifier for passive vision, and then its knowledge is transferred to a reinforcement-learning planner for active vision. Experiments on two scenarios, self-supervised learning and unsupervised domain adaptation, using a photo-realistic Habitat simulator validate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2304.14501.pdf' target='_blank'>https://arxiv.org/pdf/2304.14501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiafei Duan, Samson Yu, Nicholas Tan, Yi Ru Wang, Cheston Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14501">Read My Mind: A Multi-Modal Dataset for Human Belief Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human intentions is key to enabling effective and efficient human-robot interaction (HRI) in collaborative settings. To enable developments and evaluation of the ability of artificial intelligence (AI) systems to infer human beliefs, we introduce a large-scale multi-modal video dataset for intent prediction based on object-context relations.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2304.14003.pdf' target='_blank'>https://arxiv.org/pdf/2304.14003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Evangelos Tsagkournis, Dimitris Panagopoulos, Giannis Petousakis, Grigoris Nikolaou, Rustam Stolkin, Manolis Chiou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14003">A Supervised Machine Learning Approach to Operator Intent Recognition for Teleoperated Mobile Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In applications that involve human-robot interaction (HRI), human-robot teaming (HRT), and cooperative human-machine systems, the inference of the human partner's intent is of critical importance. This paper presents a method for the inference of the human operator's navigational intent, in the context of mobile robots that provide full or partial (e.g., shared control) teleoperation. We propose the Machine Learning Operator Intent Inference (MLOII) method, which a) processes spatial data collected by the robot's sensors; b) utilizes a supervised machine learning algorithm to estimate the operator's most probable navigational goal online. The proposed method's ability to reliably and efficiently infer the intent of the human operator is experimentally evaluated in realistically simulated exploration and remote inspection scenarios. The results in terms of accuracy and uncertainty indicate that the proposed method is comparable to another state-of-the-art method found in the literature.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2301.02555.pdf' target='_blank'>https://arxiv.org/pdf/2301.02555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02555">"No, to the Right" -- Online Language Corrections for Robotic Manipulation via Shared Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systems for language-guided human-robot interaction must satisfy two key desiderata for broad adoption: adaptivity and learning efficiency. Unfortunately, existing instruction-following agents cannot adapt, lacking the ability to incorporate online natural language supervision, and even if they could, require hundreds of demonstrations to learn even simple policies. In this work, we address these problems by presenting Language-Informed Latent Actions with Corrections (LILAC), a framework for incorporating and adapting to natural language corrections - "to the right," or "no, towards the book" - online, during execution. We explore rich manipulation domains within a shared autonomy paradigm. Instead of discrete turn-taking between a human and robot, LILAC splits agency between the human and robot: language is an input to a learned model that produces a meaningful, low-dimensional control space that the human can use to guide the robot. Each real-time correction refines the human's control space, enabling precise, extended behaviors - with the added benefit of requiring only a handful of demonstrations to learn. We evaluate our approach via a user study where users work with a Franka Emika Panda manipulator to complete complex manipulation tasks. Compared to existing learned baselines covering both open-loop instruction following and single-turn shared autonomy, we show that our corrections-aware approach obtains higher task completion rates, and is subjectively preferred by users because of its reliability, precision, and ease of use.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2301.00452.pdf' target='_blank'>https://arxiv.org/pdf/2301.00452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonghao Long, Wang Wei, Tao Huang, Yuehao Wang, Qi Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00452">Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical robot automation has attracted increasing research interest over the past decade, expecting its potential to benefit surgeons, nurses and patients. Recently, the learning paradigm of embodied intelligence has demonstrated promising ability to learn good control policies for various complex tasks, where embodied AI simulators play an essential role to facilitate relevant research. However, existing open-sourced simulators for surgical robot are still not sufficiently supporting human interactions through physical input devices, which further limits effective investigations on how the human demonstrations would affect policy learning. In this work, we study human-in-the-loop embodied intelligence with a new interactive simulation platform for surgical robot learning. Specifically, we establish our platform based on our previously released SurRoL simulator with several new features co-developed to allow high-quality human interaction via an input device. We showcase the improvement of our simulation environment with the designed new features, and validate effectiveness of incorporating human factors in embodied intelligence through the use of human demonstrations and reinforcement learning as a representative example. Promising results are obtained in terms of learning efficiency. Lastly, five new surgical robot training tasks are developed and released, with which we hope to pave the way for future research on surgical embodied intelligence. Our learning platform is publicly released and will be continuously updated in the website: https://med-air.github.io/SurRoL.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2210.11940.pdf' target='_blank'>https://arxiv.org/pdf/2210.11940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Vendrow, Duy Tho Le, Jianfei Cai, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11940">JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robotic systems operating in human environments must understand their surroundings to make accurate and safe decisions. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking. However, existing datasets either do not provide pose annotations or include scene types unrelated to robotic applications. Many datasets also lack the diversity of poses and occlusions found in crowded human scenes. To address this limitation we introduce JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation and tracking using videos captured from a social navigation robot. The dataset contains challenge scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. JRDB-Pose provides human pose annotations with per-keypoint occlusion labels and track IDs consistent across the scene. A public evaluation server is made available for fair evaluation on a held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2207.13552.pdf' target='_blank'>https://arxiv.org/pdf/2207.13552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Lombardi, Elisa Maiettini, Vadim Tikhanoff, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.13552">iCub Knows Where You Look: Exploiting Social Cues for Interactive Object Detection Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing joint interaction requires constant mutual monitoring of own actions and their effects on the other's behaviour. Such an action-effect monitoring is boosted by social cues and might result in an increasing sense of agency. Joint actions and joint attention are strictly correlated and both of them contribute to the formation of a precise temporal coordination. In human-robot interaction, the robot's ability to establish joint attention with a human partner and exploit various social cues to react accordingly is a crucial step in creating communicative robots. Along the social component, an effective human-robot interaction can be seen as a new method to improve and make the robot's learning process more natural and robust for a given task. In this work we use different social skills, such as mutual gaze, gaze following, speech and human face recognition, to develop an effective teacher-learner scenario tailored to visual object learning in dynamic environments. Experiments on the iCub robot demonstrate that the system allows the robot to learn new objects through a natural interaction with a human teacher in presence of distractors.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2201.10671.pdf' target='_blank'>https://arxiv.org/pdf/2201.10671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathaniel Dennler, Changxiao Ruan, Jessica Hadiwijoyo, Brenna Chen, Stefanos Nikolaidis, Maja Mataric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.10671">Using Design Metaphors to Understand User Expectations of Socially Interactive Robot Embodiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The physical design of a robot suggests expectations of that robot's functionality for human users and collaborators. When those expectations align with the true capabilities of the robot, interaction with the robot is enhanced. However, misalignment of those expectations can result in an unsatisfying interaction. This paper uses Mechanical Turk to evaluate user expectation through the use of design metaphors as applied to a wide range of robot embodiments. The first study (N=382) associates crowd-sourced design metaphors to different robot embodiments. The second study (N=803) assesses initial social expectations of robot embodiments. The final study (N=805) addresses the degree of abstraction of the design metaphors and the functional expectations projected on robot embodiments. Together, these results can guide robot designers toward aligning user expectations with true robot capabilities, facilitating positive human-robot interaction.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2111.00063.pdf' target='_blank'>https://arxiv.org/pdf/2111.00063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Chen, Zhengming Ding, David Crandall, Lantao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.00063">Polyline Generative Navigable Space Segmentation for Autonomous Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting navigable space is a fundamental capability for mobile robots navigating in unknown or unmapped environments. In this work, we treat visual navigable space segmentation as a scene decomposition problem and propose Polyline Segmentation Variational autoencoder Network (PSV-Net), a representation learning-based framework for learning the navigable space segmentation in a self-supervised manner. Current segmentation techniques heavily rely on fully-supervised learning strategies which demand a large amount of pixel-level annotated images. In this work, we propose a framework leveraging a Variational AutoEncoder (VAE) and an AutoEncoder (AE) to learn a polyline representation that compactly outlines the desired navigable space boundary. Through extensive experiments, we validate that the proposed PSV-Net can learn the visual navigable space with no or few labels, producing an accuracy comparable to fully-supervised state-of-the-art methods that use all available labels. In addition, we show that integrating the proposed navigable space segmentation model with a visual planner can achieve efficient mapless navigation in real environments.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/1907.01932.pdf' target='_blank'>https://arxiv.org/pdf/1907.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florentin WÃ¶rgÃ¶tter, Fatemeh Ziaeetabar, Stefan Pfeiffer, Osman Kaya, Tomas Kulvicius, Minija Tamosiunaite
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1907.01932">Action Prediction in Humans and Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient action prediction is of central importance for the fluent workflow between humans and equally so for human-robot interaction. To achieve prediction, actions can be encoded by a series of events, where every event corresponds to a change in a (static or dynamic) relation between some of the objects in a scene. Manipulation actions and others can be uniquely encoded this way and only, on average, less than 60% of the time series has to pass until an action can be predicted. Using a virtual reality setup and testing ten different manipulation actions, here we show that in most cases humans predict actions at the same event as the algorithm. In addition, we perform an in-depth analysis about the temporal gain resulting from such predictions when chaining actions and show in some robotic experiments that the percentage gain for humans and robots is approximately equal. Thus, if robots use this algorithm then their prediction-moments will be compatible to those of their human interaction partners, which should much benefit natural human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2509.14748.pdf' target='_blank'>https://arxiv.org/pdf/2509.14748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Ibrahim, Alap Kshirsagar, Dorothea Koert, Jan Peters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14748">Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication is essential for safety and efficiency in human-robot collaboration, particularly in shared workspaces. This paper investigates the impact of nonverbal communication on human-robot interaction (HRI) by integrating reactive light signals and emotional displays into a robotic system. We equipped a Franka Emika Panda robot with an LED strip on its end effector and an animated facial display on a tablet to convey movement intent through colour-coded signals and facial expressions. We conducted a human-robot collaboration experiment with 18 participants, evaluating three conditions: LED signals alone, LED signals with reactive emotional displays, and LED signals with pre-emptive emotional displays. We collected data through questionnaires and position tracking to assess anticipation of potential collisions, perceived clarity of communication, and task performance. The results indicate that while emotional displays increased the perceived interactivity of the robot, they did not significantly improve collision anticipation, communication clarity, or task efficiency compared to LED signals alone. These findings suggest that while emotional cues can enhance user engagement, their impact on task performance in shared workspaces is limited.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2509.01996.pdf' target='_blank'>https://arxiv.org/pdf/2509.01996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Sun, Xian Wang, Abhishek Kumar, Chengbin Cui, Lik-Hang Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01996">MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot interaction (HRI) in multi-object teleoperation tasks faces significant challenges due to perceptual ambiguities in virtual reality (VR) environments and the limitations of single-modality intention recognition. This paper proposes a shared control framework that combines a virtual admittance (VA) model with a Multimodal-CNN-based Human Intention Perception Network (MMIPN) to enhance teleoperation performance and user experience. The VA model employs artificial potential fields to guide operators toward target objects by adjusting admittance force and optimizing motion trajectories. MMIPN processes multimodal inputs, including gaze movement, robot motions, and environmental context, to estimate human grasping intentions, helping to overcome depth perception challenges in VR. Our user study evaluated four conditions across two factors, and the results showed that MMIPN significantly improved grasp success rates, while the VA model enhanced movement efficiency by reducing path lengths. Gaze data emerged as the most crucial input modality. These findings demonstrate the effectiveness of combining multimodal cues with implicit guidance in VR-based teleoperation, providing a robust solution for multi-object grasping tasks and enabling more natural interactions across various applications in the future.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2508.16465.pdf' target='_blank'>https://arxiv.org/pdf/2508.16465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Jean-SÃ©bastien Franco, GrÃ©gory Rogez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16465">HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2508.13444.pdf' target='_blank'>https://arxiv.org/pdf/2508.13444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Li, Jeonghwan Kim, Wontaek Kim, Donghoon Baek, Seungeun Rho, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13444">Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in whole-body robot control have enabled humanoid and legged robots to execute increasingly agile and coordinated movements. However, standardized benchmarks for evaluating robotic athletic performance in real-world settings and in direct comparison to humans remain scarce. We present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable pipeline that leverages motion-sensing console games to evaluate whole-body robot control policies. Using Just Dance on the Nintendo Switch as a representative example, our system captures, reconstructs, and retargets in-game choreography for robotic execution. We validate the system on a Unitree G1 humanoid with an open-source whole-body controller, establishing a quantitative baseline for the robot's performance against a human player. In the paper, we discuss these results, which demonstrate the feasibility of using commercial games platform as physically grounded benchmarks and motivate future work to for benchmarking embodied AI.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2508.10287.pdf' target='_blank'>https://arxiv.org/pdf/2508.10287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simindokht Jahangard, Mehrzad Mohammadi, Yi Shen, Zhixi Cai, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10287">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Vision-Language Models (VLMs) and large language models (LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI agents like robots. However, existing visual reasoning benchmarks often suffer from several limitations: they lack a clear definition of reasoning complexity, offer have no control to generate questions over varying difficulty and task customization, and fail to provide structured, step-by-step reasoning annotations (workflows). To bridge these gaps, we formalize reasoning complexity, introduce an adaptive query engine that generates customizable questions of varying complexity with detailed intermediate annotations, and extend the JRDB dataset with human-object interaction and geometric relationship annotations to create JRDB-Reasoning, a benchmark tailored for visual reasoning in human-crowded environments. Our engine and benchmark enable fine-grained evaluation of visual reasoning frameworks and dynamic assessment of visual-language models across reasoning levels.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2508.06095.pdf' target='_blank'>https://arxiv.org/pdf/2508.06095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mitchell Abrams, Thies Oelerich, Christian Hartl-Nesic, Andreas Kugi, Matthias Scheutz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06095">Incremental Language Understanding for Online Motion Planning of Robot Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction requires robots to process language incrementally, adapting their actions in real-time based on evolving speech input. Existing approaches to language-guided robot motion planning typically assume fully specified instructions, resulting in inefficient stop-and-replan behavior when corrections or clarifications occur. In this paper, we introduce a novel reasoning-based incremental parser which integrates an online motion planning algorithm within the cognitive architecture. Our approach enables continuous adaptation to dynamic linguistic input, allowing robots to update motion plans without restarting execution. The incremental parser maintains multiple candidate parses, leveraging reasoning mechanisms to resolve ambiguities and revise interpretations when needed. By combining symbolic reasoning with online motion planning, our system achieves greater flexibility in handling speech corrections and dynamically changing constraints. We evaluate our framework in real-world human-robot interaction scenarios, demonstrating online adaptions of goal poses, constraints, or task objectives. Our results highlight the advantages of integrating incremental language understanding with real-time motion planning for natural and fluid human-robot collaboration. The experiments are demonstrated in the accompanying video at www.acin.tuwien.ac.at/42d5.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2506.18466.pdf' target='_blank'>https://arxiv.org/pdf/2506.18466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matti KrÃ¼ger, Daniel Tanneberg, Chao Wang, Stephan Hasler, Michael Gienger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18466">Mirror Eyes: Explainable Human-Robot Interaction at a Glance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gaze of a person tends to reflect their interest. This work explores what happens when this statement is taken literally and applied to robots. Here we present a robot system that employs a moving robot head with a screen-based eye model that can direct the robot's gaze to points in physical space and present a reflection-like mirror image of the attended region on top of each eye. We conducted a user study with 33 participants, who were asked to instruct the robot to perform pick-and-place tasks, monitor the robot's task execution, and interrupt it in case of erroneous actions. Despite a deliberate lack of instructions about the role of the eyes and a very brief system exposure, participants felt more aware about the robot's information processing, detected erroneous actions earlier, and rated the user experience higher when eye-based mirroring was enabled compared to non-reflective eyes. These results suggest a beneficial and intuitive utilization of the introduced method in cooperative human-robot interaction.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2506.14233.pdf' target='_blank'>https://arxiv.org/pdf/2506.14233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirreza Payandeh, Anuj Pokhrel, Daeun Song, Marcos Zampieri, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14233">Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2506.13189.pdf' target='_blank'>https://arxiv.org/pdf/2506.13189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchong Zhang, Bastian Orthmann, Shichen Ji, Michael Welle, Jonne Van Haastregt, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13189">Multimodal "Puppeteer": An Exploration of Robot Teleoperation Via Virtual Counterpart with LLM-Driven Voice and Gesture Interaction in Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of robotics and augmented reality (AR) holds transformative potential for advancing human-robot interaction (HRI), offering enhancements in usability, intuitiveness, accessibility, and collaborative task performance. This paper introduces and evaluates a novel multimodal AR-based robot puppeteer framework that enables intuitive teleoperation via virtual counterpart through large language model (LLM)-driven voice commands and hand gesture interactions. Utilizing the Meta Quest 3, users interact with a virtual counterpart robot in real-time, effectively "puppeteering" its physical counterpart within an AR environment. We conducted a within-subject user study with 42 participants performing robotic cube pick-and-place with pattern matching tasks under two conditions: gesture-only interaction and combined voice-and-gesture interaction. Both objective performance metrics and subjective user experience (UX) measures were assessed, including an extended comparative analysis between roboticists and non-roboticists. The results provide key insights into how multimodal input influences contextual task efficiency, usability, and user satisfaction in AR-based HRI. Our findings offer practical design implications for designing effective AR-enhanced HRI systems.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2506.09930.pdf' target='_blank'>https://arxiv.org/pdf/2506.09930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irving Fang, Juexiao Zhang, Shengbang Tong, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09930">From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2505.17610.pdf' target='_blank'>https://arxiv.org/pdf/2505.17610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Till Freihaut, Luca Viano, Volkan Cevher, Matthieu Geist, Giorgia Ramponi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17610">Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides the first expert sample complexity characterization for learning a Nash equilibrium from expert data in Markov Games. We show that a new quantity named the single policy deviation concentrability coefficient is unavoidable in the non-interactive imitation learning setting, and we provide an upper bound for behavioral cloning (BC) featuring such coefficient. BC exhibits substantial regret in games with high concentrability coefficient, leading us to utilize expert queries to develop and introduce two novel solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response oracle and learns an $\varepsilon$-Nash equilibrium with $\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses completely the best response oracle at the cost of a worse expert query complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide numerical evidence, confirming our theoretical findings.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2505.10239.pdf' target='_blank'>https://arxiv.org/pdf/2505.10239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10239">Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In physical human-robot interaction, force feedback has been the most common sensing modality to convey the human intention to the robot. It is widely used in admittance control to allow the human to direct the robot. However, it cannot be used in scenarios where direct force feedback is not available since manipulated objects are not always equipped with a force sensor. In this work, we study one such scenario: the collaborative pushing and pulling of heavy objects on frictional surfaces, a prevalent task in industrial settings. When humans do it, they communicate through verbal and non-verbal cues, where body poses, and movements often convey more than words. We propose a novel context-aware approach using Directed Graph Neural Networks to analyze spatio-temporal human posture data to predict human motion intention for non-verbal collaborative physical manipulation. Our experiments demonstrate that robot assistance significantly reduces human effort and improves task efficiency. The results indicate that incorporating posture-based context recognition, either together with or as an alternative to force sensing, enhances robot decision-making and control efficiency.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2505.08213.pdf' target='_blank'>https://arxiv.org/pdf/2505.08213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junda Huang, Jianshu Zhou, Honghao Guo, Yunhui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08213">HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robotics progresses toward general manipulation, dexterous hands are becoming increasingly critical. However, proprioception in dexterous hands remains a bottleneck due to limitations in volume and generality. In this work, we present HandCept, a novel visual-inertial proprioception framework designed to overcome the challenges of traditional joint angle estimation methods. HandCept addresses the difficulty of achieving accurate and robust joint angle estimation in dynamic environments where both visual and inertial measurements are prone to noise and drift. It leverages a zero-shot learning approach using a wrist-mounted RGB-D camera and 9-axis IMUs, fused in real time via a latency-free Extended Kalman Filter (EKF). Our results show that HandCept achieves joint angle estimation errors between $2^{\circ}$ and $4^{\circ}$ without observable drift, outperforming visual-only and inertial-only methods. Furthermore, we validate the stability and uniformity of the IMU system, demonstrating that a common base frame across IMUs simplifies system calibration. To support sim-to-real transfer, we also open-sourced our high-fidelity rendering pipeline, which is essential for training without real-world ground truth. This work offers a robust, generalizable solution for proprioception in dexterous hands, with significant implications for robotic manipulation and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2504.14822.pdf' target='_blank'>https://arxiv.org/pdf/2504.14822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Qiu, Shijie Chen, Yu Su, Po-Yin Yen, Han-Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14822">Completing A Systematic Review in Hours instead of Months with Interactive AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2504.01588.pdf' target='_blank'>https://arxiv.org/pdf/2504.01588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Garello, Giulia Belgiovine, Gabriele Russo, Francesco Rea, Alessandra Sciutti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01588">Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating robotics into everyday scenarios like tutoring or physical training requires robots capable of adaptive, socially engaging, and goal-oriented interactions. While Large Language Models show promise in human-like communication, their standalone use is hindered by memory constraints and contextual incoherence. This work presents a multimodal, cognitively inspired framework that enhances LLM-based autonomous decision-making in social and task-oriented Human-Robot Interaction. Specifically, we develop an LLM-based agent for a robot trainer, balancing social conversation with task guidance and goal-driven motivation. To further enhance autonomy and personalization, we introduce a memory system for selecting, storing and retrieving experiences, facilitating generalized reasoning based on knowledge built across different interactions. A preliminary HRI user study and offline experiments with a synthetic dataset validate our approach, demonstrating the system's ability to manage complex interactions, autonomously drive training tasks, and build and retrieve contextual memories, advancing socially intelligent robotics.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2504.00848.pdf' target='_blank'>https://arxiv.org/pdf/2504.00848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushan Zhang, AljoÅ¡a OÅ¡ep, Laura Leal-TaixÃ©, Tim Meinhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00848">Zero-Shot 4D Lidar Panoptic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot 4D segmentation and recognition of arbitrary objects in Lidar is crucial for embodied navigation, with applications ranging from streaming perception to semantic mapping and localization. However, the primary challenge in advancing research and developing generalized, versatile methods for spatio-temporal scene understanding in Lidar lies in the scarcity of datasets that provide the necessary diversity and scale of annotations.To overcome these challenges, we propose SAL-4D (Segment Anything in Lidar--4D), a method that utilizes multi-modal robotic sensor setups as a bridge to distill recent developments in Video Object Segmentation (VOS) in conjunction with off-the-shelf Vision-Language foundation models to Lidar. We utilize VOS models to pseudo-label tracklets in short video sequences, annotate these tracklets with sequence-level CLIP tokens, and lift them to the 4D Lidar space using calibrated multi-modal sensory setups to distill them to our SAL-4D model. Due to temporal consistent predictions, we outperform prior art in 3D Zero-Shot Lidar Panoptic Segmentation (LPS) over $5$ PQ, and unlock Zero-Shot 4D-LPS.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2503.16492.pdf' target='_blank'>https://arxiv.org/pdf/2503.16492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhi Lai, Shenghai Yuan, Boya Zhang, Benjamin Kiefer, Peizheng Li, Tianchen Deng, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16492">FAM-HRI: Foundation-Model Assisted Multi-Modal Human-Robot Interaction Combining Gaze and Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective Human-Robot Interaction (HRI) is crucial for enhancing accessibility and usability in real-world robotics applications. However, existing solutions often rely on gestures or language commands, making interaction inefficient and ambiguous, particularly for users with physical impairments. In this paper, we introduce FAM-HRI, an efficient multi-modal framework for human-robot interaction that integrates language and gaze inputs via foundation models. By leveraging lightweight Meta ARIA glasses, our system captures real-time multi-modal signals and utilizes large language models (LLMs) to fuse user intention with scene context, enabling intuitive and precise robot manipulation. Our method accurately determines gaze fixation time interval, reducing noise caused by the gaze dynamic nature. Experimental evaluations demonstrate that FAM-HRI achieves a high success rate in task execution while maintaining a low interaction time, providing a practical solution for individuals with limited physical mobility or motor impairments.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2503.09758.pdf' target='_blank'>https://arxiv.org/pdf/2503.09758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Ike Obi, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09758">Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in robotics and large language models (LLMs) have sparked growing interest in human-robot collaboration and embodied intelligence. To enable the broader deployment of robots in human-populated environments, socially-aware robot navigation (SAN) has become a key research area. While deep reinforcement learning approaches that integrate human-robot interaction (HRI) with path planning have demonstrated strong benchmark performance, they often struggle to adapt to new scenarios and environments. LLMs offer a promising avenue for zero-shot navigation through commonsense inference. However, most existing LLM-based frameworks rely on centralized decision-making, lack robust verification mechanisms, and face inconsistencies in translating macro-actions into precise low-level control signals. To address these challenges, we propose SAMALM, a decentralized multi-agent LLM actor-critic framework for multi-robot social navigation. In this framework, a set of parallel LLM actors, each reflecting distinct robot personalities or configurations, directly generate control signals. These actions undergo a two-tier verification process via a global critic that evaluates group-level behaviors and individual critics that assess each robot's context. An entropy-based score fusion mechanism further enhances self-verification and re-query, improving both robustness and coordination. Experimental results confirm that SAMALM effectively balances local autonomy with global oversight, yielding socially compliant behaviors and strong adaptability across diverse multi-robot scenarios. More details and videos about this work are available at: https://sites.google.com/view/SAMALM.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2503.01301.pdf' target='_blank'>https://arxiv.org/pdf/2503.01301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Zou, Junda Huang, Boyuan Liang, Honghao Guo, Zhengyang Liu, Xin Ma, Jianshu Zhou, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01301">Few-shot Sim2Real Based on High Fidelity Rendering with Force Feedback Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperation offers a promising approach to robotic data collection and human-robot interaction. However, existing teleoperation methods for data collection are still limited by efficiency constraints in time and space, and the pipeline for simulation-based data collection remains unclear. The problem is how to enhance task performance while minimizing reliance on real-world data. To address this challenge, we propose a teleoperation pipeline for collecting robotic manipulation data in simulation and training a few-shot sim-to-real visual-motor policy. Force feedback devices are integrated into the teleoperation system to provide precise end-effector gripping force feedback. Experiments across various manipulation tasks demonstrate that force feedback significantly improves both success rates and execution efficiency, particularly in simulation. Furthermore, experiments with different levels of visual rendering quality reveal that enhanced visual realism in simulation substantially boosts task performance while reducing the need for real-world data.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2412.02863.pdf' target='_blank'>https://arxiv.org/pdf/2412.02863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Nogueira Nobrega, Ewerton de Oliveira, Martin Saska, Tiago Nascimento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02863">Proximal Control of UAVs with Federated Learning for Human-Robot Collaborative Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human-robot interaction (HRI) is a growing area of research. In HRI, complex command (action) classification is still an open problem that usually prevents the real applicability of such a technique. The literature presents some works that use neural networks to detect these actions. However, occlusion is still a major issue in HRI, especially when using uncrewed aerial vehicles (UAVs), since, during the robot's movement, the human operator is often out of the robot's field of view. Furthermore, in multi-robot scenarios, distributed training is also an open problem. In this sense, this work proposes an action recognition and control approach based on Long Short-Term Memory (LSTM) Deep Neural Networks with two layers in association with three densely connected layers and Federated Learning (FL) embedded in multiple drones. The FL enabled our approach to be trained in a distributed fashion, i.e., access to data without the need for cloud or other repositories, which facilitates the multi-robot system's learning. Furthermore, our multi-robot approach results also prevented occlusion situations, with experiments with real robots achieving an accuracy greater than 96%.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2411.17820.pdf' target='_blank'>https://arxiv.org/pdf/2411.17820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Liu, Jintong Li, Yicheng Jiang, Niranjan Sujay, Zhicheng Yang, Juexiao Zhang, John Abanes, Jing Zhang, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17820">CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating dynamic urban environments presents significant challenges for embodied agents, requiring advanced spatial reasoning and adherence to common-sense norms. Despite progress, existing visual navigation methods struggle in map-free or off-street settings, limiting the deployment of autonomous agents like last-mile delivery robots. To overcome these obstacles, we propose a scalable, data-driven approach for human-like urban navigation by training agents on thousands of hours of in-the-wild city walking and driving videos sourced from the web. We introduce a simple and scalable data processing pipeline that extracts action supervision from these videos, enabling large-scale imitation learning without costly annotations. Our model learns sophisticated navigation policies to handle diverse challenges and critical scenarios. Experimental results show that training on large-scale, diverse datasets significantly enhances navigation performance, surpassing current methods. This work shows the potential of using abundant online video data to develop robust navigation policies for embodied agents in dynamic urban settings. Project homepage is at https://ai4ce.github.io/CityWalker/.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2411.09893.pdf' target='_blank'>https://arxiv.org/pdf/2411.09893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faith Johnson, Bryan Bo Cao, Ashwin Ashok, Shubham Jain, Kristin Dana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09893">Memory Proxy Maps for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation takes inspiration from humans, who navigate in previously unseen environments using vision without detailed environment maps. Inspired by this, we introduce a novel no-RL, no-graph, no-odometry approach to visual navigation using feudal learning to build a three tiered agent. Key to our approach is a memory proxy map (MPM), an intermediate representation of the environment learned in a self-supervised manner by the high-level manager agent that serves as a simplified memory, approximating what the agent has seen. We demonstrate that recording observations in this learned latent space is an effective and efficient memory proxy that can remove the need for graphs and odometry in visual navigation tasks. For the mid-level manager agent, we develop a waypoint network (WayNet) that outputs intermediate subgoals, or waypoints, imitating human waypoint selection during local navigation. For the low-level worker agent, we learn a classifier over a discrete action space that avoids local obstacles and moves the agent towards the WayNet waypoint. The resulting feudal navigation network offers a novel approach with no RL, no graph, no odometry, and no metric map; all while achieving SOTA results on the image goal navigation task.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2410.14993.pdf' target='_blank'>https://arxiv.org/pdf/2410.14993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Donglin Bai, Shiqi Jiang, Qianxi Zhang, Yifan Yang, Xin Ding, Ting Cao, Yunxin Liu, Fengyuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14993">Making Every Frame Matter: Continuous Activity Recognition in Streaming Video via Adaptive Video Context Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video activity recognition has become increasingly important in robots and embodied AI. Recognizing continuous video activities poses considerable challenges due to the fast expansion of streaming video, which contains multi-scale and untrimmed activities. We introduce a novel system, CARS, to overcome these issues through adaptive video context modeling. Adaptive video context modeling refers to selectively maintaining activity-related features in temporal and spatial dimensions. CARS has two key designs. The first is an activity spatial feature extraction by eliminating irrelevant visual features while maintaining recognition accuracy. The second is an activity-aware state update introducing dynamic adaptability to better preserve the video context for multi-scale activity recognition. Our CARS runs at speeds $>$30 FPS on typical edge devices and outperforms all baselines by 1.2\% to 79.7\% in accuracy. Moreover, we explore applying CARS to a large video model as a video encoder. Experimental results show that our CARS can result in a 0.46-point enhancement (on a 5-point scale) on the in-distribution video activity dataset, and an improvement ranging from 1.19\% to 4\% on zero-shot video activity datasets.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2409.20548.pdf' target='_blank'>https://arxiv.org/pdf/2409.20548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20548">Robi Butler: Multimodal Remote Interaction with a Household Robot Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imagine a future when we can Zoom-call a robot to manage household chores remotely. This work takes one step in this direction. Robi Butler is a new household robot assistant that enables seamless multimodal remote interaction. It allows the human user to monitor its environment from a first-person view, issue voice or text commands, and specify target objects through hand-pointing gestures. At its core, a high-level behavior module, powered by Large Language Models (LLMs), interprets multimodal instructions to generate multistep action plans. Each plan consists of open-vocabulary primitives supported by vision-language models, enabling the robot to process both textual and gestural inputs. Zoom provides a convenient interface to implement remote interactions between the human and the robot. The integration of these components allows Robi Butler to ground remote multimodal instructions in real-world home environments in a zero-shot manner. We evaluated the system on various household tasks, demonstrating its ability to execute complex user commands with multimodal inputs. We also conducted a user study to examine how multimodal interaction influences user experiences in remote human-robot interaction. These results suggest that with the advances in robot foundation models, we are moving closer to the reality of remote household robot assistants.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2408.01384.pdf' target='_blank'>https://arxiv.org/pdf/2408.01384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bohan Zhou, Zhongbin Zhang, Jiangxing Wang, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01384">NOLO: Navigate Only Look Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The in-context learning ability of Transformer models has brought new possibilities to visual navigation. In this paper, we focus on the video navigation setting, where an in-context navigation policy needs to be learned purely from videos in an offline manner, without access to the actual environment. For this setting, we propose Navigate Only Look Once (NOLO), a method for learning a navigation policy that possesses the in-context ability and adapts to new scenes by taking corresponding context videos as input without finetuning or re-training. To enable learning from videos, we first propose a pseudo action labeling procedure using optical flow to recover the action label from egocentric videos. Then, offline reinforcement learning is applied to learn the navigation policy. Through extensive experiments on different scenes both in simulation and the real world, we show that our algorithm outperforms baselines by a large margin, which demonstrates the in-context learning ability of the learned policy. For videos and more information, visit https://sites.google.com/view/nol0.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2407.08725.pdf' target='_blank'>https://arxiv.org/pdf/2407.08725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wayne Wu, Honglin He, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08725">MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. Micromobility enabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we present MetaUrban, a compositional simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an infinite number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2407.02405.pdf' target='_blank'>https://arxiv.org/pdf/2407.02405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Lamberti, Vlad Niculescu, MichaÅ Barcis, Lorenzo Bellone, Enrico Natalizio, Luca Benini, Daniele Palossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02405">Tiny-PULP-Dronets: Squeezing Neural Networks for Faster and Lighter Inference on Multi-Tasking Autonomous Nano-Drones</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pocket-sized autonomous nano-drones can revolutionize many robotic use cases, such as visual inspection in narrow, constrained spaces, and ensure safer human-robot interaction due to their tiny form factor and weight -- i.e., tens of grams. This compelling vision is challenged by the high level of intelligence needed aboard, which clashes against the limited computational and storage resources available on PULP (parallel-ultra-low-power) MCU class navigation and mission controllers that can be hosted aboard. This work moves from PULP-Dronet, a State-of-the-Art convolutional neural network for autonomous navigation on nano-drones. We introduce Tiny-PULP-Dronet: a novel methodology to squeeze by more than one order of magnitude model size (50x fewer parameters), and number of operations (27x less multiply-and-accumulate) required to run inference with similar flight performance as PULP-Dronet. This massive reduction paves the way towards affordable multi-tasking on nano-drones, a fundamental requirement for achieving high-level intelligence.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2406.19353.pdf' target='_blank'>https://arxiv.org/pdf/2406.19353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Liu, Chengwen Zhang, Ruofan Xing, Bingda Tang, Bowen Yang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19353">CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2406.06874.pdf' target='_blank'>https://arxiv.org/pdf/2406.06874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06874">Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2406.00765.pdf' target='_blank'>https://arxiv.org/pdf/2406.00765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wakana Haijima, Kou Nakakubo, Masahiro Suzuki, Yutaka Matsuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00765">The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, as machine learning, particularly for vision and language understanding, has been improved, research in embedded AI has also evolved. VOYAGER is a well-known LLM-based embodied AI that enables autonomous exploration in the Minecraft world, but it has issues such as underutilization of visual data and insufficient functionality as a world model. In this research, the possibility of utilizing visual data and the function of LLM as a world model were investigated with the aim of improving the performance of embodied AI. The experimental results revealed that LLM can extract necessary information from visual data, and the utilization of the information improves its performance as a world model. It was also suggested that devised prompts could bring out the LLM's function as a world model.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2405.19449.pdf' target='_blank'>https://arxiv.org/pdf/2405.19449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amy Koike, Bengisu Cagiltay, Bilge Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19449">Tangible Scenography as a Holistic Design Method for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional approaches to human-robot interaction design typically examine robot behaviors in controlled environments and narrow tasks. These methods are impractical for designing robots that interact with diverse user groups in complex human environments. Drawing from the field of theater, we present the construct of scenes -- individual environments consisting of specific people, objects, spatial arrangements, and social norms -- and tangible scenography, as a holistic design approach for human-robot interactions. We created a design tool, Tangible Scenography Kit (TaSK), with physical props to aid in design brainstorming. We conducted design sessions with eight professional designers to generate exploratory designs. Designers used tangible scenography and TaSK components to create multiple scenes with specific interaction goals, characterize each scene's social environment, and design scene-specific robot behaviors. From these sessions, we found that this method can encourage designers to think beyond a robot's narrow capabilities and consider how they can facilitate complex social interactions.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2405.16559.pdf' target='_blank'>https://arxiv.org/pdf/2405.16559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koya Sakamoto, Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16559">Map-based Modular Approach for Zero-shot Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Question Answering (EQA) serves as a benchmark task to evaluate the capability of robots to navigate within novel environments and identify objects in response to human queries. However, existing EQA methods often rely on simulated environments and operate with limited vocabularies. This paper presents a map-based modular approach to EQA, enabling real-world robots to explore and map unknown environments. By leveraging foundation models, our method facilitates answering a diverse range of questions using natural language. We conducted extensive experiments in both virtual and real-world settings, demonstrating the robustness of our approach in navigating and comprehending queries within unknown environments.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2404.18375.pdf' target='_blank'>https://arxiv.org/pdf/2404.18375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanjun Bu, Alexandra Bremers, Mark Colley, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18375">Field Notes on Deploying Research Robots in Public Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction requires to be studied in the wild. In the summers of 2022 and 2023, we deployed two trash barrel service robots through the wizard-of-oz protocol in public spaces to study human-robot interactions in urban settings. We deployed the robots at two different public plazas in downtown Manhattan and Brooklyn for a collective of 20 hours of field time. To date, relatively few long-term human-robot interaction studies have been conducted in shared public spaces. To support researchers aiming to fill this gap, we would like to share some of our insights and learned lessons that would benefit both researchers and practitioners on how to deploy robots in public spaces. We share best practices and lessons learned with the HRI research community to encourage more in-the-wild research of robots in public spaces and call for the community to share their lessons learned to a GitHub repository.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2404.14965.pdf' target='_blank'>https://arxiv.org/pdf/2404.14965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchong Zhang, Yong Ma, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14965">Vision Beyond Boundaries: An Initial Design Space of Domain-specific Large Vision Models in Human-robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of large vision models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years. However, there's a noticeable gap in structured research applying LVMs to human-robot interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots. Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models. We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains. The empirical evaluation was implemented among 15 experts across six evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios. We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2403.14597.pdf' target='_blank'>https://arxiv.org/pdf/2403.14597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yehor Karpichev, Todd Charter, Jayden Hong, Amir M. Soufi Enayati, Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14597">Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole. Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies. The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2403.11384.pdf' target='_blank'>https://arxiv.org/pdf/2403.11384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Wang, Luyao Shen, Lik-Hang Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11384">Towards Massive Interaction with Generalist Robotics: A Systematic Review of XR-enabled Remote Human-Robot Interaction Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rising interest of generalist robots seek to create robots with versatility to handle multiple tasks in a variety of environments, and human will interact with such robots through immersive interfaces. In the context of human-robot interaction (HRI), this survey provides an exhaustive review of the applications of extended reality (XR) technologies in the field of remote HRI. We developed a systematic search strategy based on the PRISMA methodology. From the initial 2,561 articles selected, 100 research papers that met our inclusion criteria were included. We categorized and summarized the domain in detail, delving into XR technologies, including augmented reality (AR), virtual reality (VR), and mixed reality (MR), and their applications in facilitating intuitive and effective remote control and interaction with robotic systems. The survey highlights existing articles on the application of XR technologies, user experience enhancement, and various interaction designs for XR in remote HRI, providing insights into current trends and future directions. We also identified potential gaps and opportunities for future research to improve remote HRI systems through XR technology to guide and inform future XR and robotics research.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2403.10700.pdf' target='_blank'>https://arxiv.org/pdf/2403.10700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10700">Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to -25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark. Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our benchmark dataset. We also propose an effective method, based on a cross-modal transformer architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks. Code and dataset available at https://intelligolabs.github.io/R2RIE-CE
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2402.14281.pdf' target='_blank'>https://arxiv.org/pdf/2402.14281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14281">A Landmark-Aware Visual Navigation Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Map representations learned by expert demonstrations have shown promising research value. However, the field of visual navigation still faces challenges due to the lack of real-world human-navigation datasets that can support efficient, supervised, representation learning of environments. We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building. We collect RGBD observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space. The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization. These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments. Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors. We release our dataset with detailed documentation at https://huggingface.co/datasets/visnavdataset/lavn (DOI: 10.57967/hf/2386) and a plan for long-term preservation.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2402.12498.pdf' target='_blank'>https://arxiv.org/pdf/2402.12498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faith Johnson, Bryan Bo Cao, Ashwin Ashok, Shubham Jain, Kristin Dana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12498">Feudal Networks for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high-level manager, we learn a memory proxy map in a self supervised manner to record prior observations in a learned latent space and avoid the use of graphs and odometry. For the mid-level manager, we develop a waypoint network that outputs intermediate subgoals imitating human waypoint selection during local navigation. This waypoint network is pre-trained using a new, small set of teleoperation videos that we make publicly available, with training environments different from testing environments. The resulting feudal navigation network achieves near SOTA performance, while providing a novel no-RL, no-graph, no-odometry, no-metric map approach to the image goal navigation task.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2402.04768.pdf' target='_blank'>https://arxiv.org/pdf/2402.04768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Esteve Valls Mascaro, Yashuai Yan, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04768">Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands. Webpage: https://evm7.github.io/ECHO/
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2402.04580.pdf' target='_blank'>https://arxiv.org/pdf/2402.04580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyi Niu, Jianming Hu, Guyue Zhou, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04580">A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used in cross-domain policy transfer problems. Lastly, we summarize the open challenges that lie beyond the capabilities of current paradigms and discuss potential future directions in this field.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2401.15174.pdf' target='_blank'>https://arxiv.org/pdf/2401.15174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15174">LaMI: Large Language Models for Multi-Modal Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating "atomic actions" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers' defined guidelines. Simultaneously, it coordinates the robot's lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system's potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2401.12437.pdf' target='_blank'>https://arxiv.org/pdf/2401.12437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denizalp Goktas, Arjun Prakash, Amy Greenwald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12437">Convex-Concave Zero-sum Markov Stackelberg Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-sum Markov Stackelberg games can be used to model myriad problems, in domains ranging from economics to human robot interaction. In this paper, we develop policy gradient methods that solve these games in continuous state and action settings using noisy gradient estimates computed from observed trajectories of play. When the games are convex-concave, we prove that our algorithms converge to Stackelberg equilibrium in polynomial time. We also show that reach-avoid problems are naturally modeled as convex-concave zero-sum Markov Stackelberg games, and that Stackelberg equilibrium policies are more effective than their Nash counterparts in these problems.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2312.07459.pdf' target='_blank'>https://arxiv.org/pdf/2312.07459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlotta Sartore, Lorenzo Rapetti, Fabio Bergonti, Stefano Dafarra, Silvio Traversaro, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07459">Codesign of Humanoid Robots for Ergonomy Collaboration with Multiple Humans via Genetic Algorithms and Nonlinear Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ergonomics is a key factor to consider when designing control architectures for effective physical collaborations between humans and humanoid robots. In contrast, ergonomic indexes are often overlooked in the robot design phase, which leads to suboptimal performance in physical human-robot interaction tasks. This paper proposes a novel methodology for optimizing the design of humanoid robots with respect to ergonomic indicators associated with the interaction of multiple agents. Our approach leverages a dynamic and kinematic parameterization of the robot link and motor specifications to seek for optimal robot designs using a bilevel optimization approach. Specifically, a genetic algorithm first generates robot designs by selecting the link and motor characteristics. Then, we use nonlinear optimization to evaluate interaction ergonomy indexes during collaborative payload lifting with different humans and weights. To assess the effectiveness of our approach, we compare the optimal design obtained using bilevel optimization against the design obtained using nonlinear optimization. Our results show that the proposed approach significantly improves ergonomics in terms of energy expenditure calculated in two reference scenarios involving static and dynamic robot motions. We plan to apply our methodology to drive the design of the ergoCub2 robot, a humanoid intended for optimal physical collaboration with humans in diverse environments
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2312.03446.pdf' target='_blank'>https://arxiv.org/pdf/2312.03446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kibeom Kim, Kisung Shin, Min Whoo Lee, Moonhoen Lee, Minsu Lee, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03446">Visual Hindsight Self-Imitation Learning for Interactive Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive visual navigation tasks, which involve following instructions to reach and interact with specific targets, are challenging not only because successful experiences are very rare but also because the complex visual inputs require a substantial number of samples. Previous methods for these tasks often rely on intricately designed dense rewards or the use of expensive expert data for imitation learning. To tackle these challenges, we propose a novel approach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample efficiency through hindsight goal re-labeling and self-imitation. We also introduce a prototypical goal embedding method derived from experienced goal observations, that is particularly effective in vision-based and partially observable environments. This embedding technique allows the agent to visually reinterpret its unsuccessful attempts, enabling vision-based goal re-labeling and self-imitation from enhanced successful experiences. Experimental results show that VHS outperforms existing techniques in interactive visual navigation tasks, confirming its superior performance and sample efficiency.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2311.05818.pdf' target='_blank'>https://arxiv.org/pdf/2311.05818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Li, Jinhan Li, Wei Fu, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05818">Learning Agile Bipedal Motions on a Quadrupedal Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions. Our framework is with a hierarchical structure. At the low level is a motion-conditioned control policy that allows the quadrupedal robot to track desired base and front limb movements while balancing on two hind feet. The policy is commanded by a high-level motion generator that gives trajectories of parameterized human-like motions to the robot from multiple modalities of human input. We for the first time demonstrate various bipedal motions on a quadrupedal robot, and showcase interesting human-robot interaction modes including mimicking human videos, following natural language instructions, and physical interaction. The video is available at https://sites.google.com/view/bipedal-motions-quadruped.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2311.05334.pdf' target='_blank'>https://arxiv.org/pdf/2311.05334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlo Mazzola, Francesco Rea, Alessandra Sciutti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05334">Real-time Addressee Estimation: Deployment of a Deep-Learning Model on the iCub Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressee Estimation is the ability to understand to whom a person is talking, a skill essential for social robots to interact smoothly with humans. In this sense, it is one of the problems that must be tackled to develop effective conversational agents in multi-party and unstructured scenarios. As humans, one of the channels that mainly lead us to such estimation is the non-verbal behavior of speakers: first of all, their gaze and body pose. Inspired by human perceptual skills, in the present work, a deep-learning model for Addressee Estimation relying on these two non-verbal features is designed, trained, and deployed on an iCub robot. The study presents the procedure of such implementation and the performance of the model deployed in real-time human-robot interaction compared to previous tests on the dataset used for the training.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2309.16524.pdf' target='_blank'>https://arxiv.org/pdf/2309.16524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Esteve Valls Mascaro, Daniel Sliwowski, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16524">HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are becoming increasingly integrated into our lives, assisting us in various tasks. To ensure effective collaboration between humans and robots, it is essential that they understand our intentions and anticipate our actions. In this paper, we propose a Human-Object Interaction (HOI) anticipation framework for collaborative robots. We propose an efficient and robust transformer-based model to detect and anticipate HOIs from videos. This enhanced anticipation empowers robots to proactively assist humans, resulting in more efficient and intuitive collaborations. Our model outperforms state-of-the-art results in HOI detection and anticipation in VidHOI dataset with an increase of 1.76% and 1.04% in mAP respectively while being 15.4 times faster. We showcase the effectiveness of our approach through experimental results in a real robot, demonstrating that the robot's ability to anticipate HOIs is key for better Human-Robot Interaction. More information can be found on our project webpage: https://evm7.github.io/HOI4ABOT_page/
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2305.16925.pdf' target='_blank'>https://arxiv.org/pdf/2305.16925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16925">How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment -- typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, \eg, end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph.
  Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2305.13741.pdf' target='_blank'>https://arxiv.org/pdf/2305.13741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kibeom Kim, Hyundo Lee, Min Whoo Lee, Moonheon Lee, Minsu Lee, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13741">L-SA: Learning Under-Explored Targets in Multi-Target Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks that involve interaction with various targets are called multi-target tasks. When applying general reinforcement learning approaches for such tasks, certain targets that are difficult to access or interact with may be neglected throughout the course of training - a predicament we call Under-explored Target Problem (UTP). To address this problem, we propose L-SA (Learning by adaptive Sampling and Active querying) framework that includes adaptive sampling and active querying. In the L-SA framework, adaptive sampling dynamically samples targets with the highest increase of success rates at a high proportion, resulting in curricular learning from easy to hard targets. Active querying prompts the agent to interact more frequently with under-explored targets that need more experience or exploration. Our experimental results on visual navigation tasks show that the L-SA framework improves sample efficiency as well as success rates on various multi-target tasks with UTP. Also, it is experimentally demonstrated that the cyclic relationship between adaptive sampling and active querying effectively improves the sample richness of under-explored targets and alleviates UTP.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2304.13787.pdf' target='_blank'>https://arxiv.org/pdf/2304.13787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Varun Bhatt, Heramb Nemlekar, Matthew C. Fontaine, Bryon Tjanaka, Hejia Zhang, Ya-Chuan Hsu, Stefanos Nikolaidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13787">Surrogate Assisted Generation of Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2303.07889.pdf' target='_blank'>https://arxiv.org/pdf/2303.07889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Leusmann, Chao Wang, Michael Gienger, Albrecht Schmidt, Sven Mayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07889">Understanding the Uncertainty Loop of Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently the field of Human-Robot Interaction gained popularity, due to the wide range of possibilities of how robots can support humans during daily tasks. One form of supportive robots are socially assistive robots which are specifically built for communicating with humans, e.g., as service robots or personal companions. As they understand humans through artificial intelligence, these robots will at some point make wrong assumptions about the humans' current state and give an unexpected response. In human-human conversations, unexpected responses happen frequently. However, it is currently unclear how such robots should act if they understand that the human did not expect their response, or even showing the uncertainty of their response in the first place. For this, we explore the different forms of potential uncertainties during human-robot conversations and how humanoids can, through verbal and non-verbal cues, communicate these uncertainties.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2302.01039.pdf' target='_blank'>https://arxiv.org/pdf/2302.01039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Anna Belardinelli, Stephan Hasler, Theodoros Stouraitis, Daniel Tanneberg, Michael Gienger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01039">Explainable Human-Robot Training and Cooperation with Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The current spread of social and assistive robotics applications is increasingly highlighting the need for robots that can be easily taught and interacted with, even by users with no technical background. Still, it is often difficult to grasp what such robots know or to assess if a correct representation of the task is being formed. Augmented Reality (AR) has the potential to bridge this gap. We demonstrate three use cases where AR design elements enhance the explainability and efficiency of human-robot interaction: 1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the robot showing its plan for solving novel tasks in AR to a human for validation, and 3) a robot communicating its intentions via AR while assisting people with limited mobility during daily activities.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2301.11972.pdf' target='_blank'>https://arxiv.org/pdf/2301.11972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandra Bremers, Alexandria Pabst, Maria Teresa Parreira, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11972">Using Social Cues to Recognize Task Failures for HRI: Overview, State-of-the-Art, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots that carry out tasks and interact in complex environments will inevitably commit errors. Error detection is thus an essential ability for robots to master to work efficiently and productively. People can leverage social feedback to get an indication of whether an action was successful or not. With advances in computing and artificial intelligence (AI), it is increasingly possible for robots to achieve a similar capability of collecting social feedback. In this work, we take this one step further and propose a framework for how social cues can be used as feedback signals to recognize task failures for human-robot interaction (HRI). Our proposed framework sets out a research agenda based on insights from the literature on behavioral science, human-robot interaction, and machine learning to focus on three areas: 1) social cues as feedback (from behavioral science), 2) recognizing task failures in robots (from HRI), and 3) approaches for autonomous detection of HRI task failures based on social cues (from machine learning). We propose a taxonomy of error detection based on self-awareness and social feedback. Finally, we provide recommendations for HRI researchers and practitioners interested in developing robots that detect task errors using human social cues. This article is intended for interdisciplinary HRI researchers and practitioners, where the third theme of our analysis provides more technical details aiming toward the practical implementation of these systems.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2301.03213.pdf' target='_blank'>https://arxiv.org/pdf/2301.03213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tang, Kevin Liang, Matt Feiszli, Weiyao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03213">EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their "framed" nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2209.14823.pdf' target='_blank'>https://arxiv.org/pdf/2209.14823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Hejrati, Jouni Mattila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14823">Physical Human-Robot Interaction Control of an Upper Limb Exoskeleton with a Decentralized Neuro-Adaptive Control Scheme</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Within the concept of physical human-robot interaction (pHRI), the most important criterion is the safety of the human operator interacting with a high degree of freedom (DoF) robot. Therefore, a robust control scheme is in high demand to establish safe pHRI and stabilize nonlinear, high DoF systems. In this paper, an adaptive decentralized control strategy is designed to accomplish the abovementioned objectives. To do so, a human upper limb model and an exoskeleton model are decentralized and augmented at the subsystem level to enable a decentralized control action design. Moreover, human exogenous force (HEF) that can resist exoskeleton motion is estimated using radial basis function neural networks (RBFNNs). Estimating both human upper limb and robot rigid body parameters, along with HEF estimation, makes the controller adaptable to different operators, ensuring their physical safety. The barrier Lyapunov function (BLF) is employed to guarantee that the robot can operate in a safe workspace while ensuring stability by adjusting the control law. Unknown actuator uncertainty and constraints are also considered in this study to ensure a smooth and safe pHRI. Then, the asymptotic stability of the whole system is established by means of the virtual stability concept and virtual power flows (VPFs) under the proposed robust controller. The experimental results are presented and compared to proportional-derivative (PD) and proportional-integral-derivative (PID) controllers. To show the robustness of the designed controller and its good performance, experiments are performed at different velocities, with different human users, and in the presence of unknown disturbances. The proposed controller showed perfect performance in controlling the robot, whereas PD and PID controllers could not even ensure stable motion in the wrist joints of the robot.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2209.03827.pdf' target='_blank'>https://arxiv.org/pdf/2209.03827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minja Axelsson, Nikhil Churamani, Atahan Caldir, Hatice Gunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.03827">Participant Perceptions of a Robotic Coach Conducting Positive Psychology Exercises: A Qualitative Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a qualitative analysis of participants' perceptions of a robotic coach conducting Positive Psychology exercises, providing insights for the future design of robotic coaches. Participants (n = 20) took part in a single-session (avg. 31 +- 10 minutes) Human-Robot Interaction study in a laboratory setting. We created the design of the robotic coach, and its affective adaptation, based on user-centred design research and collaboration with a professional coach. We transcribed post-study participant interviews and conducted a Thematic Analysis. We discuss the results of that analysis, presenting aspects participants found particularly helpful (e.g., the robot asked the correct questions and helped them think of new positive things in their life), and what should be improved (e.g., the robot's utterance content should be more responsive). We found that participants had no clear preference for affective adaptation or no affective adaptation, which may be due to both positive and negative user perceptions being heightened in the case of adaptation. Based on our qualitative analysis, we highlight insights for the future design of robotic coaches, and areas for future investigation (e.g., examining how participants with different personality traits, or participants experiencing isolation, could benefit from an interaction with a robotic coach).
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2509.18311.pdf' target='_blank'>https://arxiv.org/pdf/2509.18311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin A. Christie, Sagar Parekh, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18311">Fine-Tuning Robot Policies While Maintaining User Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works introduce general-purpose robot policies. These policies provide a strong prior over how robots should behave -- e.g., how a robot arm should manipulate food items. But in order for robots to match an individual person's needs, users typically fine-tune these generalized policies -- e.g., showing the robot arm how to make their own preferred dinners. Importantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat). Other agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors. This leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents? We here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies. Our core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot's network. With the correct key, the robot's policy switches to match that user's preferences -- but with incorrect keys, the robot reverts to its baseline behaviors. We show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks. PRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches. See videos and code here: https://prop-icra26.github.io.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2509.06951.pdf' target='_blank'>https://arxiv.org/pdf/2509.06951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06951">F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2509.05614.pdf' target='_blank'>https://arxiv.org/pdf/2509.05614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzhen Wang, Jiaming Xu, Jiayi Pan, Yongkang Zhou, Guohao Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05614">SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2508.04338.pdf' target='_blank'>https://arxiv.org/pdf/2508.04338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohong Zhong, Alessandro Albini, Giammarco Caroleo, Giorgio Cannata, Perla Maiolino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04338">Improving Tactile Gesture Recognition with Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile gesture recognition systems play a crucial role in Human-Robot Interaction (HRI) by enabling intuitive communication between humans and robots. The literature mainly addresses this problem by applying machine learning techniques to classify sequences of tactile images encoding the pressure distribution generated when executing the gestures. However, some gestures can be hard to differentiate based on the information provided by tactile images alone. In this paper, we present a simple yet effective way to improve the accuracy of a gesture recognition classifier. Our approach focuses solely on processing the tactile images used as input by the classifier. In particular, we propose to explicitly highlight the dynamics of the contact in the tactile image by computing the dense optical flow. This additional information makes it easier to distinguish between gestures that produce similar tactile images but exhibit different contact dynamics. We validate the proposed approach in a tactile gesture recognition task, showing that a classifier trained on tactile images augmented with optical flow information achieved a 9% improvement in gesture classification accuracy compared to one trained on standard tactile images.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2507.23042.pdf' target='_blank'>https://arxiv.org/pdf/2507.23042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Santosh Patapati, Trisanth Srinivasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23042">Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2507.20370.pdf' target='_blank'>https://arxiv.org/pdf/2507.20370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michele Grimaldi, Carlo Cernicchiaro, Sebastian Realpe Rua, Alaaeddine El-Masri-El-Chaarani, Markus Buchholz, Loizos Michael, Pere Ridao Rodriguez, Ignacio Carlucho, Yvan R. Petillot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20370">Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic platforms have become essential for marine operations by providing regular and continuous access to offshore assets, such as underwater infrastructure inspection, environmental monitoring, and resource exploration. However, the complex and dynamic nature of underwater environments, characterized by limited visibility, unpredictable currents, and communication constraints, presents significant challenges that demand advanced autonomy while ensuring operator trust and oversight. Central to addressing these challenges are knowledge representation and reasoning techniques, particularly knowledge graphs and retrieval-augmented generation (RAG) systems, that enable robots to efficiently structure, retrieve, and interpret complex environmental data. These capabilities empower robotic agents to reason, adapt, and respond effectively to changing conditions. The primary goal of this work is to demonstrate both multi-agent autonomy and shared autonomy, where multiple robotic agents operate independently while remaining connected to a human supervisor. We show how a RAG-powered large language model, augmented with knowledge graph data and domain taxonomy, enables autonomous multi-agent decision-making and facilitates seamless human-robot interaction, resulting in 100\% mission validation and behavior completeness. Finally, ablation studies reveal that without structured knowledge from the graph and/or taxonomy, the LLM is prone to hallucinations, which can compromise decision quality.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2507.12846.pdf' target='_blank'>https://arxiv.org/pdf/2507.12846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Bandi Jai Krishna, Navid Kayhani, Oriana Peltzer, David D. Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel J. Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12846">Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2506.18256.pdf' target='_blank'>https://arxiv.org/pdf/2506.18256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Jiang, Boce Hu, Linfeng Zhao, Lawson L. S. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18256">Robot Tactile Gesture Recognition Based on Full-body Modular E-skin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of robot electronic skin technology, various tactile sensors, enhanced by AI, are unlocking a new dimension of perception for robots. In this work, we explore how robots equipped with electronic skin can recognize tactile gestures and interpret them as human commands. We developed a modular robot E-skin, composed of multiple irregularly shaped skin patches, which can be assembled to cover the robot's body while capturing real-time pressure and pose data from thousands of sensing points. To process this information, we propose an equivariant graph neural network-based recognizer that efficiently and accurately classifies diverse tactile gestures, including poke, grab, stroke, and double-pat. By mapping the recognized gestures to predefined robot actions, we enable intuitive human-robot interaction purely through tactile input.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2506.17561.pdf' target='_blank'>https://arxiv.org/pdf/2506.17561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, Lin Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17561">VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2506.03709.pdf' target='_blank'>https://arxiv.org/pdf/2506.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03709">AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2505.24786.pdf' target='_blank'>https://arxiv.org/pdf/2505.24786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Bamani Beeri, Eden Nissinman, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24786">DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic hand gestures play a pivotal role in assistive human-robot interaction (HRI), facilitating intuitive, non-verbal communication, particularly for individuals with mobility constraints or those operating robots remotely. Current gesture recognition methods are mostly limited to short-range interactions, reducing their utility in scenarios demanding robust assistive communication from afar. In this paper, we introduce a novel approach designed specifically for assistive robotics, enabling dynamic gesture recognition at extended distances of up to 30 meters, thereby significantly improving accessibility and quality of life. Our proposed Distance-aware Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust processing and classification of gesture sequences captured under challenging conditions, including significant physical attenuation, reduced resolution, and dynamic gesture variations commonly experienced in real-world assistive environments. We further introduce the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model robustness across varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures. By effectively interpreting gestures from considerable distances, DiG-Net significantly enhances the usability of assistive robots in home healthcare, industrial safety, and remote assistance scenarios, enabling seamless and intuitive interactions for users regardless of physical limitations
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2504.13944.pdf' target='_blank'>https://arxiv.org/pdf/2504.13944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tace McNamara, Jon McCormack, Maria Teresa Llano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13944">Mixer Metaphors: audio interfaces for non-musical applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The NIME conference traditionally focuses on interfaces for music and musical expression. In this paper we reverse this tradition to ask, can interfaces developed for music be successfully appropriated to non-musical applications? To help answer this question we designed and developed a new device, which uses interface metaphors borrowed from analogue synthesisers and audio mixing to physically control the intangible aspects of a Large Language Model. We compared two versions of the device, with and without the audio-inspired augmentations, with a group of artists who used each version over a one week period. Our results show that the use of audio-like controls afforded more immediate, direct and embodied control over the LLM, allowing users to creatively experiment and play with the device over its non-mixer counterpart. Our project demonstrates how cross-sensory metaphors can support creative thinking and embodied practice when designing new technological interfaces.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2504.01293.pdf' target='_blank'>https://arxiv.org/pdf/2504.01293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Xu, Jiayi Shao, Yulan Ju, Ximing Shen, Qingyuan Gao, Weijen Chen, Qing Zhang, Yun Suen Pai, Giulia Barbareschi, Matthias Hoppe, Kouta Minamizawa, Kai Kunze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01293">Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flying robots, such as quadrotor drones, offer new possibilities for human-robot interaction but often pose safety risks due to fast-spinning propellers, rigid structures, and noise. In contrast, lighter-than-air flapping-wing robots, inspired by animal movement, offer a soft, quiet, and touch-safe alternative. Building on these advantages, we present Cuddle-Fish, a soft flapping-wing floating robot designed for close-proximity interactions in indoor spaces. Through a user study with 24 participants, we explored their perceptions of the robot and experiences during a series of co-located demonstrations in which the robot moved near them. Results showed that participants felt safe, willingly engaged in touch-based interactions with the robot, and exhibited spontaneous affective behaviours, such as patting, stroking, hugging, and cheek-touching, without external prompting. They also reported positive emotional responses towards the robot. These findings suggest that the soft floating robot with flapping wings can serve as a novel and socially acceptable alternative to traditional rigid flying robots, opening new potential for applications in companionship, affective interaction, and play in everyday indoor environments.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2503.19240.pdf' target='_blank'>https://arxiv.org/pdf/2503.19240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Guo, Jianfei Zhu, Wei Fan, Chunzhi Yi, Feng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19240">Beyond Object Categories: Multi-Attribute Reference Understanding for Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring expression comprehension (REC) aims at achieving object localization based on natural language descriptions. However, existing REC approaches are constrained by object category descriptions and single-attribute intention descriptions, hindering their application in real-world scenarios. In natural human-robot interactions, users often express their desires through individual states and intentions, accompanied by guiding gestures, rather than detailed object descriptions. To address this challenge, we propose Multi-ref EC, a novel task framework that integrates state descriptions, derived intentions, and embodied gestures to locate target objects. We introduce the State-Intention-Gesture Attributes Reference (SIGAR) dataset, which combines state and intention expressions with embodied references. Through extensive experiments with various baseline models on SIGAR, we demonstrate that properly ordered multi-attribute references contribute to improved localization performance, revealing that single-attribute reference is insufficient for natural human-robot interaction scenarios. Our findings underscore the importance of multi-attribute reference expressions in advancing visual-language understanding.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2503.07547.pdf' target='_blank'>https://arxiv.org/pdf/2503.07547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nina Moorman, Michelle Zhao, Matthew B. Luebbers, Sanne Van Waveren, Reid Simmons, Henny Admoni, Sonia Chernova, Matthew Gombolay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07547">Bi-Directional Mental Model Reconciliation for Human-Robot Interaction with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interactions, human and robot agents maintain internal mental models of their environment, their shared task, and each other. The accuracy of these representations depends on each agent's ability to perform theory of mind, i.e. to understand the knowledge, preferences, and intentions of their teammate. When mental models diverge to the extent that it affects task execution, reconciliation becomes necessary to prevent the degradation of interaction. We propose a framework for bi-directional mental model reconciliation, leveraging large language models to facilitate alignment through semi-structured natural language dialogue. Our framework relaxes the assumption of prior model reconciliation work that either the human or robot agent begins with a correct model for the other agent to align to. Through our framework, both humans and robots are able to identify and communicate missing task-relevant context during interaction, iteratively progressing toward a shared mental model.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2503.02280.pdf' target='_blank'>https://arxiv.org/pdf/2503.02280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carolina Silva-Plata, Carlos Rosel, Barnabas Gavin Cangan, Hosam Alagi, BjÃ¶rn Hein, Robert K. Katzschmann, RubÃ©n FernÃ¡ndez, Yosra Mojtahedi, Stefan Escaida Navarro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02280">Model-Based Capacitive Touch Sensing in Soft Robotics: Achieving Robust Tactile Interactions for Artistic Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a touch technology to achieve tactile interactivity for human-robot interaction (HRI) in soft robotics. By combining a capacitive touch sensor with an online solid mechanics simulation provided by the SOFA framework, contact detection is achieved for arbitrary shapes. Furthermore, the implementation of the capacitive touch technology presented here is selectively sensitive to human touch (conductive objects), while it is largely unaffected by the deformations created by the pneumatic actuation of our soft robot. Multi-touch interactions are also possible. We evaluated our approach with an organic soft robotics sculpture that was created by a visual artist. In particular, we evaluate that the touch localization capabilities are robust under the deformation of the device. We discuss the potential this approach has for the arts and entertainment as well as other domains.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2502.19645.pdf' target='_blank'>https://arxiv.org/pdf/2502.19645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moo Jin Kim, Chelsea Finn, Percy Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19645">Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26$\times$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs ($Ï_0$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2501.19259.pdf' target='_blank'>https://arxiv.org/pdf/2501.19259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amogh Joshi, Sourav Sanyal, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19259">Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2501.01366.pdf' target='_blank'>https://arxiv.org/pdf/2501.01366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Austin T. Wang, ZeMing Gong, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01366">ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2501.00038.pdf' target='_blank'>https://arxiv.org/pdf/2501.00038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanbo Hou, Qiaoqiao Ren, Wenwu Wang, Dick Botteldooren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00038">Sound-Based Recognition of Touch Gestures and Emotions for Enhanced Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion recognition and touch gesture decoding are crucial for advancing human-robot interaction (HRI), especially in social environments where emotional cues and tactile perception play important roles. However, many humanoid robots, such as Pepper, Nao, and Furhat, lack full-body tactile skin, limiting their ability to engage in touch-based emotional and gesture interactions. In addition, vision-based emotion recognition methods usually face strict GDPR compliance challenges due to the need to collect personal facial data. To address these limitations and avoid privacy issues, this paper studies the potential of using the sounds produced by touching during HRI to recognise tactile gestures and classify emotions along the arousal and valence dimensions. Using a dataset of tactile gestures and emotional interactions from 28 participants with the humanoid robot Pepper, we design an audio-only lightweight touch gesture and emotion recognition model with only 0.24M parameters, 0.94MB model size, and 0.7G FLOPs. Experimental results show that the proposed sound-based touch gesture and emotion recognition model effectively recognises the arousal and valence states of different emotions, as well as various tactile gestures, when the input audio length varies. The proposed model is low-latency and achieves similar results as well-known pretrained audio neural networks (PANNs), but with much smaller FLOPs, parameters, and model size.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2412.14058.pdf' target='_blank'>https://arxiv.org/pdf/2412.14058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14058">Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2412.13211.pdf' target='_blank'>https://arxiv.org/pdf/2412.13211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arth Shukla, Stone Tao, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13211">ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2412.10694.pdf' target='_blank'>https://arxiv.org/pdf/2412.10694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junliang Li, Kai Ye, Haolan Kang, Mingxuan Liang, Yuhang Wu, Zhenhua Liu, Huiping Zhuang, Rui Huang, Yongquan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10694">Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, as robotics has advanced, human-robot collaboration has gained increasing importance. However, current robots struggle to fully and accurately interpret human intentions from voice commands alone. Traditional gripper and suction systems often fail to interact naturally with humans, lack advanced manipulation capabilities, and are not adaptable to diverse tasks, especially in unstructured environments. This paper introduces the Embodied Dexterous Grasping System (EDGS), designed to tackle object grasping in cluttered environments for human-robot interaction. We propose a novel approach to semantic-object alignment using a Vision-Language Model (VLM) that fuses voice commands and visual information, significantly enhancing the alignment of multi-dimensional attributes of target objects in complex scenarios. Inspired by human hand-object interactions, we develop a robust, precise, and efficient grasping strategy, incorporating principles like the thumb-object axis, multi-finger wrapping, and fingertip interaction with an object's contact mechanics. We also design experiments to assess Referring Expression Representation Enrichment (RERE) in referring expression segmentation, demonstrating that our system accurately detects and matches referring expressions. Extensive experiments confirm that EDGS can effectively handle complex grasping tasks, achieving stability and high success rates, highlighting its potential for further development in the field of Embodied AI.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2412.08442.pdf' target='_blank'>https://arxiv.org/pdf/2412.08442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, Alexander Toshev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08442">From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We examine the capability of Multimodal Large Language Models (MLLMs) to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. Specifically, our focus lies in areas such as Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. GEA is trained with supervised learning on a large dataset of embodied experiences and with online RL in interactive simulators. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents. The final GEA model achieves strong generalization performance to unseen tasks across diverse benchmarks compared to other generalist models and benchmark-specific approaches.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2412.01663.pdf' target='_blank'>https://arxiv.org/pdf/2412.01663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Sun, Sai Hou, Zixuan Wang, Bo Yu, Shaoshan Liu, Xu Yang, Shuai Liang, Yiming Gan, Yinhe Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01663">DaDu-E: Rethinking the Role of Large Language Model in Robotic Computing Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing complex tasks in open environments remains challenging for robots, even when using large language models (LLMs) as the core planner. Many LLM-based planners are inefficient due to their large number of parameters and prone to inaccuracies because they operate in open-loop systems. We think the reason is that only applying LLMs as planners is insufficient. In this work, we propose DaDu-E, a robust closed-loop planning framework for embodied AI robots. Specifically, DaDu-E is equipped with a relatively lightweight LLM, a set of encapsulated robot skill instructions, a robust feedback system, and memory augmentation. Together, these components enable DaDu-E to (i) actively perceive and adapt to dynamic environments, (ii) optimize computational costs while maintaining high performance, and (iii) recover from execution failures using its memory and feedback mechanisms. Extensive experiments on real-world and simulated tasks show that DaDu-E achieves task success rates comparable to embodied AI robots with larger models as planners like COME-Robot, while reducing computational requirements by $6.6 \times$. Users are encouraged to explore our system at: \url{https://rlc-lab.github.io/dadu-e/}.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2411.18413.pdf' target='_blank'>https://arxiv.org/pdf/2411.18413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Bamani Beeri, Eden Nissinman, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18413">Robust Dynamic Gesture Recognition at Ultra-Long Distances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic hand gestures play a crucial role in conveying nonverbal information for Human-Robot Interaction (HRI), eliminating the need for complex interfaces. Current models for dynamic gesture recognition suffer from limitations in effective recognition range, restricting their application to close proximity scenarios. In this letter, we present a novel approach to recognizing dynamic gestures in an ultra-range distance of up to 28 meters, enabling natural, directive communication for guiding robots in both indoor and outdoor environments. Our proposed SlowFast-Transformer (SFT) model effectively integrates the SlowFast architecture with Transformer layers to efficiently process and classify gesture sequences captured at ultra-range distances, overcoming challenges of low resolution and environmental noise. We further introduce a distance-weighted loss function shown to enhance learning and improve model robustness at varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 95.1% on a diverse dataset with challenging ultra-range gestures. This enables robots to react appropriately to human commands from a far distance, providing an essential enhancement in HRI, especially in scenarios requiring seamless and natural interaction.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2410.08852.pdf' target='_blank'>https://arxiv.org/pdf/2410.08852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michelle Zhao, Reid Simmons, Henny Admoni, Aaditya Ramdas, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08852">Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2409.14296.pdf' target='_blank'>https://arxiv.org/pdf/2409.14296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14296">HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-20 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2409.10078.pdf' target='_blank'>https://arxiv.org/pdf/2409.10078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Chu, Xuan Zhang, Zhedong Zheng, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10078">3D-TAFS: A Training-free Framework for 3D Affordance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating high-level linguistic instructions into precise robotic actions in the physical world remains challenging, particularly when considering the feasibility of interacting with 3D objects. In this paper, we introduce 3D-TAFS, a novel training-free multimodal framework for 3D affordance segmentation. To facilitate a comprehensive evaluation of such frameworks, we present IndoorAfford-Bench, a large-scale benchmark containing 9,248 images spanning 20 diverse indoor scenes across 6 areas, supporting standardized interaction queries. In particular, our framework integrates a large multimodal model with a specialized 3D vision network, enabling a seamless fusion of 2D and 3D visual understanding with language comprehension. Extensive experiments on IndoorAfford-Bench validate the proposed 3D-TAFS's capability in handling interactive 3D affordance segmentation tasks across diverse settings, showcasing competitive performance across various metrics. Our results highlight 3D-TAFS's potential for enhancing human-robot interaction based on affordance understanding in complex indoor environments, advancing the development of more intuitive and efficient robotic frameworks for real-world applications.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2409.06369.pdf' target='_blank'>https://arxiv.org/pdf/2409.06369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Rustler, Matej Misar, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06369">Adaptive Electronic Skin Sensitivity for Safe Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial electronic skins covering complete robot bodies can make physical human-robot collaboration safe and hence possible. Standards for collaborative robots (e.g., ISO/TS 15066) prescribe permissible forces and pressures during contacts with the human body. These characteristics of the collision depend on the speed of the colliding robot link but also on its effective mass. Thus, to warrant contacts complying with the Power and Force Limiting (PFL) collaborative regime but at the same time maximizing productivity, protective skin thresholds should be set individually for different parts of the robot bodies and dynamically on the run. Here we present and empirically evaluate four scenarios: (a) static and uniform - fixed thresholds for the whole skin, (b) static but different settings for robot body parts, (c) dynamically set based on every link velocity, (d) dynamically set based on effective mass of every robot link. We perform experiments in simulation and on a real 6-axis collaborative robot arm (UR10e) completely covered with sensitive skin (AIRSKIN) comprising eleven individual pads. On a mock pick-and-place scenario with transient collisions with the robot body parts and two collision reactions (stop and avoid), we demonstrate the boost in productivity in going from the most conservative setting of the skin thresholds (a) to the most adaptive setting (d). The threshold settings for every skin pad are adapted with a frequency of 25 Hz. This work can be easily extended for platforms with more degrees of freedom and larger skin coverage (humanoids) and to social human-robot interaction scenarios where contacts with the robot will be used for communication.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2408.11138.pdf' target='_blank'>https://arxiv.org/pdf/2408.11138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengwei Xie, Siang Chen, Dingchang Hu, Yixiang Dai, Kaiqin Yang, Guijin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11138">Target-Oriented Object Grasping via Multimodal Human Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of human-robot interaction and collaboration scenarios, robotic grasping still encounters numerous challenges. Traditional grasp detection methods generally analyze the entire scene to predict grasps, leading to redundancy and inefficiency. In this work, we reconsider 6-DoF grasp detection from a target-referenced perspective and propose a Target-Oriented Grasp Network (TOGNet). TOGNet specifically targets local, object-agnostic region patches to predict grasps more efficiently. It integrates seamlessly with multimodal human guidance, including language instructions, pointing gestures, and interactive clicks. Thus our system comprises two primary functional modules: a guidance module that identifies the target object in 3D space and TOGNet, which detects region-focal 6-DoF grasps around the target, facilitating subsequent motion planning. Through 50 target-grasping simulation experiments in cluttered scenes, our system achieves a success rate improvement of about 13.7%. In real-world experiments, we demonstrate that our method excels in various target-oriented grasping scenarios.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2407.21374.pdf' target='_blank'>https://arxiv.org/pdf/2407.21374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Bamani Beeri, Eden Nissinman, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21374">Dynamic Gesture Recognition in Ultra-Range Distance for Effective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach for ultra-range gesture recognition, addressing Human-Robot Interaction (HRI) challenges over extended distances. By leveraging human gestures in video data, we propose the Temporal-Spatiotemporal Fusion Network (TSFN) model that surpasses the limitations of current methods, enabling robots to understand gestures from long distances. With applications in service robots, search and rescue operations, and drone-based interactions, our approach enhances HRI in expansive environments. Experimental validation demonstrates significant advancements in gesture recognition accuracy, particularly in prolonged gesture sequences.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2406.18746.pdf' target='_blank'>https://arxiv.org/pdf/2406.18746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Tziafas, Hamidreza Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18746">Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully hand-crafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2406.07904.pdf' target='_blank'>https://arxiv.org/pdf/2406.07904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt Kira, Alexander Toshev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07904">Grounding Multimodal Large Language Models in Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2405.06593.pdf' target='_blank'>https://arxiv.org/pdf/2405.06593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Manzini, Priyankari Perali, Raisa Karnik, Mihir Godbole, Hasnat Abdullah, Robin Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06593">Non-Uniform Spatial Alignment Errors in sUAS Imagery From Wide-Area Disasters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents the first quantitative study of alignment errors between small uncrewed aerial systems (sUAS) georectified imagery and a priori building polygons and finds that alignment errors are non-uniform and irregular, which negatively impacts field robotics systems and human-robot interfaces that rely on geospatial information. There are no efforts that have considered the alignment of a priori spatial data with georectified sUAS imagery, possibly because straight-forward linear transformations often remedy any misalignment in satellite imagery. However, an attempt to develop machine learning models for an sUAS field robotics system for disaster response from nine wide-area disasters using the CRASAR-U-DROIDs dataset uncovered serious translational alignment errors. The analysis considered 21,608 building polygons in 51 orthomosaic images, covering 16787.2 Acres (26.23 square miles), and 7,880 adjustment annotations, averaging 75.36 pixels and an average intersection over union of 0.65. Further analysis found no uniformity among the angle and distance metrics of the building polygon alignments, presenting an average circular variance of 0.28 and an average distance variance of 0.45 pixels2, making it impossible to use the linear transform used to align satellite imagery. The study's primary contribution is alerting field robotics and human-robot interaction (HRI) communities to the problem of spatial alignment and that a new method will be needed to automate and communicate the alignment of spatial data in sUAS georectified imagery. This paper also contributes a description of the updated CRASAR-U-DROIDs dataset of sUAS imagery, which contains building polygons and human-curated corrections to spatial misalignment for further research in field robotics and HRI.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2404.15190.pdf' target='_blank'>https://arxiv.org/pdf/2404.15190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15190">Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in interactive environments. A key challenge in EIF is compositional task planning, typically addressed through supervised learning or few-shot in-context learning with labeled data. To this end, we introduce the Socratic Planner, a self-QA-based zero-shot planning method that infers an appropriate plan without any further training. The Socratic Planner first facilitates self-questioning and answering by the Large Language Model (LLM), which in turn helps generate a sequence of subgoals. While executing the subgoals, an embodied agent may encounter unexpected situations, such as unforeseen obstacles. The Socratic Planner then adjusts plans based on dense visual feedback through a visually-grounded re-planning mechanism. Experiments demonstrate the effectiveness of the Socratic Planner, outperforming current state-of-the-art planning models on the ALFRED benchmark across all metrics, particularly excelling in long-horizon tasks that demand complex inference. We further demonstrate its real-world applicability through deployment on a physical robot for long-horizon tasks.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2404.09846.pdf' target='_blank'>https://arxiv.org/pdf/2404.09846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09846">A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot's camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2404.00494.pdf' target='_blank'>https://arxiv.org/pdf/2404.00494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathaniel S. Dennler, Mina Kian, Stefanos Nikolaidis, Maja MatariÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00494">Designing Robot Identity: The Role of Voice, Clothing, and Task on Robot Gender Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptions of gender are a significant aspect of human-human interaction, and gender has wide-reaching social implications for robots deployed in contexts where they are expected to interact with humans. This work explored two flexible modalities for communicating gender in robots--voice and appearance--and we studied their individual and combined influences on a robot's perceived gender. We evaluated the perception of a robot's gender through three video-based studies. First, we conducted a study (n=65) on the gender perception of robot voices by varying speaker identity and pitch. Second, we conducted a study (n=93) on the gender perception of robot clothing designed for two different tasks. Finally, building on the results of the first two studies, we completed a large integrative video-based study (n=273) involving two human-robot interaction tasks. We found that voice and clothing can be used to reliably establish a robot's perceived gender, and that combining these two modalities can have different effects on the robot's perceived gender. Taken together, these results inform the design of robot voices and clothing as individual and interacting components in the perceptions of robot gender.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2403.07741.pdf' target='_blank'>https://arxiv.org/pdf/2403.07741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kira Wursthorn, Markus Hillemann, Markus Ulrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07741">Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is one of the top-performing 6D object pose estimation approaches in the BOP Challenge 2022. We apply established metrics and concepts for deep uncertainty quantification to evaluate the results. Furthermore, we propose a novel uncertainty calibration score for regression tasks to quantify the quality of the estimated uncertainty.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2402.17878.pdf' target='_blank'>https://arxiv.org/pdf/2402.17878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Stegner, David Porfirio, Laura M. Hiatt, SÃ©verin Lemaignan, Ross Mead, Bilge Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17878">End-User Development for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-user development (EUD) represents a key step towards making robotics accessible for experts and nonexperts alike. Within academia, researchers investigate novel ways that EUD tools can capture, represent, visualize, analyze, and test developer intent. At the same time, industry researchers increasingly build and ship programming tools that enable customers to interact with their robots. However, despite this growing interest, the role of EUD within HRI is not well defined. EUD struggles to situate itself within a growing array of alternative approaches to application development, such as robot learning and teleoperation. EUD further struggles due to the wide range of individuals who can be considered end users, such as independent third-party application developers, consumers, hobbyists, or even employees of the robot manufacturer. Key questions remain such as how EUD is justified over alternate approaches to application development, which contexts EUD is most suited for, who the target users of an EUD system are, and where interaction between a human and a robot takes place, amongst many other questions. We seek to address these challenges and questions by organizing the first End-User Development for Human-Robot Interaction (EUD4HRI) workshop at the 2024 International Conference of Human-Robot Interaction. The workshop will bring together researchers with a wide range of expertise across academia and industry, spanning perspectives from multiple subfields of robotics, with the primary goal being a consensus of perspectives about the role that EUD must play within human-robot interaction.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2402.11792.pdf' target='_blank'>https://arxiv.org/pdf/2402.11792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Xu, Hanbo Zhang, Xinghang Li, Huaping Liu, Xuguang Lan, Tao Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11792">SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Linguistic ambiguity is ubiquitous in our daily lives. Previous works adopted interaction between robots and humans for language disambiguation. Nevertheless, when interactive robots are deployed in daily environments, there are significant challenges for natural human-robot interaction, stemming from complex and unpredictable visual inputs, open-ended interaction, and diverse user demands. In this paper, we present SInViG, which is a self-evolving interactive visual agent for human-robot interaction based on natural languages, aiming to resolve language ambiguity, if any, through multi-turn visual-language dialogues. It continuously and automatically learns from unlabeled images and large language models, without human intervention, to be more robust against visual and linguistic complexity. Benefiting from self-evolving, it sets new state-of-the-art on several interactive visual grounding benchmarks. Moreover, our human-robot interaction experiments show that the evolved models consistently acquire more and more preferences from human users. Besides, we also deployed our model on a Franka robot for interactive manipulation tasks. Results demonstrate that our model can follow diverse user instructions and interact naturally with humans in natural language, despite the complexity and disturbance of the environment.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2402.04210.pdf' target='_blank'>https://arxiv.org/pdf/2402.04210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben Amor, Subbarao Kambhampati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04210">Task Success is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2312.03327.pdf' target='_blank'>https://arxiv.org/pdf/2312.03327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobo Hu, Youfang Lin, HeHe Fan, Shuo Wang, Zhihao Wu, Kai Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03327">Building Category Graphs Representation with Spatial and Temporal Attention for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given an object of interest, visual navigation aims to reach the object's location based on a sequence of partial observations. To this end, an agent needs to 1) learn a piece of certain knowledge about the relations of object categories in the world during training and 2) look for the target object based on the pre-learned object category relations and its moving trajectory in the current unseen environment. In this paper, we propose a Category Relation Graph (CRG) to learn the knowledge of object category layout relations and a Temporal-Spatial-Region (TSR) attention architecture to perceive the long-term spatial-temporal dependencies of objects helping the navigation. We learn prior knowledge of object layout, establishing a category relationship graph to deduce the positions of specific objects. Subsequently, we introduced TSR to capture the relationships of objects in temporal, spatial, and regions within the observation trajectories. Specifically, we propose a Temporal attention module (T) to model the temporal structure of the observation sequence, which implicitly encodes the historical moving or trajectory information. Then, a Spatial attention module (S) is used to uncover the spatial context of the current observation objects based on the category relation graph and past observations. Last, a Region attention module (R) shifts the attention to the target-relevant region. Based on the visual representation extracted by our method, the agent can better perceive the environment and easily learn superior navigation policy. Experiments on AI2-THOR demonstrate our CRG-TSR method significantly outperforms existing methods regarding both effectiveness and efficiency. The code has been included in the supplementary material and will be publicly available.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2311.15361.pdf' target='_blank'>https://arxiv.org/pdf/2311.15361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Bamani, Eden Nissinman, Inbar Meir, Lisa Koenigsberg, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15361">Ultra-Range Gesture Recognition using a Web-Camera in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand gestures play a significant role in human interactions where non-verbal intentions, thoughts and commands are conveyed. In Human-Robot Interaction (HRI), hand gestures offer a similar and efficient medium for conveying clear and rapid directives to a robotic agent. However, state-of-the-art vision-based methods for gesture recognition have been shown to be effective only up to a user-camera distance of seven meters. Such a short distance range limits practical HRI with, for example, service robots, search and rescue robots and drones. In this work, we address the Ultra-Range Gesture Recognition (URGR) problem by aiming for a recognition distance of up to 25 meters and in the context of HRI. We propose the URGR framework, a novel deep-learning, using solely a simple RGB camera. Gesture inference is based on a single image. First, a novel super-resolution model termed High-Quality Network (HQ-Net) uses a set of self-attention and convolutional layers to enhance the low-resolution image of the user. Then, we propose a novel URGR classifier termed Graph Vision Transformer (GViT) which takes the enhanced image as input. GViT combines the benefits of a Graph Convolutional Network (GCN) and a modified Vision Transformer (ViT). Evaluation of the proposed framework over diverse test data yields a high recognition rate of 98.1%. The framework has also exhibited superior performance compared to human recognition in ultra-range distances. With the framework, we analyze and demonstrate the performance of an autonomous quadruped robot directed by human gestures in complex ultra-range indoor and outdoor environments, acquiring 96% recognition rate on average.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2310.17552.pdf' target='_blank'>https://arxiv.org/pdf/2310.17552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huihan Liu, Shivin Dass, Roberto MartÃ­n-MartÃ­n, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17552">Model-Based Runtime Monitoring with Interactive Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method learns a latent-space dynamics model and a failure classifier, enabling our method to simulate future action outcomes and detect out-of-distribution and high-risk states preemptively. We train our method within an interactive imitation learning framework, where it continually updates the model from the experiences of the human-robot team collected using trustworthy deployments. Consequently, our method reduces the human workload needed over time while ensuring reliable task execution. Our method outperforms the baselines across system-level and unit-test metrics, with 23% and 40% higher success rates in simulation and on physical hardware, respectively. More information at https://ut-austin-rpl.github.io/sirius-runtime-monitor/
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2310.12147.pdf' target='_blank'>https://arxiv.org/pdf/2310.12147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanbo Zhang, Jie Xu, Yuchen Mo, Tao Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12147">InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ambiguity is ubiquitous in human communication. Previous approaches in Human-Robot Interaction (HRI) have often relied on predefined interaction templates, leading to reduced performance in realistic and open-ended scenarios. To address these issues, we present a large-scale dataset, \invig, for interactive visual grounding under language ambiguity. Our dataset comprises over 520K images accompanied by open-ended goal-oriented disambiguation dialogues, encompassing millions of object instances and corresponding question-answer pairs. Leveraging the \invig dataset, we conduct extensive studies and propose a set of baseline solutions for end-to-end interactive visual disambiguation and grounding, achieving a 45.6\% success rate during validation. To the best of our knowledge, the \invig dataset is the first large-scale dataset for resolving open-ended interactive visual grounding, presenting a practical yet highly challenging benchmark for ambiguity-aware HRI. Codes and datasets are available at: \href{https://openivg.github.io}{https://openivg.github.io}.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2310.12020.pdf' target='_blank'>https://arxiv.org/pdf/2310.12020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengqiang Zhang, Philipp Wicke, LÃ¼tfi Kerem Åenel, Luis Figueredo, Abdeldjallil Naceri, Sami Haddadin, Barbara Plank, Hinrich SchÃ¼tze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12020">LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption generation and learnable interface for incorporating explicit and implicit observation feedback to the LLM, respectively. These methods serve as the two baselines for our proposed benchmark. Experiments show that both methods struggle to solve some tasks, indicating long-horizon manipulation tasks are still challenging for current popular models. We expect the proposed public benchmark and baselines can help the community develop better models for long-horizon tabletop manipulation tasks.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2309.14931.pdf' target='_blank'>https://arxiv.org/pdf/2309.14931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Walter Jansma, Elia Trevisan, Ãlvaro Serra-GÃ³mez, Javier Alonso-Mora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14931">Interaction-Aware Sampling-Based MPC with Learned Local Goal Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion planning for autonomous robots in tight, interaction-rich, and mixed human-robot environments is challenging. State-of-the-art methods typically separate prediction and planning, predicting other agents' trajectories first and then planning the ego agent's motion in the remaining free space. However, agents' lack of awareness of their influence on others can lead to the freezing robot problem. We build upon Interaction-Aware Model Predictive Path Integral (IA-MPPI) control and combine it with learning-based trajectory predictions, thereby relaxing its reliance on communicated short-term goals for other agents. We apply this framework to Autonomous Surface Vessels (ASVs) navigating urban canals. By generating an artificial dataset in real sections of Amsterdam's canals, adapting and training a prediction model for our domain, and proposing heuristics to extract local goals, we enable effective cooperation in planning. Our approach improves autonomous robot navigation in complex, crowded environments, with potential implications for multi-agent systems and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2309.13857.pdf' target='_blank'>https://arxiv.org/pdf/2309.13857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13857">Adversarial Attacks on Video Object Segmentation with Hard Region Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation has been applied to various computer vision tasks, such as video editing, autonomous driving, and human-robot interaction. However, the methods based on deep neural networks are vulnerable to adversarial examples, which are the inputs attacked by almost human-imperceptible perturbations, and the adversary (i.e., attacker) will fool the segmentation model to make incorrect pixel-level predictions. This will rise the security issues in highly-demanding tasks because small perturbations to the input video will result in potential attack risks. Though adversarial examples have been extensively used for classification, it is rarely studied in video object segmentation. Existing related methods in computer vision either require prior knowledge of categories or cannot be directly applied due to the special design for certain tasks, failing to consider the pixel-wise region attack. Hence, this work develops an object-agnostic adversary that has adversarial impacts on VOS by first-frame attacking via hard region discovery. Particularly, the gradients from the segmentation model are exploited to discover the easily confused region, in which it is difficult to identify the pixel-wise objects from the background in a frame. This provides a hardness map that helps to generate perturbations with a stronger adversarial power for attacking the first frame. Empirical studies on three benchmarks indicate that our attacker significantly degrades the performance of several state-of-the-art video object segmentation models.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2309.09936.pdf' target='_blank'>https://arxiv.org/pdf/2309.09936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazin Hamad, Simone Nertinger, Robin J. Kirschner, Luis Figueredo, Abdeldjallil Naceri, Sami Haddadin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09936">A Concise Overview of Safety Aspects in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As of today, robots exhibit impressive agility but also pose potential hazards to humans using/collaborating with them. Consequently, safety is considered the most paramount factor in human-robot interaction (HRI). This paper presents a multi-layered safety architecture, integrating both physical and cognitive aspects for effective HRI. We outline critical requirements for physical safety layers as service modules that can be arbitrarily queried. Further, we showcase an HRI scheme that addresses human factors and perceived safety as high-level constraints on a validated impact safety paradigm. The aim is to enable safety certification of human-friendly robots across various HRI scenarios.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2309.09137.pdf' target='_blank'>https://arxiv.org/pdf/2309.09137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rashmi Bhaskara, Hrishikesh Viswanath, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09137">Trajectory Prediction for Robot Navigation using Flow-Guided Markov Neural Operator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian movements remains a complex and persistent challenge in robot navigation research. We must evaluate several factors to achieve accurate predictions, such as pedestrian interactions, the environment, crowd density, and social and cultural norms. Accurate prediction of pedestrian paths is vital for ensuring safe human-robot interaction, especially in robot navigation. Furthermore, this research has potential applications in autonomous vehicles, pedestrian tracking, and human-robot collaboration. Therefore, in this paper, we introduce FlowMNO, an Optical Flow-Integrated Markov Neural Operator designed to capture pedestrian behavior across diverse scenarios. Our paper models trajectory prediction as a Markovian process, where future pedestrian coordinates depend solely on the current state. This problem formulation eliminates the need to store previous states. We conducted experiments using standard benchmark datasets like ETH, HOTEL, ZARA1, ZARA2, UCY, and RGB-D pedestrian datasets. Our study demonstrates that FlowMNO outperforms some of the state-of-the-art deep learning methods like LSTM, GAN, and CNN-based approaches, by approximately 86.46% when predicting pedestrian trajectories. Thus, we show that FlowMNO can seamlessly integrate into robot navigation systems, enhancing their ability to navigate crowded areas smoothly.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2309.08865.pdf' target='_blank'>https://arxiv.org/pdf/2309.08865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Revanth Krishna Senthilkumaran, Mridu Prashanth, Hrishikesh Viswanath, Sathvika Kotha, Kshitij Tiwari, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08865">ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical Information System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mass casualty incidents (MCIs) pose a significant challenge to emergency medical services by overwhelming available resources and personnel. Effective victim assessment is the key to minimizing casualties during such a crisis. We introduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical Information System, to aid first responders in MCI events. It leverages speech processing, natural language processing, and deep learning to help with acuity classification. This is deployed on a quadruped that performs victim localization and preliminary injury severity assessment. First responders access victim information through a Graphical User Interface that is updated in real-time. To validate our proposed algorithmic triage protocol, we used the Unitree Go1 quadruped. The robot identifies humans, interacts with them, gets vitals and information, and assigns an acuity label. Simulations of an MCI in software and a controlled environment outdoors were conducted. The system achieved a triage-level classification precision of over 74% on average and 99% for the most critical victims, i.e. level 1 acuity, outperforming state-of-the-art deep learning-based triage labeling systems. In this paper, we showcase the potential of human-robot interaction in assisting medical personnel in MCI events.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2307.13850.pdf' target='_blank'>https://arxiv.org/pdf/2307.13850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vidhi Jain, Jayant Sravan Tamarapalli, Sahiti Yerramilli, Yonatan Bisk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13850">MAEA: Multimodal Attribution for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2307.10743.pdf' target='_blank'>https://arxiv.org/pdf/2307.10743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Franceschi, Fabio Bertini, Francesco Braghin, Loris Roveda, Nicola Pedrocchi, Manuel Beschi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10743">Predicting human motion intention for pHRI assistive control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses human intention identification during physical Human-Robot Interaction (pHRI) tasks to include this information in an assistive controller. To this purpose, human intention is defined as the desired trajectory that the human wants to follow over a finite rolling prediction horizon so that the robot can assist in pursuing it. This work investigates a Recurrent Neural Network (RNN), specifically, Long-Short Term Memory (LSTM) cascaded with a Fully Connected layer. In particular, we propose an iterative training procedure to adapt the model. Such an iterative procedure is powerful in reducing the prediction error. Still, it has the drawback that it is time-consuming and does not generalize to different users or different co-manipulated objects. To overcome this issue, Transfer Learning (TL) adapts the pre-trained model to new trajectories, users, and co-manipulated objects by freezing the LSTM layer and fine-tuning the last FC layer, which makes the procedure faster. Experiments show that the iterative procedure adapts the model and reduces prediction error. Experiments also show that TL adapts to different users and to the co-manipulation of a large object. Finally, to check the utility of adopting the proposed method, we compare the proposed controller enhanced by the intention prediction with the other two standard controllers of pHRI.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2307.10739.pdf' target='_blank'>https://arxiv.org/pdf/2307.10739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Franceschi, Manuel Beschi, Nicola Pedrocchi, Anna Valente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10739">Modeling and analysis of pHRI with Differential Game Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applications involving humans and robots working together are spreading nowadays. Alongside, modeling and control techniques that allow physical Human-Robot Interaction (pHRI) are widely investigated. To better understand its potential application in pHRI, this work investigates the Cooperative Differential Game Theory modeling of pHRI in a cooperative reaching task, specifically for reference tracking. The proposed controller based on Collaborative Game Theory is deeply analyzed and compared in simulations with two other techniques, Linear Quadratic Regulator (LQR) and Non-Cooperative Game-Theoretic Controller. The set of simulations shows how different tuning of control parameters affects the system response and control efforts of both the players for the three controllers, suggesting the use of Cooperative GT in the case the robot should assist the human, while Non-Cooperative GT represents a better choice in the case the robot should lead the action. Finally, preliminary tests with a trained human are performed to extract useful information on the real applicability and limitations of the proposed method.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2307.02949.pdf' target='_blank'>https://arxiv.org/pdf/2307.02949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Yoav Matalon, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02949">Recognition and Estimation of Human Finger Pointing with an RGB Camera for Robot Directive</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In communication between humans, gestures are often preferred or complementary to verbal expression since the former offers better spatial referral. Finger pointing gesture conveys vital information regarding some point of interest in the environment. In human-robot interaction, a user can easily direct a robot to a target location, for example, in search and rescue or factory assistance. State-of-the-art approaches for visual pointing estimation often rely on depth cameras, are limited to indoor environments and provide discrete predictions between limited targets. In this paper, we explore the learning of models for robots to understand pointing directives in various indoor and outdoor environments solely based on a single RGB camera. A novel framework is proposed which includes a designated model termed PointingNet. PointingNet recognizes the occurrence of pointing followed by approximating the position and direction of the index finger. The model relies on a novel segmentation model for masking any lifted arm. While state-of-the-art human pose estimation models provide poor pointing angle estimation accuracy of 28deg, PointingNet exhibits mean accuracy of less than 2deg. With the pointing information, the target is computed followed by planning and motion of the robot. The framework is evaluated on two robotic systems yielding accurate target reaching.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2306.14830.pdf' target='_blank'>https://arxiv.org/pdf/2306.14830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Wicke, LÃ¼fti Kerem Åenel, Shengqiang Zhang, Luis Figueredo, Abdeldjallil Naceri, Sami Haddadin, Hinrich SchÃ¼tze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14830">Towards Language-Based Modulation of Assistive Robots through Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of Geriatronics, enabling effective and transparent communication between humans and robots is crucial for enhancing the acceptance and performance of assistive robots. Our early-stage research project investigates the potential of language-based modulation as a means to improve human-robot interaction. We propose to explore real-time modulation during task execution, leveraging language cues, visual references, and multimodal inputs. By developing transparent and interpretable methods, we aim to enable robots to adapt and respond to language commands, enhancing their usability and flexibility. Through the exchange of insights and knowledge at the workshop, we seek to gather valuable feedback to advance our research and contribute to the development of interactive robotic systems for Geriatronics and beyond.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2306.10376.pdf' target='_blank'>https://arxiv.org/pdf/2306.10376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu, Sungjoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10376">CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we focus on inferring whether the given user command is clear, ambiguous, or infeasible in the context of interactive robotic agents utilizing large language models (LLMs). To tackle this problem, we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command is classified as uncertain, we further distinguish it between ambiguous or infeasible commands leveraging LLMs with situational aware context in a zero-shot manner. For ambiguous commands, we disambiguate the command by interacting with users via question generation with LLMs. We believe that proper recognition of the given commands could lead to a decrease in malfunction and undesired actions of the robot, enhancing the reliability of interactive robot agents. We present a dataset for robotic situational awareness, consisting pair of high-level commands, scene descriptions, and labels of command type (i.e., clear, ambiguous, or infeasible). We validate the proposed method on the collected dataset, pick-and-place tabletop simulation. Finally, we demonstrate the proposed approach in real-world human-robot interaction experiments, i.e., handover scenarios.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2303.17592.pdf' target='_blank'>https://arxiv.org/pdf/2303.17592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen, Wei Yang, Claudia PÃ©rez-D'Arpino, Otmar Hilliges, Dieter Fox, Yu-Wei Chao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17592">Learning Human-to-Robot Handovers from Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the first framework to learn control policies for vision-based human-to-robot handovers, a critical task for human-robot interaction. While research in Embodied AI has made significant progress in training robot agents in simulated environments, interacting with humans remains challenging due to the difficulties of simulating humans. Fortunately, recent research has developed realistic simulated environments for human-to-robot handovers. Leveraging this result, we introduce a method that is trained with a human-in-the-loop via a two-stage teacher-student framework that uses motion and grasp planning, reinforcement learning, and self-supervision. We show significant performance gains over baselines on a simulation benchmark, sim-to-sim transfer and sim-to-real transfer.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2301.00901.pdf' target='_blank'>https://arxiv.org/pdf/2301.00901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Tian, Masayoshi Tomizuka, Anca Dragan, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00901">Towards Modeling and Influencing the Dynamics of Human Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans have internal models of robots (like their physical capabilities), the world (like what will happen next), and their tasks (like a preferred goal). However, human internal models are not always perfect: for example, it is easy to underestimate a robot's inertia. Nevertheless, these models change and improve over time as humans gather more experience. Interestingly, robot actions influence what this experience is, and therefore influence how people's internal models change. In this work we take a step towards enabling robots to understand the influence they have, leverage it to better assist people, and help human models more quickly align with reality. Our key idea is to model the human's learning as a nonlinear dynamical system which evolves the human's internal model given new observations. We formulate a novel optimization problem to infer the human's learning dynamics from demonstrations that naturally exhibit human learning. We then formalize how robots can influence human learning by embedding the human's learning dynamics model into the robot planning problem. Although our formulations provide concrete problem statements, they are intractable to solve in full generality. We contribute an approximation that sacrifices the complexity of the human internal models we can represent, but enables robots to learn the nonlinear dynamics of these internal models. We evaluate our inference and planning methods in a suite of simulated environments and an in-person user study, where a 7DOF robotic arm teaches participants to be better teleoperators. While influencing human learning remains an open problem, our results demonstrate that this influence is possible and can be helpful in real human-robot interaction.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2210.14791.pdf' target='_blank'>https://arxiv.org/pdf/2210.14791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simar Kareer, Naoki Yokoyama, Dhruv Batra, Sehoon Ha, Joanne Truong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.14791">ViNL: Visual Navigation and Locomotion Over Obstacles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely "model-free", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely "zero-shot" (without any co-training). While prior works have developed learning methods for visual navigation or visual locomotion, to the best of our knowledge, this is the first fully learned approach that leverages vision to accomplish both (1) intelligent navigation in new environments, and (2) intelligent visual locomotion that aims to traverse cluttered environments without disrupting obstacles. On the task of navigation to distant goals in unknown environments, ViNL using just egocentric vision significantly outperforms prior work on robust locomotion using privileged terrain maps (+32.8% success and -4.42 collisions per meter). Additionally, we ablate our locomotion policy to show that each aspect of our approach helps reduce obstacle collisions. Videos and code at http://www.joannetruong.com/projects/vinl.html
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2210.13542.pdf' target='_blank'>https://arxiv.org/pdf/2210.13542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linfeng Zhao, Huazhe Xu, Lawson L. S. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13542">Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation, and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2207.03395.pdf' target='_blank'>https://arxiv.org/pdf/2207.03395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaunak A. Mehta, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.03395">Unified Learning from Demonstrations, Corrections, and Preferences during Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can leverage physical interaction to teach robot arms. This physical interaction takes multiple forms depending on the task, the user, and what the robot has learned so far. State-of-the-art approaches focus on learning from a single modality, or combine multiple interaction types by assuming that the robot has prior information about the human's intended task. By contrast, in this paper we introduce an algorithmic formalism that unites learning from demonstrations, corrections, and preferences. Our approach makes no assumptions about the tasks the human wants to teach the robot; instead, we learn a reward model from scratch by comparing the human's inputs to nearby alternatives. We first derive a loss function that trains an ensemble of reward models to match the human's demonstrations, corrections, and preferences. The type and order of feedback is up to the human teacher: we enable the robot to collect this feedback passively or actively. We then apply constrained optimization to convert our learned reward into a desired robot trajectory. Through simulations and a user study we demonstrate that our proposed approach more accurately learns manipulation tasks from physical human interaction than existing baselines, particularly when the robot is faced with new or unexpected objectives. Videos of our user study are available at: https://youtu.be/FSUJsTYvEKU
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2206.11626.pdf' target='_blank'>https://arxiv.org/pdf/2206.11626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Barnabas Gavin Cangan, Stefan Escaida Navarro, Bai Yang, Yu Zhang, Christian Duriez, Robert K. Katzschmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.11626">Model-Based Disturbance Estimation for a Fiber-Reinforced Soft Manipulator using Orientation Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For soft robots to work effectively in human-centered environments, they need to be able to estimate their state and external interactions based on (proprioceptive) sensors. Estimating disturbances allows a soft robot to perform desirable force control. Even in the case of rigid manipulators, force estimation at the end-effector is seen as a non-trivial problem. And indeed, other current approaches to address this challenge have shortcomings that prevent their general application. They are often based on simplified soft dynamic models, such as the ones relying on a piece-wise constant curvature (PCC) approximation or matched rigid-body models that do not represent enough details of the problem. Thus, the applications needed for complex human-robot interaction can not be built. Finite element methods (FEM) allow for predictions of soft robot dynamics in a more generic fashion. Here, using the soft robot modeling capabilities of the framework SOFA, we build a detailed FEM model of a multi-segment soft continuum robotic arm composed of compliant deformable materials and fiber-reinforced pressurized actuation chambers with a model for sensors that provide orientation output. This model is used to establish a state observer for the manipulator. Model parameters were calibrated to match imperfections of the manual fabrication process using physical experiments. We then solve a quadratic programming inverse dynamics problem to compute the components of external force that explain the pose error. Our experiments show an average force estimation error of around 1.2%. As the methods proposed are generic, these results are encouraging for the task of building soft robots exhibiting complex, reactive, sensor-based behavior that can be deployed in human-centered environments.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2111.14422.pdf' target='_blank'>https://arxiv.org/pdf/2111.14422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobo Hu, Youfang Lin, Shuo Wang, Zhihao Wu, Kai Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.14422">Agent-Centric Relation Graph for Object Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object visual navigation aims to steer an agent toward a target object based on visual observations. It is highly desirable to reasonably perceive the environment and accurately control the agent. In the navigation task, we introduce an Agent-Centric Relation Graph (ACRG) for learning the visual representation based on the relationships in the environment. ACRG is a highly effective structure that consists of two relationships, i.e., the horizontal relationship among objects and the distance relationship between the agent and objects . On the one hand, we design the Object Horizontal Relationship Graph (OHRG) that stores the relative horizontal location among objects. On the other hand, we propose the Agent-Target Distance Relationship Graph (ATDRG) that enables the agent to perceive the distance between the target and objects. For ATDRG, we utilize image depth to obtain the target distance and imply the vertical location to capture the distance relationship among objects in the vertical direction. With the above graphs, the agent can perceive the environment and output navigation actions. Experimental results in the artificial environment AI2-THOR demonstrate that ACRG significantly outperforms other state-of-the-art methods in unseen testing environments.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2109.10493.pdf' target='_blank'>https://arxiv.org/pdf/2109.10493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoki Yokoyama, Qian Luo, Dhruv Batra, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.10493">Benchmarking Augmentation Methods for Learning Robust Navigation Agents: the Winning Entry of the 2021 iGibson Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep reinforcement learning and scalable photorealistic simulation have led to increasingly mature embodied AI for various visual tasks, including navigation. However, while impressive progress has been made for teaching embodied agents to navigate static environments, much less progress has been made on more dynamic environments that may include moving pedestrians or movable obstacles. In this study, we aim to benchmark different augmentation techniques for improving the agent's performance in these challenging environments. We show that adding several dynamic obstacles into the scene during training confers significant improvements in test-time generalization, achieving much higher success rates than baseline agents. We find that this approach can also be combined with image augmentation methods to achieve even higher success rates. Additionally, we show that this approach is also more robust to sim-to-sim transfer than image augmentation methods. Finally, we demonstrate the effectiveness of this dynamic obstacle augmentation approach by using it to train an agent for the 2021 iGibson Challenge at CVPR, where it achieved 1st place for Interactive Navigation. Video link: https://www.youtube.com/watch?v=HxUX2HeOSE4
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2105.08666.pdf' target='_blank'>https://arxiv.org/pdf/2105.08666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing-Cheng Pang, Tian Xu, Shengyi Jiang, Yu-Ren Liu, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.08666">Reinforcement Learning With Sparse-Executing Actions via Sparsity Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has demonstrated impressive performance in decision-making tasks like embodied control, autonomous driving and financial trading. In many decision-making tasks, the agents often encounter the problem of executing actions under limited budgets. However, classic RL methods typically overlook the challenges posed by such sparse-executing actions. They operate under the assumption that all actions can be taken for a unlimited number of times, both in the formulation of the problem and in the development of effective algorithms. To tackle the issue of limited action execution in RL, this paper first formalizes the problem as a Sparse Action Markov Decision Process (SA-MDP), in which specific actions in the action space can only be executed for a limited time. Then, we propose a policy optimization algorithm, Action Sparsity REgularization (ASRE), which adaptively handles each action with a distinct preference. ASRE operates through two steps: First, ASRE evaluates action sparsity by constrained action sampling. Following this, ASRE incorporates the sparsity evaluation into policy learning by way of an action distribution regularization. We provide theoretical identification that validates the convergence of ASRE to a regularized optimal value function. Experiments on tasks with known sparse-executing actions, where classical RL algorithms struggle to train policy efficiently, ASRE effectively constrains the action sampling and outperforms baselines. Moreover, we present that ASRE can generally improve the performance in Atari games, demonstrating its broad applicability.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2103.08022.pdf' target='_blank'>https://arxiv.org/pdf/2103.08022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoki Yokoyama, Sehoon Ha, Dhruv Batra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.08022">Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent's dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exemplifies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in an real robot to navigate an apartment, and show that they can generalize in a zero-shot manner.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2509.18447.pdf' target='_blank'>https://arxiv.org/pdf/2509.18447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Madan, Jiawei Lin, Mahika Goel, Angchen Xie, Xiaoyu Liang, Marcus Lee, Justin Guo, Pranav N. Thakkar, Rohan Banerjee, Jose Barreiros, Kate Tsui, Tom Silver, Tapomayukh Bhattacharjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18447">PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives--trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2509.17430.pdf' target='_blank'>https://arxiv.org/pdf/2509.17430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gunjan Chhablani, Xiaomeng Ye, Muhammad Zubair Irshad, Zsolt Kira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17430">EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2509.16122.pdf' target='_blank'>https://arxiv.org/pdf/2509.16122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carter Sifferman, Mohit Gupta, Michael Gleicher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16122">Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We provide a method for detecting and localizing objects near a robot arm using arm-mounted miniature time-of-flight sensors. A key challenge when using arm-mounted sensors is differentiating between the robot itself and external objects in sensor measurements. To address this challenge, we propose a computationally lightweight method which utilizes the raw time-of-flight information captured by many off-the-shelf, low-resolution time-of-flight sensor. We build an empirical model of expected sensor measurements in the presence of the robot alone, and use this model at runtime to detect objects in proximity to the robot. In addition to avoiding robot self-detections in common sensor configurations, the proposed method enables extra flexibility in sensor placement, unlocking configurations which achieve more efficient coverage of a radius around the robot arm. Our method can detect small objects near the arm and localize the position of objects along the length of a robot link to reasonable precision. We evaluate the performance of the method with respect to object type, location, and ambient light level, and identify limiting factors on performance inherent in the measurement principle. The proposed method has potential applications in collision avoidance and in facilitating safe human-robot interaction.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2509.02749.pdf' target='_blank'>https://arxiv.org/pdf/2509.02749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgia Buracchio, Ariele Callegari, Massimo Donini, Cristina Gena, Antonio Lieto, Alberto Lillo, Claudio Mattutino, Alessandro Mazzei, Linda Pigureddu, Manuel Striani, Fabiana Vernero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02749">The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper presents an experiment on the effects of adaptive emotional alignment between agents, considered a prerequisite for empathic communication, in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the robot's persuasive effectiveness, (ii) the user's communication style, and (iii) the attribution of mental states and empathy to the robot. In an experiment with 42 participants, two conditions were compared: one with neutral communication and another where the robot provided responses adapted to the emotions expressed by the users. The results show that emotional alignment does not influence users' communication styles or have a persuasive effect. However, it significantly influences attribution of mental states to the robot and its perceived empathy
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2509.00218.pdf' target='_blank'>https://arxiv.org/pdf/2509.00218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandra Landowska, Aislinn D Gomez Bergin, Ayodeji O. Abioye, Jayati Deshmukh, Andriana Bouadouki, Maria Wheadon, Athina Georgara, Dominic Price, Tuyen Nguyen, Shuang Ao, Lokesh Singh, Yi Long, Raffaele Miele, Joel E. Fischer, Sarvapali D. Ramchurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00218">Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting -- UKAIRS 2025 (Copy)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces and overviews a multidisciplinary project aimed at developing responsible and adaptive multi-human multi-robot (MHMR) systems for complex, dynamic settings. The project integrates co-design, ethical frameworks, and multimodal sensing to create AI-driven robots that are emotionally responsive, context-aware, and aligned with the needs of diverse users. We outline the project's vision, methodology, and early outcomes, demonstrating how embodied AI can support sustainable, ethical, and human-centred futures.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2508.20664.pdf' target='_blank'>https://arxiv.org/pdf/2508.20664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kan Chen, Zhen Meng, Xiangmin Xu, Jiaming Yang, Emma Li, Philip G. Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20664">Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time human-device interaction in industrial Metaverse faces challenges such as high computational load, limited bandwidth, and strict latency. This paper proposes a task-oriented edge-assisted cross-system framework using digital twins (DTs) to enable responsive interactions. By predicting operator motions, the system supports: 1) proactive Metaverse rendering for visual feedback, and 2) preemptive control of remote devices. The DTs are decoupled into two virtual functions-visual display and robotic control-optimizing both performance and adaptability. To enhance generalizability, we introduce the Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates the framework's effectiveness: in a Trajectory-Based Drawing Control task, it reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene representation task for nuclear decommissioning, it achieves a PSNR of 22.11, SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's capability to ensure spatial precision and visual fidelity in real-time, high-risk industrial environments.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2507.02521.pdf' target='_blank'>https://arxiv.org/pdf/2507.02521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayodeji O. Abioye, Jayati Deshmukh, Athina Georgara, Dominic Price, Tuyen Nguyen, Aleksandra Landowska, Amel Bennaceur, Joel E. Fischer, Sarvapali D. Ramchurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02521">Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research investigates strategies for multi-robot coordination in multi-human environments. It proposes a multi-objective learning-based coordination approach to addressing the problem of path planning, navigation, task scheduling, task allocation, and human-robot interaction in multi-human multi-robot (MHMR) settings.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2507.01667.pdf' target='_blank'>https://arxiv.org/pdf/2507.01667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Monaci, Philippe Weinzaepfel, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01667">What does really matter in image goal navigation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image goal navigation requires two different skills: firstly, core navigation skills, including the detection of free space and obstacles, and taking decisions based on an internal representation; and secondly, computing directional information by comparing visual observations to the goal image. Current state-of-the-art methods either rely on dedicated image-matching, or pre-training of computer vision modules on relative pose estimation. In this paper, we study whether this task can be efficiently solved with end-to-end training of full agents with RL, as has been claimed by recent work. A positive answer would have impact beyond Embodied AI and allow training of relative pose estimation from reward for navigation alone. In a large study we investigate the effect of architectural choices like late fusion, channel stacking, space-to-depth projections and cross-attention, and their role in the emergence of relative pose estimators from navigation training. We show that the success of recent methods is influenced up to a certain extent by simulator settings, leading to shortcuts in simulation. However, we also show that these capabilities can be transferred to more realistic setting, up to some extend. We also find evidence for correlations between navigation performance and probed (emerging) relative pose estimation performance, an important sub skill.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2506.17967.pdf' target='_blank'>https://arxiv.org/pdf/2506.17967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariya Hendriksen, Tabish Rashid, David Bignell, Raluca Georgescu, Abdelhak Lemkhenter, Katja Hofmann, Sam Devlin, Sarah Parisot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17967">Adapting Vision-Language Models for Evaluating World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models -- generative models that simulate environment dynamics conditioned on past observations and actions -- are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency -- capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce a evaluation protocol targeting two recognition tasks -- action recognition and character recognition -- each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a method for adapting VLMs to rollout evaluation under data and compute constraints. We conduct a large-scale study comparing full, partial, and parameter-efficient finetuning across task formats, context lengths, sampling strategies, and data compositions. The resulting unified evaluator matches the performance of task-specific baselines using a single checkpoint. Human studies confirm strong alignment with human judgments, establishing UNIVERSE as a scalable, semantics-aware evaluator for world models.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2506.17221.pdf' target='_blank'>https://arxiv.org/pdf/2506.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17221">VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2506.09937.pdf' target='_blank'>https://arxiv.org/pdf/2506.09937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Gu, Yuanliang Ju, Shengxiang Sun, Igor Gilitschenski, Haruki Nishimura, Masha Itkina, Florian Shkurti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09937">SAFE: Multitask Failure Detection for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $Ï_0$, and $Ï_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2506.06630.pdf' target='_blank'>https://arxiv.org/pdf/2506.06630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heeju Ko, Sungjune Kim, Gyeongrok Oh, Jeongyoon Yoon, Honglak Lee, Sujin Jang, Seungryong Kim, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06630">Active Test-time Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2506.02112.pdf' target='_blank'>https://arxiv.org/pdf/2506.02112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuweiyi Chen, Tian Xia, Sihan Xu, Jianing Yang, Joyce Chai, Zezhou Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02112">SAB3R: Semantic-Augmented Backbone in 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new task, Map and Locate, which unifies the traditionally distinct objectives of open-vocabulary segmentation - detecting and segmenting object instances based on natural language queries - and 3D reconstruction, the process of estimating a scene's 3D structure from visual inputs. Specifically, Map and Locate involves generating a point cloud from an unposed video and segmenting object instances based on open-vocabulary queries. This task serves as a critical step toward real-world embodied AI applications and introduces a practical task that bridges reconstruction, recognition and reorganization. To tackle this task, we introduce a simple yet effective baseline, which we denote as SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer vision, and incorporates a lightweight distillation strategy. This method transfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP and DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary frozen networks, our model generates per-pixel semantic features and constructs cohesive point maps in a single forward pass. Compared to separately deploying MASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the Map and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic segmentation and 3D tasks to comprehensively validate its effectiveness.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2505.15954.pdf' target='_blank'>https://arxiv.org/pdf/2505.15954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Paykari, Ali Alfatemi, Damian M. Lyons, Mohamed Rahouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15954">Integrating Robotic Navigation with Blockchain: A Novel PoS-Based Approach for Heterogeneous Robotic Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work explores a novel integration of blockchain methodologies with Wide Area Visual Navigation (WAVN) to address challenges in visual navigation for a heterogeneous team of mobile robots deployed for unstructured applications in agriculture, forestry, etc. Focusing on overcoming challenges such as GPS independence, environmental changes, and computational limitations, the study introduces the Proof of Stake (PoS) mechanism, commonly used in blockchain systems, into the WAVN framework \cite{Lyons_2022}. This integration aims to enhance the cooperative navigation capabilities of robotic teams by prioritizing robot contributions based on their navigation reliability. The methodology involves a stake weight function, consensus score with PoS, and a navigability function, addressing the computational complexities of robotic cooperation and data validation. This innovative approach promises to optimize robotic teamwork by leveraging blockchain principles, offering insights into the scalability, efficiency, and overall system performance. The project anticipates significant advancements in autonomous navigation and the broader application of blockchain technology beyond its traditional financial context.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2505.13186.pdf' target='_blank'>https://arxiv.org/pdf/2505.13186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Scholl, Alexander Dietrich, Sebastian Wolf, Jinoh Lee, Alin-Albu SchÃ¤ffer, Gitta Kutyniok, Maged Iskandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13186">Interpretable Robotic Friction Learning via Symbolic Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately modeling the friction torque in robotic joints has long been challenging due to the request for a robust mathematical description. Traditional model-based approaches are often labor-intensive, requiring extensive experiments and expert knowledge, and they are difficult to adapt to new scenarios and dependencies. On the other hand, data-driven methods based on neural networks are easier to implement but often lack robustness, interpretability, and trustworthiness--key considerations for robotic hardware and safety-critical applications such as human-robot interaction. To address the limitations of both approaches, we propose the use of symbolic regression (SR) to estimate the friction torque. SR generates interpretable symbolic formulas similar to those produced by model-based methods while being flexible to accommodate various dynamic effects and dependencies. In this work, we apply SR algorithms to approximate the friction torque using collected data from a KUKA LWR-IV+ robot. Our results show that SR not only yields formulas with comparable complexity to model-based approaches but also achieves higher accuracy. Moreover, SR-derived formulas can be seamlessly extended to include load dependencies and other dynamic factors.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2504.06124.pdf' target='_blank'>https://arxiv.org/pdf/2504.06124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin A. Christie, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06124">Safe Interaction via Monte Carlo Linear-Quadratic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: https://youtu.be/KJuHeiWVuWY.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2504.03629.pdf' target='_blank'>https://arxiv.org/pdf/2504.03629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cody Simons, Aritra Samanta, Amit K. Roy-Chowdhury, Konstantinos Karydis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03629">SeGuE: Semantic Guided Exploration for Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of embodied AI applications has enabled robots to perform complex tasks which require a sophisticated understanding of their environment. To enable successful robot operation in such settings, maps must be constructed so that they include semantic information, in addition to geometric information. In this paper, we address the novel problem of semantic exploration, whereby a mobile robot must autonomously explore an environment to fully map both its structure and the semantic appearance of features. We develop a method based on next-best-view exploration, where potential poses are scored based on the semantic features visible from that pose. We explore two alternative methods for sampling potential views and demonstrate the effectiveness of our framework in both simulation and physical experiments. Automatic creation of high-quality semantic maps can enable robots to better understand and interact with their environments and enable future embodied AI applications to be more easily deployed.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2503.23760.pdf' target='_blank'>https://arxiv.org/pdf/2503.23760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Scheibl, Birte Richter, Alissa Müller, Michael Beetz, Britta Wrede
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23760">Towards a cognitive architecture to enable natural language interaction in co-constructive task learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research addresses the question, which characteristics a cognitive architecture must have to leverage the benefits of natural language in Co-Constructive Task Learning (CCTL). To provide context, we first discuss Interactive Task Learning (ITL), the mechanisms of the human memory system, and the significance of natural language and multi-modality. Next, we examine the current state of cognitive architectures, analyzing their capabilities to inform a concept of CCTL grounded in multiple sources. We then integrate insights from various research domains to develop a unified framework. Finally, we conclude by identifying the remaining challenges and requirements necessary to achieve CCTL in Human-Robot Interaction (HRI).
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2503.19692.pdf' target='_blank'>https://arxiv.org/pdf/2503.19692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AndrÃ© GroÃ, Birte Richter, Bjarne Thomzik, Britta Wrede
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19692">Leveraging Cognitive States for Adaptive Scaffolding of Understanding in Explanatory Tasks in HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how scaffolding strategies influence human understanding in human-robot interaction is important for developing effective assistive systems. This empirical study investigates linguistic scaffolding strategies based on negation as an important means that de-biases the user from potential errors but increases processing costs and hesitations as a means to ameliorate processing costs. In an adaptive strategy, the user state with respect to the current state of understanding and processing capacity was estimated via a scoring scheme based on task performance, prior scaffolding strategy, and current eye gaze behavior. In the study, the adaptive strategy of providing negations and hesitations was compared with a non-adaptive strategy of providing only affirmations. The adaptive scaffolding strategy was generated using the computational model SHIFT. Our findings indicate that using adaptive scaffolding strategies with SHIFT tends to (1) increased processing costs, as reflected in longer reaction times, but (2) improved task understanding, evidenced by a lower error rate of almost 23%. We assessed the efficiency of SHIFT's selected scaffolding strategies across different cognitive states, finding that in three out of five states, the error rate was lower compared to the baseline condition. We discuss how these results align with the assumptions of the SHIFT model and highlight areas for refinement. Moreover, we demonstrate how scaffolding strategies, such as negation and hesitation, contribute to more effective human-robot explanatory dialogues.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2503.16447.pdf' target='_blank'>https://arxiv.org/pdf/2503.16447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AndrÃ© GroÃ, Birte Richter, Britta Wrede
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16447">SHIFT: An Interdisciplinary Framework for Scaffolding Human Attention and Understanding in Explanatory Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a domain-independent approach for adaptive scaffolding in robotic explanation generation to guide tasks in human-robot interaction. We present a method for incorporating interdisciplinary research results into a computational model as a pre-configured scoring system implemented in a framework called SHIFT. This involves outlining a procedure for integrating concepts from disciplines outside traditional computer science into a robotics computational framework. Our approach allows us to model the human cognitive state into six observable states within the human partner model. To study the pre-configuration of the system, we implement a reinforcement learning approach on top of our model. This approach allows adaptation to individuals who deviate from the configuration of the scoring system. Therefore, in our proof-of-concept evaluation, the model's adaptability on four different user types shows that the models' adaptation performs better, i.e., recouped faster after exploration and has a higher accumulated reward with our pre-configured scoring system than without it. We discuss further strategies of speeding up the learning phase to enable a realistic adaptation behavior to real users. The system is accessible through docker and supports querying via ROS.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2503.06469.pdf' target='_blank'>https://arxiv.org/pdf/2503.06469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>George Tang, Aditya Agarwal, Weiqiao Han, Trevor Darrell, Yutong Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06469">Vector Quantized Feature Fields for Fast 3D Semantic Lifting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We generalize lifting to semantic lifting by incorporating per-view masks that indicate relevant pixels for lifting tasks. These masks are determined by querying corresponding multiscale pixel-aligned feature maps, which are derived from scene representations such as distilled feature fields and feature point clouds. However, storing per-view feature maps rendered from distilled feature fields is impractical, and feature point clouds are expensive to store and query. To enable lightweight on-demand retrieval of pixel-aligned relevance masks, we introduce the Vector-Quantized Feature Field. We demonstrate the effectiveness of the Vector-Quantized Feature Field on complex indoor and outdoor scenes. Semantic lifting, when paired with a Vector-Quantized Feature Field, can unlock a myriad of applications in scene representation and embodied intelligence. Specifically, we showcase how our method enables text-driven localized scene editing and significantly improves the efficiency of embodied question answering.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2503.01058.pdf' target='_blank'>https://arxiv.org/pdf/2503.01058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Chen, Ni Ou, Xuyang Zhang, Zhiyuan Wu, Yongqiang Zhao, Yupeng Wang, Nathan Lepora, Lorenzo Jamone, Jiankang Deng, Shan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01058">General Force Sensation for Tactile Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic tactile sensors, including vision-based and taxel-based sensors, enable agile manipulation and safe human-robot interaction through force sensation. However, variations in structural configurations, measured signals, and material properties create domain gaps that limit the transferability of learned force sensation across different tactile sensors. Here, we introduce GenForce, a general framework for achieving transferable force sensation across both homogeneous and heterogeneous tactile sensors in robotic systems. By unifying tactile signals into marker-based binary tactile images, GenForce enables the transfer of existing force labels to arbitrary target sensors using a marker-to-marker translation technique with a few paired data. This process equips uncalibrated tactile sensors with force prediction capabilities through spatiotemporal force prediction models trained on the transferred data. Extensive experimental results validate GenForce's generalizability, accuracy, and robustness across sensors with diverse marker patterns, structural designs, material properties, and sensing principles. The framework significantly reduces the need for costly and labor-intensive labeled data collection, enabling the rapid deployment of multiple tactile sensors on robotic hands requiring force sensing capabilities.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2503.00283.pdf' target='_blank'>https://arxiv.org/pdf/2503.00283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Nikhil Antony, Maia Stiber, Chien-Ming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00283">Xpress: A System For Dynamic, Context-Aware Robot Facial Expressions using Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expressions are vital in human communication and significantly influence outcomes in human-robot interaction (HRI), such as likeability, trust, and companionship. However, current methods for generating robotic facial expressions are often labor-intensive, lack adaptability across contexts and platforms, and have limited expressive ranges--leading to repetitive behaviors that reduce interaction quality, particularly in long-term scenarios. We introduce Xpress, a system that leverages language models (LMs) to dynamically generate context-aware facial expressions for robots through a three-phase process: encoding temporal flow, conditioning expressions on context, and generating facial expression code. We demonstrated Xpress as a proof-of-concept through two user studies (n=15x2) and a case study with children and parents (n=13), in storytelling and conversational scenarios to assess the system's context-awareness, expressiveness, and dynamism. Results demonstrate Xpress's ability to dynamically produce expressive and contextually appropriate facial expressions, highlighting its versatility and potential in HRI applications.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2502.09278.pdf' target='_blank'>https://arxiv.org/pdf/2502.09278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Onat Åahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09278">ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View Gaussian Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have significantly improved 3D generation, enabling the use of assets generated from an image for embodied AI simulations. However, the one-to-many nature of the image-to-3D problem limits their use due to inconsistent content and quality across views. Previous models optimize a 3D model by sampling views from a view-conditioned diffusion prior, but diffusion models cannot guarantee view consistency. Instead, we present ConsistentDreamer, where we first generate a set of fixed multi-view prior images and sample random views between them with another diffusion model through a score distillation sampling (SDS) loss. Thereby, we limit the discrepancies between the views guided by the SDS loss and ensure a consistent rough shape. In each iteration, we also use our generated multi-view prior images for fine-detail reconstruction. To balance between the rough shape and the fine-detail optimizations, we introduce dynamic task-dependent weights based on homoscedastic uncertainty, updated automatically in each iteration. Additionally, we employ opacity, depth distortion, and normal alignment losses to refine the surface for mesh extraction. Our method ensures better view consistency and visual quality compared to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2502.07380.pdf' target='_blank'>https://arxiv.org/pdf/2502.07380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Han, Preet Shah, Sidharth Rajagopal, Yanda Bao, Sanghun Jung, Sidharth Talia, Gabriel Guo, Bryan Xu, Bhaumik Mehta, Emma Romig, Rosario Scalise, Byron Boots
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07380">Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) has been pivotal in recent robotics milestones and is poised to play a prominent role in the future. However, these advances can rely on proprietary simulators, expensive hardware, and a daunting range of tools and skills. As a result, broader communities are disconnecting from the state-of-the-art; education curricula are poorly equipped to teach indispensable modern robotics skills involving hardware, deployment, and iterative development. To address this gap between the broader and scientific communities, we contribute Wheeled Lab, an ecosystem which integrates accessible, open-source wheeled robots with Isaac Lab, an open-source robot learning and simulation framework, that is widely adopted in the state-of-the-art. To kickstart research and education, this work demonstrates three state-of-the-art zero-shot policies for small-scale RC cars developed through Wheeled Lab: controlled drifting, elevation traversal, and visual navigation. The full stack, from hardware to software, is low-cost and open-source. Videos and additional materials can be found at: https://uwrobotlearning.github.io/WheeledLab/
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2502.06838.pdf' target='_blank'>https://arxiv.org/pdf/2502.06838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Jieya Zhou, Su Zheng, Shuo Yin, Kaichao Liang, Shoubo Hu, Xiao Chen, Bei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06838">TorchResist: Open-Source Differentiable Resist Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent decades have witnessed remarkable advancements in artificial intelligence (AI), including large language models (LLMs), image and video generative models, and embodied AI systems. These advancements have led to an explosive increase in the demand for computational power, challenging the limits of Moore's Law. Optical lithography, a critical technology in semiconductor manufacturing, faces significant challenges due to its high costs. To address this, various lithography simulators have been developed. However, many of these simulators are limited by their inadequate photoresist modeling capabilities. This paper presents TorchResist, an open-source, differentiable photoresist simulator.TorchResist employs an analytical approach to model the photoresist process, functioning as a white-box system with at most twenty interpretable parameters. Leveraging modern differentiable programming techniques and parallel computing on GPUs, TorchResist enables seamless co-optimization with other tools across multiple related tasks. Our experimental results demonstrate that TorchResist achieves superior accuracy and efficiency compared to existing solutions. The source code is publicly available.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2501.13518.pdf' target='_blank'>https://arxiv.org/pdf/2501.13518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Benavent-Lledo, David Mulero-PÃ©rez, David Ortiz-Perez, Jose Garcia-Rodriguez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13518">Text-driven Online Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting actions as they occur is essential for applications like video surveillance, autonomous driving, and human-robot interaction. Known as online action detection, this task requires classifying actions in streaming videos, handling background noise, and coping with incomplete actions. Transformer architectures are the current state-of-the-art, yet the potential of recent advancements in computer vision, particularly vision-language models (VLMs), remains largely untapped for this problem, partly due to high computational costs. In this paper, we introduce TOAD: a Text-driven Online Action Detection architecture that supports zero-shot and few-shot learning. TOAD leverages CLIP (Contrastive Language-Image Pretraining) textual embeddings, enabling efficient use of VLMs without significant computational overhead. Our model achieves 82.46% mAP on the THUMOS14 dataset, outperforming existing methods, and sets new baselines for zero-shot and few-shot performance on the THUMOS14 and TVSeries datasets.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2412.05552.pdf' target='_blank'>https://arxiv.org/pdf/2412.05552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengze Zhou, Yicong Hong, Zun Wang, Chongyang Zhao, Mohit Bansal, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05552">SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into a unified and generic framework -- we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose a novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present a versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2411.14092.pdf' target='_blank'>https://arxiv.org/pdf/2411.14092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Woehrle, Arun N. Sivakumar, Naveen Uppalapati, Girish Chowdhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14092">MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous under-canopy navigation faces additional challenges compared to over-canopy settings - for example the tight spacing between the crop rows, degraded GPS accuracy and excessive clutter. Keypoint-based visual navigation has been shown to perform well in these conditions, however the differences between agricultural environments in terms of lighting, season, soil and crop type mean that a domain shift will likely be encountered at some point of the robot deployment. In this paper, we explore the use of Meta-Learning to overcome this domain shift using a minimal amount of data. We train a base-learner that can quickly adapt to new conditions, enabling more robust navigation in low-data regimes.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2411.09299.pdf' target='_blank'>https://arxiv.org/pdf/2411.09299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Arreghini, Antonio Paolillo, Gabriele Abbate, Alessandro Giusti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09299">Hearing the Robot's Mind: Sonification for Explicit Feedback in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are required not only to understand human intentions but also to effectively communicate their intentions or own internal states to users. This study explores the use of sonification to provide explicit auditory feedback, enhancing mutual understanding in HRI. We introduce a novel sonification approach that conveys the robot's internal state, linked to its perception of nearby individuals and their interaction intentions. The approach is evaluated through a two-fold user study: an online video-based survey with $26$ participants and live experiments with $10$ participants. Results indicate that while sonification improves the robot's expressivity and communication effectiveness, the design of the auditory feedback needs refinement to enhance user experience. Participants found the auditory cues useful but described the sounds as uninteresting and unpleasant. These findings underscore the importance of carefully designed auditory feedback in developing more effective and engaging HRI systems.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2411.03483.pdf' target='_blank'>https://arxiv.org/pdf/2411.03483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caio Mucchiani, Dimitrios Chatziparaschis, Konstantinos Karydis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03483">Augmented-Reality Enabled Crop Monitoring with Robot Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of augmented reality (AR), extended reality (XR), and virtual reality (VR) technologies in agriculture has shown significant promise in enhancing various agricultural practices. Mobile robots have also been adopted as assessment tools in precision agriculture, improving economic efficiency and productivity, and minimizing undesired effects such as weeds and pests. Despite considerable work on both fronts, the combination of a versatile User Interface (UI) provided by an AR headset with the integration and direct interaction and control of a mobile field robot has not yet been fully explored or standardized. This work aims to address this gap by providing real-time data input and control output of a mobile robot for precision agriculture through a virtual environment enabled by an AR headset interface. The system leverages open-source computational tools and off-the-shelf hardware for effective integration. Distinctive case studies are presented where growers or technicians can interact with a legged robot via an AR headset and a UI. Users can teleoperate the robot to gather information in an area of interest, request real-time graphed status of an area, or have the robot autonomously navigate to selected areas for measurement updates. The proposed system utilizes a custom local navigation method with a fixed holographic coordinate system in combination with QR codes. This step toward fusing AR and robotics in agriculture aims to provide practical solutions for real-time data management and control enabled by human-robot interaction. The implementation can be extended to various robot applications in agriculture and beyond, promoting a unified framework for on-demand and autonomous robot operation in the field.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2410.05554.pdf' target='_blank'>https://arxiv.org/pdf/2410.05554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maulik Bhatt, Iman Askari, Yue Yu, Ufuk Topcu, Huazhen Fang, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05554">MultiNash-PF: A Particle Filtering Approach for Computing Multiple Local Generalized Nash Equilibria in Trajectory Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern robotic systems frequently engage in complex multi-agent interactions, many of which are inherently multi-modal, i.e., they can lead to multiple distinct outcomes. To interact effectively, robots must recognize the possible interaction modes and adapt to the one preferred by other agents. In this work, we propose MultiNash-PF, an efficient algorithm for capturing the multimodality in multi-agent interactions. We model interaction outcomes as equilibria of a game-theoretic planner, where each equilibrium corresponds to a distinct interaction mode. Our framework formulates interactive planning as Constrained Potential Trajectory Games (CPTGs), in which local Generalized Nash Equilibria (GNEs) represent plausible interaction outcomes. We propose to integrate the potential game approach with implicit particle filtering, a sample-efficient method for non-convex trajectory optimization. We utilize implicit particle filtering to identify the coarse estimates of multiple local minimizers of the game's potential function. MultiNash-PF then refines these estimates with optimization solvers, obtaining different local GNEs. We show through numerical simulations that MultiNash-PF reduces computation time by up to 50\% compared to a baseline. We further demonstrate the effectiveness of our algorithm in real-world human-robot interaction scenarios, where it successfully accounts for the multi-modal nature of interactions and resolves potential conflicts in real-time.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2410.03287.pdf' target='_blank'>https://arxiv.org/pdf/2410.03287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Arreghini, Gabriele Abbate, Alessandro Giusti, Antonio Paolillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03287">A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a service robot that offers chocolate treats to people passing in its proximity: it has the capability of predicting in advance a person's intention to interact, and to actuate an "offering" gesture, subtly extending the tray of chocolates towards a given target. We run the system for more than 5 hours across 3 days and two different crowded public locations; the system implements three possible behaviors that are randomly toggled every few minutes: passive (e.g. never performing the offering gesture); or active, triggered by either a naive distance-based rule, or a smart approach that relies on various behavioral cues of the user. We collect a real-world dataset that includes information on 1777 users with several spontaneous human-robot interactions and study the influence of robot actions on people's behavior. Our comprehensive analysis suggests that users are more prone to engage with the robot when it proactively starts the interaction. We release the dataset and provide insights to make our work reproducible for the community. Also, we report qualitative observations collected during the acquisition campaign and identify future challenges and research directions in the domain of social human-robot interaction.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2409.19459.pdf' target='_blank'>https://arxiv.org/pdf/2409.19459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cody Simons, Zhichao Liu, Brandon Marcus, Amit K. Roy-Chowdhury, Konstantinos Karydis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19459">Language-guided Robust Navigation for Mobile Robots in Dynamically-changing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we develop an embodied AI system for human-in-the-loop navigation with a wheeled mobile robot. We propose a direct yet effective method of monitoring the robot's current plan to detect changes in the environment that impact the intended trajectory of the robot significantly and then query a human for feedback. We also develop a means to parse human feedback expressed in natural language into local navigation waypoints and integrate it into a global planning system, by leveraging a map of semantic features and an aligned obstacle map. Extensive testing in simulation and physical hardware experiments with a resource-constrained wheeled robot tasked to navigate in a real-world environment validate the efficacy and robustness of our method. This work can support applications like precision agriculture and construction, where persistent monitoring of the environment provides a human with information about the environment state.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2409.13837.pdf' target='_blank'>https://arxiv.org/pdf/2409.13837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mani Amani, Reza Akhavian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13837">Adaptive Robot Perception in Construction Environments using 4D BIM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) is a pivotal component of robot perception for physical Human Robot Interaction (pHRI) tasks. In construction robotics, it is vital that robots have an accurate and robust perception of worker activities. This enhanced perception is the foundation of trustworthy and safe Human-Robot Collaboration (HRC) in an industrial setting. Many developed HAR algorithms lack the robustness and adaptability to ensure seamless HRC. Recent works have employed multi-modal approaches to increase feature considerations. This paper further expands previous research to include 4D building information modeling (BIM) schedule data. We created a pipeline that transforms high-level BIM schedule activities into a set of low-level tasks in real-time. The framework then utilizes this subset as a tool to restrict the solution space that the HAR algorithm can predict activities from. By limiting this subspace through 4D BIM schedule data, the algorithm has a higher chance of predicting the true possible activities from a smaller pool of possibilities in a localized setting as compared to calculating all global possibilities at every point. Results indicate that the proposed approach achieves higher confidence predictions over the base model without leveraging the BIM data.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2409.00015.pdf' target='_blank'>https://arxiv.org/pdf/2409.00015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Bakirtzis, Andrea Aler Tubella, Andreas Theodorou, David Danks, Ufuk Topcu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00015">Navigating the sociotechnical labyrinth: Dynamic certification for responsible embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sociotechnical requirements shape the governance of artificially intelligent (AI) systems. In an era where embodied AI technologies are rapidly reshaping various facets of contemporary society, their inherent dynamic adaptability presents a unique blend of opportunities and challenges. Traditional regulatory mechanisms, often designed for static -- or slower-paced -- technologies, find themselves at a crossroads when faced with the fluid and evolving nature of AI systems. Moreover, typical problems in AI, for example, the frequent opacity and unpredictability of the behaviour of the systems, add additional sociotechnical challenges.
  To address these interconnected issues, we introduce the concept of dynamic certification, an adaptive regulatory framework specifically crafted to keep pace with the continuous evolution of AI systems. The complexity of these challenges requires common progress in multiple domains: technical, socio-governmental, and regulatory. Our proposed transdisciplinary approach is designed to ensure the safe, ethical, and practical deployment of AI systems, aligning them bidirectionally with the real-world contexts in which they operate. By doing so, we aim to bridge the gap between rapid technological advancement and effective regulatory oversight, ensuring that AI systems not only achieve their intended goals but also adhere to ethical standards and societal values.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2408.10861.pdf' target='_blank'>https://arxiv.org/pdf/2408.10861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengming Zhu, Zhiwen Zeng, Weijia Yao, Wei Dai, Huimin Lu, Zongtan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10861">DVRP-MHSI: Dynamic Visualization Research Platform for Multimodal Human-Swarm Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a significant amount of research on algorithms and control methods for distributed collaborative robots. However, the emergence of collective behavior in a swarm is still difficult to predict and control. Nevertheless, human interaction with the swarm helps render the swarm more predictable and controllable, as human operators can utilize intuition or knowledge that is not always available to the swarm. Therefore, this paper designs the Dynamic Visualization Research Platform for Multimodal Human-Swarm Interaction (DVRP-MHSI), which is an innovative open system that can perform real-time dynamic visualization and is specifically designed to accommodate a multitude of interaction modalities (such as brain-computer, eye-tracking, electromyographic, and touch-based interfaces), thereby expediting progress in human-swarm interaction research. Specifically, the platform consists of custom-made low-cost omnidirectional wheeled mobile robots, multitouch screens and two workstations. In particular, the mutitouch screens can recognize human gestures and the shapes of objects placed on them, and they can also dynamically render diverse scenes. One of the workstations processes communication information within robots and the other one implements human-robot interaction methods. The development of DVRP-MHSI frees researchers from hardware or software details and allows them to focus on versatile swarm algorithms and human-swarm interaction methods without being limited to fixed scenarios, tasks, and interfaces. The effectiveness and potential of the platform for human-swarm interaction studies are validated by several demonstrative experiments.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2408.08158.pdf' target='_blank'>https://arxiv.org/pdf/2408.08158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Bovo, Steven Abreu, Karan Ahuja, Eric J Gonzalez, Li-Te Cheng, Mar Gonzalez-Franco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08158">EmBARDiment: an Embodied AI Agent for Productivity in XR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>XR devices running chat-bots powered by Large Language Models (LLMs) have the to become always-on agents that enable much better productivity scenarios. Current screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query. We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment. Our work minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2408.07671.pdf' target='_blank'>https://arxiv.org/pdf/2408.07671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Alcaraz-Herrera, Michail-Antisthenis Tsompanas, Andrew Adamatzky, Igor Balaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07671">NeuroEvolution algorithms applied in the designing process of biohybrid actuators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robots diverge from traditional rigid robotics, offering unique advantages in adaptability, safety, and human-robot interaction. In some cases, soft robots can be powered by biohybrid actuators and the design process of these systems is far from straightforward. We analyse here two algorithms that may assist the design of these systems, namely, NEAT (NeuroEvolution of Augmented Topologies) and HyperNEAT (Hypercube-based NeuroEvolution of Augmented Topologies). These algorithms exploit the evolution of the structure of actuators encoded through neural networks. To evaluate these algorithms, we compare them with a similar approach using the Age Fitness Pareto Optimization (AFPO) algorithm, with a focus on assessing the maximum displacement achieved by the discovered biohybrid morphologies. Additionally, we investigate the effects of optimization against both the volume of these morphologies and the distance they can cover. To further accelerate the computational process, the proposed methodology is implemented in a client-server setting; so, the most demanding calculations can be executed on specialized and efficient hardware. The results indicate that the HyperNEAT-based approach excels in identifying morphologies with minimal volumes that still achieve satisfactory displacement targets.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2407.17464.pdf' target='_blank'>https://arxiv.org/pdf/2407.17464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Abiri, Ali Rabiee, Sima Ghafoori, Anna Cetera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17464">Toward human-centered shared autonomy AI paradigms for human-robot teaming in healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With recent advancements in AI and computation tools, intelligent paradigms emerged to empower different fields such as healthcare robots with new capabilities. Advanced AI robotic algorithms (e.g., reinforcement learning) can be trained and developed to autonomously make individual decisions to achieve a desired and usually fixed goal. However, such independent decisions and goal achievements might not be ideal for a healthcare robot that usually interacts with a dynamic end-user or a patient. In such a complex human-robot interaction (teaming) framework, the dynamic user continuously wants to be involved in decision-making as well as introducing new goals while interacting with their present environment in real-time. To address this challenge, an adaptive shared autonomy AI paradigm is required to be developed for the two interactive agents (Human & AI agents) with a foundation based on human-centered factors to avoid any possible ethical issues and guarantee no harm to humanity.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2407.08898.pdf' target='_blank'>https://arxiv.org/pdf/2407.08898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrestha Mohanty, Negar Arabzadeh, Andrea Tupini, Yuxuan Sun, Alexey Skrynnik, Artem Zholus, Marc-Alexandre CÃ´tÃ©, Julia Kiseleva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08898">IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating Interactive Task-Solving Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seamless interaction between AI agents and humans using natural language remains a key goal in AI research. This paper addresses the challenges of developing interactive agents capable of understanding and executing grounded natural language instructions through the IGLU competition at NeurIPS. Despite advancements, challenges such as a scarcity of appropriate datasets and the need for effective evaluation platforms persist. We introduce a scalable data collection tool for gathering interactive grounded language instructions within a Minecraft-like environment, resulting in a Multi-Modal dataset with around 9,000 utterances and over 1,000 clarification questions. Additionally, we present a Human-in-the-Loop interactive evaluation platform for qualitative analysis and comparison of agent performance through multi-turn communication with human annotators. We offer to the community these assets referred to as IDAT (IGLU Dataset And Toolkit) which aim to advance the development of intelligent, interactive AI agents and provide essential resources for further research.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2407.03122.pdf' target='_blank'>https://arxiv.org/pdf/2407.03122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Gao, Bo Ai, Joel Loo, Vinay, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03122">IntentionNet: Map-Lite Visual Navigation at the Kilometre Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work explores the challenges of creating a scalable and robust robot navigation system that can traverse both indoor and outdoor environments to reach distant goals. We propose a navigation system architecture called IntentionNet that employs a monolithic neural network as the low-level planner/controller, and uses a general interface that we call intentions to steer the controller. The paper proposes two types of intentions, Local Path and Environment (LPE) and Discretised Local Move (DLM), and shows that DLM is robust to significant metric positioning and mapping errors. The paper also presents Kilo-IntentionNet, an instance of the IntentionNet system using the DLM intention that is deployed on a Boston Dynamics Spot robot, and which successfully navigates through complex indoor and outdoor environments over distances of up to a kilometre with only noisy odometry.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2407.00632.pdf' target='_blank'>https://arxiv.org/pdf/2407.00632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengying Wu, Yao Mu, Kangjie Zhou, Ji Ma, Junting Chen, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00632">CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents. However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2406.19967.pdf' target='_blank'>https://arxiv.org/pdf/2406.19967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19967">Into the Unknown: Generating Geospatial Descriptions for New Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2406.17531.pdf' target='_blank'>https://arxiv.org/pdf/2406.17531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucrezia Grassi, Carmine Tommaso Recchiuto, Antonio Sgorbissa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17531">Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system's pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system's performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2406.10586.pdf' target='_blank'>https://arxiv.org/pdf/2406.10586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedetta Matcovich, Cristina Gena, Fabiana Vernero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10586">How personality and memory of a robot can influence user modeling in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, robotics has evolved, placing robots in social contexts, and giving rise to Human-Robot Interaction (HRI). HRI aims to improve user satisfaction by designing autonomous social robots with user modeling functionalities and user-adapted interactions, storing data on people to achieve personalized interactions. Personality, a vital factor in human interactions, influences temperament, social preferences, and cognitive abilities. Despite much research on personality traits influencing human-robot interactions, little attention has been paid to the influence of the robot's personality on the user model. Personality can influence not only temperament and how people interact with each other but also what they remember about an interaction or the person they interact with. A robot's personality traits could therefore influence what it remembers about the user and thus modify the user model and the consequent interactions. However, no studies investigating such conditioning have been found. This paper addresses this gap by proposing distinct user models that reflect unique robotic personalities, exploring the interplay between individual traits, memory, and social interactions to replicate human-like processes, providing users with more engaging and natural experiences
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2406.05132.pdf' target='_blank'>https://arxiv.org/pdf/2406.05132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05132">3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of language and 3D perception is crucial for embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is a lack of large-scale datasets with dense grounding between language and 3D scenes. We introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons of models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the importance of large-scale 3D-text datasets for embodied AI research. Our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with resources and insights to lead to more reliable and better-grounded 3D-LLMs. Project website: https://3d-grand.github.io
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2405.02929.pdf' target='_blank'>https://arxiv.org/pdf/2405.02929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fares Abawi, Di Fu, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02929">Unified Dynamic Scanpath Predictors Outperform Individually Trained Neural Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous research on scanpath prediction has mainly focused on group models, disregarding the fact that the scanpaths and attentional behaviors of individuals are diverse. The disregard of these differences is especially detrimental to social human-robot interaction, whereby robots commonly emulate human gaze based on heuristics or predefined patterns. However, human gaze patterns are heterogeneous and varying behaviors can significantly affect the outcomes of such human-robot interactions. To fill this gap, we developed a deep learning-based social cue integration model for saliency prediction to instead predict scanpaths in videos. Our model learned scanpaths by recursively integrating fixation history and social cues through a gating mechanism and sequential attention. We evaluated our approach on gaze datasets of dynamic social scenes, observed under the free-viewing condition. The introduction of fixation history into our models makes it possible to train a single unified model rather than the resource-intensive approach of training individual models for each set of scanpaths. We observed that the late neural integration approach surpasses early fusion when training models on a large dataset, in comparison to a smaller dataset with a similar distribution. Results also indicate that a single unified model, trained on all the observers' scanpaths, performs on par or better than individually trained models. We hypothesize that this outcome is a result of the group saliency representations instilling universal attention in the model, while the supervisory signal and fixation history guide it to learn personalized attentional behaviors, providing the unified model a benefit over individual models due to its implicit representation of universal attention.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2404.10179.pdf' target='_blank'>https://arxiv.org/pdf/2404.10179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Rory Lawton, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York, Nick Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10179">Scaling Instructable Agents Across Many Simulated Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2404.02359.pdf' target='_blank'>https://arxiv.org/pdf/2404.02359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahiti Yerramilli, Jayant Sravan Tamarapalli, Jonathan Francis, Eric Nyberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02359">Attribution Regularization for Multimodal Paradigms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach aims to mitigate the issue of unimodal dominance and improve the performance of multimodal machine learning systems. Through extensive experimentation and evaluation, the effectiveness and generalizability of the proposed technique will be assessed. The findings of this research project have the potential to significantly contribute to the advancement of multimodal machine learning and facilitate its application in various domains, including multimedia analysis, human-computer interaction, and embodied AI research.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2403.12149.pdf' target='_blank'>https://arxiv.org/pdf/2403.12149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mani Amani, Reza Akhavian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12149">Ergonomic Optimization in Worker-Robot Bimanual Object Handover: Implementing REBA Using Reinforcement Learning in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots can serve as safety catalysts on construction job sites by taking over hazardous and repetitive tasks while alleviating the risks associated with existing manual workflows. Research on the safety of physical human-robot interaction (pHRI) is traditionally focused on addressing the risks associated with potential collisions. However, it is equally important to ensure that the workflows involving a collaborative robot are inherently safe, even though they may not result in an accident. For example, pHRI may require the human counterpart to use non-ergonomic body postures to conform to the robot hardware and physical configurations. Frequent and long-term exposure to such situations may result in chronic health issues. Safety and ergonomics assessment measures can be understood by robots if they are presented in algorithmic fashions so optimization for body postures is attainable. While frameworks such as Rapid Entire Body Assessment (REBA) have been an industry standard for many decades, they lack a rigorous mathematical structure which poses challenges in using them immediately for pHRI safety optimization purposes. Furthermore, learnable approaches have limited robustness outside of their training data, reducing generalizability. In this paper, we propose a novel framework that approaches optimization through Reinforcement Learning, ensuring precise, online ergonomic scores as compared to approximations, while being able to generalize and tune the regiment to any human and any task. To ensure practicality, the training is done in virtual reality utilizing Inverse Kinematics to simulate human movement mechanics. Experimental findings are compared to ergonomically naive object handover heuristics and indicate promising results where the developed framework can find the optimal object handover coordinates in pHRI contexts for manual material handling exemplary situations.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2403.04745.pdf' target='_blank'>https://arxiv.org/pdf/2403.04745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kensuke Nakamura, Ran Tian, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04745">Not All Errors Are Made Equal: A Regret Metric for Detecting System-level Trajectory Prediction Failures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot decision-making increasingly relies on data-driven human prediction models when operating around people. While these models are known to mispredict in out-of-distribution interactions, only a subset of prediction errors impact downstream robot performance. We propose characterizing such "system-level" prediction failures via the mathematical notion of regret: high-regret interactions are precisely those in which mispredictions degraded closed-loop robot performance. We further introduce a probabilistic generalization of regret that calibrates failure detection across disparate deployment contexts and renders regret compatible with reward-based and reward-free (e.g., generative) planners. In simulated autonomous driving interactions and social navigation interactions deployed on hardware, we showcase that our system-level failure metric can be used offline to automatically extract closed-loop human-robot interactions that state-of-the-art generative human predictors and robot planners previously struggled with. We further find that the very presence of high-regret data during human predictor fine-tuning is highly predictive of robot re-deployment performance improvements. Fine-tuning with the informative but significantly smaller high-regret data (23% of deployment data) is competitive with fine-tuning on the full deployment dataset, indicating a promising avenue for efficiently mitigating system-level human-robot interaction failures. Project website: https://cmu-intentlab.github.io/not-all-errors/
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2402.06665.pdf' target='_blank'>https://arxiv.org/pdf/2402.06665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Marc Rigter, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Danica Kragic, Bernhard SchÃ¶lkopf, Cheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06665">The Essential Role of Causality in Foundation World Models for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2402.03824.pdf' target='_blank'>https://arxiv.org/pdf/2402.03824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuseppe Paolo, Jonas Gonzalez-Billandon, BalÃ¡zs KÃ©gl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03824">A call for embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2401.16566.pdf' target='_blank'>https://arxiv.org/pdf/2401.16566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanyu Tian, Martin Huber, Christopher E. Mower, Zhe Han, Changsheng Li, Xingguang Duan, Christos Bergeles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16566">Excitation Trajectory Optimization for Dynamic Parameter Identification Using Virtual Constraints in Hands-on Robotic System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel, more computationally efficient method for optimizing robot excitation trajectories for dynamic parameter identification, emphasizing self-collision avoidance. This addresses the system identification challenges for getting high-quality training data associated with co-manipulated robotic arms that can be equipped with a variety of tools, a common scenario in industrial but also clinical and research contexts. Utilizing the Unified Robotics Description Format (URDF) to implement a symbolic Python implementation of the Recursive Newton-Euler Algorithm (RNEA), the approach aids in dynamically estimating parameters such as inertia using regression analyses on data from real robots. The excitation trajectory was evaluated and achieved on par criteria when compared to state-of-the-art reported results which didn't consider self-collision and tool calibrations. Furthermore, physical Human-Robot Interaction (pHRI) admittance control experiments were conducted in a surgical context to evaluate the derived inverse dynamics model showing a 30.1\% workload reduction by the NASA TLX questionnaire.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2401.12808.pdf' target='_blank'>https://arxiv.org/pdf/2401.12808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilaria Consoli, Claudio Mattutino, Cristina Gena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12808">A Robot Expressing Emotions Through Gestures: Everyone Outside of Italy Would Understand this?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of our research activities on affective computing and human-robot interaction we are working on both the recognition of human's emotions and the expression of emotions by robots. In our vision, robots will be increasingly present in schools, factories, and homes, and their empathetic behavior may foster their acceptance. In particular, in one of our research, we sought to replicate gestures associated with specific emotions on a social robot, NAO. Our focus was on Ekman's six primary emotions, along with five emotions selected from Plutchik's wheel of emotions. In our opinion the cultural component linked to the expression of emotions through gestures certainly influenced both us and the participants. Thus, we would like to investigate the influence of our culture in the gestural expression of emotion.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2312.13655.pdf' target='_blank'>https://arxiv.org/pdf/2312.13655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Ahmed Jaafar, Brian Reily, Christopher Reardon, Hao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13655">Compositional Zero-Shot Learning for Attribute-Based Object Reference in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-enabled robots have been widely studied over the past years to enable natural human-robot interaction and teaming in various real-world applications. Language-enabled robots must be able to comprehend referring expressions to identify a particular object from visual perception using a set of referring attributes extracted from natural language. However, visual observations of an object may not be available when it is referred to, and the number of objects and attributes may also be unbounded in open worlds. To address the challenges, we implement an attribute-based compositional zero-shot learning method that uses a list of attributes to perform referring expression comprehension in open worlds. We evaluate the approach on two datasets including the MIT-States and the Clothing 16K. The preliminary experimental results show that our implemented approach allows a robot to correctly identify the objects referred to by human commands.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2312.12339.pdf' target='_blank'>https://arxiv.org/pdf/2312.12339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Lekkala, Henghui Bao, Sumedh Sontakke, Laurent Itti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12339">Value Explicit Pretraining for Learning Transferable Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Value Explicit Pretraining (VEP), a method that learns generalizable representations for transfer reinforcement learning. VEP enables learning of new tasks that share similar objectives as previously learned tasks, by learning an encoder for objective-conditioned representations, irrespective of appearance changes and environment dynamics. To pre-train the encoder from a sequence of observations, we use a self-supervised contrastive loss that results in learning temporally smooth representations. VEP learns to relate states across different tasks based on the Bellman return estimate that is reflective of task progress. Experiments using a realistic navigation simulator and Atari benchmark show that the pretrained encoder produced by our method outperforms current SoTA pretraining methods on the ability to generalize to unseen tasks. VEP achieves up to a 2 times improvement in rewards on Atari and visual navigation, and up to a 3 times improvement in sample efficiency. For videos of policy performance visit our https://sites.google.com/view/value-explicit-pretraining/
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2311.12943.pdf' target='_blank'>https://arxiv.org/pdf/2311.12943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kushal Kedia, Atiksh Bhardwaj, Prithwish Dan, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12943">InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data, which we open-source.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2311.06038.pdf' target='_blank'>https://arxiv.org/pdf/2311.06038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JosÃ© Celestino, Manuel Marques, Jacinto C. Nascimento, JoÃ£o Paulo Costeira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06038">2D Image head pose estimation via latent space regression under occlusion settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head orientation is a challenging Computer Vision problem that has been extensively researched having a wide variety of applications. However, current state-of-the-art systems still underperform in the presence of occlusions and are unreliable for many task applications in such scenarios. This work proposes a novel deep learning approach for the problem of head pose estimation under occlusions. The strategy is based on latent space regression as a fundamental key to better structure the problem for occluded scenarios. Our model surpasses several state-of-the-art methodologies for occluded HPE, and achieves similar accuracy for non-occluded scenarios. We demonstrate the usefulness of the proposed approach with: (i) two synthetically occluded versions of the BIWI and AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii) a real-life application to human-robot interaction scenarios where face occlusions often occur. Specifically, the autonomous feeding from a robotic arm.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2310.18847.pdf' target='_blank'>https://arxiv.org/pdf/2310.18847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Lekkala, Chen Liu, Laurent Itti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18847">Bird's Eye View Based Pretrained World model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim2Real transfer has gained popularity because it helps transfer from inexpensive simulators to real world. This paper presents a novel system that fuses components in a traditional World Model into a robust system, trained entirely within a simulator, that Zero-Shot transfers to the real world. To facilitate transfer, we use an intermediary representation that is based on \textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a simulator by first learning to translate from complex \textit{First-Person View (FPV)} based RGB images to BEV representations, then learning to navigate using those representations. Later, when tested in the real world, the robot uses the perception model that translates FPV-based RGB images to embeddings that were learned by the FPV to BEV translator and that can be used by the downstream policy. The incorporation of state-checking modules using \textit{Anchor images} and Mixture Density LSTM not only interpolates uncertain and missing observations but also enhances the robustness of the model in the real-world. We trained the model using data from a Differential drive robot in the CARLA simulator. Our methodology's effectiveness is shown through the deployment of trained models onto a real-world Differential drive robot. Lastly we release a comprehensive codebase, dataset and models for training and deployment (\url{https://sites.google.com/view/value-explicit-pretraining}).
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2310.00906.pdf' target='_blank'>https://arxiv.org/pdf/2310.00906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Rahouti, Damian Lyons, Senthil Kumar Jagatheesaperumal, Kaiqi Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00906">A Decentralized Cooperative Navigation Approach for Visual Homing Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual homing is a lightweight approach to visual navigation. Given the stored information of an initial 'home' location, the navigation task back to this location is achieved from any other location by comparing the stored home information to the current image and extracting a motion vector. A challenge that constrains the applicability of visual homing is that the home location must be within the robot's field of view to initiate the homing process. Thus, we propose a blockchain approach to visual navigation for a heterogeneous robot team over a wide area of visual navigation. Because it does not require map data structures, the approach is useful for robot platforms with a small computational footprint, and because it leverages current visual information, it supports a resilient and adaptive path selection. Further, we present a lightweight Proof-of-Work (PoW) mechanism for reaching consensus in the untrustworthy visual homing network.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2309.12479.pdf' target='_blank'>https://arxiv.org/pdf/2309.12479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Srouji, Yao-Hung Hubert Tsai, Hugues Thomas, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12479">Human Following in Mobile Platforms with Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human following is a crucial feature of human-robot interaction, yet it poses numerous challenges to mobile agents in real-world scenarios. Some major hurdles are that the target person may be in a crowd, obstructed by others, or facing away from the agent. To tackle these challenges, we present a novel person re-identification module composed of three parts: a 360-degree visual registration, a neural-based person re-identification using human faces and torsos, and a motion tracker that records and predicts the target person's future position. Our human-following system also addresses other challenges, including identifying fast-moving targets with low latency, searching for targets that move out of the camera's sight, collision avoidance, and adaptively choosing different following mechanisms based on the distance between the target person and the mobile agent. Extensive experiments show that our proposed person re-identification module significantly enhances the human-following feature compared to other baseline variants.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2309.05251.pdf' target='_blank'>https://arxiv.org/pdf/2309.05251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhang, ZeMing Gong, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05251">Multi3DRefer: Grounding Text Description to Multiple 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2308.16493.pdf' target='_blank'>https://arxiv.org/pdf/2308.16493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riley Tavassoli, Mani Amani, Reza Akhavian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16493">Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU signal that would be lost by mapping the IMU data to a discrete activity label. Further, we demonstrate our methodology's efficacy through experiments involving human activity recognition using IMU data and visual inputs. Our results show that using multiple modalities as input improves the VLM's scene understanding and enhances its overall performance in various tasks, thus paving the way for more versatile and capable language models in multi-modal contexts.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2308.05221.pdf' target='_blank'>https://arxiv.org/pdf/2308.05221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangjie Shi, Leslie Ball, Govind Thattai, Desheng Zhang, Lucy Hu, Qiaozi Gao, Suhaila Shakiah, Xiaofeng Gao, Aishwarya Padmakumar, Bofei Yang, Cadence Chung, Dinakar Guthy, Gaurav Sukhatme, Karthika Arumugam, Matthew Wen, Osman Ipek, Patrick Lange, Rohan Khanna, Shreyas Pansare, Vasu Sharma, Chao Zhang, Cris Flagg, Daniel Pressel, Lavina Vaz, Luke Dai, Prasoon Goyal, Sattvik Sahai, Shaohua Liu, Yao Lu, Anna Gottardi, Shui Hu, Yang Liu, Dilek Hakkani-Tur, Kate Bland, Heather Rocker, James Jeun, Yadunandana Rao, Michael Johnston, Akshaya Iyengar, Arindam Mandal, Prem Natarajan, Reza Ghanadan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05221">Alexa, play with robot: Introducing the First Alexa Prize SimBot Challenge on Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Alexa Prize program has empowered numerous university students to explore, experiment, and showcase their talents in building conversational agents through challenges like the SocialBot Grand Challenge and the TaskBot Challenge. As conversational agents increasingly appear in multimodal and embodied contexts, it is important to explore the affordances of conversational interaction augmented with computer vision and physical embodiment. This paper describes the SimBot Challenge, a new challenge in which university teams compete to build robot assistants that complete tasks in a simulated physical environment. This paper provides an overview of the SimBot Challenge, which included both online and offline challenge phases. We describe the infrastructure and support provided to the teams including Alexa Arena, the simulated environment, and the ML toolkit provided to teams to accelerate their building of vision and language models. We summarize the approaches the participating teams took to overcome research challenges and extract key lessons learned. Finally, we provide analysis of the performance of the competing SimBots during the competition.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2306.09643.pdf' target='_blank'>https://arxiv.org/pdf/2306.09643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phillip Lippe, Sara Magliacane, Sindy LÃ¶we, Yuki M. Asano, Taco Cohen, Efstratios Gavves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09643">BISCUIT: Causal Representation Learning from Binary Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying the causal variables of an environment and how to intervene on them is of core value in applications such as robotics and embodied AI. While an agent can commonly interact with the environment and may implicitly perturb the behavior of some of these causal variables, often the targets it affects remain unknown. In this paper, we show that causal variables can still be identified for many common setups, e.g., additive Gaussian noise models, if the agent's interactions with a causal variable can be described by an unknown binary variable. This happens when each causal variable has two different mechanisms, e.g., an observational and an interventional one. Using this identifiability result, we propose BISCUIT, a method for simultaneously learning causal variables and their corresponding binary interaction variables. On three robotic-inspired datasets, BISCUIT accurately identifies causal variables and can even be scaled to complex, realistic environments for embodied AI.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2305.16986.pdf' target='_blank'>https://arxiv.org/pdf/2305.16986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengze Zhou, Yicong Hong, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16986">NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2305.10455.pdf' target='_blank'>https://arxiv.org/pdf/2305.10455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhou Xian, Theophile Gervet, Zhenjia Xu, Yi-Ling Qiao, Tsun-Hsuan Wang, Yian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10455">Towards Generalist Robots: A Promising Paradigm via Generative Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This document serves as a position paper that outlines the authors' vision for a potential pathway towards generalist robots. The purpose of this document is to share the excitement of the authors with the community and highlight a promising research direction in robotics and AI. The authors believe the proposed paradigm is a feasible path towards accomplishing the long-standing goal of robotics research: deploying robots, or embodied AI agents more broadly, in various non-factory real-world settings to perform diverse tasks. This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research. Instead of directly using or adapting these models to produce low-level policies and actions, it advocates for a fully automated generative pipeline (termed as generative simulation), which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots. The authors are actively pursuing this direction, but in the meantime, they recognize that the ambitious goal of building generalist robots with large-scale policy training demands significant resources such as computing power and hardware, and research groups in academia alone may face severe resource constraints in implementing the entire vision. Therefore, the authors believe sharing their thoughts at this early stage could foster discussions, attract interest towards the proposed pathway and related topics from industry groups, and potentially spur significant technical advancements in the field.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2304.12529.pdf' target='_blank'>https://arxiv.org/pdf/2304.12529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Ye, Hengxu You, Jing Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12529">Improved Trust in Human-Robot Collaboration with ChatGPT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human robot collaboration is becoming increasingly important as robots become more involved in various aspects of human life in the era of Artificial Intelligence. However, the issue of human operators trust in robots remains a significant concern, primarily due to the lack of adequate semantic understanding and communication between humans and robots. The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach. This paper explores the impact of ChatGPT on trust in a human-robot collaboration assembly task. This study designs a robot control system called RoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human operators fetch, and place tools, while human operators can communicate with and control the robot arm using natural language. A human-subject experiment showed that incorporating ChatGPT in robots significantly increased trust in human-robot collaboration, which can be attributed to the robot's ability to communicate more effectively with humans. Furthermore, ChatGPT ability to understand the nuances of human language and respond appropriately helps to build a more natural and intuitive human-robot interaction. The findings of this study have significant implications for the development of human-robot collaboration systems.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2304.12289.pdf' target='_blank'>https://arxiv.org/pdf/2304.12289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuo-Hao Zeng, Luca Weihs, Roozbeh Mottaghi, Ali Farhadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12289">Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A common assumption when training embodied agents is that the impact of taking an action is stable; for instance, executing the "move ahead" action will always move the agent forward by a fixed distance, perhaps with some small amount of actuator-induced noise. This assumption is limiting; an agent may encounter settings that dramatically alter the impact of actions: a move ahead action on a wet floor may send the agent twice as far as it expects and using the same action with a broken wheel might transform the expected translation into a rotation. Instead of relying that the impact of an action stably reflects its pre-defined semantic meaning, we propose to model the impact of actions on-the-fly using latent embeddings. By combining these latent action embeddings with a novel, transformer-based, policy head, we design an Action Adaptive Policy (AAP). We evaluate our AAP on two challenging visual navigation tasks in the AI2-THOR and Habitat environments and show that our AAP is highly performant even when faced, at inference-time with missing actions and, previously unseen, perturbed action space. Moreover, we observe significant improvement in robustness against these actions when evaluating in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2304.11753.pdf' target='_blank'>https://arxiv.org/pdf/2304.11753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahabedin Sagheb, Soham Gandhi, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11753">Should Collaborative Robots be Transparent?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We often assume that robots which collaborate with humans should behave in ways that are transparent (e.g., legible, explainable). These transparent robots intentionally choose actions that convey their internal state to nearby humans: for instance, a transparent robot might exaggerate its trajectory to indicate its goal. But while transparent behavior seems beneficial for human-robot interaction, is it actually optimal? In this paper we consider collaborative settings where the human and robot have the same objective, and the human is uncertain about the robot's type (i.e., the robot's internal state). We extend a recursive combination of Bayesian Nash equilibrium and the Bellman equation to solve for optimal robot policies. Interestingly, we discover that it is not always optimal for collaborative robots to be transparent; instead, human and robot teams can sometimes achieve higher rewards when the robot is opaque. In contrast to transparent robots, opaque robots select actions that withhold information from the human. Our analysis suggests that opaque behavior becomes optimal when either (a) human-robot interactions have a short time horizon or (b) users are slow to learn from the robot's actions. We extend this theoretical analysis to user studies across 43 total participants in both online and in-person settings. We find that -- during short interactions -- users reach higher rewards when working with opaque partners, and subjectively rate opaque robots as about equal to transparent robots. See videos of our experiments here: https://youtu.be/u8q1Z7WHUuI
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2303.14285.pdf' target='_blank'>https://arxiv.org/pdf/2303.14285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Fu, Fares Abawi, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14285">The Robot in the Room: Influence of Robot Facial Expressions and Gaze on Human-Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot facial expressions and gaze are important factors for enhancing human-robot interaction (HRI), but their effects on human collaboration and perception are not well understood, for instance, in collaborative game scenarios. In this study, we designed a collaborative triadic HRI game scenario, where two participants worked together to insert objects into a shape sorter. One participant assumed the role of a guide. The guide instructed the other participant, who played the role of an actor, by placing occluded objects into the sorter. A humanoid robot issued instructions, observed the interaction, and displayed social cues to elicit changes in the two participants' behavior. We measured human collaboration as a function of task completion time and the participants' perceptions of the robot by rating its behavior as intelligent or random. Participants also evaluated the robot by filling out the Godspeed questionnaire. We found that human collaboration was higher when the robot displayed a happy facial expression at the beginning of the game compared to a neutral facial expression. We also found that participants perceived the robot as more intelligent when it displayed a positive facial expression at the end of the game. The robot's behavior was also perceived as intelligent when directing its gaze toward the guide at the beginning of the interaction, not the actor. These findings provide insights into how robot facial expressions and gaze influence human behavior and perception in collaboration.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2210.08408.pdf' target='_blank'>https://arxiv.org/pdf/2210.08408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruipeng Zhang, Chenning Yu, Jingkai Chen, Chuchu Fan, Sicun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.08408">Learning-based Motion Planning in Dynamic Environments Using GNNs and Temporal Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based methods have shown promising performance for accelerating motion planning, but mostly in the setting of static environments. For the more challenging problem of planning in dynamic environments, such as multi-arm assembly tasks and human-robot interaction, motion planners need to consider the trajectories of the dynamic obstacles and reason about temporal-spatial interactions in very large state spaces. We propose a GNN-based approach that uses temporal encoding and imitation learning with data aggregation for learning both the embeddings and the edge prioritization policies. Experiments show that the proposed methods can significantly accelerate online planning over state-of-the-art complete dynamic planning algorithms. The learned models can often reduce costly collision checking operations by more than 1000x, and thus accelerating planning by up to 95%, while achieving high success rates on hard instances as well.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2111.12860.pdf' target='_blank'>https://arxiv.org/pdf/2111.12860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhad Nazari, Navid Mohajer, Darius Nahavandi, Abbas Khosravi, Saeid Nahavandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.12860">Applied Exoskeleton Technology: A Comprehensive Review of Physical and Cognitive Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exoskeletons and orthoses are wearable mobile systems providing mechanical benefits to the users. Despite significant improvements in the last decades, the technology is not fully mature to be adopted for strenuous and non-programmed tasks. To accommodate this insufficiency, different aspects of this technology need to be analysed and improved. Numerous studies have tried to address some aspects of exoskeletons, e.g. mechanism design, intent prediction, and control scheme. However, most works have focused on a specific element of design or application without providing a comprehensive review framework. This study aims to analyse and survey the contributing aspects to this technology's improvement and broad adoption. To address this, after introducing assistive devices and exoskeletons, the main design criteria will be investigated from both physical Human-Robot Interaction (HRI) perspectives. In order to establish an intelligent HRI strategy and enable intuitive control for users, cognitive HRI will be investigated after a brief introduction to various approaches to their control strategies. The study will be further developed by outlining several examples of known assistive devices in different categories. And some guidelines for exoskeleton selection and possible mitigation of current limitations will be discussed.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/1911.10322.pdf' target='_blank'>https://arxiv.org/pdf/1911.10322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Lekkala, Sami Abu-El-Haija, Laurent Itti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1911.10322">Meta Adaptation using Importance Weighted Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has gained immense popularity because of its high sample-efficiency. However, in real-world scenarios, where the trajectory distribution of most of the tasks dynamically shifts, model fitting on continuously aggregated data alone would be futile. In some cases, the distribution shifts, so much, that it is difficult for an agent to infer the new task. We propose a novel algorithm to generalize on any related task by leveraging prior knowledge on a set of specific tasks, which involves assigning importance weights to each past demonstration. We show experiments where the robot is trained from a diversity of environmental tasks and is also able to adapt to an unseen environment, using few-shot learning. We also developed a prototype robot system to test our approach on the task of visual navigation, and experimental results obtained were able to confirm these suppositions.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/1809.08835.pdf' target='_blank'>https://arxiv.org/pdf/1809.08835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changan Chen, Yuejiang Liu, Sven Kreiss, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1809.08835">Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2509.19002.pdf' target='_blank'>https://arxiv.org/pdf/2509.19002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19002">VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2508.19788.pdf' target='_blank'>https://arxiv.org/pdf/2508.19788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19788">Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for estimating accident-prone regions in everyday indoor scenes, aimed at improving real-time risk awareness in service robots operating in human-centric environments. As robots become integrated into daily life, particularly in homes, the ability to anticipate and respond to environmental hazards is crucial for ensuring user safety, trust, and effective human-robot interaction. Our approach models object-level risk and context through a semantic graph-based propagation algorithm. Each object is represented as a node with an associated risk score, and risk propagates asymmetrically from high-risk to low-risk objects based on spatial proximity and accident relationship. This enables the robot to infer potential hazards even when they are not explicitly visible or labeled. Designed for interpretability and lightweight onboard deployment, our method is validated on a dataset with human-annotated risk regions, achieving a binary risk detection accuracy of 75%. The system demonstrates strong alignment with human perception, particularly in scenes involving sharp or unstable objects. These results underline the potential of context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces. This framework could serve as a foundation for future systems that make context-driven safety decisions, provide real-time alerts, or autonomously assist users in avoiding or mitigating hazards within home environments.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2508.07650.pdf' target='_blank'>https://arxiv.org/pdf/2508.07650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Helong Huang, Min Cen, Kai Tan, Xingyue Quan, Guowei Huang, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07650">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2508.07501.pdf' target='_blank'>https://arxiv.org/pdf/2508.07501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoye Zuo, Nikos Athanasiou, Ginger Delmas, Yiming Huang, Xingyu Fu, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07501">FormCoach: Lift Smarter, Not Harder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Good form is the difference between strength and strain, yet for the fast-growing community of at-home fitness enthusiasts, expert feedback is often out of reach. FormCoach transforms a simple camera into an always-on, interactive AI training partner, capable of spotting subtle form errors and delivering tailored corrections in real time, leveraging vision-language models (VLMs). We showcase this capability through a web interface and benchmark state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference video pairs spanning 22 strength and mobility exercises. To accelerate research in AI-driven coaching, we release both the dataset and an automated, rubric-based evaluation pipeline, enabling standardized comparison across models. Our benchmarks reveal substantial gaps compared to human-level coaching, underscoring both the challenges and opportunities in integrating nuanced, context-aware movement analysis into interactive AI systems. By framing form correction as a collaborative and creative process between humans and machines, FormCoach opens a new frontier in embodied AI.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2508.05294.pdf' target='_blank'>https://arxiv.org/pdf/2508.05294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge PeÃ±a Queralta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05294">Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those works advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2508.01126.pdf' target='_blank'>https://arxiv.org/pdf/2508.01126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01126">UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2507.19684.pdf' target='_blank'>https://arxiv.org/pdf/2507.19684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bermet Burkanova, Payam Jome Yazdian, Chuxuan Zhang, Trinity Evans, Paige TuttÃ¶sÃ­, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19684">Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2507.10812.pdf' target='_blank'>https://arxiv.org/pdf/2507.10812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuxuan Zhang, Yasaman Etesam, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10812">React to This (RTT): A Nonverbal Turing Test for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an approach to test embodied AI agents for interaction awareness and believability, particularly in scenarios where humans push them to their limits. Turing introduced the Imitation Game as a way to explore the question: "Can machines think?" The Total Turing Test later expanded this concept beyond purely verbal communication, incorporating perceptual and physical interaction. Building on this, we propose a new guiding question: "Can machines react?" and introduce the React to This (RTT) test for nonverbal behaviors, presenting results from an initial experiment.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2507.03049.pdf' target='_blank'>https://arxiv.org/pdf/2507.03049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ferran GebellÃ­, AnaÃ­s Garrell, Jan-Gerrit Habekost, SÃ©verin Lemaignan, Stefan Wermter, Raquel Ros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03049">Personalised Explanations in Long-term Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of Human-Robot Interaction (HRI), a fundamental challenge is to facilitate human understanding of robots. The emerging domain of eXplainable HRI (XHRI) investigates methods to generate explanations and evaluate their impact on human-robot interactions. Previous works have highlighted the need to personalise the level of detail of these explanations to enhance usability and comprehension. Our paper presents a framework designed to update and retrieve user knowledge-memory models, allowing for adapting the explanations' level of detail while referencing previously acquired concepts. Three architectures based on our proposed framework that use Large Language Models (LLMs) are evaluated in two distinct scenarios: a hospital patrolling robot and a kitchen assistant robot. Experimental results demonstrate that a two-stage architecture, which first generates an explanation and then personalises it, is the framework architecture that effectively reduces the level of detail only when there is related user knowledge.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2506.20795.pdf' target='_blank'>https://arxiv.org/pdf/2506.20795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephanie KÃ¤s, Anton Burenko, Louis Markert, Onur Alp Culha, Dennis Mack, Timm Linder, Bastian Leibe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20795">How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures enable non-verbal human-robot communication, especially in noisy environments like agile production. Traditional deep learning-based gesture recognition relies on task-specific architectures using images, videos, or skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs) and Vision Language Models (VLMs) with their strong generalization abilities offer potential to reduce system complexity by replacing dedicated task-specific modules. This study investigates adapting such models for dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing skeleton-based approach). We introduce NUGGET, a dataset tailored for human-robot communication in intralogistics environments, to evaluate the different gesture recognition approaches. In our experiments, HD-GCN achieves best performance, but V-JEPA comes close with a simple, task-specific classification head - thus paving a possible way towards reducing system complexity, by using it as a shared multi-task model. In contrast, Gemini struggles to differentiate gestures based solely on textual descriptions in the zero-shot setting, highlighting the need of further research on suitable input representations for gestures.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2506.19747.pdf' target='_blank'>https://arxiv.org/pdf/2506.19747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephanie KÃ¤s, Sven Peter, Henrik Thillmann, Anton Burenko, David Benjamin Adrian, Dennis Mack, Timm Linder, Bastian Leibe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19747">Systematic Comparison of Projection Methods for Monocular 3D Human Pose Estimation on Fisheye Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fisheye cameras offer robots the ability to capture human movements across a wider field of view (FOV) than standard pinhole cameras, making them particularly useful for applications in human-robot interaction and automotive contexts. However, accurately detecting human poses in fisheye images is challenging due to the curved distortions inherent to fisheye optics. While various methods for undistorting fisheye images have been proposed, their effectiveness and limitations for poses that cover a wide FOV has not been systematically evaluated in the context of absolute human pose estimation from monocular fisheye images. To address this gap, we evaluate the impact of pinhole, equidistant and double sphere camera models, as well as cylindrical projection methods, on 3D human pose estimation accuracy. We find that in close-up scenarios, pinhole projection is inadequate, and the optimal projection method varies with the FOV covered by the human pose. The usage of advanced fisheye models like the double sphere model significantly enhances 3D human pose estimation accuracy. We propose a heuristic for selecting the appropriate projection model based on the detection bounding box to enhance prediction quality. Additionally, we introduce and evaluate on our novel dataset FISHnCHIPS, which features 3D human skeleton annotations in fisheye images, including images from unconventional angles, such as extreme close-ups, ground-mounted cameras, and wide-FOV poses, available at: https://www.vision.rwth-aachen.de/fishnchips
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2506.17516.pdf' target='_blank'>https://arxiv.org/pdf/2506.17516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhou Chen, Sanjoy Kundu, Harsimran S. Baweja, Sathyanarayanan N. Aakur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17516">EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active event perception, the ability to dynamically detect, track, and summarize events in real time, is essential for embodied intelligence in tasks such as human-AI collaboration, assistive robotics, and autonomous navigation. However, existing approaches often depend on predefined action spaces, annotated datasets, and extrinsic rewards, limiting their adaptability and scalability in dynamic, real-world scenarios. Inspired by cognitive theories of event perception and predictive coding, we propose EASE, a self-supervised framework that unifies spatiotemporal representation learning and embodied control through free energy minimization. EASE leverages prediction errors and entropy as intrinsic signals to segment events, summarize observations, and actively track salient actors, operating without explicit annotations or external rewards. By coupling a generative perception model with an action-driven control policy, EASE dynamically aligns predictions with observations, enabling emergent behaviors such as implicit memory, target continuity, and adaptability to novel environments. Extensive evaluations in simulation and real-world settings demonstrate EASE's ability to achieve privacy-preserving and scalable event perception, providing a robust foundation for embodied systems in unscripted, dynamic tasks.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2506.15607.pdf' target='_blank'>https://arxiv.org/pdf/2506.15607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shailesh, Alok Raj, Nayan Kumar, Priya Shukla, Andrew Melnik, Micheal Beetz, Gora Chand Nandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15607">GRIM: Task-Oriented Grasping with Conditioning on Generative Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-Oriented Grasping (TOG) presents a significant challenge, requiring a nuanced understanding of task semantics, object affordances, and the functional constraints dictating how an object should be grasped for a specific task. To address these challenges, we introduce GRIM (Grasp Re-alignment via Iterative Matching), a novel training-free framework for task-oriented grasping. Initially, a coarse alignment strategy is developed using a combination of geometric cues and principal component analysis (PCA)-reduced DINO features for similarity scoring. Subsequently, the full grasp pose associated with the retrieved memory instance is transferred to the aligned scene object and further refined against a set of task-agnostic, geometrically stable grasps generated for the scene object, prioritizing task compatibility. In contrast to existing learning-based methods, GRIM demonstrates strong generalization capabilities, achieving robust performance with only a small number of conditioning examples.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2506.15293.pdf' target='_blank'>https://arxiv.org/pdf/2506.15293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Chiossi, Julian Rasch, Robin Welsch, Albrecht Schmidt, Florian Michahelles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15293">Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2506.05651.pdf' target='_blank'>https://arxiv.org/pdf/2506.05651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05651">Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding relationships between objects is central to visual intelligence, with applications in embodied AI, assistive systems, and scene understanding. Yet, most visual relationship detection (VRD) models rely on a fixed predicate set, limiting their generalization to novel interactions. A key challenge is the inability to visually ground semantically plausible, but unannotated, relationships hypothesized from external knowledge. This work introduces an iterative visual grounding framework that leverages large language models (LLMs) as structured relational priors. Inspired by expectation-maximization (EM), our method alternates between generating candidate scene graphs from detected objects using an LLM (expectation) and training a visual model to align these hypotheses with perceptual evidence (maximization). This process bootstraps relational understanding beyond annotated data and enables generalization to unseen predicates. Additionally, we introduce a new benchmark for open-world VRD on Visual Genome with 21 held-out predicates and evaluate under three settings: seen, unseen, and mixed. Our model outperforms LLM-only, few-shot, and debiased baselines, achieving mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on these three sets. These results highlight the promise of grounded LLM priors for scalable open-world visual understanding.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2505.24266.pdf' target='_blank'>https://arxiv.org/pdf/2505.24266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanren Qiao, Sixu Lin, Ronglai Zuo, Zhizheng Wu, Kui Jia, Guiliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24266">SignBot: Learning Human-to-Humanoid Sign Language Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language is a natural and visual form of language that uses movements and expressions to convey meaning, serving as a crucial means of communication for individuals who are deaf or hard-of-hearing (DHH). However, the number of people proficient in sign language remains limited, highlighting the need for technological advancements to bridge communication gaps and foster interactions with minorities. Based on recent advancements in embodied humanoid robots, we propose SignBot, a novel framework for human-robot sign language interaction. SignBot integrates a cerebellum-inspired motion control component and a cerebral-oriented module for comprehension and interaction. Specifically, SignBot consists of: 1) Motion Retargeting, which converts human sign language datasets into robot-compatible kinematics; 2) Motion Control, which leverages a learning-based paradigm to develop a robust humanoid control policy for tracking sign language gestures; and 3) Generative Interaction, which incorporates translator, responser, and generator of sign language, thereby enabling natural and effective communication between robots and humans. Simulation and real-world experimental results demonstrate that SignBot can effectively facilitate human-robot interaction and perform sign language motions with diverse robots and datasets. SignBot represents a significant advancement in automatic sign language interaction on embodied humanoid robot platforms, providing a promising solution to improve communication accessibility for the DHH community.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2505.21567.pdf' target='_blank'>https://arxiv.org/pdf/2505.21567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Jiang, Zihao Zheng, Xiuping Cui, Maoliang Li, JIayu Chen, Xiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21567">EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2505.06378.pdf' target='_blank'>https://arxiv.org/pdf/2505.06378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Wei, Zhuoqi Zeng, Yue Zhong, Jiawen Kang, Ryan Wen Liu, M. Shamim Hossain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06378">Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of large language models and embodied Artificial Intelligence (AI) in the intelligent transportation scenarios, the combination of them in intelligent transportation spawns the Vehicular Embodied AI Network (VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local advanced AI applications are defined as vehicular embodied AI agents, enabling capabilities such as environment perception and multi-agent collaboration. Due to computation latency and resource constraints, the local AI applications and services running on vehicular embodied AI agents need to be migrated, and subsequently referred to as vehicular embodied AI agent twins, which drive the advancement of vehicular embodied AI networks to offload intensive tasks to Roadside Units (RSUs), mitigating latency problems while maintaining service quality. Recognizing workload imbalance among RSUs in traditional approaches, we model AV-RSU interactions as a Stackelberg game to optimize bandwidth resource allocation for efficient migration. A Tiny Multi-Agent Bidirectional LSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to approximate the Stackelberg equilibrium through decentralized coordination. Furthermore, a personalized neural network pruning algorithm based on Path eXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities by identifying task-critical parameters in trained models, reducing model complexity with less performance degradation. Experimental validation confirms the algorithm's effectiveness in balancing system load and minimizing delays, demonstrating significant improvements in vehicular embodied AI agent deployment.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2505.05074.pdf' target='_blank'>https://arxiv.org/pdf/2505.05074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Apicella, Alessio Xompero, Andrea Cavallaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05074">Visual Affordances: Enabling Robots to Understand Object Functionality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. Predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. In this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. To address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. To favour transparency, we introduce the Affordance Sheet, a document to detail the proposed solution, the datasets, and the validation. As the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. Using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. Our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2504.01301.pdf' target='_blank'>https://arxiv.org/pdf/2504.01301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Kobayashi, Masato Kobayashi, Thanpimon Buamanee, Yuki Uranishi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01301">Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Bi-LAT, a novel imitation learning framework that unifies bilateral control with natural language processing to achieve precise force modulation in robotic manipulation. Bi-LAT leverages joint position, velocity, and torque data from leader-follower teleoperation while also integrating visual and linguistic cues to dynamically adjust applied force. By encoding human instructions such as "softly grasp the cup" or "strongly twist the sponge" through a multimodal Transformer-based model, Bi-LAT learns to distinguish nuanced force requirements in real-world tasks. We demonstrate Bi-LAT's performance in (1) unimanual cup-stacking scenario where the robot accurately modulates grasp force based on language commands, and (2) bimanual sponge-twisting task that requires coordinated force control. Experimental results show that Bi-LAT effectively reproduces the instructed force levels, particularly when incorporating SigLIP among tested language encoders. Our findings demonstrate the potential of integrating natural language cues into imitation learning, paving the way for more intuitive and adaptive human-robot interaction. For additional material, please visit: https://mertcookimg.github.io/bi-lat/
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2503.16965.pdf' target='_blank'>https://arxiv.org/pdf/2503.16965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Hu, Jing Li, Zhongzhu Pu, Hou Pong Chan, Yu Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16965">Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Models exhibited immense potential for embodied AI, yet they often lack the sophisticated situational reasoning required for complex decision-making. This paper shows that VLMs can achieve surprisingly strong decision-making performance when visual scenes are represented merely as text-only descriptions, suggesting foundational reasoning can be effectively learned from language. Motivated by this insight, we propose Praxis-VLM, a reasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO algorithm on textual scenarios to instill robust reasoning capabilities, where models learn to evaluate actions and their consequences. These reasoning skills, acquired purely from text, successfully transfer to multimodal inference with visual inputs, significantly reducing reliance on scarce paired image-text training data. Experiments across diverse decision-making benchmarks demonstrate that Praxis-VLM substantially outperforms standard supervised fine-tuning, exhibiting superior performance and generalizability. Further analysis confirms that our models engage in explicit and effective reasoning, underpinning their enhanced performance and adaptability.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2503.16960.pdf' target='_blank'>https://arxiv.org/pdf/2503.16960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steve Benford, Eike Schneiders, Juan Pablo Martinez Avila, Praminda Caleb-Solly, Patrick Robert Brundell, Simon Castle-Green, Feng Zhou, Rachael Garrett, Kristina HÃ¶Ã¶k, Sarah Whatley, Kate Marsh, Paul Tennent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16960">Somatic Safety: An Embodied Approach Towards Safe Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots enter the messy human world so the vital matter of safety takes on a fresh complexion with physical contact becoming inevitable and even desirable. We report on an artistic-exploration of how dancers, working as part of a multidisciplinary team, engaged in contact improvisation exercises to explore the opportunities and challenges of dancing with cobots. We reveal how they employed their honed bodily senses and physical skills to engage with the robots aesthetically and yet safely, interleaving improvised physical manipulations with reflections to grow their knowledge of how the robots behaved and felt. We introduce somatic safety, a holistic mind-body approach in which safety is learned, felt and enacted through bodily contact with robots in addition to being reasoned about. We conclude that robots need to be better designed for people to hold them and might recognise tacit safety cues among people.We propose that safety should be learned through iterative bodily experience interleaved with reflection.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2503.16548.pdf' target='_blank'>https://arxiv.org/pdf/2503.16548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elisabeth Menendez, Michael Gienger, Santiago MartÃ­nez, Carlos Balaguer, Anna Belardinelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16548">SemanticScanpath: Combining Gaze and Speech for Situated Human-Robot Interaction Using LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have substantially improved the conversational capabilities of social robots. Nevertheless, for an intuitive and fluent human-robot interaction, robots should be able to ground the conversation by relating ambiguous or underspecified spoken utterances to the current physical situation and to the intents expressed non verbally by the user, for example by using referential gaze. Here we propose a representation integrating speech and gaze to enable LLMs to obtain higher situated awareness and correctly resolve ambiguous requests. Our approach relies on a text-based semantic translation of the scanpath produced by the user along with the verbal requests and demonstrates LLM's capabilities to reason about gaze behavior, robustly ignoring spurious glances or irrelevant objects. We validate the system across multiple tasks and two scenarios, showing its generality and accuracy, and demonstrate its implementation on a robotic platform, closing the loop from request interpretation to execution.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2503.16451.pdf' target='_blank'>https://arxiv.org/pdf/2503.16451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Tan, Boyuan Li, Chuhao Jin, Wenbing Huang, Xiting Wang, Ruihua Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16451">Think-Then-React: Towards Unconstrained Human Action-to-Reaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games. Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion. To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions. First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts. Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding. Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2503.15496.pdf' target='_blank'>https://arxiv.org/pdf/2503.15496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulio Antonio Abbo, Maria Jose Pinto-Bernal, Martijn Catrycke, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15496">Fast Multi-Party Open-Ended Conversation with a Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the implementation and evaluation of a conversational agent designed for multi-party open-ended interactions. Leveraging state-of-the-art technologies such as voice direction of arrival, voice recognition, face tracking, and large language models, the system aims to facilitate natural and intuitive human-robot conversations. Deployed on the Furhat robot, the system was tested with 30 participants engaging in open-ended group conversations and then in two overlapping discussions. Quantitative metrics, such as latencies and recognition accuracy, along with qualitative measures from user questionnaires, were collected to assess performance. The results highlight the system's effectiveness in managing multi-party interactions, though improvements are needed in response relevance and latency. This study contributes valuable insights for advancing human-robot interaction, particularly in enhancing the naturalness and engagement in group conversations.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2502.14917.pdf' target='_blank'>https://arxiv.org/pdf/2502.14917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14917">Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2502.01536.pdf' target='_blank'>https://arxiv.org/pdf/2502.01536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01536">VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators. However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a Real-to-Sim-to-Real framework that generates photorealistic and physically interactive "digital twin" simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2501.07468.pdf' target='_blank'>https://arxiv.org/pdf/2501.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Liu, Xu Cao, Tingting Chen, Yankai Jiang, Junjie You, Minghua Wu, Xiaosong Wang, Mengling Feng, Yaochu Jin, Jintai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07468">From Screens to Scenes: A Survey of Embodied AI in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Healthcare systems worldwide face persistent challenges in efficiency, accessibility, and personalization. Powered by modern AI technologies such as multimodal large language models and world models, Embodied AI (EmAI) represents a transformative frontier, offering enhanced autonomy and the ability to interact with the physical world to address these challenges. As an interdisciplinary and rapidly evolving research domain, "EmAI in healthcare" spans diverse fields such as algorithms, robotics, and biomedicine. This complexity underscores the importance of timely reviews and analyses to track advancements, address challenges, and foster cross-disciplinary collaboration. In this paper, we provide a comprehensive overview of the "brain" of EmAI for healthcare, wherein we introduce foundational AI algorithms for perception, actuation, planning, and memory, and focus on presenting the healthcare applications spanning clinical interventions, daily care & companionship, infrastructure support, and biomedical research. Despite its promise, the development of EmAI for healthcare is hindered by critical challenges such as safety concerns, gaps between simulation platforms and real-world applications, the absence of standardized benchmarks, and uneven progress across interdisciplinary domains. We discuss the technical barriers and explore ethical considerations, offering a forward-looking perspective on the future of EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI systems is also introduced to guide further development. By providing systematic insights, this work aims to inspire innovation and practical applications, paving the way for a new era of intelligent, patient-centered healthcare.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2501.05628.pdf' target='_blank'>https://arxiv.org/pdf/2501.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulio Antonio Abbo, Tony Belpaeme, Micol Spitale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05628">Concerns and Values in Human-Robot Interactions: A Focus on Social Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots, as AI with physical instantiation, inhabit our social and physical world, where their actions have both social and physical consequences, posing challenges for researchers when designing social robots. This study starts with a scoping review to identify discussions and potential concerns arising from interactions with robotic systems. Two focus groups of technology ethics experts then validated a comprehensive list of key topics and values in human-robot interaction (HRI) literature. These insights were integrated into the HRI Value Compass web tool, to help HRI researchers identify ethical values in robot design. The tool was evaluated in a pilot study. This work benefits the HRI community by highlighting key concerns in human-robot interactions and providing an instrument to help researchers design robots that align with human values, ensuring future robotic systems adhere to these values in social applications.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2412.20826.pdf' target='_blank'>https://arxiv.org/pdf/2412.20826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanjun Bu, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20826">ReStory: VLM-augmentation of Social Human-Robot Interaction Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internet-scaled datasets are a luxury for human-robot interaction (HRI) researchers, as collecting natural interaction data in the wild is time-consuming and logistically challenging. The problem is exacerbated by robots' different form factors and interaction modalities. Inspired by recent work on ethnomethodological and conversation analysis (EMCA) in the domain of HRI, we propose ReStory, a method that has the potential to augment existing in-the-wild human-robot interaction datasets leveraging Vision Language Models. While still requiring human supervision, ReStory is capable of synthesizing human-interpretable interaction scenarios in the form of storyboards. We hope our proposed approach provides HRI researchers and interaction designers with a new angle to utilizing their valuable and scarce data.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2411.14322.pdf' target='_blank'>https://arxiv.org/pdf/2411.14322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun P S, Andrew Melnik, Gora Chand Nandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14322">SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting and Dense Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Experience Goal Visual Rearrangement task stands as a foundational challenge within Embodied AI, requiring an agent to construct a robust world model that accurately captures the goal state. The agent uses this world model to restore a shuffled scene to its original configuration, making an accurate representation of the world essential for successfully completing the task. In this work, we present a novel framework that leverages on 3D Gaussian Splatting as a 3D scene representation for experience goal visual rearrangement task. Recent advances in volumetric scene representation like 3D Gaussian Splatting, offer fast rendering of high quality and photo-realistic novel views. Our approach enables the agent to have consistent views of the current and the goal setting of the rearrangement task, which enables the agent to directly compare the goal state and the shuffled state of the world in image space. To compare these views, we propose to use a dense feature matching method with visual features extracted from a foundation model, leveraging its advantages of a more universal feature representation, which facilitates robustness, and generalization. We validate our approach on the AI2-THOR rearrangement challenge benchmark and demonstrate improvements over the current state of the art methods
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2411.10016.pdf' target='_blank'>https://arxiv.org/pdf/2411.10016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kavindie Katuwandeniya, Leimin Tian, Dana KuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10016">'What did the Robot do in my Absence?' Video Foundation Models to Enhance Intermittent Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of Video Foundation Models (ViFMs) for generating robot data summaries to enhance intermittent human supervision of robot teams. We propose a novel framework that produces both generic and query-driven summaries of long-duration robot vision data in three modalities: storyboards, short videos, and text. Through a user study involving 30 participants, we evaluate the efficacy of these summary methods in allowing operators to accurately retrieve the observations and actions that occurred while the robot was operating without supervision over an extended duration (40 min). Our findings reveal that query-driven summaries significantly improve retrieval accuracy compared to generic summaries or raw data, albeit with increased task duration. Storyboards are found to be the most effective presentation modality, especially for object-related queries. This work represents, to our knowledge, the first zero-shot application of ViFMs for generating multi-modal robot-to-human communication in intermittent supervision contexts, demonstrating both the promise and limitations of these models in human-robot interaction (HRI) scenarios.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2411.02236.pdf' target='_blank'>https://arxiv.org/pdf/2411.02236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Sokolov, Swapnil Bhosale, Xiatian Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02236">3D Audio-Visual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing the sounding objects in scenes is a longstanding objective in embodied AI, with diverse applications in robotics and AR/VR/MR. To that end, Audio-Visual Segmentation (AVS), taking as condition an audio signal to identify the masks of the target sounding objects in an input image with synchronous camera and microphone sensors, has been recently advanced. However, this paradigm is still insufficient for real-world operation, as the mapping from 2D images to 3D scenes is missing. To address this fundamental limitation, we introduce a novel research problem, 3D Audio-Visual Segmentation, extending the existing AVS to the 3D output space. This problem poses more challenges due to variations in camera extrinsics, audio scattering, occlusions, and diverse acoustics across sounding object categories. To facilitate this research, we create the very first simulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D scene environments with grounded spatial audio under single-instance and multi-instance settings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator to generate comprehensive annotations of sounding object locations and corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet, characterized by integrating the ready-to-use knowledge from pretrained 2D audio-visual foundation models synergistically with 3D visual scene representation through spatial audio-aware mask alignment and refinement. Extensive experiments demonstrate that EchoSegnet can effectively segment sounding objects in 3D space on our new benchmark, representing a significant advancement in the field of embodied AI. Project page: https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2411.00007.pdf' target='_blank'>https://arxiv.org/pdf/2411.00007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Raoufi, Pawel Romanczuk, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00007">LARS: Light Augmented Reality System for Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Light Augmented Reality System LARS as an open-source and cost-effective tool. LARS leverages light-projected visual scenes for indirect robot-robot and human-robot interaction through the real environment. It operates in real-time and is compatible with a range of robotic platforms, from miniature to middle-sized robots. LARS can support researchers in conducting experiments with increased freedom, reliability, and reproducibility. This XR tool makes it possible to enrich the environment with full control by adding complex and dynamic objects while keeping the properties of robots as realistic as they are.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2410.13407.pdf' target='_blank'>https://arxiv.org/pdf/2410.13407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Yang, Nieqing Cao, Yan Ding, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13407">BestMan: A Modular Mobile Manipulator Platform for Embodied AI with Unified Simulation-Hardware APIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) emphasizes agents' ability to perceive, understand, and act in physical environments. Simulation platforms play a crucial role in advancing this field by enabling the validation and optimization of algorithms. However, existing platforms face challenges such as multilevel technical integration complexity, insufficient modularity, interface heterogeneity, and adaptation to diverse hardware. We present BestMan, a simulation platform based on PyBullet, designed to address these issues. BestMan introduces an integrated multilevel skill chain for seamless coordination across perception, planning, and control; a highly modular architecture for flexible algorithm integration; unified interfaces for smooth simulation-to-reality transfer; and a hardware-agnostic approach for adapting to various mobile manipulator configurations. These features collectively simplify development and enhance platform expandability, making BestMan a valuable tool for Embodied AI research.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2410.12822.pdf' target='_blank'>https://arxiv.org/pdf/2410.12822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Rigter, Tarun Gupta, Agrin Hilmkil, Chao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12822">AVID: Adapting Video Diffusion Models to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale generative models have achieved remarkable success in a number of domains. However, for sequential decision-making problems, such as robotics, action-labelled data is often scarce and therefore scaling-up foundation models for decision-making remains a challenge. A potential solution lies in leveraging widely-available unlabelled videos to train world models that simulate the consequences of actions. If the world model is accurate, it can be used to optimize decision-making in downstream tasks. Image-to-video diffusion models are already capable of generating highly realistic synthetic videos. However, these models are not action-conditioned, and the most powerful models are closed-source which means they cannot be finetuned. In this work, we propose to adapt pretrained video diffusion models to action-conditioned world models, without access to the parameters of the pretrained model. Our approach, AVID, trains an adapter on a small domain-specific dataset of action-labelled videos. AVID uses a learned mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos. We evaluate AVID on video game and real-world robotics data, and show that it outperforms existing baselines for diffusion model adaptation.1 Our results demonstrate that if utilized correctly, pretrained video models have the potential to be powerful tools for embodied AI.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2408.16776.pdf' target='_blank'>https://arxiv.org/pdf/2408.16776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Sheidlower, Mavis Murdock, Emma Bethel, Reuben M. Aronson, Elaine Schaertl Short
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16776">Online Behavior Modification for Expressive User Control of RL-Trained Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) is an effective method for robots to learn tasks. However, in typical RL, end-users have little to no control over how the robot does the task after the robot has been deployed. To address this, we introduce the idea of online behavior modification, a paradigm in which users have control over behavior features of a robot in real time as it autonomously completes a task using an RL-trained policy. To show the value of this user-centered formulation for human-robot interaction, we present a behavior diversity based algorithm, Adjustable Control Of RL Dynamics (ACORD), and demonstrate its applicability to online behavior modification in simulation and a user study. In the study (n=23) users adjust the style of paintings as a robot traces a shape autonomously. We compare ACORD to RL and Shared Autonomy (SA), and show ACORD affords user-preferred levels of control and expression, comparable to SA, but with the potential for autonomous execution and robustness of RL.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2408.11347.pdf' target='_blank'>https://arxiv.org/pdf/2408.11347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takanori Ugai, Kensho Hara, Shusaku Egami, Ken Fukuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11347">Multimodal Datasets and Benchmarks for Reasoning about Dynamic Spatio-Temporality in Everyday Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We used a 3D simulator to create artificial video data with standardized annotations, aiming to aid in the development of Embodied AI. Our question answering (QA) dataset measures the extent to which a robot can understand human behavior and the environment in a home setting. Preliminary experiments suggest our dataset is useful in measuring AI's comprehension of daily life. \end{abstract}
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2408.10588.pdf' target='_blank'>https://arxiv.org/pdf/2408.10588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijing Shao, Duotun Wang, Qing-Yao Tian, Yao-Dong Yang, Hengyu Meng, Zeyu Cai, Bo Dong, Yu Zhang, Kang Zhang, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10588">DEGAS: Detailed Expressions on Full-Body Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although neural rendering has made significant advances in creating lifelike, animatable full-body and head avatars, incorporating detailed expressions into full-body avatars remains largely unexplored. We present DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for full-body avatars with rich facial expressions. Trained on multiview videos of a given subject, our method learns a conditional variational autoencoder that takes both the body motion and facial expression as driving signals to generate Gaussian maps in the UV layout. To drive the facial expressions, instead of the commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to adopt the expression latent space trained solely on 2D portrait images, bridging the gap between 2D talking faces and 3D avatars. Leveraging the rendering capability of 3DGS and the rich expressiveness of the expression latent space, the learned avatars can be reenacted to reproduce photorealistic rendering images with subtle and accurate facial expressions. Experiments on an existing dataset and our newly proposed dataset of full-body talking avatars demonstrate the efficacy of our method. We also propose an audio-driven extension of our method with the help of 2D talking faces, opening new possibilities for interactive AI agents.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2407.18550.pdf' target='_blank'>https://arxiv.org/pdf/2407.18550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, Jonghyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18550">ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulated virtual environments have been widely used to learn robotic agents that perform daily household tasks. These environments encourage research progress by far, but often provide limited object interactability, visual appearance different from real-world environments, or relatively smaller environment sizes. This prevents the learned models in the virtual scenes from being readily deployable. To bridge the gap between these learning environments and deploying (i.e., real) environments, we propose the ReALFRED benchmark that employs real-world scenes, objects, and room layouts to learn agents to complete household tasks by understanding free-form language instructions and interacting with objects in large, multi-room and 3D-captured scenes. Specifically, we extend the ALFRED benchmark with updates for larger environmental spaces with smaller visual domain gaps. With ReALFRED, we analyze previously crafted methods for the ALFRED benchmark and observe that they consistently yield lower performance in all metrics, encouraging the community to develop methods in more realistic environments. Our code and data are publicly available.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2407.17348.pdf' target='_blank'>https://arxiv.org/pdf/2407.17348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Feng, David S. Martinez Lema, Mohammadhossein Malmir, Hang Li, Jianxiang Feng, Zhaopeng Chen, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17348">DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for Task-Oriented Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DexGanGrasp, a dexterous grasping synthesis method that generates and evaluates grasps with single view in real time. DexGanGrasp comprises a Conditional Generative Adversarial Networks (cGANs)-based DexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor to assess the stability of these grasps. Extensive simulation and real-world expriments showcases the effectiveness of our proposed method, outperforming the baseline FFHNet with an 18.57% higher success rate in real-world evaluation. We further extend DexGanGrasp to DexAfford-Prompt, an open-vocabulary affordance grounding pipeline for dexterous grasping leveraging Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs), to achieve task-oriented grasping with successful real-world deployments.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2407.11560.pdf' target='_blank'>https://arxiv.org/pdf/2407.11560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Duarte, Michele Polito, Laura Gastaldi, Pedro Neto, Stefano Pastorelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11560">Demonstration of real-time event camera to collaborative robot communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time robot actuation is one of the main challenges to overcome in human-robot interaction. Most visual sensors are either too slow or their data are too complex to provide meaningful information and low latency input to a robotic system. Data output of an event camera is high-frequency and extremely lightweight, with only 8 bytes per event. To evaluate the hypothesis of using event cameras as data source for a real-time robotic system, the position of a waving hand is acquired from the event data and transmitted to a collaborative robot as a movement command. A total time delay of 110 ms was measured between the original movement and the robot movement, where much of the delay is caused by the robot dynamics.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2406.11133.pdf' target='_blank'>https://arxiv.org/pdf/2406.11133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Hoggenmuller, Wen-Ying Lee, Luke Hespanhol, Malte Jung, Martin Tomitsch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11133">Eliciting New Perspectives in RtD Studies through Annotated Portfolios: A Case Study of Robotic Artefacts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate how to elicit new perspectives in research-through-design (RtD) studies through annotated portfolios. Situating the usage in human-robot interaction (HRI), we used two robotic artefacts as a case study: we first created our own annotated portfolio and subsequently ran online workshops during which we asked HRI experts to annotate our robotic artefacts. We report on the different aspects revealed about the value, use, and further improvements of the robotic artefacts through using the annotated portfolio technique ourselves versus using it with experts. We suggest that annotated portfolios - when performed by external experts - allow design researchers to obtain a form of creative and generative peer critique. Our paper offers methodological considerations for conducting expert annotation sessions. Further, we discuss the use of annotated portfolios to unveil designerly HRI knowledge in RtD studies.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2406.04882.pdf' target='_blank'>https://arxiv.org/pdf/2406.04882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04882">InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to navigate following diverse language instructions in unexplored environments is an attractive goal for human-robot interaction. However, this goal is challenging because different navigation tasks require different strategies. The scarcity of instruction navigation data hinders training an instruction navigation model with varied strategies. Therefore, previous methods are all constrained to one specific type of navigation instruction. In this work, we propose InstructNav, a generic instruction navigation system. InstructNav makes the first endeavor to handle various instruction navigation tasks without any navigation training or pre-built maps. To reach this goal, we introduce Dynamic Chain-of-Navigation (DCoN) to unify the planning process for different types of navigation instructions. Furthermore, we propose Multi-sourced Value Maps to model key elements in instruction navigation so that linguistic DCoN planning can be converted into robot actionable trajectories. With InstructNav, we complete the R2R-CE task in a zero-shot way for the first time and outperform many task-training methods. Besides, InstructNav also surpasses the previous SOTA method by 10.48% on the zero-shot Habitat ObjNav and by 86.34% on demand-driven navigation DDN. Real robot experiments on diverse indoor scenes further demonstrate our method's robustness in coping with the environment and instruction variations.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2405.18324.pdf' target='_blank'>https://arxiv.org/pdf/2405.18324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas Bhat, Joseph B. Lyons, Cong Shi, X. Jessie Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18324">Value Alignment and Trust in Human-Robot Interaction: Insights from Simulation and User Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of AI technologies, humans and robots are increasingly teaming up to perform collaborative tasks. To enable smooth and effective collaboration, the topic of value alignment (operationalized herein as the degree of dynamic goal alignment within a task) between the robot and the human is gaining increasing research attention. Prior literature on value alignment makes an inherent assumption that aligning the values of the robot with that of the human benefits the team. This assumption, however, has not been empirically verified. Moreover, prior literature does not account for human's trust in the robot when analyzing human-robot value alignment. Thus, a research gap needs to be bridged by answering two questions: How does alignment of values affect trust? Is it always beneficial to align the robot's values with that of the human? We present a simulation study and a human-subject study to answer these questions. Results from the simulation study show that alignment of values is important for trust when the overall risk level of the task is high. We also present an adaptive strategy for the robot that uses Inverse Reinforcement Learning (IRL) to match the values of the robot with those of the human during interaction. Our simulations suggest that such an adaptive strategy is able to maintain trust across the full spectrum of human values. We also present results from an empirical study that validate these findings from simulation. Results indicate that real-time personalized value alignment is beneficial to trust and perceived performance by the human when the robot does not have a good prior on the human's values.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2405.05792.pdf' target='_blank'>https://arxiv.org/pdf/2405.05792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko SÃ¼nderhauf, Feras Dayoub, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05792">RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on "image segments", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a "continuous sense of a place", defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of "hops over segments" and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2405.04732.pdf' target='_blank'>https://arxiv.org/pdf/2405.04732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Sashank Dorbala, Prasoon Goyal, Robinson Piramuthu, Michael Johnston, Reza Ghanadhan, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04732">Is the House Ready For Sleeptime? Generating and Evaluating Situational Queries for Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present and tackle the problem of Embodied Question Answering (EQA) with Situational Queries (S-EQA) in a household environment. Unlike prior EQA work tackling simple queries that directly reference target objects and properties ("What is the color of the car?"), situational queries (such as "Is the house ready for sleeptime?") are challenging as they require the agent to correctly identify multiple object-states (Doors: Closed, Lights: Off, etc.) and reach a consensus on their states for an answer. Towards this objective, we first introduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an LLM's output to generate unique situational queries and corresponding consensus object information. PGE is used to generate 2K datapoints in the VirtualHome simulator, which is then annotated for ground truth answers via a large scale user-study conducted on M-Turk. With a high rate of answerability (97.26%) on this study, we establish that LLMs are good at generating situational data. However, in evaluating the data using an LLM, we observe a low correlation of 46.2% with the ground truth human annotations; indicating that while LLMs are good at generating situational data, they struggle to answer them according to consensus. When asked for reasoning, we observe the LLM often goes against commonsense in justifying its answer. Finally, we utilize PGE to generate situational data in a real-world environment, exposing LLM hallucination in generating reliable object-states when a structured scene graph is unavailable. To the best of our knowledge, this is the first work to introduce EQA in the context of situational queries and also the first to present a generative approach for query creation. We aim to foster research on improving the real-world usability of embodied agents through this work.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2404.04069.pdf' target='_blank'>https://arxiv.org/pdf/2404.04069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuba Girgin, Emre Girgin, Yigit Yildirim, Emre Ugur, Mehmet Haklidir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04069">Bidirectional Human Interactive AI Framework for Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trustworthiness is a crucial concept in the context of human-robot interaction. Cooperative robots must be transparent regarding their decision-making process, especially when operating in a human-oriented environment. This paper presents a comprehensive end-to-end framework aimed at fostering trustworthy bidirectional human-robot interaction in collaborative environments for the social navigation of mobile robots. In this framework, the robot communicates verbally while the human guides with gestures. Our method enables a mobile robot to predict the trajectory of people and adjust its route in a socially-aware manner. In case of conflict between human and robot decisions, detected through visual examination, the route is dynamically modified based on human preference while verbal communication is maintained. We present our pipeline, framework design, and preliminary experiments that form the foundation of our proposition.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2403.15813.pdf' target='_blank'>https://arxiv.org/pdf/2403.15813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yigit Yildirim, Mehmet Suzer, Emre Ugur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15813">Learning Early Social Maneuvers for Enhanced Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Socially compliant navigation is an integral part of safety features in Human-Robot Interaction. Traditional approaches to mobile navigation prioritize physical aspects, such as efficiency, but social behaviors gain traction as robots appear more in daily life. Recent techniques to improve the social compliance of navigation often rely on predefined features or reward functions, introducing assumptions about social human behavior. To address this limitation, we propose a novel Learning from Demonstration (LfD) framework for social navigation that exclusively utilizes raw sensory data. Additionally, the proposed system contains mechanisms to consider the future paths of the surrounding pedestrians, acknowledging the temporal aspect of the problem. The final product is expected to reduce the anxiety of people sharing their environment with a mobile robot, helping them trust that the robot is aware of their presence and will not harm them. As the framework is currently being developed, we outline its components, present experimental results, and discuss future work towards realizing this framework.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2403.10994.pdf' target='_blank'>https://arxiv.org/pdf/2403.10994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanjun Bu, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10994">SSUP-HRI: Social Signaling in Urban Public Human-Robot Interaction dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces our dataset featuring human-robot interactions (HRI) in urban public environments. This dataset is rich with social signals that we believe can be modeled to help understand naturalistic human-robot interaction. Our dataset currently comprises approximately 15 hours of video footage recorded from the robots' perspectives, within which we annotated a total of 274 observable interactions featuring a wide range of naturalistic human-robot interactions. The data was collected by two mobile trash barrel robots deployed in Astor Place, New York City, over the course of a week. We invite the HRI community to access and utilize our dataset. To the best of our knowledge, this is the first dataset showcasing robot deployments in a complete public, non-controlled setting involving urban residents.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2312.08611.pdf' target='_blank'>https://arxiv.org/pdf/2312.08611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Melnik, Michael BÃ¼ttner, Leon Harz, Lyon Brown, Gora Chand Nandi, Arjun PS, Gaurav Kumar Yadav, Rahul Kala, Robert Haschke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08611">UniTeam: Open Vocabulary Mobile Manipulation Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report introduces our UniTeam agent - an improved baseline for the "HomeRobot: Open Vocabulary Mobile Manipulation" challenge. The challenge poses problems of navigation in unfamiliar environments, manipulation of novel objects, and recognition of open-vocabulary object classes. This challenge aims to facilitate cross-cutting research in embodied AI using recent advances in machine learning, computer vision, natural language, and robotics. In this work, we conducted an exhaustive evaluation of the provided baseline agent; identified deficiencies in perception, navigation, and manipulation skills; and improved the baseline agent's performance. Notably, enhancements were made in perception - minimizing misclassifications; navigation - preventing infinite loop commitments; picking - addressing failures due to changing object visibility; and placing - ensuring accurate positioning for successful object placement.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2312.07886.pdf' target='_blank'>https://arxiv.org/pdf/2312.07886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Huang, Boyuan Yang, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07886">Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are capable of reasoning over diverse input data modalities through pre-trained encoders. However, the growing diversity of input data modalities prevents incorporating all modalities into LLMs, especially when LLMs are deployed on resource-constrained edge devices for embodied AI applications. Instead, a better option is to adaptively involve only the useful modalities at runtime, depending on the current environmental contexts and task requirements. For such modality adaptation, existing work adopts fixed connections between encoders and the LLM's input layer, leading to high training cost at runtime and ineffective cross-modal interaction. In this paper, we address these limitations by presenting mPnP-LLM, a new technique that allows fully elastic, automated and prompt runtime modality adaptation, by connecting unimodal encoders to a flexible set of last LLM blocks and making such latent connections fully trainable at runtime. Experiments over the nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction and 30% GPU memory usage reduction, while retaining on-par accuracy with the existing schemes. Under the same compute budget, mPnP-LLM improves the task accuracy by up to 4% compared to the best existing scheme.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2311.16051.pdf' target='_blank'>https://arxiv.org/pdf/2311.16051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas Bhat, Joseph B. Lyons, Cong Shi, X. Jessie Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16051">Evaluating the Impact of Personalized Value Alignment in Human-Robot Interaction: Insights into Trust and Team Performance Outcomes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper examines the effect of real-time, personalized alignment of a robot's reward function to the human's values on trust and team performance. We present and compare three distinct robot interaction strategies: a non-learner strategy where the robot presumes the human's reward function mirrors its own, a non-adaptive-learner strategy in which the robot learns the human's reward function for trust estimation and human behavior modeling, but still optimizes its own reward function, and an adaptive-learner strategy in which the robot learns the human's reward function and adopts it as its own. Two human-subject experiments with a total number of 54 participants were conducted. In both experiments, the human-robot team searches for potential threats in a town. The team sequentially goes through search sites to look for threats. We model the interaction between the human and the robot as a trust-aware Markov Decision Process (trust-aware MDP) and use Bayesian Inverse Reinforcement Learning (IRL) to estimate the reward weights of the human as they interact with the robot. In Experiment 1, we start our learning algorithm with an informed prior of the human's values/goals. In Experiment 2, we start the learning algorithm with an uninformed prior. Results indicate that when starting with a good informed prior, personalized value alignment does not seem to benefit trust or team performance. On the other hand, when an informed prior is unavailable, alignment to the human's values leads to high trust and higher perceived performance while maintaining the same objective team performance.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2311.13781.pdf' target='_blank'>https://arxiv.org/pdf/2311.13781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanying Zhang, Shen Zhao, Fanyang Meng, Songtao Wu, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13781">Dynamic Compositional Graph Convolutional Network for Efficient Composite Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With potential applications in fields including intelligent surveillance and human-robot interaction, the human motion prediction task has become a hot research topic and also has achieved high success, especially using the recent Graph Convolutional Network (GCN). Current human motion prediction task usually focuses on predicting human motions for atomic actions. Observing that atomic actions can happen at the same time and thus formulating the composite actions, we propose the composite human motion prediction task. To handle this task, we first present a Composite Action Generation (CAG) module to generate synthetic composite actions for training, thus avoiding the laborious work of collecting composite action samples. Moreover, we alleviate the effect of composite actions on demand for a more complicated model by presenting a Dynamic Compositional Graph Convolutional Network (DC-GCN). Extensive experiments on the Human3.6M dataset and our newly collected CHAMP dataset consistently verify the efficiency of our DC-GCN method, which achieves state-of-the-art motion prediction accuracies and meanwhile needs few extra computational costs than traditional GCN-based human motion methods.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2311.05562.pdf' target='_blank'>https://arxiv.org/pdf/2311.05562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shiuan Tung, Matthew B. Luebbers, Alessandro Roncone, Bradley Hayes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05562">Improving Human Legibility in Collaborative Robot Tasks through Augmented Reality and Workspace Preparation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the intentions of human teammates is critical for safe and effective human-robot interaction. The canonical approach for human-aware robot motion planning is to first predict the human's goal or path, and then construct a robot plan that avoids collision with the human. This method can generate unsafe interactions if the human model and subsequent predictions are inaccurate. In this work, we present an algorithmic approach for both arranging the configuration of objects in a shared human-robot workspace, and projecting ``virtual obstacles'' in augmented reality, optimizing for legibility in a given task. These changes to the workspace result in more legible human behavior, improving robot predictions of human goals, thereby improving task fluency and safety. To evaluate our approach, we propose two user studies involving a collaborative tabletop task with a manipulator robot, and a warehouse navigation task with a mobile robot.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2310.10610.pdf' target='_blank'>https://arxiv.org/pdf/2310.10610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10610">Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our ultimate goal is to build robust policies for robots that assist people. What makes this hard is that people can behave unexpectedly at test time, potentially interacting with the robot outside its training distribution and leading to failures. Even just measuring robustness is a challenge. Adversarial perturbations are the default, but they can paint the wrong picture: they can correspond to human motions that are unlikely to occur during natural interactions with people. A robot policy might fail under small adversarial perturbations but work under large natural perturbations. We propose that capturing robustness in these interactive settings requires constructing and analyzing the entire natural-adversarial frontier: the Pareto-frontier of human policies that are the best trade-offs between naturalness and low robot performance. We introduce RIGID, a method for constructing this frontier by training adversarial human policies that trade off between minimizing robot reward and acting human-like (as measured by a discriminator). On an Assistive Gym task, we use RIGID to analyze the performance of standard collaborative Reinforcement Learning, as well as the performance of existing methods meant to increase robustness. We also compare the frontier RIGID identifies with the failures identified in expert adversarial interaction, and with naturally-occurring failures during user interaction. Overall, we find evidence that RIGID can provide a meaningful measure of robustness predictive of deployment performance, and uncover failure cases in human-robot interaction that are difficult to find manually. https://ood-human.github.io.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2309.15616.pdf' target='_blank'>https://arxiv.org/pdf/2309.15616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arindam Roychoudhury, Shahram Khorshidi, Subham Agrawal, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15616">Perception for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose of Review: The field of humanoid robotics, perception plays a fundamental role in enabling robots to interact seamlessly with humans and their surroundings, leading to improved safety, efficiency, and user experience. This scientific study investigates various perception modalities and techniques employed in humanoid robots, including visual, auditory, and tactile sensing by exploring recent state-of-the-art approaches for perceiving and understanding the internal state, the environment, objects, and human activities.
  Recent Findings: Internal state estimation makes extensive use of Bayesian filtering methods and optimization techniques based on maximum a-posteriori formulation by utilizing proprioceptive sensing. In the area of external environment understanding, with an emphasis on robustness and adaptability to dynamic, unforeseen environmental changes, the new slew of research discussed in this study have focused largely on multi-sensor fusion and machine learning in contrast to the use of hand-crafted, rule-based systems. Human robot interaction methods have established the importance of contextual information representation and memory for understanding human intentions.
  Summary: This review summarizes the recent developments and trends in the field of perception in humanoid robots. Three main areas of application are identified, namely, internal state estimation, external environment estimation, and human robot interaction. The applications of diverse sensor modalities in each of these areas are considered and recent significant works are discussed.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2309.11689.pdf' target='_blank'>https://arxiv.org/pdf/2309.11689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Patankar, Khiem Phi, Dasharadhan Mahalingam, Nilanjan Chakraborty, IV Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11689">Task-Oriented Grasping with Point Cloud Representation of Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the problem of task-oriented grasp synthesis from partial point cloud data using an eye-in-hand camera configuration. In task-oriented grasp synthesis, a grasp has to be selected so that the object is not lost during manipulation, and it is also ensured that adequate force/moment can be applied to perform the task. We formalize the notion of a gross manipulation task as a constant screw motion (or a sequence of constant screw motions) to be applied to the object after grasping. Using this notion of task, and a corresponding grasp quality metric developed in our prior work, we use a neural network to approximate a function for predicting the grasp quality metric on a cuboid shape. We show that by using a bounding box obtained from the partial point cloud of an object, and the grasp quality metric mentioned above, we can generate a good grasping region on the bounding box that can be used to compute an antipodal grasp on the actual object. Our algorithm does not use any manually labeled data or grasping simulator, thus making it very efficient to implement and integrate with screw linear interpolation-based motion planners. We present simulation as well as experimental results that show the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2308.00945.pdf' target='_blank'>https://arxiv.org/pdf/2308.00945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaohui Guo, X. Jessie Yang, Cong Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00945">Reward Shaping for Building Trustworthy Robots in Sequential Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trust-aware human-robot interaction (HRI) has received increasing research attention, as trust has been shown to be a crucial factor for effective HRI. Research in trust-aware HRI discovered a dilemma -- maximizing task rewards often leads to decreased human trust, while maximizing human trust would compromise task performance. In this work, we address this dilemma by formulating the HRI process as a two-player Markov game and utilizing the reward-shaping technique to improve human trust while limiting performance loss. Specifically, we show that when the shaping reward is potential-based, the performance loss can be bounded by the potential functions evaluated at the final states of the Markov game. We apply the proposed framework to the experience-based trust model, resulting in a linear program that can be efficiently solved and deployed in real-world applications. We evaluate the proposed framework in a simulation scenario where a human-robot team performs a search-and-rescue mission. The results demonstrate that the proposed framework successfully modifies the robot's optimal policy, enabling it to increase human trust at a minimal task performance cost.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2307.16206.pdf' target='_blank'>https://arxiv.org/pdf/2307.16206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shusaku Egami, Takanori Ugai, Mikiko Oono, Koji Kitamura, Ken Fukuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16206">Synthesizing Event-centric Knowledge Graphs of Daily Activities Using Virtual Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) is expected to be embodied in software agents, robots, and cyber-physical systems that can understand the various contextual information of daily life in the home environment to support human behavior and decision making in various situations. Scene graph and knowledge graph (KG) construction technologies have attracted much attention for knowledge-based embodied question answering meeting this expectation. However, collecting and managing real data on daily activities under various experimental conditions in a physical space are quite costly, and developing AI that understands the intentions and contexts is difficult. In the future, data from both virtual spaces, where conditions can be easily modified, and physical spaces, where conditions are difficult to change, are expected to be combined to analyze daily living activities. However, studies on the KG construction of daily activities using virtual space and their application have yet to progress. The potential and challenges must still be clarified to facilitate AI development for human daily life. Thus, this study proposes the VirtualHome2KG framework to generate synthetic KGs of daily life activities in virtual space. This framework augments both the synthetic video data of daily activities and the contextual semantic data corresponding to the video contents based on the proposed event-centric schema and virtual space simulation results. Therefore, context-aware data can be analyzed, and various applications that have conventionally been difficult to develop due to the insufficient availability of relevant data and semantic information can be developed. We also demonstrate herein the utility and potential of the proposed VirtualHome2KG framework through several use cases, including the analysis of daily activities by querying, embedding, and clustering, and fall risk detection among ...
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2306.17379.pdf' target='_blank'>https://arxiv.org/pdf/2306.17379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas H. Weisswange, Hifza Javed, Manuel Dietrich, Tuan Vu Pham, Maria Teresa Parreira, Michael Sack, Nawid Jamali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17379">What Could a Social Mediator Robot Do? Lessons from Real-World Mediation Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of social robots as instruments for social mediation has been gaining traction in the field of Human-Robot Interaction (HRI). So far, the design of such robots and their behaviors is often driven by technological platforms and experimental setups in controlled laboratory environments. To address complex social relationships in the real world, it is crucial to consider the actual needs and consequences of the situations found therein. This includes understanding when a mediator is necessary, what specific role such a robot could play, and how it moderates human social dynamics. In this paper, we discuss six relevant roles for robotic mediators that we identified by investigating a collection of videos showing realistic group situations. We further discuss mediation behaviors and target measures to evaluate the success of such interventions. We hope that our findings can inspire future research on robot-assisted social mediation by highlighting a wider set of mediation applications than those found in prior studies. Specifically, we aim to inform the categorization and selection of interaction scenarios that reflect real situations, where a mediation robot can have a positive and meaningful impact on group dynamics.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2306.17374.pdf' target='_blank'>https://arxiv.org/pdf/2306.17374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hifza Javed, Nawid Jamali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17374">Group Dynamics: Survey of Existing Multimodal Models and Considerations for Social Mediation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social mediator robots facilitate human-human interactions by producing behavior strategies that positively influence how humans interact with each other in social settings. As robots for social mediation gain traction in the field of human-human-robot interaction, their ability to "understand" the humans in their environments becomes crucial. This objective requires models of human understanding that consider multiple humans in an interaction as a collective entity and represent the group dynamics that exist among its members. Group dynamics are defined as the influential actions, processes, and changes that occur within and between group interactants. Since an individual's behavior may be deeply influenced by their interactions with other group members, the social dynamics existing within a group can influence the behaviors, attitudes, and opinions of each individual and the group as a whole. Therefore, models of group dynamics are critical for a social mediator robot to be effective in its role. In this paper, we survey existing models of group dynamics and categorize them into models of social dominance, affect, social cohesion, conflict resolution, and engagement. We highlight the multimodal features these models utilize, and emphasize the importance of capturing the interpersonal aspects of a social interaction. Finally, we make a case for models of relational affect as an approach that may be able to capture a representation of human-human interactions that can be useful for social mediation.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2306.16633.pdf' target='_blank'>https://arxiv.org/pdf/2306.16633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hifza Javed, Nawid Jamali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16633">Modeling Group Dynamics for Personalized Robot-Mediated Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of human-human-robot interaction (HHRI) uses social robots to positively influence how humans interact with each other. This objective requires models of human understanding that consider multiple humans in an interaction as a collective entity and represent the group dynamics that exist within it. Understanding group dynamics is important because these can influence the behaviors, attitudes, and opinions of each individual within the group, as well as the group as a whole. Such an understanding is also useful when personalizing an interaction between a robot and the humans in its environment, where a group-level model can facilitate the design of robot behaviors that are tailored to a given group, the dynamics that exist within it, and the specific needs and preferences of the individual interactants. In this paper, we highlight the need for group-level models of human understanding in human-human-robot interaction research and how these can be useful in developing personalization techniques. We survey existing models of group dynamics and categorize them into models of social dominance, affect, social cohesion, and conflict resolution. We highlight the important features these models utilize, evaluate their potential to capture interpersonal aspects of a social interaction, and highlight their value for personalization techniques. Finally, we identify directions for future work, and make a case for models of relational affect as an approach that can better capture group-level understanding of human-human interactions and be useful in personalizing human-human-robot interactions.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2306.15228.pdf' target='_blank'>https://arxiv.org/pdf/2306.15228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Datta, Ryan Hoque, Anrui Gu, Eugen Solowjow, Ken Goldberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15228">IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has been applied to a range of robotic tasks, but can struggle when robots encounter edge cases that are not represented in the training data (i.e., distribution shift). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human supervisors during task execution and learn from them over time, but different supervisors may demonstrate the task in different ways. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose Implicit Interactive Fleet Learning (IIFL), an algorithm that builds on IBC for interactive imitation learning from multiple heterogeneous human supervisors. A key insight in IIFL is a novel approach for uncertainty quantification in EBMs using Jeffreys divergence. While IIFL is more computationally expensive than explicit methods, results suggest that IIFL achieves a 2.8x higher success rate in simulation experiments and a 4.5x higher return on human effort in a physical block pushing task over (Explicit) IFL, IBC, and other baselines.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2303.04011.pdf' target='_blank'>https://arxiv.org/pdf/2303.04011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sacha Morin, Miguel Saavedra-Ruiz, Liam Paull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04011">One-4-All: Neural Potential Fields for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice requires tuning a number of pruning heuristics. These heuristics are necessary to avoid spurious edges, limit runtime memory usage and maintain reasonably fast graph queries in large environments. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over image embeddings. Our system is trained offline on non-expert exploration sequences of RGB data and controls, and does not require any depth or pose measurements. We show that O4A can reach long-range goals in 8 simulated Gibson indoor environments and that resulting embeddings are topologically similar to ground truth maps, even if no pose is observed. We further demonstrate successful real-world navigation using a Jackal UGV platform.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2303.01586.pdf' target='_blank'>https://arxiv.org/pdf/2303.01586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaozi Gao, Govind Thattai, Suhaila Shakiah, Xiaofeng Gao, Shreyas Pansare, Vasu Sharma, Gaurav Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zheng, Lucy Hu, Karthika Arumugam, Shui Hu, Matthew Wen, Dinakar Guthy, Cadence Chung, Rohan Khanna, Osman Ipek, Leslie Ball, Kate Bland, Heather Rocker, Yadunandana Rao, Michael Johnston, Reza Ghanadan, Arindam Mandal, Dilek Hakkani Tur, Prem Natarajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01586">Alexa Arena: A User-Centric Interactive Platform for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research. Alexa Arena provides a variety of multi-room layouts and interactable objects, for the creation of human-robot interaction (HRI) missions. With user-friendly graphics and control mechanisms, Alexa Arena supports the development of gamified robotic tasks readily accessible to general human users, thus opening a new venue for high-efficiency HRI data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled instruction-following benchmark and provide baseline results for it. We make Alexa Arena publicly available to facilitate research in building generalizable and assistive embodied agents.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2210.16220.pdf' target='_blank'>https://arxiv.org/pdf/2210.16220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni Franzese, Leandro de Souza Rosa, Tim Verburg, Luka Peternel, Jens Kober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.16220">Interactive Imitation Learning of Bimanual Movement Primitives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing bimanual tasks with dual robotic setups can drastically increase the impact on industrial and daily life applications. However, performing a bimanual task brings many challenges, like synchronization and coordination of the single-arm policies. This article proposes the Safe, Interactive Movement Primitives Learning (SIMPLe) algorithm, to teach and correct single or dual arm impedance policies directly from human kinesthetic demonstrations. Moreover, it proposes a novel graph encoding of the policy based on Gaussian Process Regression (GPR) where the single-arm motion is guaranteed to converge close to the trajectory and then towards the demonstrated goal. Regulation of the robot stiffness according to the epistemic uncertainty of the policy allows for easily reshaping the motion with human feedback and/or adapting to external perturbations. We tested the SIMPLe algorithm on a real dual-arm setup where the teacher gave separate single-arm demonstrations and then successfully synchronized them only using kinesthetic feedback or where the original bimanual demonstration was locally reshaped to pick a box at a different height.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2209.05135.pdf' target='_blank'>https://arxiv.org/pdf/2209.05135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Tavella, Aphrodite Galata, Angelo Cangelosi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05135">Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters. Our results show that our approach is able to successfully imitate these fine-grained movements without additional information, highlighting its potential for real-world applications in robotics.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2207.14556.pdf' target='_blank'>https://arxiv.org/pdf/2207.14556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Amir Tafrishi, Ankit A. Ravankar, Yasuhisa Hirata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.14556">PSM: A Predictive Safety Model for Body Motion Based On the Spring-Damper Pendulum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantifying the safety of the human body orientation is an important issue in human-robot interaction. Knowing the changing physical constraints on human motion can improve inspection of safe human motions and bring essential information about stability and normality of human body orientations with real-time risk assessment. Also, this information can be used in cooperative robots and monitoring systems to evaluate and interact in the environment more freely. Furthermore, the workspace area can be more deterministic with the known physical characteristics of safety. Based on this motivation, we propose a novel predictive safety model (PSM) that relies on the information of an inertial measurement unit on the human chest. The PSM encompasses a 3-Dofs spring-damper pendulum model that predicts human motion based on a safe motion dataset. The estimated safe orientation of humans is obtained by integrating a safety dataset and an elastic spring-damper model in a way that the proposed approach can realize complex motions at different safety levels. We did experiments in a real-world scenario to verify our novel proposed model. This novel approach can be used in different guidance/assistive robots and health monitoring systems to support and evaluate the human condition, particularly elders.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2205.15862.pdf' target='_blank'>https://arxiv.org/pdf/2205.15862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Ali, Doreen Jirak, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.15862">Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots are expected to get more involved in people's everyday lives, frameworks that enable intuitive user interfaces are in demand. Hand gesture recognition systems provide a natural way of communication and, thus, are an integral part of seamless Human-Robot Interaction (HRI). Recent years have witnessed an immense evolution of computational models powered by deep learning. However, state-of-the-art models fall short in expanding across different gesture domains, such as emblems and co-speech. In this paper, we propose a novel hybrid hand gesture recognition system. Our architecture enables learning both static and dynamic gestures: by capturing a so-called "snapshot" of the gesture performance at its peak, we integrate the hand pose along with the dynamic movement. Moreover, we present a method for analyzing the motion profile of a gesture to uncover its dynamic characteristics and which allows regulating a static channel based on the amount of motion. Our evaluation demonstrates the superiority of our approach on two gesture benchmarks compared to a CNNLSTM baseline. We also provide an analysis on a gesture class basis that unveils the potential of our Snapture architecture for performance improvements. Thanks to its modular implementation, our framework allows the integration of other multimodal data like facial expressions and head tracking, which are important cues in HRI scenarios, into one architecture. Thus, our work contributes both to gesture recognition research and machine learning applications for non-verbal communication with robots.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2204.07183.pdf' target='_blank'>https://arxiv.org/pdf/2204.07183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Theodora Kontogianni, Ekin Celikkan, Siyu Tang, Konrad Schindler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.07183">Interactive Object Segmentation in 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an interactive approach for 3D instance segmentation, where users can iteratively collaborate with a deep learning model to segment objects in a 3D point cloud directly. Current methods for 3D instance segmentation are generally trained in a fully-supervised fashion, which requires large amounts of costly training labels, and does not generalize well to classes unseen during training. Few works have attempted to obtain 3D segmentation masks using human interactions. Existing methods rely on user feedback in the 2D image domain. As a consequence, users are required to constantly switch between 2D images and 3D representations, and custom architectures are employed to combine multiple input modalities. Therefore, integration with existing standard 3D models is not straightforward. The core idea of this work is to enable users to interact directly with 3D point clouds by clicking on desired 3D objects of interest~(or their background) to interactively segment the scene in an open-world setting. Specifically, our method does not require training data from any target domain, and can adapt to new environments where no appropriate training sets are available. Our system continuously adjusts the object segmentation based on the user feedback and achieves accurate dense 3D segmentation masks with minimal human effort (few clicks per object). Besides its potential for efficient labeling of large-scale and varied 3D datasets, our approach, where the user directly interacts with the 3D environment, enables new applications in AR/VR and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2202.07138.pdf' target='_blank'>https://arxiv.org/pdf/2202.07138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kebing Jin, Hankz Hankui Zhuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.07138">Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication between human and intelligent agents. This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications. We also explore some potential future issues between AI planning and natural language processing. To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2509.19954.pdf' target='_blank'>https://arxiv.org/pdf/2509.19954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinhao Song, Yurui Du, Ophelie Saussus, Sofie De Schrijver, Irene Caprara, Peter Janssen, Renaud Detry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19954">Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human-robot interaction. RT-V2 jointly models a user's long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human-computer interaction studies with keyboard input, and brain-machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2509.19843.pdf' target='_blank'>https://arxiv.org/pdf/2509.19843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Ziliotto, Jelin Raphael Akkara, Alessandro Daniele, Lamberto Ballan, Luciano Serafini, Tommaso Campari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19843">PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors. In this work, we introduce PersONAL (PERSonalized Object Navigation And Localization, a comprehensive benchmark designed to study personalization in Embodied AI. Agents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics. The benchmark supports two evaluation modes: (1) active navigation in unseen environments, and (2) object grounding in previously mapped scenes. Experiments with state-of-the-art baselines reveal a substantial gap to human performance, highlighting the need for embodied agents capable of perceiving, reasoning, and memorizing over personalized information; paving the way towards real-world assistive robot.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2509.16670.pdf' target='_blank'>https://arxiv.org/pdf/2509.16670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhuan Lu, Xinyue Song, Wenjun Ke, Zhizhi Yu, Wenhao Yang, Jianguo Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16670">Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio grounding, or speech-driven open-set object detection, aims to localize and identify objects directly from speech, enabling generalization beyond predefined categories. This task is crucial for applications like human-robot interaction where textual input is impractical. However, progress in this domain faces a fundamental bottleneck from the scarcity of large-scale, paired audio-image data, and is further constrained by previous methods that rely on indirect, text-mediated pipelines. In this paper, we introduce Speech-to-See (Speech2See), an end-to-end approach built on a pre-training and fine-tuning paradigm. Specifically, in the pre-training stage, we design a Query-Guided Semantic Aggregation module that employs learnable queries to condense redundant speech embeddings into compact semantic representations. During fine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts (MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation. Extensive experiments show that Speech2See achieves robust and adaptable performance across multiple benchmarks, demonstrating its strong generalization ability and broad applicability.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2509.14932.pdf' target='_blank'>https://arxiv.org/pdf/2509.14932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Jülg, Pierre Krack, Seongjin Bien, Yannik Blei, Khaled Gamal, Ken Nakahara, Johannes Hechtl, Roberto Calandra, Wolfram Burgard, Florian Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14932">Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) mark a major shift in robot learning. They replace specialized architectures and task-tailored components of expert policies with large-scale data collection and setup-specific fine-tuning. In this machine learning-focused workflow that is centered around models and scalable training, traditional robotics software frameworks become a bottleneck, while robot simulations offer only limited support for transitioning from and to real-world experiments. In this work, we close this gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from the ground up to support research in robot learning with large-scale generalist policies. At its core, RCS features a modular and easily extensible layered architecture with a unified interface for simulated and physical robots, facilitating sim-to-real transfer. Despite its minimal footprint and dependencies, it offers a complete feature set, enabling both real-world experiments and large-scale training in simulation. Our contribution is twofold: First, we introduce the architecture of RCS and explain its design principles. Second, we evaluate its usability and performance along the development cycle of VLA and RL policies. Our experiments also provide an extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed light on how simulation data can improve real-world policy performance. Our code, datasets, weights, and videos are available at: https://robotcontrolstack.github.io/
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2509.11402.pdf' target='_blank'>https://arxiv.org/pdf/2509.11402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandra Rossi, Patrick Holthaus, Gabriella Lakatos, Sílvia Moros, Ali Fallahi, Murat Kirtay, Marie Postma, Erhan Oztop
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11402">TRUST 2025: SCRITA and RTSS @ RO-MAN 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The TRUST workshop is the result of a collaboration between two established workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic Societies). This joint initiative brings together the complementary goals of these workshops to advance research on trust from both the human and robot perspectives. Website: https://scrita.herts.ac.uk/2025/
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2509.09889.pdf' target='_blank'>https://arxiv.org/pdf/2509.09889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulia Botta, Marco Botta, Cristina Gena, Alessandro Mazzei, Massimo Donini, Alberto Lillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09889">Using the Pepper Robot to Support Sign Language Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are increasingly experimented in public and assistive settings, but their accessibility for Deaf users remains quite underexplored. Italian Sign Language (LIS) is a fully-fledged natural language that relies on complex manual and non-manual components. Enabling robots to communicate using LIS could foster more inclusive human robot interaction, especially in social environments such as hospitals, airports, or educational settings. This study investigates whether a commercial social robot, Pepper, can produce intelligible LIS signs and short signed LIS sentences. With the help of a Deaf student and his interpreter, an expert in LIS, we co-designed and implemented 52 LIS signs on Pepper using either manual animation techniques or a MATLAB based inverse kinematics solver. We conducted a exploratory user study involving 12 participants proficient in LIS, both Deaf and hearing. Participants completed a questionnaire featuring 15 single-choice video-based sign recognition tasks and 2 open-ended questions on short signed sentences. Results shows that the majority of isolated signs were recognized correctly, although full sentence recognition was significantly lower due to Pepper's limited articulation and temporal constraints. Our findings demonstrate that even commercially available social robots like Pepper can perform a subset of LIS signs intelligibly, offering some opportunities for a more inclusive interaction design. Future developments should address multi-modal enhancements (e.g., screen-based support or expressive avatars) and involve Deaf users in participatory design to refine robot expressivity and usability.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2509.09594.pdf' target='_blank'>https://arxiv.org/pdf/2509.09594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09594">ObjectReact: Learning Object-Relative Control for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2509.02761.pdf' target='_blank'>https://arxiv.org/pdf/2509.02761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-Tür, Gokhan Tur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02761">Plan Verification for LLM-Based Embodied Task Completion Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2508.10378.pdf' target='_blank'>https://arxiv.org/pdf/2508.10378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Chen, Shu Miao, Chunyu Wu, Jingsong Mu, Bo OuYang, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10378">A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Upper-limb exoskeletons are primarily designed to provide assistive support by accurately interpreting and responding to human intentions. In home-care scenarios, exoskeletons are expected to adapt their assistive configurations based on the semantic information of the task, adjusting appropriately in accordance with the nature of the object being manipulated. However, existing solutions often lack the ability to understand task semantics or collaboratively plan actions with the user, limiting their generalizability. To address this challenge, this paper introduces a semantic-aware framework that integrates large language models into the task planning framework, enabling the delivery of safe and intent-integrative assistance. The proposed approach begins with the exoskeleton operating in transparent mode to capture the wearer's intent during object grasping. Once semantic information is extracted from the task description, the system automatically configures appropriate assistive parameters. In addition, a diffusion-based anomaly detector is used to continuously monitor the state of human-robot interaction and trigger real-time replanning in response to detected anomalies. During task execution, online trajectory refinement and impedance control are used to ensure safety and regulate human-robot interaction. Experimental results demonstrate that the proposed method effectively aligns with the wearer's cognition, adapts to semantically varying tasks, and responds reliably to anomalies.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2508.05543.pdf' target='_blank'>https://arxiv.org/pdf/2508.05543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05543">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2508.05021.pdf' target='_blank'>https://arxiv.org/pdf/2508.05021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weifan Zhang, Tingguang Li, Yuzhen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05021">MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation in unknown environments based solely on natural language descriptions is a key capability for intelligent robots. In this work, we propose a navigation framework built upon off-the-shelf Visual Language Models (VLMs), enhanced with two human-inspired mechanisms: perspective-based active grounding, which dynamically adjusts the robot's viewpoint for improved visual inspection, and historical memory backtracking, which enables the system to retain and re-evaluate uncertain observations over time. Unlike existing approaches that passively rely on incidental visual inputs, our method actively optimizes perception and leverages memory to resolve ambiguity, significantly improving vision-language grounding in complex, unseen environments. Our framework operates in a zero-shot manner, achieving strong generalization to diverse and open-ended language descriptions without requiring labeled data or model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show that our method outperforms state-of-the-art approaches in language-driven object navigation. We further demonstrate its practicality through real-world deployment on a quadruped robot, achieving robust and effective navigation performance.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2508.02219.pdf' target='_blank'>https://arxiv.org/pdf/2508.02219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02219">CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2506.21732.pdf' target='_blank'>https://arxiv.org/pdf/2506.21732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ameya Salvi, Venkat Krovi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21732">Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based lane keeping is a topic of significant interest in the robotics and autonomous ground vehicles communities in various on-road and off-road applications. The skid-steered vehicle architecture has served as a useful vehicle platform for human controlled operations. However, systematic modeling, especially of the skid-slip wheel terrain interactions (primarily in off-road settings) has created bottlenecks for automation deployment. End-to-end learning based methods such as imitation learning and deep reinforcement learning, have gained prominence as a viable deployment option to counter the lack of accurate analytical models. However, the systematic formulation and subsequent verification/validation in dynamic operation regimes (particularly for skid-steered vehicles) remains a work in progress. To this end, a novel approach for structured formulation for learning visual navigation is proposed and investigated in this work. Extensive software simulations, hardware evaluations and ablation studies now highlight the significantly improved performance of the proposed approach against contemporary literature.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2506.11773.pdf' target='_blank'>https://arxiv.org/pdf/2506.11773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Leng, Megha Thukral, Yaqi Liu, Hrudhai Rajasekhar, Shruthi K. Hiremath, Jiaman He, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11773">AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge in developing robust and generalizable Human Activity Recognition (HAR) systems for smart homes is the lack of large and diverse labeled datasets. Variations in home layouts, sensor configurations, and individual behaviors further exacerbate this issue. To address this, we leverage the idea of embodied AI agents-virtual agents that perceive and act within simulated environments guided by internal world models. We introduce AgentSense, a virtual data generation pipeline in which agents live out daily routines in simulated smart homes, with behavior guided by Large Language Models (LLMs). The LLM generates diverse synthetic personas and realistic routines grounded in the environment, which are then decomposed into fine-grained actions. These actions are executed in an extended version of the VirtualHome simulator, which we augment with virtual ambient sensors that record the agents' activities. Our approach produces rich, privacy-preserving sensor data that reflects real-world diversity. We evaluate AgentSense on five real HAR datasets. Models pretrained on the generated data consistently outperform baselines, especially in low-resource settings. Furthermore, combining the generated virtual sensor data with a small amount of real data achieves performance comparable to training on full real-world datasets. These results highlight the potential of using LLM-guided embodied agents for scalable and cost-effective sensor data generation in HAR.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2506.09581.pdf' target='_blank'>https://arxiv.org/pdf/2506.09581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel Ã. GonzÃ¡lez-Santamarta, Francisco J. RodrÃ­guez-Lera, David SobrÃ­n-Hidalgo, Ãngel Manuel Guerrero-Higueras, Vicente MatellÃn-Olivera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09581">Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have experienced great advancements in the last year resulting in an increase of these models in several fields to face natural language tasks. The integration of these models in robotics can also help to improve several aspects such as human-robot interaction, navigation, planning and decision-making. Therefore, this paper introduces llama\_ros, a tool designed to integrate quantized Large Language Models (LLMs) into robotic systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine, llama\_ros enables the efficient execution of quantized LLMs as edge artificial intelligence (AI) in robotics systems with resource-constrained environments, addressing the challenges of computational efficiency and memory limitations. By deploying quantized LLMs, llama\_ros empowers robots to leverage the natural language understanding and generation for enhanced decision-making and interaction which can be paired with prompt engineering, knowledge graphs, ontologies or other tools to improve the capabilities of autonomous robots. Additionally, this paper provides insights into some use cases of using llama\_ros for planning and explainability in robotics.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2506.08006.pdf' target='_blank'>https://arxiv.org/pdf/2506.08006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Mo, Ziyang Leng, Leon Liu, Weizhen Wang, Honglin He, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08006">Dreamland: Controllable World Creation with Simulator and Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2505.17016.pdf' target='_blank'>https://arxiv.org/pdf/2505.17016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhan Tan, Kairan Dou, Yue Zhao, Philipp KrÃ¤henbÃ¼hl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17016">Interactive Post-Training for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2505.01862.pdf' target='_blank'>https://arxiv.org/pdf/2505.01862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linus Nwankwo, Bjoern Ellensohn, Ozan Ãzdenizci, Elmar Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01862">ReLI: A Language-Agnostic Approach to Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at https://linusnep.github.io/ReLI/.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2504.18355.pdf' target='_blank'>https://arxiv.org/pdf/2504.18355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Xiling Li, Korbinian Rudolf, Nils Blank, Rudolf Lioutikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18355">Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a "this looks like that" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2504.16373.pdf' target='_blank'>https://arxiv.org/pdf/2504.16373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasra Chandio, Diana Romero, Salma Elmalaki, Fatima Anwar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16373">What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain if sensor and system-logged behavioral signals capture how users experience that collaboration. This disconnect stems from a fundamental gap: behavioral signals are observable and continuous, while collaboration is interpreted subjectively, shaped by internal states like presence, cognitive availability, and social awareness. Our core insight is that sensor signals serve as observable manifestations of subjective experiences in MR collaboration, and they can be captured through sensor data such as shared gaze, speech, spatial movement, and other system-logged performance metrics. We propose the Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links observable interaction patterns to users' subjective perceptions of collaboration and internal cognitive states through sensor-based indicators and task performance metrics. To validate this model, we conducted a study with 48 participants across 12 MR groups engaged in a collaborative image-sorting task. Our findings show a correlation between sensed behavior and perceived collaboration, particularly through shared attention and proximity.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2504.14135.pdf' target='_blank'>https://arxiv.org/pdf/2504.14135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Embley-Riches, Jianwei Liu, Simon Julier, Dimitrios Kanoulas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14135">Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity simulation is essential for robotics research, enabling safe and efficient testing of perception, control, and navigation algorithms. However, achieving both photorealistic rendering and accurate physics modeling remains a challenge. This paper presents a novel simulation framework--the Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced rendering capabilities with MuJoCo's high-precision physics simulation. Our approach enables realistic robotic perception while maintaining accurate physical interactions, facilitating benchmarking and dataset generation for vision-based robotics applications. The system supports complex environmental effects, such as smoke, fire, and water dynamics, which are critical for evaluating robotic performance under adverse conditions. We benchmark visual navigation and SLAM methods within our framework, demonstrating its utility for testing real-world robustness in controlled yet diverse scenarios. By bridging the gap between physics accuracy and photorealistic rendering, our framework provides a powerful tool for advancing robotics research and sim-to-real transfer.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2504.10414.pdf' target='_blank'>https://arxiv.org/pdf/2504.10414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya, Qixing Huang, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10414">HUMOTO: A 4D Dataset of Mocap Human Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of human-object interactions for motion generation, computer vision, and robotics applications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO captures interactions with 63 precisely modeled objects and 72 articulated parts. Our innovations include a scene-driven LLM scripting pipeline creating complete, purposeful tasks with natural progression, and a mocap-and-camera recording setup to effectively handle occlusions. Spanning diverse activities from cooking to outdoor picnics, HUMOTO preserves both physical accuracy and logical task flow. Professional artists rigorously clean and verify each sequence, minimizing foot sliding and object penetrations. We also provide benchmarks compared to other datasets. HUMOTO's comprehensive full-body motion and simultaneous multi-object interactions address key data-capturing challenges and provide opportunities to advance realistic human-object interaction modeling across research domains with practical applications in animation, robotics, and embodied AI systems. Project: https://jiaxin-lu.github.io/humoto/ .
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2504.03153.pdf' target='_blank'>https://arxiv.org/pdf/2504.03153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Natalie Tirabassi, Sathish A. P. Kumar, Sumit Jha, Arvind Ramanathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03153">MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MORAL (a multimodal reinforcement learning framework for decision making in autonomous laboratories) that enhances sequential decision-making in autonomous robotic laboratories through the integration of visual and textual inputs. Using the BridgeData V2 dataset, we generate fine-tuned image captions with a pretrained BLIP-2 vision-language model and combine them with visual features through an early fusion strategy. The fused representations are processed using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents. Experimental results demonstrate that multimodal agents achieve a 20% improvement in task completion rates and significantly outperform visual-only and textual-only baselines after sufficient training. Compared to transformer-based and recurrent multimodal RL models, our approach achieves superior performance in cumulative reward and caption quality metrics (BLEU, METEOR, ROUGE-L). These results highlight the impact of semantically aligned language cues in enhancing agent learning efficiency and generalization. The proposed framework contributes to the advancement of multimodal reinforcement learning and embodied AI systems in dynamic, real-world environments.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2504.02679.pdf' target='_blank'>https://arxiv.org/pdf/2504.02679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Bianchin, Robert Lefringhausen, Elisa Gaetan, Samuel Tesfazgi, Sandra Hirche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02679">A Set-Theoretic Robust Control Approach for Linear Quadratic Games with Unknown Counterparts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring robust decision-making in multi-agent systems is challenging when agents have distinct, possibly conflicting objectives and lack full knowledge of each other s strategies. This is apparent in safety-critical applications such as human-robot interaction and assisted driving, where uncertainty arises not only from unknown adversary strategies but also from external disturbances. To address this, the paper proposes a robust adaptive control approach based on linear quadratic differential games. Our method allows a controlled agent to iteratively refine its belief about the adversary strategy and disturbances using a set-membership approach, while simultaneously adapting its policy to guarantee robustness against the uncertain adversary policy and improve performance over time. We formally derive theoretical guarantees on the robustness of the proposed control scheme and its convergence to epsilon-Nash strategies. The effectiveness of our approach is demonstrated in a numerical simulation.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2504.00839.pdf' target='_blank'>https://arxiv.org/pdf/2504.00839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00839">Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2503.20916.pdf' target='_blank'>https://arxiv.org/pdf/2503.20916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cosima du Pasquier, Jennifer Grannen, Chuer Pan, Serin L. Huber, Aliyah Smith, Monroe Kennedy, Shuran Song, Dorsa Sadigh, Allison M. Okamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20916">A Study of Perceived Safety for Soft Robotics in Caregiving Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this project, we focus on human-robot interaction in caregiving scenarios like bathing, where physical contact is inevitable and necessary for proper task execution because force must be applied to the skin. Using finite element analysis, we designed a 3D-printed gripper combining positive and negative pressure for secure yet compliant handling. Preliminary tests showed it exerted a lower, more uniform pressure profile than a standard rigid gripper. In a user study, participants' trust in robots significantly increased after they experienced a brief bathing demonstration performed by a robotic arm equipped with the soft gripper. These results suggest that soft robotics can enhance perceived safety and acceptance in intimate caregiving scenarios.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2503.10307.pdf' target='_blank'>https://arxiv.org/pdf/2503.10307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgy Ponimatkin, Martin CÃ­fka, TomÃ¡Å¡ SouÄek, MÃ©dÃ©ric Fourmy, Yann LabbÃ©, Vladimir Petrik, Josef Sivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10307">6D Object Pose Tracking in Internet Videos for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We seek to extract a temporally consistent 6D pose trajectory of a manipulated object from an Internet instructional video. This is a challenging set-up for current 6D pose estimation methods due to uncontrolled capturing conditions, subtle but dynamic object motions, and the fact that the exact mesh of the manipulated object is not known. To address these challenges, we present the following contributions. First, we develop a new method that estimates the 6D pose of any object in the input image without prior knowledge of the object itself. The method proceeds by (i) retrieving a CAD model similar to the depicted object from a large-scale model database, (ii) 6D aligning the retrieved CAD model with the input image, and (iii) grounding the absolute scale of the object with respect to the scene. Second, we extract smooth 6D object trajectories from Internet videos by carefully tracking the detected objects across video frames. The extracted object trajectories are then retargeted via trajectory optimization into the configuration space of a robotic manipulator. Third, we thoroughly evaluate and ablate our 6D pose estimation method on YCB-V and HOPE-Video datasets as well as a new dataset of instructional videos manually annotated with approximate 6D object trajectories. We demonstrate significant improvements over existing state-of-the-art RGB 6D pose estimation methods. Finally, we show that the 6D object motion estimated from Internet videos can be transferred to a 7-axis robotic manipulator both in a virtual simulator as well as in a real world set-up. We also successfully apply our method to egocentric videos taken from the EPIC-KITCHENS dataset, demonstrating potential for Embodied AI applications.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2503.05833.pdf' target='_blank'>https://arxiv.org/pdf/2503.05833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias JÃ¼lg, Wolfram Burgard, Florian Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05833">Refined Policy Distillation: From VLA Generalists to RL Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action Models (VLAs) have demonstrated remarkable generalization capabilities in real-world experiments. However, their success rates are often not on par with expert policies, and they require fine-tuning when the setup changes. In this work, we introduce Refined Policy Distillation (RPD), a novel Reinforcement Learning (RL)-based policy refinement method that bridges this performance gap through a combination of on-policy RL with behavioral cloning. The core idea of RPD is to distill and refine VLAs into compact, high-performing expert policies by guiding the student policy during RL exploration using the actions of a teacher VLA, resulting in increased sample efficiency and faster convergence. We complement our method by fine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in simulation. While this is a key requirement for applying RL, it also yields new insights beyond existing studies on VLA performance in real-world settings. Our experimental results across various manipulation tasks show that RPD enables the RL student to learn expert policies that outperform the VLA teacher in both dense and sparse reward settings, while also achieving faster convergence than the RL baseline. Our approach is even robust to changes in camera perspective and can generalize to task variations that the underlying VLA cannot solve. Our code, dataset, VLA checkpoints, and videos are available at https://refined-policy-distillation.github.io
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2502.11777.pdf' target='_blank'>https://arxiv.org/pdf/2502.11777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddiqui Muhammad Yasir, Hyunsik Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11777">Deep Neural Networks for Accurate Depth Estimation with Latent Space Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth estimation plays a pivotal role in advancing human-robot interactions, especially in indoor environments where accurate 3D scene reconstruction is essential for tasks like navigation and object handling. Monocular depth estimation, which relies on a single RGB camera, offers a more affordable solution compared to traditional methods that use stereo cameras or LiDAR. However, despite recent progress, many monocular approaches struggle with accurately defining depth boundaries, leading to less precise reconstructions. In response to these challenges, this study introduces a novel depth estimation framework that leverages latent space features within a deep convolutional neural network to enhance the precision of monocular depth maps. The proposed model features dual encoder-decoder architecture, enabling both color-to-depth and depth-to-depth transformations. This structure allows for refined depth estimation through latent space encoding. To further improve the accuracy of depth boundaries and local features, a new loss function is introduced. This function combines latent loss with gradient loss, helping the model maintain the integrity of depth boundaries. The framework is thoroughly tested using the NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in complex indoor scenarios. The results clearly show that this approach effectively reduces depth ambiguities and blurring, making it a promising solution for applications in human-robot interaction and 3D scene reconstruction.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2502.04012.pdf' target='_blank'>https://arxiv.org/pdf/2502.04012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angus B. Clark, Xinran Wang, Alex Ranne, Nicolas Rojas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04012">Malleable Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This chapter is about the fundamentals of fabrication, control, and human-robot interaction of a new type of collaborative robotic manipulators, called malleable robots, which are based on adjustable architectures of varying stiffness for achieving high dexterity with lower mobility arms. Collaborative robots, or cobots, commonly integrate six or more degrees of freedom (DOF) in a serial arm in order to allow positioning in constrained spaces and adaptability across tasks. Increasing the dexterity of robotic arms has been indeed traditionally accomplished by increasing the number of degrees of freedom of the system; however, once a robotic task has been established (e.g., a pick-and-place operation), the motion of the end-effector can be normally achieved using less than 6-DOF (i.e., lower mobility). The aim of malleable robots is to close the technological gap that separates current cobots from achieving flexible, accessible manufacturing automation with a reduced number of actuators.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2501.07888.pdf' target='_blank'>https://arxiv.org/pdf/2501.07888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07888">Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8% over GPT-4o and 5.8% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6% performance advantage over GPT-4o and +24.9% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2412.19139.pdf' target='_blank'>https://arxiv.org/pdf/2412.19139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dejie Yang, Zijing Zhao, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19139">PlanLLM: Video Procedure Planning with Refinable Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video procedure planning, i.e., planning a sequence of action steps given the video frames of start and goal states, is an essential ability for embodied AI. Recent works utilize Large Language Models (LLMs) to generate enriched action step description texts to guide action step decoding. Although LLMs are introduced, these methods decode the action steps into a closed-set of one-hot vectors, limiting the model's capability of generalizing to new steps or tasks. Additionally, fixed action step descriptions based on world-level commonsense may contain noise in specific instances of visual states. In this paper, we propose PlanLLM, a cross-modal joint learning framework with LLMs for video procedure planning. We propose an LLM-Enhanced Planning module which fully uses the generalization ability of LLMs to produce free-form planning output and to enhance action step decoding. We also propose Mutual Information Maximization module to connect world-level commonsense of step descriptions and sample-specific information of visual states, enabling LLMs to employ the reasoning ability to generate step sequences. With the assistance of LLMs, our method can both closed-set and open vocabulary procedure planning tasks. Our PlanLLM achieves superior performance on three benchmarks, demonstrating the effectiveness of our designs.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2412.18335.pdf' target='_blank'>https://arxiv.org/pdf/2412.18335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Li, Weiqi Huang, Zan Wang, Wei Liang, Huijun Di, Feng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18335">FloNa: Floor Plan Guided Embodied Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally rely on floor plans to navigate in unfamiliar environments, as they are readily available, reliable, and provide rich geometrical guidance. However, existing visual navigation settings overlook this valuable prior knowledge, leading to limited efficiency and accuracy. To eliminate this gap, we introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the first attempt to incorporate floor plan into embodied visual navigation. While the floor plan offers significant advantages, two key challenges emerge: (1) handling the spatial inconsistency between the floor plan and the actual scene layout for collision-free navigation, and (2) aligning observed images with the floor plan sketch despite their distinct modalities. To address these challenges, we propose FloDiff, a novel diffusion policy framework incorporating a localization module to facilitate alignment between the current observation and the floor plan. We further collect $20k$ navigation episodes across $117$ scenes in the iGibson simulator to support the training and evaluation. Extensive experiments demonstrate the effectiveness and efficiency of our framework in unfamiliar scenes using floor plan knowledge. Project website: https://gauleejx.github.io/flona/.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2412.14480.pdf' target='_blank'>https://arxiv.org/pdf/2412.14480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saumya Saxena, Blake Buchanan, Chris Paxton, Peiqi Liu, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14480">GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment to answer a situated question with confidence. This problem remains challenging in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient planning and exploration. To address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantics-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps. We further demonstrate GraphEQA in multiple real-world home and office environments.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2412.10402.pdf' target='_blank'>https://arxiv.org/pdf/2412.10402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Ziliotto, Tommaso Campari, Luciano Serafini, Lamberto Ballan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10402">TANGO: Training-free Embodied AI Agents for Open-world Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2412.04700.pdf' target='_blank'>https://arxiv.org/pdf/2412.04700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Zebin Huang, Yutong Li, Xinliang Guo, Vincent Crocher, Ignacio Carlucho, Mustafa Suphi Erden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04700">SpasticMyoElbow: Physical Human-Robot Interaction Simulation Framework for Modelling Elbow Spasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic devices hold great potential for efficient and reliable assessment of neuromotor abnormalities in post-stroke patients. However, spasticity caused by stroke is still assessed manually in clinical settings. The limited and variable nature of data collected from patients has long posed a major barrier to quantitatively modelling spasticity with robotic measurements and fully validating robotic assessment techniques. This paper presents a simulation framework developed to support the design and validation of elbow spasticity models and mitigate data problems. The framework consists of a simulation environment of robot-assisted spasticity assessment, two motion controllers for the robot and human models, and a stretch reflex controller. Our framework allows simulation based on synthetic data without experimental data from human subjects. Using this framework, we replicated the constant-velocity stretch experiment typically used in robot-assisted spasticity assessment and evaluated four types of spasticity models. Our results show that a spasticity reflex model incorporating feedback on both muscle fibre velocity and length more accurately captures joint resistance characteristics during passive elbow stretching in spastic patients than a force-dependent model. When integrated with an appropriate spasticity model, this simulation framework has the potential to generate extensive datasets of virtual patients for future research on spasticity assessment.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2410.14868.pdf' target='_blank'>https://arxiv.org/pdf/2410.14868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sung-Wook Lee, Xuhui Kang, Yen-Ling Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14868">Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, diffusion policy has shown impressive results in handling multi-modal tasks in robotic manipulation. However, it has fundamental limitations in out-of-distribution failures that persist due to compounding errors and its limited capability to extrapolate. One way to address these limitations is robot-gated DAgger, an interactive imitation learning with a robot query system to actively seek expert help during policy rollout. While robot-gated DAgger has high potential for learning at scale, existing methods like Ensemble-DAgger struggle with highly expressive policies: They often misinterpret policy disagreements as uncertainty at multi-modal decision points. To address this problem, we introduce Diff-DAgger, an efficient robot-gated DAgger algorithm that leverages the training objective of diffusion policy. We evaluate Diff-DAgger across different robot tasks including stacking, pushing, and plugging, and show that Diff-DAgger improves the task failure prediction by 39.0%, the task completion rate by 20.6%, and reduces the wall-clock time by a factor of 7.8. We hope that this work opens up a path for efficiently incorporating expressive yet data-hungry policies into interactive robot learning settings. The project website is available at: https://diffdagger.github.io.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2410.13755.pdf' target='_blank'>https://arxiv.org/pdf/2410.13755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Cheng, Jonathan Eden, Bastien Berret, Atsushi Takagi, Etienne Burdet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13755">Interacting humans and robots can improve sensory prediction by adapting their viscoelasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To manipulate objects or dance together, humans and robots exchange energy and haptic information. While the exchange of energy in human-robot interaction has been extensively investigated, the underlying exchange of haptic information is not well understood. Here, we develop a computational model of the mechanical and sensory interactions between agents that can tune their viscoelasticity while considering their sensory and motor noise. The resulting stochastic-optimal-information-and-effort (SOIE) controller predicts how the exchange of haptic information and the performance can be improved by adjusting viscoelasticity. This controller was first implemented on a robot-robot experiment with a tracking task which showed its superior performance when compared to either stiff or compliant control. Importantly, the optimal controller also predicts how connected humans alter their muscle activation to improve haptic communication, with differentiated viscoelasticity adjustment to their own sensing noise and haptic perturbations. A human-robot experiment then illustrated the applicability of this optimal control strategy for robots, yielding improved tracking performance and effective haptic communication as the robot adjusted its viscoelasticity according to its own and the user's noise characteristics. The proposed SOIE controller may thus be used to improve haptic communication and collaboration of humans and robots.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2410.11377.pdf' target='_blank'>https://arxiv.org/pdf/2410.11377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresa Pekarek Rosin, Vanessa Hassouna, Xiaowen Sun, Luca Krohm, Henri-Leon Kordt, Michael Beetz, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11377">A Framework for Adapting Human-Robot Interaction to Diverse User Groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate natural and intuitive interactions with diverse user groups in real-world settings, social robots must be capable of addressing the varying requirements and expectations of these groups while adapting their behavior based on user feedback. While previous research often focuses on specific demographics, we present a novel framework for adaptive Human-Robot Interaction (HRI) that tailors interactions to different user groups and enables individual users to modulate interactions through both minor and major interruptions. Our primary contributions include the development of an adaptive, ROS-based HRI framework with an open-source code base. This framework supports natural interactions through advanced speech recognition and voice activity detection, and leverages a large language model (LLM) as a dialogue bridge. We validate the efficiency of our framework through module tests and system trials, demonstrating its high accuracy in age recognition and its robustness to repeated user inputs and plan changes.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2409.17702.pdf' target='_blank'>https://arxiv.org/pdf/2409.17702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonard BÃ¤rmann, Chad DeChant, Joana Plewnia, Fabian Peller-Konrad, Daniel Bauer, Tamim Asfour, Alex Waibel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17702">Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verbalization of robot experience, i.e., summarization of and question answering about a robot's past, is a crucial ability for improving human-robot interaction. Previous works applied rule-based systems or fine-tuned deep models to verbalize short (several-minute-long) streams of episodic data, limiting generalization and transferability. In our work, we apply large pretrained models to tackle this task with zero or few examples, and specifically focus on verbalizing life-long experiences. For this, we derive a tree-like data structure from episodic memory (EM), with lower levels representing raw perception and proprioception data, and higher levels abstracting events to natural language concepts. Given such a hierarchical representation built from the experience stream, we apply a large language model as an agent to interactively search the EM given a user's query, dynamically expanding (initially collapsed) tree nodes to find the relevant information. The approach keeps computational costs low even when scaling to months of robot experience data. We evaluate our method on simulated household robot data, human egocentric videos, and real-world robot recordings, demonstrating its flexibility and scalability.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2409.06124.pdf' target='_blank'>https://arxiv.org/pdf/2409.06124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Cheng, Shixian Shen, Ekaterina Ivanova, Gerolamo Carboni, Atsushi Takagi, Etienne Burdet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06124">Human Impedance Modulation to Improve Visuo-Haptic Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans activate muscles to shape the mechanical interaction with their environment, but can they harness this control mechanism to best sense the environment? We investigated how participants adapt their muscle activation to visual and haptic information when tracking a randomly moving target with a robotic interface. The results exhibit a differentiated effect of these sensory modalities, where participants' muscle cocontraction increases with the haptic noise and decreases with the visual noise, in apparent contradiction to previous results. These results can be explained, and reconciled with previous findings, when considering muscle spring like mechanics, where stiffness increases with cocontraction to regulate motion guidance. Increasing cocontraction to more closely follow the motion plan favors accurate visual over haptic information, while decreasing it avoids injecting visual noise and relies on accurate haptic information. We formulated this active sensing mechanism as the optimization of visuo-haptic information and effort. This OIE model can explain the adaptation of muscle activity to unimodal and multimodal sensory information when interacting with fixed or dynamic environments, or with another human, and can be used to optimize human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2409.03457.pdf' target='_blank'>https://arxiv.org/pdf/2409.03457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changfei Fu, Weinan Chen, Wenjun Xu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03457">FLAF: Focal Line and Feature-constrained Active View Planning for Visual Teach and Repeat</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents FLAF, a focal line and feature-constrained active view planning method for tracking failure avoidance in feature-based visual navigation of mobile robots. Our FLAF-based visual navigation is built upon a feature-based visual teach and repeat (VT\&R) framework, which supports many robotic applications by teaching a robot to navigate on various paths that cover a significant portion of daily autonomous navigation requirements. However, tracking failure in feature-based visual simultaneous localization and mapping (VSLAM) caused by textureless regions in human-made environments is still limiting VT\&R to be adopted in the real world. To address this problem, the proposed view planner is integrated into a feature-based visual SLAM system to build up an active VT\&R system that avoids tracking failure. In our system, a pan-tilt unit (PTU)-based active camera is mounted on the mobile robot. Using FLAF, the active camera-based VSLAM operates during the teaching phase to construct a complete path map and in the repeat phase to maintain stable localization. FLAF orients the robot toward more map points to avoid mapping failures during path learning and toward more feature-identifiable map points beneficial for localization while following the learned trajectory. Experiments in real scenarios demonstrate that FLAF outperforms the methods that do not consider feature-identifiability, and our active VT\&R system performs well in complex environments by effectively dealing with low-texture regions.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2409.01630.pdf' target='_blank'>https://arxiv.org/pdf/2409.01630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxiao Zhang, Xiangrui Kong, Thomas Braunl, Jin B. Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01630">SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose \textit{SafeEmbodAI}, a safety framework for integrating mobile robots into embodied AI systems. \textit{SafeEmbodAI} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2408.13153.pdf' target='_blank'>https://arxiv.org/pdf/2408.13153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sasha Wald, Kavya Puthuveetil, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13153">Do Mistakes Matter? Comparing Trust Responses of Different Age Groups to Errors Made by Physically Assistive Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trust is a key factor in ensuring acceptable human-robot interaction, especially in settings where robots may be assisting with critical activities of daily living. When practically deployed, robots are bound to make occasional mistakes, yet the degree to which these errors will impact a care recipient's trust in the robot, especially in performing physically assistive tasks, remains an open question. To investigate this, we conducted experiments where participants interacted with physically assistive robots which would occasionally make intentional mistakes while performing two different tasks: bathing and feeding. Our study considered the error response of two populations: younger adults at a university (median age 26) and older adults at an independent living facility (median age 83). We observed that the impact of errors on a users' trust in the robot depends on both their age and the task that the robot is performing. We also found that older adults tend to evaluate the robot on factors unrelated to the robot's performance, making their trust in the system more resilient to errors when compared to younger adults. Code and supplementary materials are available on our project webpage.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2408.10123.pdf' target='_blank'>https://arxiv.org/pdf/2408.10123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-Williams, Sethu Vijayakumar, Kun Shao, Laura Sevilla-Lara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10123">Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance, defined as the potential actions that an object offers, is crucial for embodied AI agents. For example, such knowledge directs an agent to grasp a knife by the handle for cutting or by the blade for safe handover. While existing approaches have made notable progress, affordance research still faces three key challenges: data scarcity, poor generalization, and real-world deployment. Specifically, there is a lack of large-scale affordance datasets with precise segmentation maps, existing models struggle to generalize across different domains or novel object and affordance classes, and little work demonstrates deployability in real-world scenarios. In this work, we address these issues by proposing a complete affordance learning system that (1) takes in egocentric videos and outputs precise affordance annotations without human labeling, (2) leverages geometric information and vision foundation models to improve generalization, and (3) introduces a framework that facilitates affordance-oriented robotic manipulation such as tool grasping and robot-to-human tool handover. Experimental results show that our model surpasses the state-of-the-art by 13.8% in mIoU, and the framework achieves 77.1% successful grasping among 179 trials, including evaluations on seen, unseen classes, and cluttered scenes. Project page: https://reagan1311.github.io/affgrasp.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2407.02220.pdf' target='_blank'>https://arxiv.org/pdf/2407.02220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangrui Kong, Wenxiao Zhang, Jin Hong, Thomas Braunl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02220">Embodied AI in Mobile Robots: Coverage Path Planning with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and solving mathematical problems, leading to advancements in various fields. We propose an LLM-embodied path planning framework for mobile agents, focusing on solving high-level coverage path planning issues and low-level control. Our proposed multi-layer architecture uses prompted LLMs in the path planning phase and integrates them with the mobile agents' low-level actuators. To evaluate the performance of various LLMs, we propose a coverage-weighted path planning metric to assess the performance of the embodied models. Our experiments show that the proposed framework improves LLMs' spatial inference abilities. We demonstrate that the proposed multi-layer framework significantly enhances the efficiency and accuracy of these tasks by leveraging the natural language understanding and generative capabilities of LLMs. Our experiments show that this framework can improve LLMs' 2D plane reasoning abilities and complete coverage path planning tasks. We also tested three LLM kernels: gpt-4o, gemini-1.5-flash, and claude-3.5-sonnet. The experimental results show that claude-3.5 can complete the coverage planning task in different scenarios, and its indicators are better than those of the other models.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2406.11769.pdf' target='_blank'>https://arxiv.org/pdf/2406.11769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Atanov, Jiawei Fu, Rishubh Singh, Isabella Yu, Andrew Spielberg, Amir Zamir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11769">Solving Vision Tasks with Simple Photoreceptors Instead of Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A de facto standard in solving computer vision problems is to use a common high-resolution camera and choose its placement on an agent (i.e., position and orientation) based on human intuition. On the other hand, extremely simple and well-designed visual sensors found throughout nature allow many organisms to perform diverse, complex behaviors. In this work, motivated by these examples, we raise the following questions: 1. How effective simple visual sensors are in solving vision tasks? 2. What role does their design play in their effectiveness? We explore simple sensors with resolutions as low as one-by-one pixel, representing a single photoreceptor First, we demonstrate that just a few photoreceptors can be enough to solve many tasks, such as visual navigation and continuous control, reasonably well, with performance comparable to that of a high-resolution camera. Second, we show that the design of these simple visual sensors plays a crucial role in their ability to provide useful information and successfully solve these tasks. To find a well-performing design, we present a computational design optimization algorithm and evaluate its effectiveness across different tasks and domains, showing promising results. Finally, we perform a human survey to evaluate the effectiveness of intuitive designs devised manually by humans, showing that the computationally found design is among the best designs in most cases.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2406.01915.pdf' target='_blank'>https://arxiv.org/pdf/2406.01915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonghan Lim, Sujani Patel, Alex Evans, John Pimley, Yifei Li, Ilya Kovalenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01915">Enhancing Human-Robot Collaborative Assembly in Manufacturing Systems Using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of human-robot collaboration has the ability to improve manufacturing system performance by leveraging the unique strengths of both humans and robots. On the shop floor, human operators contribute with their adaptability and flexibility in dynamic situations, while robots provide precision and the ability to perform repetitive tasks. However, the communication gap between human operators and robots limits the collaboration and coordination of human-robot teams in manufacturing systems. Our research presents a human-robot collaborative assembly framework that utilizes a large language model for enhancing communication in manufacturing environments. The framework facilitates human-robot communication by integrating voice commands through natural language for task management. A case study for an assembly task demonstrates the framework's ability to process natural language inputs and address real-time assembly challenges, emphasizing adaptability to language variation and efficiency in error resolution. The results suggest that large language models have the potential to improve human-robot interaction for collaborative manufacturing assembly applications.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2405.18860.pdf' target='_blank'>https://arxiv.org/pdf/2405.18860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Zhang, Dongjiang Li, Yihang Li, Zecui Zeng, Lin Zhao, Lei Sun, Yue Chen, Xuelong Wei, Yibing Zhan, Lusong Li, Xiaodong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18860">Empowering Embodied Manipulation: A Bimanual-Mobile Robot Manipulation Dataset for Household Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancements in embodied AI are increasingly enabling robots to tackle complex real-world tasks, such as household manipulation. However, the deployment of robots in these environments remains constrained by the lack of comprehensive bimanual-mobile robot manipulation data that can be learned. Existing datasets predominantly focus on single-arm manipulation tasks, while the few dual-arm datasets available often lack mobility features, task diversity, comprehensive sensor data, and robust evaluation metrics; they fail to capture the intricate and dynamic nature of household manipulation tasks that bimanual-mobile robots are expected to perform. To overcome these limitations, we propose BRMData, a Bimanual-mobile Robot Manipulation Dataset specifically designed for household applications. BRMData encompasses 10 diverse household tasks, including single-arm and dual-arm tasks, as well as both tabletop and mobile manipulations, utilizing multi-view and depth-sensing data information. Moreover, BRMData features tasks of increasing difficulty, ranging from single-object to multi-object grasping, non-interactive to human-robot interactive scenarios, and rigid-object to flexible-object manipulation, closely simulating real-world household applications. Additionally, we introduce a novel Manipulation Efficiency Score (MES) metric to evaluate both the precision and efficiency of robot manipulation methods in household tasks. We thoroughly evaluate and analyze the performance of advanced robot manipulation learning methods using our BRMData, aiming to drive the development of bimanual-mobile robot manipulation technologies. The dataset is now open-sourced and available at https://embodiedrobot.github.io/.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2404.19594.pdf' target='_blank'>https://arxiv.org/pdf/2404.19594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhad Nawaz, Shaoting Peng, Lars Lindemann, Nadia Figueroa, Nikolai Matni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19594">Reactive Temporal Logic-based Planning and Control for Interactive Robotic Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes. Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt. To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans. We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes. At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic~(STL). Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2404.06611.pdf' target='_blank'>https://arxiv.org/pdf/2404.06611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>J. Taery Kim, Archit Naik, Isuru Jayarathne, Sehoon Ha, Jouh Yeong Chew
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06611">Modeling social interaction dynamics using temporal graph networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating intelligent systems, such as robots, into dynamic group settings poses challenges due to the mutual influence of human behaviors and internal states. A robust representation of social interaction dynamics is essential for effective human-robot collaboration. Existing approaches often narrow their focus to facial expressions or speech, overlooking the broader context. We propose employing an adapted Temporal Graph Networks to comprehensively represent social interaction dynamics while enabling its practical implementation. Our method incorporates temporal multi-modal behavioral data including gaze interaction, voice activity and environmental context. This representation of social interaction dynamics is trained as a link prediction problem using annotated gaze interaction data. The F1-score outperformed the baseline model by 37.0%. This improvement is consistent for a secondary task of next speaker prediction which achieves an improvement of 29.0%. Our contributions are two-fold, including a model to representing social interaction dynamics which can be used for many downstream human-robot interaction tasks like human state inference and next speaker prediction. More importantly, this is achieved using a more concise yet efficient message passing method, significantly reducing it from 768 to 14 elements, while outperforming the baseline model.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2404.01702.pdf' target='_blank'>https://arxiv.org/pdf/2404.01702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Petr Vanc, Radoslav Skoviera, Karla Stepanova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01702">Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2404.00353.pdf' target='_blank'>https://arxiv.org/pdf/2404.00353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Ruo, Lorenzo Sabattini, Valeria Villani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00353">CBF-Based STL Motion Planning for Social Navigation in Crowded Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A motion planning methodology based on the combination of Control Barrier Functions (CBF) and Signal Temporal Logic (STL) is employed in this paper. This methodology allows task completion at any point within a specified time interval, considering a dynamic system subject to velocity constraints. In this work, we apply this approach into the context of Socially Responsible Navigation (SRN), introducing a rotation constraint. This constraint is designed to maintain the user within the robot's field of view (FOV), enhancing human-robot interaction with the concept of side-by-side human-robot companion. This angular constraint offers the possibility to customize social navigation to specific needs, thereby enabling safe SRN. Its validation is carried out through simulations demonstrating the system's effectiveness in adhering to spatio-temporal constraints, including those related to robot velocity, rotation, and the presence of static and dynamic obstacles.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2403.19795.pdf' target='_blank'>https://arxiv.org/pdf/2403.19795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Austin Narcomey, Nathan Tsoi, Ruta Desai, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19795">Learning Human Preferences Over Robot Behavior as Soft Planning Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preference learning has long been studied in Human-Robot Interaction (HRI) in order to adapt robot behavior to specific user needs and desires. Typically, human preferences are modeled as a scalar function; however, such a formulation confounds critical considerations on how the robot should behave for a given task, with desired -- but not required -- robot behavior. In this work, we distinguish between such required and desired robot behavior by leveraging a planning framework. Specifically, we propose a novel problem formulation for preference learning in HRI where various types of human preferences are encoded as soft planning constraints. Then, we explore a data-driven method to enable a robot to infer preferences by querying users, which we instantiate in rearrangement tasks in the Habitat 2.0 simulator. We show that the proposed approach is promising at inferring three types of preferences even under varying levels of noise in simulated user choices between potential robot behaviors. Our contributions open up doors to adaptable planning-based robot behavior in the future.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2403.15959.pdf' target='_blank'>https://arxiv.org/pdf/2403.15959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15959">Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks where robots must anticipate human intent, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human's intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP's ability to predict and adapt to a diverse set of dynamic human intents.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2403.06041.pdf' target='_blank'>https://arxiv.org/pdf/2403.06041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Xu, Rui Zhou, Yida Yin, Huidong Gao, Masayoshi Tomizuka, Jiachen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06041">MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors. We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics. We also presented experiments that demonstrate the capability of MATRIX to serve as data augmentation for imitation-based motion planning.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2403.04910.pdf' target='_blank'>https://arxiv.org/pdf/2403.04910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karan Muvvala, Andrew M. Wells, Morteza Lahijanian, Lydia E. Kavraki, Moshe Y. Vardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04910">Stochastic Games for Interactive Manipulation Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become more prevalent, the complexity of robot-robot, robot-human, and robot-environment interactions increases. In these interactions, a robot needs to consider not only the effects of its own actions, but also the effects of other agents' actions and the possible interactions between agents. Previous works have considered reactive synthesis, where the human/environment is modeled as a deterministic, adversarial agent; as well as probabilistic synthesis, where the human/environment is modeled via a Markov chain. While they provide strong theoretical frameworks, there are still many aspects of human-robot interaction that cannot be fully expressed and many assumptions that must be made in each model. In this work, we propose stochastic games as a general model for human-robot interaction, which subsumes the expressivity of all previous representations. In addition, it allows us to make fewer modeling assumptions and leads to more natural and powerful models of interaction. We introduce the semantics of this abstraction and show how existing tools can be utilized to synthesize strategies to achieve complex tasks with guarantees. Further, we discuss the current computational limitations and improve the scalability by two orders of magnitude by a new way of constructing models for PRISM-games.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2403.03462.pdf' target='_blank'>https://arxiv.org/pdf/2403.03462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ayub, Chrystopher Nehaniv, Kerstin Dautenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03462">Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans. We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months. Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2403.01068.pdf' target='_blank'>https://arxiv.org/pdf/2403.01068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philippe Nadeau, Miguel Rogel Garcia, Emmett Wise, Jonathan Kelly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01068">Automated Continuous Force-Torque Sensor Bias Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Six axis force-torque sensors are commonly attached to the wrist of serial robots to measure the external forces and torques acting on the robot's end-effector. These measurements are used for load identification, contact detection, and human-robot interaction amongst other applications. Typically, the measurements obtained from the force-torque sensor are more accurate than estimates computed from joint torque readings, as the former is independent of the robot's dynamic and kinematic models. However, the force-torque sensor measurements are affected by a bias that drifts over time, caused by the compounding effects of temperature changes, mechanical stresses, and other factors. In this work, we present a pipeline that continuously estimates the bias and the drift of the bias of a force-torque sensor attached to the wrist of a robot. The first component of the pipeline is a Kalman filter that estimates the kinematic state (position, velocity, and acceleration) of the robot's joints. The second component is a kinematic model that maps the joint-space kinematics to the task-space kinematics of the force-torque sensor. Finally, the third component is a Kalman filter that estimates the bias and the drift of the bias of the force-torque sensor assuming that the inertial parameters of the gripper attached to the distal end of the force-torque sensor are known with certainty.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2402.12067.pdf' target='_blank'>https://arxiv.org/pdf/2402.12067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Laurenz Wiskott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12067">Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation requires a whole range of capabilities. A crucial one of these is the ability of an agent to determine its own location and heading in an environment. Prior works commonly assume this information as given, or use methods which lack a suitable inductive bias and accumulate error over time. In this work, we show how the method of slow feature analysis (SFA), inspired by neuroscience research, overcomes both limitations by generating interpretable representations of visual data that encode location and heading of an agent. We employ SFA in a modern reinforcement learning context, analyse and compare representations and illustrate where hierarchical SFA can outperform other feature extractors on navigation tasks.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2401.15762.pdf' target='_blank'>https://arxiv.org/pdf/2401.15762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhin Farhad Riya, Shahinul Hoque, Xiaopeng Zhao, Jinyuan Stella Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15762">Smart Driver Monitoring Robotic System to Enhance Road Safety : A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future of transportation is being shaped by technology, and one revolutionary step in improving road safety is the incorporation of robotic systems into driver monitoring infrastructure. This literature review explores the current landscape of driver monitoring systems, ranging from traditional physiological parameter monitoring to advanced technologies such as facial recognition to steering analysis. Exploring the challenges faced by existing systems, the review then investigates the integration of robots as intelligent entities within this framework. These robotic systems, equipped with artificial intelligence and sophisticated sensors, not only monitor but actively engage with the driver, addressing cognitive and emotional states in real-time. The synthesis of existing research reveals a dynamic interplay between human and machine, offering promising avenues for innovation in adaptive, personalized, and ethically responsible human-robot interactions for driver monitoring. This review establishes a groundwork for comprehending the intricacies and potential avenues within this dynamic field. It encourages further investigation and advancement at the intersection of human-robot interaction and automotive safety, introducing a novel direction. This involves various sections detailing technological enhancements that can be integrated to propose an innovative and improved driver monitoring system.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2401.15760.pdf' target='_blank'>https://arxiv.org/pdf/2401.15760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahinul Hoque, Farhin Farhad Riya, Jinyuan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15760">HRI Challenges Influencing Low Usage of Robotic Systems in Disaster Response and Rescue Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The breakthrough in AI and Machine Learning has brought a new revolution in robotics, resulting in the construction of more sophisticated robotic systems. Not only can these robotic systems benefit all domains, but also can accomplish tasks that seemed to be unimaginable a few years ago. From swarms of autonomous small robots working together to more very heavy and large objects, to seemingly indestructible robots capable of going to the harshest environments, we can see robotic systems designed for every task imaginable. Among them, a key scenario where robotic systems can benefit is in disaster response scenarios and rescue operations. Robotic systems are capable of successfully conducting tasks such as removing heavy materials, utilizing multiple advanced sensors for finding objects of interest, moving through debris and various inhospitable environments, and not the least have flying capabilities. Even with so much potential, we rarely see the utilization of robotic systems in disaster response scenarios and rescue missions. Many factors could be responsible for the low utilization of robotic systems in such scenarios. One of the key factors involve challenges related to Human-Robot Interaction (HRI) issues. Therefore, in this paper, we try to understand the HRI challenges involving the utilization of robotic systems in disaster response and rescue operations. Furthermore, we go through some of the proposed robotic systems designed for disaster response scenarios and identify the HRI challenges of those systems. Finally, we try to address the challenges by introducing ideas from various proposed research works.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2312.02010.pdf' target='_blank'>https://arxiv.org/pdf/2312.02010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02010">Towards Learning a Generalist Model for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation. We conduct extensive experiments to evaluate the performance and generalizability of our model. The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA. Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN. Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2311.18703.pdf' target='_blank'>https://arxiv.org/pdf/2311.18703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Jarne Ornia, Giannis Delimpaltadakis, Jens Kober, Javier Alonso-Mora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18703">Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularisation) to randomise their actions in favor of exploration. This often makes it challenging for other agents and humans to predict an agent's behavior, triggering unsafe scenarios (e.g. in human-robot interaction). We propose a novel method to induce predictable behavior in RL agents, termed Predictability-Aware RL (PARL), employing the agent's trajectory entropy rate to quantify predictability. Our method maximizes a linear combination of a standard discounted reward and the negative entropy rate, thus trading off optimality with predictability. We show how the entropy rate can be formally cast as an average reward, how entropy-rate value functions can be estimated from a learned model and incorporate this in policy-gradient algorithms, and demonstrate how this approach produces predictable (near-optimal) policies in tasks inspired by human-robot use-cases.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2311.18473.pdf' target='_blank'>https://arxiv.org/pdf/2311.18473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Cai, Teng Wang, Guangran Cheng, Lele Xu, Changyin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18473">DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, learning-based approaches have demonstrated significant promise in addressing intricate navigation tasks. Traditional methods for training deep neural network navigation policies rely on meticulously designed reward functions or extensive teleoperation datasets as navigation demonstrations. However, the former is often confined to simulated environments, and the latter demands substantial human labor, making it a time-consuming process. Our vision is for robots to autonomously learn navigation skills and adapt their behaviors to environmental changes without any human intervention. In this work, we discuss the self-supervised navigation problem and present Dynamic Graph Memory (DGMem), which facilitates training only with on-board observations. With the help of DGMem, agents can actively explore their surroundings, autonomously acquiring a comprehensive navigation policy in a data-efficient manner without external feedback. Our method is evaluated in photorealistic 3D indoor scenes, and empirical studies demonstrate the effectiveness of DGMem.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2311.05401.pdf' target='_blank'>https://arxiv.org/pdf/2311.05401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandra Rossi, Patrick Holthaus, Gabriella Lakatos, SÃ­lvia Moros, Lewis Riches
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05401">SCRITA 2023: Trust, Acceptance and Social Cues in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This workshop focused on identifying the challenges and dynamics between people and robots to foster short interactions and long-lasting relationships in different fields, from educational, service, collaborative, companion, care-home and medical robotics. For that, this workshop facilitated a discussion about people's trust towards robots "in the field", inviting workshop participants to contribute their past experiences and lessons learnt.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2310.17722.pdf' target='_blank'>https://arxiv.org/pdf/2310.17722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17722">Large Language Models as Generalizable Policies for Embodied Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2310.15321.pdf' target='_blank'>https://arxiv.org/pdf/2310.15321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Barbara Sienkiewicz, Gabriela Sejnova, Paul Gajewski, Michal Vavrecka, Bipin Indurkhya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15321">How language of interaction affects the user perception of a robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spoken language is the most natural way for a human to communicate with a robot. It may seem intuitive that a robot should communicate with users in their native language. However, it is not clear if a user's perception of a robot is affected by the language of interaction.
  We investigated this question by conducting a study with twenty-three native Czech participants who were also fluent in English. The participants were tasked with instructing the Pepper robot on where to place objects on a shelf. The robot was controlled remotely using the Wizard-of-Oz technique. We collected data through questionnaires, video recordings, and a post-experiment feedback session. The results of our experiment show that people perceive an English-speaking robot as more intelligent than a Czech-speaking robot (z = 18.00, p-value = 0.02). This finding highlights the influence of language on human-robot interaction. Furthermore, we discuss the feedback obtained from the participants via the post-experiment sessions and its implications for HRI design.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2310.08669.pdf' target='_blank'>https://arxiv.org/pdf/2310.08669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao-Hung Hubert Tsai, Vansh Dhar, Jialu Li, Bowen Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08669">Multimodal Large Language Model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation. In contrast, our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2310.07937.pdf' target='_blank'>https://arxiv.org/pdf/2310.07937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangguo Yu, Qihao Yuan, Kailai Li, Hamidreza Kasaei, Ming Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07937">Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual target navigation is a critical capability for autonomous robots operating in unknown environments, particularly in human-robot interaction scenarios. While classical and learning-based methods have shown promise, most existing approaches lack common-sense reasoning and are typically designed for single-robot settings, leading to reduced efficiency and robustness in complex environments. To address these limitations, we introduce Co-NavGPT, a novel framework that integrates a Vision Language Model (VLM) as a global planner to enable common-sense multi-robot visual target navigation. Co-NavGPT aggregates sub-maps from multiple robots with diverse viewpoints into a unified global map, encoding robot states and frontier regions. The VLM uses this information to assign frontiers across the robots, facilitating coordinated and efficient exploration. Experiments on the Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT outperforms existing baselines in terms of success rate and navigation efficiency, without requiring task-specific training. Ablation studies further confirm the importance of semantic priors from the VLM. We also validate the framework in real-world scenarios using quadrupedal robots. Supplementary video and code are available at: https://sites.google.com/view/co-navgpt2.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2309.10001.pdf' target='_blank'>https://arxiv.org/pdf/2309.10001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junan Lin, Zhichao Sun, Enjie Cao, Taein Kwon, Mahdi Rad, Marc Pollefeys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10001">CaSAR: Contact-aware Skeletal Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeletal Action recognition from an egocentric view is important for applications such as interfaces in AR/VR glasses and human-robot interaction, where the device has limited resources. Most of the existing skeletal action recognition approaches use 3D coordinates of hand joints and 8-corner rectangular bounding boxes of objects as inputs, but they do not capture how the hands and objects interact with each other within the spatial context. In this paper, we present a new framework called Contact-aware Skeletal Action Recognition (CaSAR). It uses novel representations of hand-object interaction that encompass spatial information: 1) contact points where the hand joints meet the objects, 2) distant points where the hand joints are far away from the object and nearly not involved in the current action. Our framework is able to learn how the hands touch or stay away from the objects for each frame of the action sequence, and use this information to predict the action class. We demonstrate that our approach achieves the state-of-the-art accuracy of 91.3% and 98.4% on two public datasets, H2O and FPHA, respectively.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2309.04316.pdf' target='_blank'>https://arxiv.org/pdf/2309.04316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonard BÃ¤rmann, Rainer Kartmann, Fabian Peller-Konrad, Jan Niehues, Alex Waibel, Tamim Asfour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04316">Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2309.03708.pdf' target='_blank'>https://arxiv.org/pdf/2309.03708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manal Helal, Patrick Holthaus, Gabriella Lakatos, Farshid Amirabdollahian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03708">Chat Failures and Troubles: Reasons and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2308.07241.pdf' target='_blank'>https://arxiv.org/pdf/2308.07241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07241">Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accomplishing household tasks requires to plan step-by-step actions considering the consequences of previous actions. However, the state-of-the-art embodied agents often make mistakes in navigating the environment and interacting with proper objects due to imperfect learning by imitating experts or algorithmic planners without such knowledge. To improve both visual navigation and object interaction, we propose to consider the consequence of taken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory) that incorporates semantic context (e.g., appropriate objects to interact with) in a sequence of actions, and the changed spatial arrangement and states of interacted objects (e.g., location that the object has been moved to) in inferring the subsequent actions. We empirically show that the agent with the proposed CAPEAM achieves state-of-the-art performance in various metrics using a challenging interactive instruction following benchmark in both seen and unseen environments by large margins (up to +10.70% in unseen env.).
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2308.01555.pdf' target='_blank'>https://arxiv.org/pdf/2308.01555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Zhang, Wei Chai, Jiankun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01555">Mani-GPT: A Generative Model for Interactive Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, human dialogues are multi-round and diverse. Furthermore, human instructions can be unclear and human responses are unrestricted. Interactive robots face difficulties in understanding human intents and generating suitable strategies for assisting individuals through manipulation. In this article, we propose Mani-GPT, a Generative Pre-trained Transformer (GPT) for interactive robotic manipulation. The proposed model has the ability to understand the environment through object information, understand human intent through dialogues, generate natural language responses to human input, and generate appropriate manipulation plans to assist the human. This makes the human-robot interaction more natural and humanized. In our experiment, Mani-GPT outperforms existing algorithms with an accuracy of 84.6% in intent recognition and decision-making for actions. Furthermore, it demonstrates satisfying performance in real-world dialogue tests with users, achieving an average response accuracy of 70%.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2307.00114.pdf' target='_blank'>https://arxiv.org/pdf/2307.00114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ayub, Chrystopher L. Nehaniv, Kerstin Dautenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00114">A Personalized Household Assistive Robot that Learns and Creates New Breakfast Options through Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For robots to assist users with household tasks, they must first learn about the tasks from the users. Further, performing the same task every day, in the same way, can become boring for the robot's user(s), therefore, assistive robots must find creative ways to perform tasks in the household. In this paper, we present a cognitive architecture for a household assistive robot that can learn personalized breakfast options from its users and then use the learned knowledge to set up a table for breakfast. The architecture can also use the learned knowledge to create new breakfast options over a longer period of time. The proposed cognitive architecture combines state-of-the-art perceptual learning algorithms, computational implementation of cognitive models of memory encoding and learning, a task planner for picking and placing objects in the household, a graphical user interface (GUI) to interact with the user and a novel approach for creating new breakfast options using the learned knowledge. The architecture is integrated with the Fetch mobile manipulator robot and validated, as a proof-of-concept system evaluation in a large indoor environment with multiple kitchen objects. Experimental results demonstrate the effectiveness of our architecture to learn personalized breakfast options from the user and generate new breakfast options never learned by the robot.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2305.16332.pdf' target='_blank'>https://arxiv.org/pdf/2305.16332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ayub, Zachary De Francesco, Patrick Holthaus, Chrystopher L. Nehaniv, Kerstin Dautenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16332">Continual Learning through Human-Robot Interaction: Human Perceptions of a Continual Learning Robot in Repeated Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For long-term deployment in dynamic real-world environments, assistive robots must continue to learn and adapt to their environments. Researchers have developed various computational models for continual learning (CL) that can allow robots to continually learn from limited training data, and avoid forgetting previous knowledge. While these CL models can mitigate forgetting on static, systematically collected datasets, it is unclear how human users might perceive a robot that continually learns over multiple interactions with them. In this paper, we developed a system that integrates CL models for object recognition with a Fetch mobile manipulator robot and allows human participants to directly teach and test the robot over multiple sessions. We conducted an in-person study with 60 participants that interacted with our system in 300 sessions (5 sessions per participant). We conducted a between-subject study with three different CL models to understand human perceptions of continual learning robots over multiple sessions. Our results suggest that participants' perceptions of trust, competence, and usability of a continual learning robot significantly decrease over multiple sessions if the robot forgets previously learned objects. However, the perceived task load on participants for teaching and testing the robot remains the same over multiple sessions even if the robot forgets previously learned objects. Our results also indicate that state-of-the-art CL models might perform unreliably when applied on robots interacting with human participants. Further, continual learning robots are not perceived as very trustworthy or competent by human participants, regardless of the underlying continual learning model or the session number.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2304.14823.pdf' target='_blank'>https://arxiv.org/pdf/2304.14823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joyjit Mukherjee, Ankit Chatterjee, Shreeshan Jena, Nitesh Kumar, Suriya Prakash Muthukrishnan, Sitikantha Roy, Shubhendu Bhasin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14823">Adaptive Gravity Compensation Control of a Cable-Driven Upper-Arm Soft Exosuit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an adaptive gravity compensation (AGC) control strategy for a cable-driven upper-limb exosuit intended to assist the wearer with lifting tasks. Unlike most model-based control techniques used for this human-robot interaction task, the proposed control design does not assume knowledge of the anthropometric parameters of the wearer's arm and the payload. Instead, the uncertainties in human arm parameters, such as mass, length, and payload, are estimated online using an indirect adaptive control law that compensates for the gravity moment about the elbow joint. Additionally, the AGC controller is agnostic to the desired joint trajectory followed by the human arm. For the purpose of controller design, the human arm is modeled using a 1-DOF manipulator model. Further, a cable-driven actuator model is proposed that maps the assistive elbow torque to the actuator torque. The performance of the proposed method is verified through a co-simulation, wherein the control input realized in MATLAB is applied to the human bio-mechanical model in OpenSim under varying payload conditions. Significant reductions in human effort in terms of human muscle torque and metabolic cost are observed with the proposed control strategy. Further, simulation results show that the performance of the AGC controller converges to that of the gravity compensation (GC) controller, demonstrating the efficacy of AGC-based online parameter learning.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2304.12288.pdf' target='_blank'>https://arxiv.org/pdf/2304.12288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanibek Rysbek, Ki Hwan Oh, Afagh Mehri Shervedani, Timotej Klemencic, Milos Zefran, Barbara Di Eugenio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12288">Robots Taking Initiative in Collaborative Object Manipulation: Lessons from Physical Human-Human Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical Human-Human Interaction (pHHI) involves the use of multiple sensory modalities. Studies of communication through spoken utterances and gestures are well established, but communication through force signals is not well understood. In this paper, we focus on investigating the mechanisms employed by humans during the negotiation through force signals, and how the robot can communicate task goals, comprehend human intent, and take the lead as needed. To achieve this, we formulate a task that requires active force communication and propose a taxonomy that extends existing literature. Also, we conducted a study to observe how humans behave during collaborative manipulation tasks. An important contribution of this work is the novel features based on force-kinematic signals that demonstrate predictive power to recognize symbolic human intent. Further, we show the feasibility of developing a real-time intent classifier based on the novel features and speculate the role it plays in high-level robot controllers for physical Human-Robot Interaction (pHRI). This work provides important steps to achieve more human-like fluid interaction in physical co-manipulation tasks that are applicable and not limited to humanoid, assistive robots, and human-in-the-loop automation.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2304.01334.pdf' target='_blank'>https://arxiv.org/pdf/2304.01334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramzi Abou Chahine, Steven Vasquez, Pooyan Fazli, Hasti Seifi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01334">Clustering Social Touch Gestures for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social touch provides a rich non-verbal communication channel between humans and robots. Prior work has identified a set of touch gestures for human-robot interaction and described them with natural language labels (e.g., stroking, patting). Yet, no data exists on the semantic relationships between the touch gestures in users' minds. To endow robots with touch intelligence, we investigated how people perceive the similarities of social touch labels from the literature. In an online study, 45 participants grouped 36 social touch labels based on their perceived similarities and annotated their groupings with descriptive names. We derived quantitative similarities of the gestures from these groupings and analyzed the similarities using hierarchical clustering. The analysis resulted in 9 clusters of touch gestures formed around the social, emotional, and contact characteristics of the gestures. We discuss the implications of our results for designing and evaluating touch sensing and interactions with social robots.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2303.18176.pdf' target='_blank'>https://arxiv.org/pdf/2303.18176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hatice Gunes, Nikhil Churamani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18176">Affective Computing for Human-Robot Interaction Research: Four Critical Lessons for the Hitchhiker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social Robotics and Human-Robot Interaction (HRI) research relies on different Affective Computing (AC) solutions for sensing, perceiving and understanding human affective behaviour during interactions. This may include utilising off-the-shelf affect perception models that are pre-trained on popular affect recognition benchmarks and directly applied to situated interactions. However, the conditions in situated human-robot interactions differ significantly from the training data and settings of these models. Thus, there is a need to deepen our understanding of how AC solutions can be best leveraged, customised and applied for situated HRI. This paper, while critiquing the existing practices, presents four critical lessons to be noted by the hitchhiker when applying AC for HRI research. These lessons conclude that: (i) The six basic emotions categories are irrelevant in situated interactions, (ii) Affect recognition accuracy (%) improvements are unimportant, (iii) Affect recognition does not generalise across contexts, and (iv) Affect recognition alone is insufficient for adaptation and personalisation. By describing the background and the context for each lesson, and demonstrating how these lessons have been learnt, this paper aims to enable the hitchhiker to successfully and insightfully leverage AC solutions for advancing HRI research.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2303.00423.pdf' target='_blank'>https://arxiv.org/pdf/2303.00423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Weber, Wolfgang Fuhl, Enkelejda Kasneci, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00423">Multiperspective Teaching of Unknown Objects via Shared-gaze-based Multimodal Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For successful deployment of robots in multifaceted situations, an understanding of the robot for its environment is indispensable. With advancing performance of state-of-the-art object detectors, the capability of robots to detect objects within their interaction domain is also enhancing. However, it binds the robot to a few trained classes and prevents it from adapting to unfamiliar surroundings beyond predefined scenarios. In such scenarios, humans could assist robots amidst the overwhelming number of interaction entities and impart the requisite expertise by acting as teachers. We propose a novel pipeline that effectively harnesses human gaze and augmented reality in a human-robot collaboration context to teach a robot novel objects in its surrounding environment. By intertwining gaze (to guide the robot's attention to an object of interest) with augmented reality (to convey the respective class information) we enable the robot to quickly acquire a significant amount of automatically labeled training data on its own. Training in a transfer learning fashion, we demonstrate the robot's capability to detect recently learned objects and evaluate the influence of different machine learning models and learning procedures as well as the amount of training data involved. Our multimodal approach proves to be an efficient and natural way to teach the robot novel objects based on a few instances and allows it to detect classes for which no training dataset is available. In addition, we make our dataset publicly available to the research community, which consists of RGB and depth data, intrinsic and extrinsic camera parameters, along with regions of interest.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2302.12532.pdf' target='_blank'>https://arxiv.org/pdf/2302.12532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Liu, Xiaolin Wei, Bo Li, Junjie Cao, Yu-Kun Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12532">Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of the existing audio-driven 3D facial animation methods suffered from the lack of detailed facial expression and head pose, resulting in unsatisfactory experience of human-robot interaction. In this paper, a novel pose-controllable 3D facial animation synthesis method is proposed by utilizing hierarchical audio-vertex attention. To synthesize real and detailed expression, a hierarchical decomposition strategy is proposed to encode the audio signal into both a global latent feature and a local vertex-wise control feature. Then the local and global audio features combined with vertex spatial features are used to predict the final consistent facial animation via a graph convolutional neural network by fusing the intrinsic spatial topology structure of the face model and the corresponding semantic feature of the audio. To accomplish pose-controllable animation, we introduce a novel pose attribute augmentation method by utilizing the 2D talking face technique. Experimental results indicate that the proposed method can produce more realistic facial expressions and head posture movements. Qualitative and quantitative experiments show that the proposed method achieves competitive performance against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2302.03971.pdf' target='_blank'>https://arxiv.org/pdf/2302.03971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Holthaus, Trenton Schulz, Gabriella Lakatos, Rebekka Soma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03971">Communicative Robot Signals: Presenting a New Typology for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new typology for classifying signals from robots when they communicate with humans. For inspiration, we use ethology, the study of animal behaviour and previous efforts from literature as guides in defining the typology. The typology is based on communicative signals that consist of five properties: the origin where the signal comes from, the deliberateness of the signal, the signal's reference, the genuineness of the signal, and its clarity (i.e., how implicit or explicit it is). Using the accompanying worksheet, the typology is straightforward to use to examine communicative signals from previous human-robot interactions and provides guidance for designers to use the typology when designing new robot behaviours.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2302.00191.pdf' target='_blank'>https://arxiv.org/pdf/2302.00191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Lew, Sydney Thompson, Nathan Tsoi, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00191">Shutter, the Robot Photographer: Leveraging Behavior Trees for Public, In-the-Wild Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying interactive systems in-the-wild requires adaptability to situations not encountered in lab environments. Our work details our experience about the impact of architecture choice on behavior reusability and reactivity while deploying a public interactive system. In particular, we introduce Shutter, a robot photographer and a platform for public interaction. In designing Shutter's architecture, we focused on adaptability for in-the-wild deployment, while developing a reusable platform to facilitate future research in public human-robot interaction. We find that behavior trees allow reactivity, especially in group settings, and encourage designing reusable behaviors.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2301.09899.pdf' target='_blank'>https://arxiv.org/pdf/2301.09899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Petr Vanc, Jan Kristof Behrens, Karla Stepanova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09899">Context-aware robot control using gesture episodes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots became a popular tool for increasing productivity in partly automated manufacturing plants. Intuitive robot teaching methods are required to quickly and flexibly adapt the robot programs to new tasks. Gestures have an essential role in human communication. However, in human-robot-interaction scenarios, gesture-based user interfaces are so far used rarely, and if they employ a one-to-one mapping of gestures to robot control variables. In this paper, we propose a method that infers the user's intent based on gesture episodes, the context of the situation, and common sense. The approach is evaluated in a simulated table-top manipulation setting. We conduct deterministic experiments with simulated users and show that the system can even handle personal preferences of each user.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2210.05556.pdf' target='_blank'>https://arxiv.org/pdf/2210.05556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Terry Yue Zhuo, Yaqing Liao, Yuecheng Lei, Lizhen Qu, Gerard de Melo, Xiaojun Chang, Yazhou Ren, Zenglin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05556">ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ViLPAct, a novel vision-language benchmark for human activity planning. It is designed for a task where embodied AI agents can reason and forecast future actions of humans based on video clips about their initial activities and intents in text. The dataset consists of 2.9k videos from \charades extended with intents via crowdsourcing, a multi-choice question test set, and four strong baselines. One of the baselines implements a neurosymbolic approach based on a multi-modal knowledge base (MKB), while the other ones are deep generative models adapted from recent state-of-the-art (SOTA) methods. According to our extensive experiments, the key challenges are compositional generalization and effective use of information from both modalities.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2112.14602.pdf' target='_blank'>https://arxiv.org/pdf/2112.14602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianzhao Li, Ostap Okhrin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.14602">Modified DDPG car-following model with a real-world human driving experience with CARLA simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the autonomous driving field, fusion of human knowledge into Deep Reinforcement Learning (DRL) is often based on the human demonstration recorded in a simulated environment. This limits the generalization and the feasibility of application in real-world traffic. We propose a two-stage DRL method to train a car-following agent, that modifies the policy by leveraging the real-world human driving experience and achieves performance superior to the pure DRL agent. Training a DRL agent is done within CARLA framework with Robot Operating System (ROS). For evaluation, we designed different driving scenarios to compare the proposed two-stage DRL car-following agent with other agents. After extracting the "good" behavior from the human driver, the agent becomes more efficient and reasonable, which makes this autonomous agent more suitable for Human-Robot Interaction (HRI) traffic.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2109.07201.pdf' target='_blank'>https://arxiv.org/pdf/2109.07201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Jeanne Kirschner, Henning Mayer, Lisa Burr, Nico Mansfeld, Saeed Abdolshah, Sami Haddadin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.07201">Expectable Motion Unit: Avoiding Hazards From Human Involuntary Motions in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotics, many control and planning schemes have been developed to ensure human physical safety in human-robot interaction. The human psychological state and the expectation towards the robot, however, are typically neglected. Even if the robot behaviour is regarded as biomechanically safe, humans may still react with a rapid involuntary motion (IM) caused by a startle or surprise. Such sudden, uncontrolled motions can jeopardize safety and should be prevented by any means. In this letter, we propose the Expectable Motion Unit (EMU), which ensures that a certain probability of IM occurrence is not exceeded in a typical HRI setting. Based on a model of IM occurrence generated through an experiment with 29 participants, we establish the mapping between robot velocity, robot-human distance, and the relative frequency of IM occurrence. This mapping is processed towards a real-time capable robot motion generator that limits the robot velocity during task execution if necessary. The EMU is combined in a holistic safety framework that integrates both the physical and psychological safety knowledge. A validation experiment showed that the EMU successfully avoids human IM in five out of six cases.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2106.06604.pdf' target='_blank'>https://arxiv.org/pdf/2106.06604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Gleirscher, Radu Calinescu, James Douthwaite, Benjamin Lesage, Colin Paterson, Jonathan Aitken, Rob Alexander, James Law
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.06604">Verified Synthesis of Optimal Safety Controllers for Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a tool-supported approach for the synthesis, verification and validation of the control software responsible for the safety of the human-robot interaction in manufacturing processes that use collaborative robots. In human-robot collaboration, software-based safety controllers are used to improve operational safety, e.g., by triggering shutdown mechanisms or emergency stops to avoid accidents. Complex robotic tasks and increasingly close human-robot interaction pose new challenges to controller developers and certification authorities. Key among these challenges is the need to assure the correctness of safety controllers under explicit (and preferably weak) assumptions. Our controller synthesis, verification and validation approach is informed by the process, risk analysis, and relevant safety regulations for the target application. Controllers are selected from a design space of feasible controllers according to a set of optimality criteria, are formally verified against correctness criteria, and are translated into executable code and validated in a digital twin. The resulting controller can detect the occurrence of hazards, move the process into a safe state, and, in certain circumstances, return the process to an operational state from which it can resume its original task. We show the effectiveness of our software engineering approach through a case study involving the development of a safety controller for a manufacturing work cell equipped with a collaborative robot.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2102.13008.pdf' target='_blank'>https://arxiv.org/pdf/2102.13008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Kumar Thakur, MD-Nazmus Samin Sunbeam, Vinicius G. Goecks, Ellen Novoseller, Ritwik Bera, Vernon J. Lawhern, Gregory M. Gremillion, John Valasek, Nicholas R. Waytowich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2102.13008">Imitation Learning with Human Eye Gaze via Multi-Objective Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Approaches for teaching learning agents via human demonstrations have been widely studied and successfully applied to multiple domains. However, the majority of imitation learning work utilizes only behavioral information from the demonstrator, i.e. which actions were taken, and ignores other useful information. In particular, eye gaze information can give valuable insight towards where the demonstrator is allocating visual attention, and holds the potential to improve agent performance and generalization. In this work, we propose Gaze Regularized Imitation Learning (GRIL), a novel context-aware, imitation learning architecture that learns concurrently from both human demonstrations and eye gaze to solve tasks where visual attention provides important context. We apply GRIL to a visual navigation task, in which an unmanned quadrotor is trained to search for and navigate to a target vehicle in a photorealistic simulated environment. We show that GRIL outperforms several state-of-the-art gaze-based imitation learning algorithms, simultaneously learns to predict human visual attention, and generalizes to scenarios not present in the training data. Supplemental videos and code can be found at https://sites.google.com/view/gaze-regularized-il/.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2007.03340.pdf' target='_blank'>https://arxiv.org/pdf/2007.03340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Gleirscher, Radu Calinescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.03340">Safety Controller Synthesis for Collaborative Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot collaboration (HRC), software-based automatic safety controllers (ASCs) are used in various forms (e.g. shutdown mechanisms, emergency brakes, interlocks) to improve operational safety. Complex robotic tasks and increasingly close human-robot interaction pose new challenges to ASC developers and certification authorities. Key among these challenges is the need to assure the correctness of ASCs under reasonably weak assumptions. To address this need, we introduce and evaluate a tool-supported ASC synthesis method for HRC in manufacturing. Our ASC synthesis is: (i) informed by the manufacturing process, risk analysis, and regulations; (ii) formally verified against correctness criteria; and (iii) selected from a design space of feasible controllers according to a set of optimality criteria. The synthesised ASC can detect the occurrence of hazards, move the process into a safe state, and, in certain circumstances, return the process to an operational state from which it can resume its original task.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2509.12008.pdf' target='_blank'>https://arxiv.org/pdf/2509.12008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Song, Cesare Tonola, Stefano Savazzi, Sanaz Kianoush, Nicola Pedrocchi, Stephan Sigg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12008">Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly prevalent in both homes and industrial settings, the demand for intuitive and efficient human-machine interaction continues to rise. Gesture recognition offers an intuitive control method that does not require physical contact with devices and can be implemented using various sensing technologies. Wireless solutions are particularly flexible and minimally invasive. While camera-based vision systems are commonly used, they often raise privacy concerns and can struggle in complex or poorly lit environments. In contrast, radar sensing preserves privacy, is robust to occlusions and lighting, and provides rich spatial data such as distance, relative velocity, and angle. We present a gesture-controlled robotic arm using mm-wave radar for reliable, contactless motion recognition. Nine gestures are recognized and mapped to real-time commands with precision. Case studies are conducted to demonstrate the system practicality, performance and reliability for gesture-based robotic manipulation. Unlike prior work that treats gesture recognition and robotic control separately, our system unifies both into a real-time pipeline for seamless, contactless human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2509.06031.pdf' target='_blank'>https://arxiv.org/pdf/2509.06031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhui Huang, Yuhe Gong, Changsheng Li, Xingguang Duan, Luis Figueredo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06031">ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ZLATTE, a geometry-aware, learning-free framework for language-driven trajectory reshaping in human-robot interaction. Unlike prior learning-based methods, ZLATTE leverages Vision-Language Models to register objects as geometric primitives and employs a Large Language Model to translate natural language instructions into explicit geometric and kinematic constraints. These constraints are integrated into a potential field optimization to adapt initial trajectories while preserving feasibility and safety. A multi-agent strategy further enhances robustness under complex or conflicting commands. Simulation and real-world experiments demonstrate that ZLATTE achieves smoother, safer, and more interpretable trajectory modifications compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2508.20959.pdf' target='_blank'>https://arxiv.org/pdf/2508.20959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Curtis C. Johnson, Daniel Webb, David Hill, Marc D. Killpack
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20959">Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling tactile sensing for robust whole-body manipulation is a significant challenge, often limited by wiring complexity, data throughput, and system reliability. This paper presents a complete architecture designed to overcome these barriers. Our approach pairs open-source, fabric-based sensors with custom readout electronics that reduce signal crosstalk to less than 3.3% through hardware-based mitigation. Critically, we introduce a novel, daisy-chained SPI bus topology that avoids the practical limitations of common wireless protocols and the prohibitive wiring complexity of USB hub-based systems. This architecture streams synchronized data from over 8,000 taxels across 1 square meter of sensing area at update rates exceeding 50 FPS, confirming its suitability for real-time control. We validate the system's efficacy in a whole-body grasping task where, without feedback, the robot's open-loop trajectory results in an uncontrolled application of force that slowly crushes a deformable cardboard box. With real-time tactile feedback, the robot transforms this motion into a gentle, stable grasp, successfully manipulating the object without causing structural damage. This work provides a robust and well-characterized platform to enable future research in advanced whole-body control and physical human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2508.18539.pdf' target='_blank'>https://arxiv.org/pdf/2508.18539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaijie Xu, Clark Verbrugge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18539">Adaptive Visual Navigation Assistant in 3D RPGs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In complex 3D game environments, players rely on visual affordances to spot map transition points. Efficient identification of such points is important to client-side auto-mapping, and provides an objective basis for evaluating map cue presentation. In this work, we formalize the task of detecting traversable Spatial Transition Points (STPs)-connectors between two sub regions-and selecting the singular Main STP (MSTP), the unique STP that lies on the designer-intended critical path toward the player's current macro-objective, from a single game frame, proposing this as a new research focus. We introduce a two-stage deep-learning pipeline that first detects potential STPs using Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses local and global visual features. Both stages benefit from parameter-efficient adapters, and we further introduce an optional retrieval-augmented fusion step. Our primary goal is to establish the feasibility of this problem and set baseline performance metrics. We validate our approach on a custom-built, diverse dataset collected from five Action RPG titles. Our experiments reveal a key trade-off: while full-network fine-tuning produces superior STP detection with sufficient data, adapter-only transfer is significantly more robust and effective in low-data scenarios and for the MSTP selection task. By defining this novel problem, providing a baseline pipeline and dataset, and offering initial insights into efficient model adaptation, we aim to contribute to future AI-driven navigation aids and data-informed level-design tools.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2508.18539.pdf' target='_blank'>https://arxiv.org/pdf/2508.18539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaijie Xu, Clark Verbrugge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18539">Adaptive Visual Navigation Assistant in 3D RPGs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In complex 3D game environments, players rely on visual affordances to spot map transition points. Efficient identification of such points is important to client-side auto-mapping, and provides an objective basis for evaluating map cue presentation. In this work, we formalize the task of detecting traversable Spatial Transition Points (STPs)-connectors between two sub regions-and selecting the singular Main STP (MSTP), the unique STP that lies on the designer-intended critical path toward the player's current macro-objective, from a single game frame, proposing this as a new research focus. We introduce a two-stage deep-learning pipeline that first detects potential STPs using Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses local and global visual features. Both stages benefit from parameter-efficient adapters, and we further introduce an optional retrieval-augmented fusion step. Our primary goal is to establish the feasibility of this problem and set baseline performance metrics. We validate our approach on a custom-built, diverse dataset collected from five Action RPG titles. Our experiments reveal a key trade-off: while full-network fine-tuning produces superior STP detection with sufficient data, adapter-only transfer is significantly more robust and effective in low-data scenarios and for the MSTP selection task. By defining this novel problem, providing a baseline pipeline and dataset, and offering initial insights into efficient model adaptation, we aim to contribute to future AI-driven navigation aids and data-informed level-design tools.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2508.12637.pdf' target='_blank'>https://arxiv.org/pdf/2508.12637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shankaranarayanan H, Satyapreet Singh Yadav, Adithya Krishna, Ajay Vikram P, Mahesh Mehendale, Chetan Singh Thakur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12637">HOMI: Ultra-Fast EdgeAI platform for Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras offer significant advantages for edge robotics applications due to their asynchronous operation and sparse, event-driven output, making them well-suited for tasks requiring fast and efficient closed-loop control, such as gesture-based human-robot interaction. Despite this potential, existing event processing solutions remain limited, often lacking complete end-to-end implementations, exhibiting high latency, and insufficiently exploiting event data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI accelerator. We have developed hardware-optimized pre-processing pipelines supporting both constant-time and constant-event modes for histogram accumulation, linear and exponential time surfaces. Our general-purpose implementation caters to both accuracy-driven and low-latency applications. HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when configured for high accuracy operation and provides a throughput of 1000 fps for low-latency configuration. The hardware-optimised pipeline maintains a compact memory footprint and utilises only 33% of the available LUT resources on the FPGA, leaving ample headroom for further latency reduction, model parallelisation, multi-task deployments, or integration of more complex architectures.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2508.10561.pdf' target='_blank'>https://arxiv.org/pdf/2508.10561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Gargano, Jasin Machkour, Mimma Nardelli, Enzo Pasquale Scilingo, Michael Muma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10561">Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Affective Computing, a key challenge lies in reliably linking subjective emotional experiences with objective physiological markers. This preliminary study addresses the issue of reproducibility by identifying physiological features from cardiovascular and electrodermal signals that are associated with continuous self-reports of arousal levels. Using the Continuously Annotated Signal of Emotion dataset, we analyzed 164 features extracted from cardiac and electrodermal signals of 30 participants exposed to short emotion-evoking videos. Feature selection was performed using the Terminating-Random Experiments (T-Rex) method, which performs variable selection systematically controlling a user-defined target False Discovery Rate. Remarkably, among all candidate features, only two electrodermal-derived features exhibited reproducible and statistically significant associations with arousal, achieving a 100\% confirmation rate. These results highlight the necessity of rigorous reproducibility assessments in physiological features selection, an aspect often overlooked in Affective Computing. Our approach is particularly promising for applications in safety-critical environments requiring trustworthy and reliable white box models, such as mental disorder recognition and human-robot interaction systems.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2508.08831.pdf' target='_blank'>https://arxiv.org/pdf/2508.08831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo-Hsun Chen, Nevindu M. Batagoda, Dan Negrut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08831">DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2508.05497.pdf' target='_blank'>https://arxiv.org/pdf/2508.05497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico ScarÃ¬, Olger Siebinga, Arkady Zgonnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05497">Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicle's driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2508.05310.pdf' target='_blank'>https://arxiv.org/pdf/2508.05310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jelle Luijkx, Zlatan AjanoviÄ, Laura Ferranti, Jens Kober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05310">ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2508.05208.pdf' target='_blank'>https://arxiv.org/pdf/2508.05208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Ngo, Rachel, Ramchurn, Roma Patel, Alan Chamberlain, Ayse Kucukyilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05208">Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an evaluation of 18 children's in-the-wild experiences with the autonomous robot arm performer NED (Never-Ending Dancer) within the Thingamabobas installation, showcased across the UK. We detail NED's design, including costume, behaviour, and human interactions, all integral to the installation. Our observational analysis revealed three key challenges in child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of robot expressivity and reciprocity, and 3) Unmet expectations. Our findings show that children are naturally curious, and adept at interacting with a robotic art performer. However, our observations emphasise the critical need to optimise human-robot interaction (HRI) systems through careful consideration of audience's capabilities, perceptions, and expectations, within the performative arts context, to enable engaging and meaningful experiences, especially for young audiences.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2508.03514.pdf' target='_blank'>https://arxiv.org/pdf/2508.03514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pavlos Panagiotidis, Victor Zhi Heung Ngo, Sean Myatt, Roma Patel, Rachel Ramchurn, Alan Chamberlain, Ayse Kucukyilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03514">Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose theatre-in-the-loop, a framework for developing expressive robot behaviours tailored to artistic performance through a director-guided puppeteering workflow. Leveraging theatrical methods, we use narrative objectives to direct a puppeteer in generating improvised robotic gestures that convey specific emotions. These improvisations are captured and curated to build a dataset of reusable movement templates for standalone playback in future autonomous performances. Initial trials demonstrate the feasibility of this approach, illustrating how the workflow enables precise sculpting of robotic gestures into coherent emotional arcs while revealing challenges posed by the robot's mechanical constraints. We argue that this practice-led framework provides a model for interdisciplinary teams creating socially expressive robot behaviours, contributing to (1) theatre as an interactive training ground for human-robot interaction and (2) co-creation methodologies between humans and machines.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2508.02505.pdf' target='_blank'>https://arxiv.org/pdf/2508.02505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Lombardi, Carmela Calabrese, Davide Ghiglino, Caterina Foglino, Davide De Tommaso, Giulia Da Lisca, Lorenzo Natale, Agnieszka Wykowska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02505">Would you let a humanoid play storytelling with your child? A usability study on LLM-powered narrative Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in human-robot interaction research lies in developing robotic systems that can effectively perceive and interpret social cues, facilitating natural and adaptive interactions. In this work, we present a novel framework for enhancing the attention of the iCub humanoid robot by integrating advanced perceptual abilities to recognise social cues, understand surroundings through generative models, such as ChatGPT, and respond with contextually appropriate social behaviour. Specifically, we propose an interaction task implementing a narrative protocol (storytelling task) in which the human and the robot create a short imaginary story together, exchanging in turn cubes with creative images placed on them. To validate the protocol and the framework, experiments were performed to quantify the degree of usability and the quality of experience perceived by participants interacting with the system. Such a system can be beneficial in promoting effective human robot collaborations, especially in assistance, education and rehabilitation scenarios where the social awareness and the robot responsiveness play a pivotal role.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2508.01723.pdf' target='_blank'>https://arxiv.org/pdf/2508.01723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danyang Li, Zenghui Yang, Guangpeng Qi, Songtao Pang, Guangyong Shang, Qiang Ma, Zheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01723">OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grounding natural language instructions to visual observations is fundamental for embodied agents operating in open-world environments. Recent advances in visual-language mapping have enabled generalizable semantic representations by leveraging vision-language models (VLMs). However, these methods often fall short in aligning free-form language commands with specific scene instances, due to limitations in both instance-level semantic consistency and instruction interpretation. We present OpenMap, a zero-shot open-vocabulary visual-language map designed for accurate instruction grounding in navigation tasks. To address semantic inconsistencies across views, we introduce a Structural-Semantic Consensus constraint that jointly considers global geometric structure and vision-language similarity to guide robust 3D instance-level aggregation. To improve instruction interpretation, we propose an LLM-assisted Instruction-to-Instance Grounding module that enables fine-grained instance selection by incorporating spatial context and expressive target descriptions. We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and instruction-to-target retrieval tasks. Experimental results show that OpenMap outperforms state-of-the-art baselines in zero-shot settings, demonstrating the effectiveness of our method in bridging free-form language and 3D perception for embodied navigation.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2507.18820.pdf' target='_blank'>https://arxiv.org/pdf/2507.18820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachel Ringe, Robin Nolte, Nima Zargham, Robert Porzel, Rainer Malaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18820">MetaMorph -- A Metamodelling Approach For Robot Morphology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot appearance crucially shapes Human-Robot Interaction (HRI) but is typically described via broad categories like anthropomorphic, zoomorphic, or technical. More precise approaches focus almost exclusively on anthropomorphic features, which fail to classify robots across all types, limiting the ability to draw meaningful connections between robot design and its effect on interaction. In response, we present MetaMorph, a comprehensive framework for classifying robot morphology. Using a metamodeling approach, MetaMorph was synthesized from 222 robots in the IEEE Robots Guide, offering a structured method for comparing visual features. This model allows researchers to assess the visual distances between robot models and explore optimal design traits tailored to different tasks and contexts.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2507.16124.pdf' target='_blank'>https://arxiv.org/pdf/2507.16124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16124">Benchmarking LLM Privacy Recognition for Social Robot Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2507.07818.pdf' target='_blank'>https://arxiv.org/pdf/2507.07818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Xu, Jiaqian Yu, Xiongfeng Peng, Yiwei Chen, Weiming Li, Jaewook Yoo, Sunghyun Chunag, Dongwook Lee, Daehyun Ji, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07818">MoSE: Skill-by-Skill Mixture-of-Experts Learning for Embodied Autonomous Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To meet the growing demand for smarter, faster, and more efficient embodied AI solutions, we introduce a novel Mixture-of-Expert (MoE) method that significantly boosts reasoning and learning efficiency for embodied autonomous systems. General MoE models demand extensive training data and complex optimization, which limits their applicability in embodied AI such as autonomous driving (AD) and robotic manipulation. In this work, we propose a skill-oriented MoE called MoSE, which mimics the human learning and reasoning process skill-by-skill, step-by-step. We introduce a skill-oriented routing mechanism that begins with defining and annotating specific skills, enabling experts to identify the necessary competencies for various scenarios and reasoning tasks, thereby facilitating skill-by-skill learning. To better align with multi-step planning in human reasoning and in end-to-end driving models, we build a hierarchical skill dataset and pretrain the router to encourage the model to think step-by-step. Unlike other multi-round dialogues, MoSE integrates valuable auxiliary tasks (e.g. perception-prediction-planning for AD, and high-level and low-level planning for robots) in one single forward process without introducing any extra computational cost. With less than 3B sparsely activated parameters, our model effectively grows more diverse expertise and outperforms models on both AD corner-case reasoning tasks and robot reasoning tasks with less than 40% of the parameters.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2507.01843.pdf' target='_blank'>https://arxiv.org/pdf/2507.01843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01843">MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixture-of-Experts (MoE) approaches have recently gained traction in robotics applications due to their ability to dynamically allocate computational resources and specialize sub-networks for distinct tasks or environmental contexts, enabling more efficient decision-making. Such systems often comprise sparsely activated experts combined under a single monolithic architecture and require a well-configured internal routing mechanism, which does not allow for selective low-level expert and router customization and requires additional training. We propose MoIRA, an architecture-agnostic modular MoE framework designed to coordinate existing experts with an external text-based router. MoIRA incorporates two zero-shot routing options: embedding-based similarity and prompt-driven language model inference. In our experiments, we choose large Vision-Language-Action models, gr00t-N1 and $Ï_0$, as the underlying experts, and train low-rank adapters for low-overhead inference. We evaluate MoIRA on various GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it consistently outperforms generalist models and competes with other MoE pipelines. Additionally, we analyse the robustness of the proposed approach to the variations of the instructions. While relying solely on textual descriptions of tasks and experts, MoIRA demonstrates the practical viability of modular deployment with precise, low-effort routing and provides an alternative, scalable foundation for future multi-expert robotic systems.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2506.17462.pdf' target='_blank'>https://arxiv.org/pdf/2506.17462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernard Lange, Anil Yildiz, Mansur Arief, Shehryar Khattak, Mykel Kochenderfer, Georgios Georgakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17462">General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed data flows, limiting generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot integrations typically depend on pre-mapped spaces, hard-coded representations, and myopic exploration. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose navigation framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools available within modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query the robotic modules, reason over multimodal inputs, and select appropriate navigation actions. This approach enables robust navigation and reasoning in previously unmapped environments, providing a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves state-of-the-art performance, demonstrating effective exploration, navigation, and embodied question answering without relying on handcrafted plans, fixed input representations, or pre-existing maps.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2506.13583.pdf' target='_blank'>https://arxiv.org/pdf/2506.13583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernhard Hilpert, Muhan Hou, Kim Baraka, Joost Broekens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13583">Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) agents often exhibit learning behaviors that are not intuitively interpretable by human observers, which can result in suboptimal feedback in collaborative teaching settings. Yet, how humans perceive and interpret RL agent's learning behavior is largely unknown. In a bottom-up approach with two experiments, this work provides a data-driven understanding of the factors of human observers' understanding of the agent's learning process. A novel, observation-based paradigm to directly assess human inferences about agent learning was developed. In an exploratory interview study (\textit{N}=9), we identify four core themes in human interpretations: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second confirmatory study (\textit{N}=34) applied an expanded version of the paradigm across two tasks (navigation/manipulation) and two RL algorithms (tabular/function approximation). Analyses of 816 responses confirmed the reliability of the paradigm and refined the thematic framework, revealing how these themes evolve over time and interrelate. Our findings provide a human-centered understanding of how people make sense of agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2506.10462.pdf' target='_blank'>https://arxiv.org/pdf/2506.10462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana MÃ¼ller, Sabina Jeschke, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10462">Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the impact of a group-adaptive conversation design in two socially interactive agents (SIAs) through two real-world studies. Both SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped with a conversational artificial intelligence (CAI) backend combining hybrid retrieval and generative models. The studies were carried out in an in-the-wild setting with a total of $N = 188$ participants who interacted with the SIAs - in dyads, triads or larger groups - at a German museum. Although the results did not reveal a significant effect of the group-sensitive conversation design on perceived satisfaction, the findings provide valuable insights into the challenges of adapting CAI for multi-party interactions and across different embodiments (robot vs.\ virtual agent), highlighting the need for multimodal strategies beyond linguistic pluralization. These insights contribute to the fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and broader Human-Machine Interaction (HMI), providing insights for future research on effective dialogue adaptation in group settings.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2506.08892.pdf' target='_blank'>https://arxiv.org/pdf/2506.08892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tauhid Tanjim, Jonathan St. George, Kevin Ching, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08892">Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human-robot interaction (HRI) field has recognized the importance of enabling robots to interact with teams. Human teams rely on effective communication for successful collaboration in time-sensitive environments. Robots can play a role in enhancing team coordination through real-time assistance. Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to investigate how multimodal robot communication in action teams affects workload and human perception of robots. We explore team collaboration in a medical training scenario where a robotic crash cart (RCC) provides verbal and non-verbal cues to help users remember to perform iterative tasks and search for supplies. Our findings show that verbal cues for object search tasks and visual cues for task reminders reduce team workload and increase perceived ease of use and perceived usefulness more effectively than a robot with no feedback. Our work contributes to multimodal interaction research in the HRI field, highlighting the need for more human-robot teaming research to understand best practices for integrating collaborative robots in time-sensitive environments such as in hospitals, search and rescue, and manufacturing applications.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2506.01563.pdf' target='_blank'>https://arxiv.org/pdf/2506.01563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfan Bao, Yan Pan, Tianhu Peng, Dimitrios Kanoulas, Chengxu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01563">Hierarchical Intention-Aware Expressive Motion Generation for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot interaction requires robots to identify human intentions and generate expressive, socially appropriate motions in real-time. Existing approaches often rely on fixed motion libraries or computationally expensive generative models. We propose a hierarchical framework that combines intention-aware reasoning via in-context learning (ICL) with real-time motion generation using diffusion models. Our system introduces structured prompting with confidence scoring, fallback behaviors, and social context awareness to enable intention refinement and adaptive response. Leveraging large-scale motion datasets and efficient latent-space denoising, the framework generates diverse, physically plausible gestures suitable for dynamic humanoid interactions. Experimental validation on a physical platform demonstrates the robustness and social alignment of our method in realistic scenarios.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2506.00098.pdf' target='_blank'>https://arxiv.org/pdf/2506.00098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Welte, Rania Rayyes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00098">Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous manipulation is a crucial yet highly complex challenge in humanoid robotics, demanding precise, adaptable, and sample-efficient learning methods. As humanoid robots are usually designed to operate in human-centric environments and interact with everyday objects, mastering dexterous manipulation is critical for real-world deployment. Traditional approaches, such as reinforcement learning and imitation learning, have made significant strides, but they often struggle due to the unique challenges of real-world dexterous manipulation, including high-dimensional control, limited training data, and covariate shift. This survey provides a comprehensive overview of these challenges and reviews existing learning-based methods for real-world dexterous manipulation, spanning imitation learning, reinforcement learning, and hybrid approaches. A promising yet underexplored direction is interactive imitation learning, where human feedback actively refines a robots behavior during training. While interactive imitation learning has shown success in various robotic tasks, its application to dexterous manipulation remains limited. To address this gap, we examine current interactive imitation learning techniques applied to other robotic tasks and discuss how these methods can be adapted to enhance dexterous manipulation. By synthesizing state-of-the-art research, this paper highlights key challenges, identifies gaps in current methodologies, and outlines potential directions for leveraging interactive imitation learning to improve dexterous robotic skills.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2505.24257.pdf' target='_blank'>https://arxiv.org/pdf/2505.24257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24257">Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An embodied AI assistant operating on egocentric video must integrate spatial cues across time - for instance, determining where an object A, glimpsed a few moments ago lies relative to an object B encountered later. We introduce Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs by posing questions about object pairs that are not co-visible in the same frame. We evaluated seven state-of-the-art VLMs and found that models lag behind human performance by 28%, with steeper declines in accuracy (60% to 30 %) as the temporal gap widens. Our analysis further reveals that providing trajectories or bird's-eye-view projections to VLMs results in only marginal improvements, whereas providing oracle 3D coordinates leads to a substantial 20% performance increase. This highlights a core bottleneck of multi-frame VLMs in constructing and maintaining 3D scene representations over time from visual signals. Disjoint-3DQA therefore sets a clear, measurable challenge for long-horizon spatial reasoning and aims to catalyze future research at the intersection of vision, language, and embodied AI.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2505.20503.pdf' target='_blank'>https://arxiv.org/pdf/2505.20503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Lisondra, Beno Benhabib, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20503">Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2505.16384.pdf' target='_blank'>https://arxiv.org/pdf/2505.16384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoming Huang, Musen Zhang, Jianxin Yang, Zhen Li, Jinkai Li, Yao Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16384">MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze can provide rich information on human psychological activities, and has garnered significant attention in the field of Human-Robot Interaction (HRI). However, existing gaze estimation methods merely predict either the gaze direction or the Point-of-Gaze (PoG) on the screen, failing to provide sufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze analysis in 3D space. Moreover, the variations of eye shape and structure among individuals also impede the generalization capability of these methods. In this study, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an efficient calibration module, to predict the 6-DoF gaze information that is applicable for the real-word HRI. Our basic model encodes both the directional and positional features from facial images, and predicts gaze results with dedicated information flow and multiple decoders. To reduce the impact of individual variations, we propose a novel calibration module, namely Easy-Calibration, to fine-tune the basic model with subject-specific data, which is efficient to implement without the need of a screen. Experimental results demonstrate that our method achieves state-of-the-art performance on the public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2505.10872.pdf' target='_blank'>https://arxiv.org/pdf/2505.10872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Jiang, Chuhao Zhou, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10872">REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2505.01998.pdf' target='_blank'>https://arxiv.org/pdf/2505.01998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoliang Chen, Xin Yu, Le Chang, Yunhe Huang, Jiashuai He, Shibo Zhang, Jin Li, Likai Lin, Ziyu Zeng, Xianling Tu, Shuyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01998">A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations, covering far-field localization, weak signal detection, and multilingual speech recognition, demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2504.05748.pdf' target='_blank'>https://arxiv.org/pdf/2504.05748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tri Tung Nguyen Nguyen, Quang Tien Dam, Dinh Tuan Tran, Joo-Ho Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05748">When Less Is More: A Sparse Facial Motion Structure For Listening Motion Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human behavior modeling is critical for successful human-robot interaction. Current state-of-the-art approaches for predicting listening head behavior during dyadic conversations employ continuous-to-discrete representations, where continuous facial motion sequence is converted into discrete latent tokens. However, non-verbal facial motion presents unique challenges owing to its temporal variance and multi-modal nature. State-of-the-art discrete motion token representation struggles to capture underlying non-verbal facial patterns making training the listening head inefficient with low-fidelity generated motion. This study proposes a novel method for representing and predicting non-verbal facial motion by encoding long sequences into a sparse sequence of keyframes and transition frames. By identifying crucial motion steps and interpolating intermediate frames, our method preserves the temporal structure of motion while enhancing instance-wise diversity during the learning process. Additionally, we apply this novel sparse representation to the task of listening head prediction, demonstrating its contribution to improving the explanation of facial motion patterns.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2504.02724.pdf' target='_blank'>https://arxiv.org/pdf/2504.02724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen, David MÃ¼ller, Agon Serifi, Ruben Grandia, Georg Wiedebach, Michael A. Hopkins, Espen Knoop, Moritz BÃ¤cher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02724">Autonomous Human-Robot Interaction via Operator Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperated robotic characters can perform expressive interactions with humans, relying on the operators' experience and social intuition. In this work, we propose to create autonomous interactive robots, by training a model to imitate operator data. Our model is trained on a dataset of human-robot interactions, where an expert operator is asked to vary the interactions and mood of the robot, while the operator commands as well as the pose of the human and robot are recorded. Our approach learns to predict continuous operator commands through a diffusion process and discrete commands through a classifier, all unified within a single transformer architecture. We evaluate the resulting model in simulation and with a user study on the real system. We show that our method enables simple autonomous human-robot interactions that are comparable to the expert-operator baseline, and that users can recognize the different robot moods as generated by our model. Finally, we demonstrate a zero-shot transfer of our model onto a different robotic platform with the same operator interface.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2503.15518.pdf' target='_blank'>https://arxiv.org/pdf/2503.15518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Tang, Chao Tang, Steven Gong, Thomas M. Kwok, Yue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15518">Robot Character Generation and Adaptive Human-Robot Interaction with Personality Shaping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for designing emotionally agile robots with dynamic personalities and memory-based learning, with the aim of performing adaptive and non-deterministic interactions with humans while conforming to shared social understanding. While existing work has largely focused on emotion recognition and static response systems, many approaches rely on sentiment analysis and action mapping frameworks that are pre-defined with limited dimensionality and fixed configurations, lacking the flexibility of dynamic personality traits and memory-enabled adaptation. Other systems are often restricted to limited modes of expression and fail to develop a causal relationship between human behavior and the robot's proactive physical actions, resulting in constrained adaptability and reduced responsiveness in complex, dynamic interactions. Our methodology integrates the Big Five Personality Traits, Appraisal Theory, and abstracted memory layers through Large Language Models (LLMs). The LLM generates a parameterized robot personality based on the Big Five, processes human language and sentiments, evaluates human behavior using Appraisal Theory, and generates emotions and selects appropriate actions adapted by historical context over time. We validated the framework by testing three robots with distinct personalities in identical background contexts and found that personality, appraisal, and memory influence the adaptability of human-robot interactions. The impact of the individual components was further validated through ablation tests. We conclude that this system enables robots to engage in meaningful and personalized interactions with users, and holds significant potential for applications in domains such as pet robots, assistive robots, educational robots, and collaborative functional robots, where cultivating tailored relationships and enriching user experiences are essential.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2503.14328.pdf' target='_blank'>https://arxiv.org/pdf/2503.14328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renzi Wang, Mathijs Schuurmans, Panagiotis Patrinos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14328">Risk-Sensitive Model Predictive Control for Interaction-Aware Planning -- A Sequential Convexification Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers risk-sensitive model predictive control for stochastic systems with a decision-dependent distribution. This class of systems is commonly found in human-robot interaction scenarios. We derive computationally tractable convex upper bounds to both the objective function, and to frequently used penalty terms for collision avoidance, allowing us to efficiently solve the generally nonconvex optimal control problem as a sequence of convex problems. Simulations of a robot navigating a corridor demonstrate the effectiveness and the computational advantage of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2503.00480.pdf' target='_blank'>https://arxiv.org/pdf/2503.00480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Christou, Daniel F. N. Gordon, Theodoros Stouraitis, Juan C. Moreno, Sethu Vijayakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00480">Model-based optimisation for the personalisation of robot-assisted gait training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalised rehabilitation can be key to promoting gait independence and quality of life. Robots can enhance therapy by systematically delivering support in gait training, but often use one-size-fits-all control methods, which can be suboptimal. Here, we describe a model-based optimisation method for designing and fine-tuning personalised robotic controllers. As a case study, we formulate the objective of providing assistance as needed as an optimisation problem, and we demonstrate how musculoskeletal modelling can be used to develop personalised interventions. Eighteen healthy participants (age = 26 +/- 4) were recruited and the personalised control parameters for each were obtained to provide assistance as needed during a unilateral tracking task. A comparison was carried out between the personalised controller and the non-personalised controller. In simulation, a significant improvement was predicted when the personalised parameters were used. Experimentally, responses varied: six subjects showed significant improvements with the personalised parameters, eight subjects showed no obvious change, while four subjects performed worse. High interpersonal and intra-personal variability was observed with both controllers. This study highlights the importance of personalised control in robot-assisted gait training, and the need for a better estimation of human-robot interaction and human behaviour to realise the benefits of model-based optimisation.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2502.11142.pdf' target='_blank'>https://arxiv.org/pdf/2502.11142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11142">NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2502.10678.pdf' target='_blank'>https://arxiv.org/pdf/2502.10678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yate Ge, Meiying Li, Xipeng Huang, Yuanda Hu, Qi Wang, Xiaohua Sun, Weiwei Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10678">GenComUI: Exploring Generative Visual Aids as Medium to Support Task-Oriented Human-Robot Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the integration of generative visual aids in human-robot task communication. We developed GenComUI, a system powered by large language models that dynamically generates contextual visual aids (such as map annotations, path indicators, and animations) to support verbal task communication and facilitate the generation of customized task programs for the robot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communication in spatial tasks. To evaluate its effectiveness, we conducted a user experiment (n = 20) comparing GenComUI with a voice-only baseline. The results demonstrate that generative visual aids, through both qualitative and quantitative analysis, enhance verbal task communication by providing continuous visual feedback, thus promoting natural and effective human-robot communication. Additionally, the study offers a set of design implications, emphasizing how dynamically generated visual aids can serve as an effective communication medium in human-robot interaction. These findings underscore the potential of generative visual aids to inform the design of more intuitive and effective human-robot communication, particularly for complex communication scenarios in human-robot interaction and LLM-based end-user development.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2502.04873.pdf' target='_blank'>https://arxiv.org/pdf/2502.04873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Wang, Diwen Liu, Jizhuo Chen, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04873">Training-free Task-oriented Grasp Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a training-free pipeline for task-oriented grasp generation that combines pre-trained grasp generation models with vision-language models (VLMs). Unlike traditional approaches that focus solely on stable grasps, our method incorporates task-specific requirements by leveraging the semantic reasoning capabilities of VLMs. We evaluate five querying strategies, each utilizing different visual representations of candidate grasps, and demonstrate significant improvements over a baseline method in both grasp success and task compliance rates, with absolute gains of up to 36.9\% in overall success rate. Our results underline the potential of VLMs to enhance task-oriented manipulation, providing insights for future research in robotic grasping and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2502.03822.pdf' target='_blank'>https://arxiv.org/pdf/2502.03822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiatao Sun, Shuo Yang, Yinxing Chen, Francis Fan, Yiyan Liang, Daniel Rakita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03822">Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion policies trained via offline behavioral cloning have recently gained traction in robotic motion generation. While effective, these policies typically require a large number of trainable parameters. This model size affords powerful representations but also incurs high computational cost during training. Ideally, it would be beneficial to dynamically adjust the trainable portion as needed, balancing representational power with computational efficiency. For example, while overparameterization enables diffusion policies to capture complex robotic behaviors via offline behavioral cloning, the increased computational demand makes online interactive imitation learning impractical due to longer training time. To address this challenge, we present a framework, called DRIFT, that uses the Singular Value Decomposition to enable dynamic rank adjustment during diffusion policy training. We implement and demonstrate the benefits of this framework in DRIFT-DAgger, an imitation learning algorithm that can seamlessly slide between an offline bootstrapping phase and an online interactive phase. We perform extensive experiments to better understand the proposed framework, and demonstrate that DRIFT-DAgger achieves improved sample efficiency and faster training with minimal impact on model performance. The project website is available at: https://apollo-lab-yale.github.io/25-RSS-DRIFT-website/.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2502.02054.pdf' target='_blank'>https://arxiv.org/pdf/2502.02054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minwoo Kim, Geunsik Bae, Jinwoo Lee, Woojae Shin, Changseung Kim, Myong-Yol Choi, Heejung Shin, Hyondong Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02054">RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2502.01092.pdf' target='_blank'>https://arxiv.org/pdf/2502.01092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dabin Kim, Inkyu Jang, Youngsoo Han, Sunwoo Hwang, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01092">Enhancing Feature Tracking Reliability for Visual Navigation using Real-Time Safety Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are extensively used for localizing a robot's pose, particularly in environments where global localization tools such as GPS or motion capture systems are unavailable. In many visual navigation systems, localization is achieved by detecting and tracking visual features or landmarks, which provide information about the sensor's relative pose. For reliable feature tracking and accurate pose estimation, it is crucial to maintain visibility of a sufficient number of features. This requirement can sometimes conflict with the robot's overall task objective. In this paper, we approach it as a constrained control problem. By leveraging the invariance properties of visibility constraints within the robot's kinematic model, we propose a real-time safety filter based on quadratic programming. This filter takes a reference velocity command as input and produces a modified velocity that minimally deviates from the reference while ensuring the information score from the currently visible features remains above a user-specified threshold. Numerical simulations demonstrate that the proposed safety filter preserves the invariance condition and ensures the visibility of more features than the required minimum. We also validated its real-world performance by integrating it into a visual simultaneous localization and mapping (SLAM) algorithm, where it maintained high estimation quality in challenging environments, outperforming a simple tracking controller.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2501.15068.pdf' target='_blank'>https://arxiv.org/pdf/2501.15068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjiang Li, Bo Peng, Chang Li, Ning Qiao, Qi Zheng, Lei Sun, Yusen Qin, Bangguo Li, Yifeng Luan, Bo Wu, Yibing Zhan, Mingang Sun, Tong Xu, Lusong Li, Hui Shen, Xiaodong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15068">An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied manipulation is a fundamental ability in the realm of embodied artificial intelligence. Although current embodied manipulation models show certain generalizations in specific settings, they struggle in new environments and tasks due to the complexity and diversity of real-world scenarios. The traditional end-to-end data collection and training manner leads to significant data demands. Decomposing end-to-end tasks into atomic skills helps reduce data requirements and improves the task success rate. However, existing methods are limited by predefined skill sets that cannot be dynamically updated. To address the issue, we introduce a three-wheeled data-driven method to build an atomic skill library. We divide tasks into subtasks using the Vision-Language-Planning (VLP). Then, atomic skill definitions are formed by abstracting the subtasks. Finally, an atomic skill library is constructed via data collection and Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands dynamically with the three-wheel update strategy, the range of tasks it can cover grows naturally. In this way, our method shifts focus from end-to-end tasks to atomic skills, significantly reducing data costs while maintaining high performance and enabling efficient adaptation to new tasks. Extensive experiments in real-world settings demonstrate the effectiveness and efficiency of our approach.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2501.05750.pdf' target='_blank'>https://arxiv.org/pdf/2501.05750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonia Raychaudhuri, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05750">Semantic Mapping in Indoor Embodied AI -- A Survey on Advances, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent embodied agents (e.g. robots) need to perform complex semantic tasks in unfamiliar environments. Among many skills that the agents need to possess, building and maintaining a semantic map of the environment is most crucial in long-horizon tasks. A semantic map captures information about the environment in a structured way, allowing the agent to reference it for advanced reasoning throughout the task. While existing surveys in embodied AI focus on general advancements or specific tasks like navigation and manipulation, this paper provides a comprehensive review of semantic map-building approaches in embodied AI, specifically for indoor navigation. We categorize these approaches based on their structural representation (spatial grids, topological graphs, dense point-clouds or hybrid maps) and the type of information they encode (implicit features or explicit environmental data). We also explore the strengths and limitations of the map building techniques, highlight current challenges, and propose future research directions. We identify that the field is moving towards developing open-vocabulary, queryable, task-agnostic map representations, while high memory demands and computational inefficiency still remaining to be open challenges. This survey aims to guide current and future researchers in advancing semantic mapping techniques for embodied AI systems.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2501.00785.pdf' target='_blank'>https://arxiv.org/pdf/2501.00785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Atmaraaj Gopal, Arihiro Yorita, Naoyuki Kubota, Matthias RÃ¤tsch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00785">Natural Multimodal Fusion-Based Human-Robot Interaction: Application With Voice and Deictic Posture via Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing Human-Robot Interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly due to difficulties with complex syntax or sign language. To address the challenge, this paper introduces a multi-modal interaction framework that combines voice and deictic posture information to create a more natural HRI system. The visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity using a Universal Robots UR3e manipulator. Our method demonstrates significantly better performance in HRI in terms of accuracy and robustness. To benefit the research community and the general public, we will make our code and design open-source.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2412.18516.pdf' target='_blank'>https://arxiv.org/pdf/2412.18516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David SobrÃ­n-Hidalgo, Ãngel Manuel Guerrero-Higueras, Vicente MatellÃ¡n-Olivera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18516">Generating Explanations for Autonomous Robots: a Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building trust between humans and robots has long interested the robotics community. Various studies have aimed to clarify the factors that influence the development of user trust. In Human-Robot Interaction (HRI) environments, a critical aspect of trust development is the robot's ability to make its behavior understandable. The concept of an eXplainable Autonomous Robot (XAR) addresses this requirement. However, giving a robot self-explanatory abilities is a complex task. Robot behavior includes multiple skills and diverse subsystems. This complexity led to research into a wide range of methods for generating explanations about robot behavior. This paper presents a systematic literature review that analyzes existing strategies for generating explanations in robots and studies the current XAR trends. Results indicate promising advancements in explainability systems. However, these systems are still unable to fully cover the complex behavior of autonomous robots. Furthermore, we also identify a lack of consensus on the theoretical concept of explainability, and the need for a robust methodology to assess explainability methods and tools has been identified.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2412.13726.pdf' target='_blank'>https://arxiv.org/pdf/2412.13726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuga Yano, Akinobu Mizutani, Yukiya Fukuda, Daiju Kanaoka, Tomohiro Ono, Hakaru Tamukoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13726">Unified Understanding of Environment, Task, and Human for Human-Robot Interaction in Real-World Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate human--robot interaction (HRI) tasks in real-world scenarios, service robots must adapt to dynamic environments and understand the required tasks while effectively communicating with humans. To accomplish HRI in practice, we propose a novel indoor dynamic map, task understanding system, and response generation system. The indoor dynamic map optimizes robot behavior by managing an occupancy grid map and dynamic information, such as furniture and humans, in separate layers. The task understanding system targets tasks that require multiple actions, such as serving ordered items. Task representations that predefine the flow of necessary actions are applied to achieve highly accurate understanding. The response generation system is executed in parallel with task understanding to facilitate smooth HRI by informing humans of the subsequent actions of the robot. In this study, we focused on waiter duties in a restaurant setting as a representative application of HRI in a dynamic environment. We developed an HRI system that could perform tasks such as serving food and cleaning up while communicating with customers. In experiments conducted in a simulated restaurant environment, the proposed HRI system successfully communicated with customers and served ordered food with 90\% accuracy. In a questionnaire administered after the experiment, the HRI system of the robot received 4.2 points out of 5. These outcomes indicated the effectiveness of the proposed method and HRI system in executing waiter tasks in real-world environments.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2412.10726.pdf' target='_blank'>https://arxiv.org/pdf/2412.10726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wu, Chuhao Zhou, Yen Heng Wong, Lin Gu, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10726">NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Vision-Language Models (VLMs) has significantly advanced the development of Embodied Question Answering (EQA), enhancing agents' abilities in language understanding and reasoning within complex and realistic scenarios. However, EQA in real-world scenarios remains challenging, as human-posed questions often contain noise that can interfere with an agent's exploration and response, bringing challenges especially for language beginners and non-expert users. To address this, we introduce a NoisyEQA benchmark designed to evaluate an agent's ability to recognize and correct noisy questions. This benchmark introduces four common types of noise found in real-world applications: Latent Hallucination Noise, Memory Noise, Perception Noise, and Semantic Noise generated through an automated dataset creation framework. Additionally, we also propose a 'Self-Correction' prompting mechanism and a new evaluation metric to enhance and measure both noise detection capability and answer quality. Our comprehensive evaluation reveals that current EQA agents often struggle to detect noise in questions, leading to responses that frequently contain erroneous information. Through our Self-Correct Prompting mechanism, we can effectively improve the accuracy of agent answers.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2412.00711.pdf' target='_blank'>https://arxiv.org/pdf/2412.00711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carson Kohlbrenner, Caleb Escobedo, S. Sandra Bae, Alexander Dickhans, Alessandro Roncone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00711">GenTact Toolbox: A Computational Design Pipeline to Procedurally Generate Context-Driven 3D Printed Whole-Body Artificial Skins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing whole-body tactile skins for robots remains a challenging task, as existing solutions often prioritize modular, one-size-fits-all designs, which, while versatile, fail to account for the robot's specific shape and the unique demands of its operational context. In this work, we introduce GenTact Toolbox, a computational pipeline for creating versatile whole-body tactile skins tailored to both robot shape and application domain. Our method includes procedural mesh generation for conforming to a robot's topology, task-driven simulation to refine sensor distribution, and multi-material 3D printing for shape-agnostic fabrication. We validate our approach by creating and deploying six capacitive sensing skins on a Franka Research 3 robot arm in a human-robot interaction scenario. This work represents a shift from "one-size-fits-all" tactile sensors toward context-driven, highly adaptable designs that can be customized for a wide range of robotic systems and applications. The project website is available at https://hiro-group.ronc.one/gentacttoolbox
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2411.07644.pdf' target='_blank'>https://arxiv.org/pdf/2411.07644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rotem Atari, Eran Bamani, Avishai Sintov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07644">Human Arm Pose Estimation with a Shoulder-worn Force-Myography Device for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human pose estimation is essential for effective Human-Robot Interaction (HRI). By observing a user's arm movements, robots can respond appropriately, whether it's providing assistance or avoiding collisions. While visual perception offers potential for human pose estimation, it can be hindered by factors like poor lighting or occlusions. Additionally, wearable inertial sensors, though useful, require frequent calibration as they do not provide absolute position information. Force-myography (FMG) is an alternative approach where muscle perturbations are externally measured. It has been used to observe finger movements, but its application to full arm state estimation is unexplored. In this letter, we investigate the use of a wearable FMG device that can observe the state of the human arm for real-time applications of HRI. We propose a Transformer-based model to map FMG measurements from the shoulder of the user to the physical pose of the arm. The model is also shown to be transferable to other users with limited decline in accuracy. Through real-world experiments with a robotic arm, we demonstrate collision avoidance without relying on visual perception.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2411.07223.pdf' target='_blank'>https://arxiv.org/pdf/2411.07223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Luo, Yilun Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07223">Grounding Video Models to Actions through Goal Conditioned Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large video models, pretrained on massive amounts of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data are available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2411.06736.pdf' target='_blank'>https://arxiv.org/pdf/2411.06736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyeong Park, Junmo Cho, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06736">MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2411.05022.pdf' target='_blank'>https://arxiv.org/pdf/2411.05022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amar Halilovic, Senka Krivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05022">Towards Probabilistic Planning of Explanations for Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotics, ensuring that autonomous systems are comprehensible and accountable to users is essential for effective human-robot interaction. This paper introduces a novel approach that integrates user-centered design principles directly into the core of robot path planning processes. We propose a probabilistic framework for automated planning of explanations for robot navigation, where the preferences of different users regarding explanations are probabilistically modeled to tailor the stochasticity of the real-world human-robot interaction and the communication of decisions of the robot and its actions towards humans. This approach aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs by anticipating the types of explanations that will satisfy individual users.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2410.18373.pdf' target='_blank'>https://arxiv.org/pdf/2410.18373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhen Li, Longbing Cao, Xiao-Ming Wu, Xiaohan Yu, Runze Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18373">UGotMe: An Embodied System for Affective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multiparty conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, realtime response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve realtime response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://pi3-141592653.github.io/UGotMe/.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2410.17058.pdf' target='_blank'>https://arxiv.org/pdf/2410.17058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yenan Shen, Naomi Ehrich Leonard, Bassam Bamieh, Juncal Arbelaiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17058">Optimal gait design for nonlinear soft robotic crawlers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robots offer a frontier in robotics with enormous potential for safe human-robot interaction and agility in uncertain environments. A stepping stone towards unlocking their potential is a control theory tailored to soft robotics, including a principled framework for gait design. We analyze the problem of optimal gait design for a soft crawling body - the crawler. The crawler is an elastic body with the control signal defined as actuation forces between segments of the body. We consider the simplest such crawler: a two-segmented body with a passive mechanical connection modeling the viscoelastic body dynamics and a symmetric control force modeling actuation between the two body segments. The model accounts for the nonlinear asymmetric friction with the ground, which together with the symmetric actuation forces enable the crawler's locomotion. Using a describing-function analysis, we show that when the body is forced sinusoidally, the optimal actuator contraction frequency corresponds to the body's natural frequency when operating with only passive dynamics. We then use the framework of Optimal Periodic Control (OPC) to design optimal force cycles of arbitrary waveform and the corresponding crawling gaits. We provide a hill-climbing algorithm to solve the OPC problem numerically. Our proposed methods and results inform the design of optimal forcing and gaits for more complex and multi-segmented crawling soft bodies.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2410.05791.pdf' target='_blank'>https://arxiv.org/pdf/2410.05791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruocheng Wang, Pei Xu, Haochen Shi, Elizabeth Schumann, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05791">FÃ¼rElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2409.18452.pdf' target='_blank'>https://arxiv.org/pdf/2409.18452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenzhang Xiao, Seung Yun Song, Yu Chen, Mahshid Mansouri, JoÃ£o Ramos, Adam W. Bleakney, William R. Norris, Elizabeth T. Hsiao-Wecksler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18452">Exploiting Physical Human-Robot Interaction to Provide a Unique Rolling Experience with a Riding Ballbot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces the development of hands-free control schemes for a riding ballbot, designed to allow riders including manual wheelchair users to control its movement through torso leaning and twisting. The hardware platform, Personal Unique Rolling Experience (PURE), utilizes a ballbot drivetrain, a dynamically stable mobile robot that uses a ball as its wheel to provide omnidirectional maneuverability. To accommodate users with varying torso motion functions, the hanads-free control scheme should be adjustable based on the rider's torso function and personal preferences. Therefore, concepts of (a) impedance control and (b) admittance control were integrated into the control scheme. A duo-agent optimization framework was utilized to assess the efficiency of this rider-ballbot system for a safety-critical task: braking from 1.4 m/s. The candidate control schemes were further implemented in the physical robot hardware and validated with two experienced users, demonstrating the efficiency and robustness of the hands-free admittance control scheme (HACS). This interface, which utilized physical human-robot interaction (pHRI) as the input, resulted in lower braking effort and shorter braking distance and time. Subsequently, 12 novice participants (six able-bodied users and six manual wheelchair users) with different levels of torso motion capability were then recruited to benchmark the braking performance with HACS. The indoor navigation capability of PURE was further demonstrated with these participants in courses simulating narrow hallways, tight turns, and navigation through static and dynamic obstacles. By exploiting pHRI, the proposed admittance-style control scheme provided effective control of the ballbot via torso motions. This interface enables PURE to provide a personal unique rolling experience to manual wheelchair users for safe and agile indoor navigation.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2409.15922.pdf' target='_blank'>https://arxiv.org/pdf/2409.15922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukai Huang, Shu-Wei Liu, Nir Lipovetzky, Trevor Cohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15922">The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2409.11635.pdf' target='_blank'>https://arxiv.org/pdf/2409.11635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang Tien Dam, Tri Tung Nguyen Nguyen, Yuki Endo, Dinh Tuan Tran, Joo-Ho Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11635">PainDiffusion: Learning to Express Pain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate pain expression synthesis is essential for improving clinical training and human-robot interaction. Current Robotic Patient Simulators (RPSs) lack realistic pain facial expressions, limiting their effectiveness in medical training. In this work, we introduce PainDiffusion, a generative model that synthesizes naturalistic facial pain expressions. Unlike traditional heuristic or autoregressive methods, PainDiffusion operates in a continuous latent space, ensuring smoother and more natural facial motion while supporting indefinite-length generation via diffusion forcing. Our approach incorporates intrinsic characteristics such as pain expressiveness and emotion, allowing for personalized and controllable pain expression synthesis. We train and evaluate our model using the BioVid HeatPain Database. Additionally, we integrate PainDiffusion into a robotic system to assess its applicability in real-time rehabilitation exercises. Qualitative studies with clinicians reveal that PainDiffusion produces realistic pain expressions, with a 31.2% (std 4.8%) preference rate against ground-truth recordings. Our results suggest that PainDiffusion can serve as a viable alternative to real patients in clinical training and simulation, bridging the gap between synthetic and naturalistic pain expression. Code and videos are available at: https://damtien444.github.io/paindf/
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2409.07013.pdf' target='_blank'>https://arxiv.org/pdf/2409.07013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Chen, Mahshid Mansouri, Chenzhang Xiao, Ze Wang, Elizabeth T. Hsiao-Wecksler, William R. Norris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07013">Enabling Shared-Control for A Riding Ballbot System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a shared-control approach for collision avoidance in a self-balancing riding ballbot, called PURE, marked by its dynamic stability, omnidirectional movement, and hands-free interface. Integrated with a sensor array and a novel Passive Artificial Potential Field (PAPF) method, PURE provides intuitive navigation with deceleration assistance and haptic/audio feedback, effectively mitigating collision risks. This approach addresses the limitations of traditional APF methods, such as control oscillations and unnecessary speed reduction in challenging scenarios. A human-robot interaction experiment, with 20 manual wheelchair users and able-bodied individuals, was conducted to evaluate the performance of indoor navigation and obstacle avoidance with the proposed shared-control algorithm. Results indicated that shared-control significantly reduced collisions and cognitive load without affecting travel speed, offering intuitive and safe operation. These findings highlight the shared-control system's suitability for enhancing collision avoidance in self-balancing mobility devices, a relatively unexplored area in assistive mobility research.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2409.05593.pdf' target='_blank'>https://arxiv.org/pdf/2409.05593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muraleekrishna Gopinathan, Jumana Abu-Khalaf, David Suter, Martin Masek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05593">StratXplore: Strategic Novelty-seeking and Instruction-aligned Exploration for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called \textit{StratXplore}, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2408.01024.pdf' target='_blank'>https://arxiv.org/pdf/2408.01024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangwoo Shin, Seunghyun Kim, Youngsoo Jang, Moontae Lee, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01024">Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied instruction-following (EIF), the integration of pretrained language models (LMs) as task planners emerges as a significant branch, where tasks are planned at the skill level by prompting LMs with pretrained skills and user instructions. However, grounding these pretrained skills in different domains remains challenging due to their intricate entanglement with the domain-specific knowledge. To address this challenge, we present a semantic skill grounding (SemGro) framework that leverages the hierarchical nature of semantic skills. SemGro recognizes the broad spectrum of these skills, ranging from short-horizon low-semantic skills that are universally applicable across domains to long-horizon rich-semantic skills that are highly specialized and tailored for particular domains. The framework employs an iterative skill decomposition approach, starting from the higher levels of semantic skill hierarchy and then moving downwards, so as to ground each planned skill to an executable level within the target domain. To do so, we use the reasoning capabilities of LMs for composing and decomposing semantic skills, as well as their multi-modal extension for assessing the skill feasibility in the target domain. Our experiments in the VirtualHome benchmark show the efficacy of SemGro in 300 cross-domain EIF scenarios.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2407.18009.pdf' target='_blank'>https://arxiv.org/pdf/2407.18009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana MÃ¼ller, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18009">Egocentric Robots in a Human-Centric World? Exploring Group-Robot-Interaction in Public Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of social robots in real-world scenarios is increasing, supporting humans in various contexts. However, they still struggle to grasp social dynamics, especially in public spaces, sometimes resulting in violations of social norms, such as interrupting human conversations. This behavior, originating from a limited processing of social norms, might be perceived as robot-centered. Understanding social dynamics, particularly in group-robot-interactions (GRI), underscores the need for further research and development in human-robot-interaction (HRI). Enhancing the interaction abilities of social robots, especially in GRIs, can improve their effectiveness in real-world applications on a micro-level, as group interactions lead to increased motivation and comfort. In this study, we assessed the influence of the interaction condition (dyadic vs. triadic) on the perceived extraversion (ext.) of social robots in public spaces. The research involved 40 HRIs, including 24 dyadic (i.e., one human and one robot) interactions and 16 triadic interactions, which involve at least three entities, including the robot.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2407.04158.pdf' target='_blank'>https://arxiv.org/pdf/2407.04158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brendon Boldt, David Mortensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04158">ELCC: the Emergent Language Corpus Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Emergent Language Corpus Collection (ELCC): a collection of corpora generated from open source implementations of emergent communication systems across the literature. These systems include a variety of signalling game environments as well as more complex environments like a social deduction game and embodied navigation. Each corpus is annotated with metadata describing the characteristics of the source system as well as a suite of analyses of the corpus (e.g., size, entropy, average message length, performance as transfer learning data). Currently, research studying emergent languages requires directly running different systems which takes time away from actual analyses of such languages, makes studies which compare diverse emergent languages rare, and presents a barrier to entry for researchers without a background in deep learning. The availability of a substantial collection of well-documented emergent language corpora, then, will enable research which can analyze a wider variety of emergent languages, which more effectively uncovers general principles in emergent communication rather than artifacts of particular environments. We provide some quantitative and qualitative analyses with ELCC to demonstrate potential use cases of the resource in this vein.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2407.03340.pdf' target='_blank'>https://arxiv.org/pdf/2407.03340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iveta BeÄkovÃ¡, Å tefan PÃ³coÅ¡, Giulia Belgiovine, Marco Matarese, Omar Eldardeer, Alessandra Sciutti, Carlo Mazzola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03340">A Multi-Modal Explainability Approach for Human-Aware Robots in Multi-Party Conversation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The addressee estimation (understanding to whom somebody is talking) is a fundamental task for human activity recognition in multi-party conversation scenarios. Specifically, in the field of human-robot interaction, it becomes even more crucial to enable social robots to participate in such interactive contexts. However, it is usually implemented as a binary classification task, restricting the robot's capability to estimate whether it was addressed \review{or not, which} limits its interactive skills. For a social robot to gain the trust of humans, it is also important to manifest a certain level of transparency and explainability. Explainable artificial intelligence thus plays a significant role in the current machine learning applications and models, to provide explanations for their decisions besides excellent performance. In our work, we a) present an addressee estimation model with improved performance in comparison with the previous state-of-the-art; b) further modify this model to include inherently explainable attention-based segments; c) implement the explainable addressee estimation as part of a modular cognitive architecture for multi-party conversation in an iCub robot; d) validate the real-time performance of the explainable model in multi-party human-robot interaction; e) propose several ways to incorporate explainability and transparency in the aforementioned architecture; and f) perform an online user study to analyze the effect of various explanations on how human participants perceive the robot.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2406.06843.pdf' target='_blank'>https://arxiv.org/pdf/2406.06843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jikai Wang, Qifan Zhang, Yu-Wei Chao, Bowen Wen, Xiaohu Guo, Yu Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06843">HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a data capture system and a new dataset, HO-Cap, for 3D reconstruction and pose tracking of hands and objects in videos. The system leverages multiple RGBD cameras and a HoloLens headset for data collection, avoiding the use of expensive 3D scanners or mocap systems. We propose a semi-automatic method for annotating the shape and pose of hands and objects in the collected videos, significantly reducing the annotation time compared to manual labeling. With this system, we captured a video dataset of humans interacting with objects to perform various tasks, including simple pick-and-place actions, handovers between hands, and using objects according to their affordance, which can serve as human demonstrations for research in embodied AI and robot manipulation. Our data capture setup and annotation framework will be available for the community to use in reconstructing 3D shapes of objects and human hands and tracking their poses in videos.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2406.05486.pdf' target='_blank'>https://arxiv.org/pdf/2406.05486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sue Lim, Ralf SchmÃ¤lzle, Gary Bente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05486">Artificial social influence via human-embodied AI agent interaction in immersive virtual reality (VR): Effects of similarity-matching during health conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactions with artificial intelligence (AI) based agents can positively influence human behavior and judgment. However, studies to date focus on text-based conversational agents (CA) with limited embodiment, restricting our understanding of how social influence principles, such as similarity, apply to AI agents (i.e., artificial social influence). We address this gap by leveraging the latest advances in AI (language models) and combining them with immersive virtual reality (VR). Specifically, we built VR-ECAs, or embodied conversational agents that can naturally converse with humans about health-related topics in a virtual environment. Then we manipulated interpersonal similarity via gender matching and examined its effects on biobehavioral (i.e., gaze), social (e.g., agent likeability), and behavioral outcomes (i.e., healthy snack selection). We found an interesting interaction effect between agent and participant gender on biobehavioral outcomes: discussing health with opposite-gender agents tended to enhance gaze duration, with the effect stronger for male participants compared to their female counterparts. A similar directional pattern was observed for healthy snack selection, though it was not statistically significant. In addition, female participants liked the VR-ECAs more than their male counterparts, regardless of the gender of the VR-ECAs. Finally, participants experienced greater presence while conversing with VR-embodied agents than chatting with text-only agents. Overall, our findings highlight embodiment as a crucial factor of influence of AI on human behavior, and our paradigm enables new experimental research at the intersection of social influence, human-AI communication, and immersive virtual reality (VR).
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2406.01832.pdf' target='_blank'>https://arxiv.org/pdf/2406.01832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Martini, Harshil Parekh, Shaoting Peng, Nicola Bombieri, Nadia Figueroa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01832">A Robust Filter for Marker-less Multi-person Tracking in Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pursuing natural and marker-less human-robot interaction (HRI) has been a long-standing robotics research focus, driven by the vision of seamless collaboration without physical markers. Marker-less approaches promise an improved user experience, but state-of-the-art struggles with the challenges posed by intrinsic errors in human pose estimation (HPE) and depth cameras. These errors can lead to issues such as robot jittering, which can significantly impact the trust users have in collaborative systems. We propose a filtering pipeline that refines incomplete 3D human poses from an HPE backbone and a single RGB-D camera to address these challenges, solving for occlusions that can degrade the interaction. Experimental results show that using the proposed filter leads to more consistent and noise-free motion representation, reducing unexpected robot movements and enabling smoother interaction.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2406.00375.pdf' target='_blank'>https://arxiv.org/pdf/2406.00375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehasis Banerjee, Sayan Paul, Ruddradev Roychoudhury, Abhijan Bhattacharya, Chayan Sarkar, Ashis Sau, Pradip Pramanick, Brojeshwar Bhowmick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00375">Teledrive: An Embodied AI based Telepresence System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents Teledrive, a telepresence robotic system with embodied AI features that empowers an operator to navigate the telerobot in any unknown remote place with minimal human intervention. We conceive Teledrive in the context of democratizing remote care-giving for elderly citizens as well as for isolated patients, affected by contagious diseases. In particular, this paper focuses on the problem of navigating to a rough target area (like bedroom or kitchen) rather than pre-specified point destinations. This ushers in a unique AreaGoal based navigation feature, which has not been explored in depth in the contemporary solutions. Further, we describe an edge computing-based software system built on a WebRTC-based communication framework to realize the aforementioned scheme through an easy-to-use speech-based human-robot interaction. Moreover, to enhance the ease of operation for the remote caregiver, we incorporate a person following feature, whereby a robot follows a person on the move in its premises as directed by the operator. Moreover, the system presented is loosely coupled with specific robot hardware, unlike the existing solutions. We have evaluated the efficacy of the proposed system through baseline experiments, user study, and real-life deployment.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2405.16818.pdf' target='_blank'>https://arxiv.org/pdf/2405.16818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor A. Kich, Jair A. Bottega, Raul Steinmetz, Ricardo B. Grando, Ayanori Yorozu, Akihisa Ohya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16818">Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces YamaS, a simulator integrating Unity3D Engine with Robotic Operating System for robot navigation research and aims to facilitate the development of both Deep Reinforcement Learning (Deep-RL) and Natural Language Processing (NLP). It supports single and multi-agent configurations with features like procedural environment generation, RGB vision, and dynamic obstacle navigation. Unique to YamaS is its ability to construct single and multi-agent environments, as well as generating agent's behaviour through textual descriptions. The simulator's fidelity is underscored by comparisons with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality (VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive platform for developers and researchers. This fusion establishes YamaS as a versatile and valuable tool for the development and testing of autonomous systems, contributing to the fields of robot simulation and AI-driven training methodologies.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2405.13841.pdf' target='_blank'>https://arxiv.org/pdf/2405.13841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amar Halilovic, Senka Krivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13841">Robot Explanation Identity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To bring robots into human everyday life, their capacity for social interaction must increase. One way for robots to acquire social skills is by assigning them the concept of identity. This research focuses on the concept of \textit{Explanation Identity} within the broader context of robots' roles in society, particularly their ability to interact socially and explain decisions. Explanation Identity refers to the combination of characteristics and approaches robots use to justify their actions to humans. Drawing from different technical and social disciplines, we introduce Explanation Identity as a multidisciplinary concept and discuss its importance in Human-Robot Interaction. Our theoretical framework highlights the necessity for robots to adapt their explanations to the user's context, demonstrating empathy and ethical integrity. This research emphasizes the dynamic nature of robot identity and guides the integration of explanation capabilities in social robots, aiming to improve user engagement and acceptance.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2405.13477.pdf' target='_blank'>https://arxiv.org/pdf/2405.13477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Li, Florian A. Kunneman, Koen V. Hindriks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13477">A Near-Real-Time Processing Ego Speech Filtering Pipeline Designed for Speech Interruption During Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With current state-of-the-art automatic speech recognition (ASR) systems, it is not possible to transcribe overlapping speech audio streams separately. Consequently, when these ASR systems are used as part of a social robot like Pepper for interaction with a human, it is common practice to close the robot's microphone while it is talking itself. This prevents the human users to interrupt the robot, which limits speech-based human-robot interaction. To enable a more natural interaction which allows for such interruptions, we propose an audio processing pipeline for filtering out robot's ego speech using only a single-channel microphone. This pipeline takes advantage of the possibility to feed the robot ego speech signal, generated by a text-to-speech API, as training data into a machine learning model. The proposed pipeline combines a convolutional neural network and spectral subtraction to extract overlapping human speech from the audio recorded by the robot-embedded microphone. When evaluating on a held-out test set, we find that this pipeline outperforms our previous approach to this task, as well as state-of-the-art target speech extraction systems that were retrained on the same dataset. We have also integrated the proposed pipeline into a lightweight robot software development framework to make it available for broader use. As a step towards demonstrating the feasibility of deploying our pipeline, we use this framework to evaluate the effectiveness of the pipeline in a small lab-based feasibility pilot using the social robot Pepper. Our results show that when participants interrupt the robot, the pipeline can extract the participant's speech from one-second streaming audio buffers received by the robot-embedded single-channel microphone, hence in near-real time.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2405.04889.pdf' target='_blank'>https://arxiv.org/pdf/2405.04889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim TÃ¸rresen, Ryo Kurazume
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04889">Fast LiDAR Upsampling using Conditional Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2404.19253.pdf' target='_blank'>https://arxiv.org/pdf/2404.19253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Roy, Dana Kulic, Elizabeth Croft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19253">Learning to Communicate Functional States with Nonverbal Expressions for Improved Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots must effectively communicate their internal state to humans to enable a smooth interaction. Nonverbal communication is widely used to communicate information during human-robot interaction, however, such methods may also be misunderstood, leading to communication errors. In this work, we explore modulating the acoustic parameter values (pitch bend, beats per minute, beats per loop) of nonverbal auditory expressions to convey functional robot states (accomplished, progressing, stuck). We propose a reinforcement learning (RL) algorithm based on noisy human feedback to produce accurately interpreted nonverbal auditory expressions. The proposed approach was evaluated through a user study with 24 participants. The results demonstrate that: 1. Our proposed RL-based approach is able to learn suitable acoustic parameter values which improve the users' ability to correctly identify the state of the robot. 2. Algorithm initialization informed by previous user data can be used to significantly speed up the learning process. 3. The method used for algorithm initialization strongly influences whether participants converge to similar sounds for each robot state. 4. Modulation of pitch bend has the largest influence on user association between sounds and robotic states.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2404.19174.pdf' target='_blank'>https://arxiv.org/pdf/2404.19174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, Erickson R. Nascimento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19174">XFeat: Accelerated Features for Lightweight Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations. Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2404.18628.pdf' target='_blank'>https://arxiv.org/pdf/2404.18628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Maiorca, Seyed Abolfazl Ghasemzadeh, Thierry Ravet, FranÃ§ois Cresson, Thierry Dutoit, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18628">Self-Avatar Animation in Virtual Reality: Impact of Motion Signals Artifacts on the Full-Body Pose Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) applications have revolutionized user experiences by immersing individuals in interactive 3D environments. These environments find applications in numerous fields, including healthcare, education, or architecture. A significant aspect of VR is the inclusion of self-avatars, representing users within the virtual world, which enhances interaction and embodiment. However, generating lifelike full-body self-avatar animations remains challenging, particularly in consumer-grade VR systems, where lower-body tracking is often absent. One method to tackle this problem is by providing an external source of motion information that includes lower body information such as full Cartesian positions estimated from RGB(D) cameras. Nevertheless, the limitations of these systems are multiples: the desynchronization between the two motion sources and occlusions are examples of significant issues that hinder the implementations of such systems. In this paper, we aim to measure the impact on the reconstruction of the articulated self-avatar's full-body pose of (1) the latency between the VR motion features and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4) the inaccuracy of the position estimation algorithm. In addition, we analyze the motion reconstruction errors using ground truth and 3D Cartesian coordinates estimated from \textit{YOLOv8} pose estimation. These analyzes show that the studied methods are significantly sensitive to any degradation tested, especially regarding the velocity reconstruction error.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2404.13206.pdf' target='_blank'>https://arxiv.org/pdf/2404.13206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunxi Dai, Xiaohan Liu, Roberto Shu, Ralph Hollis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13206">Wheelchair Maneuvering with a Single-Spherical-Wheeled Balancing Mobile Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a control framework to effectively maneuver wheelchairs with a dynamically stable mobile manipulator. Wheelchairs are a type of nonholonomic cart system, maneuvering such systems with mobile manipulators (MM) is challenging mostly due to the following reasons: 1) These systems feature nonholonomic constraints and considerably varying inertial parameters that require online identification and adaptation. 2) These systems are widely used in human-centered environments, which demand the MM to operate in potentially crowded spaces while ensuring compliance for safe physical human-robot interaction (pHRI). We propose a control framework that plans whole-body motion based on quasi-static analysis to maneuver heavy nonholonomic carts while maintaining overall compliance. We validated our approach experimentally by maneuvering a wheelchair with a bimanual mobile manipulator, the CMU ballbot. The experiments demonstrate the proposed framework is able to track desired wheelchair velocity with loads varying from 11.8 kg to 79.4 kg at a maximum linear velocity of 0.45 m/s and angular velocity of 0.3 rad/s. Furthermore, we verified that the proposed method can generate human-like motion smoothness of the wheelchair while ensuring safe interactions with the environment.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2404.11248.pdf' target='_blank'>https://arxiv.org/pdf/2404.11248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patricia Piedade, Ana Henriques, Filipa Rocha, Isabel Neto, Hugo Nicolau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11248">Ethical Concerns when Working with Mixed-Ability Groups of Children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accessibility research has gained traction, yet ethical gaps persist in the inclusion of individuals with disabilities, especially children. Inclusive research practices are essential to ensure research and design solutions cater to the needs of all individuals, regardless of their abilities. Working with children with disabilities in Human-Computer Interaction and Human-Robot Interaction presents a unique set of ethical dilemmas. These young participants often require additional care, support, and accommodations, which can fall off researchers' resources or expertise. The lack of clear guidance on navigating these challenges further aggravates the problem. To provide a base and address this issue, we adopt a critical reflective approach, evaluating our impact by analyzing two case studies involving children with disabilities in HCI/HRI research.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2404.09705.pdf' target='_blank'>https://arxiv.org/pdf/2404.09705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David SobrÃ­n-Hidalgo, Miguel Ãngel GonzÃ¡lez-Santamarta, Ãngel Manuel Guerrero-Higueras, Francisco Javier RodrÃ­guez-Lera, Vicente MatellÃ¡n-Olivera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09705">Enhancing Robot Explanation Capabilities through Vision-Language Models: a Preliminary Study by Interpreting Visual Inputs for Improved Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an improved system based on our prior work, designed to create explanations for autonomous robot actions during Human-Robot Interaction (HRI). Previously, we developed a system that used Large Language Models (LLMs) to interpret logs and produce natural language explanations. In this study, we expand our approach by incorporating Vision-Language Models (VLMs), enabling the system to analyze textual logs with the added context of visual input. This method allows for generating explanations that combine data from the robot's logs and the images it captures. We tested this enhanced system on a basic navigation task where the robot needs to avoid a human obstacle. The findings from this preliminary study indicate that adding visual interpretation improves our system's explanations by precisely identifying obstacles and increasing the accuracy of the explanations provided.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2404.04004.pdf' target='_blank'>https://arxiv.org/pdf/2404.04004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Jeanne Kirschner, Carina M. Micheler, Yangcan Zhou, Sebastian Siegner, Mazin Hamad, Claudio Glowalla, Jan Neumann, Nader Rajaei, Rainer Burgkart, Sami Haddadin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04004">Towards Safe Robot Use with Edged or Pointed Objects: A Surrogate Study Assembling a Human Hand Injury Protection Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of pointed or edged tools or objects is one of the most challenging aspects of today's application of physical human-robot interaction (pHRI). One reason for this is that the severity of harm caused by such edged or pointed impactors is less well studied than for blunt impactors. Consequently, the standards specify well-reasoned force and pressure thresholds for blunt impactors and advise avoiding any edges and corners in contacts. Nevertheless, pointed or edged impactor geometries cannot be completely ruled out in real pHRI applications. For example, to allow edged or pointed tools such as screwdrivers near human operators, the knowledge of injury severity needs to be extended so that robot integrators can perform well-reasoned, time-efficient risk assessments. In this paper, we provide the initial datasets on injury prevention for the human hand based on drop tests with surrogates for the human hand, namely pig claws and chicken drumsticks. We then demonstrate the ease and efficiency of robot use using the dataset for contact on two examples. Finally, our experiments provide a set of injuries that may also be expected for human subjects under certain robot mass-velocity constellations in collisions. To extend this work, testing on human samples and a collaborative effort from research institutes worldwide is needed to create a comprehensive human injury avoidance database for any pHRI scenario and thus for safe pHRI applications including edged and pointed geometries.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2403.17270.pdf' target='_blank'>https://arxiv.org/pdf/2403.17270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Gupta, Hyonyoung Shin, Emily Norman, Keri K. Stephens, Nanshu Lu, Luis Sentis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17270">Human Stress Response and Perceived Safety during Encounters with Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the rise of mobile robot deployments in home and work settings, perceived safety of users and bystanders is understudied in the human-robot interaction (HRI) literature. To address this, we present a study designed to identify elements of a human-robot encounter that correlate with observed stress response. Stress is a key component of perceived safety and is strongly associated with human physiological response. In this study a Boston Dynamics Spot and a Unitree Go1 navigate autonomously through a shared environment occupied by human participants wearing multimodal physiological sensors to track their electrocardiography (ECG) and electrodermal activity (EDA). The encounters are varied through several trials and participants self-rate their stress levels after each encounter. The study resulted in a multidimensional dataset archiving various objective and subjective aspects of a human-robot encounter, containing insights for understanding perceived safety in such encounters. To this end, acute stress responses were decoded from the human participants' ECG and EDA and compared across different human-robot encounter conditions. Statistical analysis of data indicate that on average (1) participants feel more stress during encounters compared to baselines, (2) participants feel more stress encountering multiple robots compared to a single robot and (3) participants stress increases during navigation behavior compared with search behavior.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2403.17124.pdf' target='_blank'>https://arxiv.org/pdf/2403.17124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17124">Grounding Language Plans in Demonstrations Through Counterfactual Perturbations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2403.14163.pdf' target='_blank'>https://arxiv.org/pdf/2403.14163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14163">Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs. The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL). The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms. For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav).
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2403.12039.pdf' target='_blank'>https://arxiv.org/pdf/2403.12039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Li, Taskin Padir, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12039">StereoNavNet: Learning to Navigate using Stereo Cameras with Auxiliary Occupancy Voxels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation has received significant attention recently. Most of the prior works focus on predicting navigation actions based on semantic features extracted from visual encoders. However, these approaches often rely on large datasets and exhibit limited generalizability. In contrast, our approach draws inspiration from traditional navigation planners that operate on geometric representations, such as occupancy maps. We propose StereoNavNet (SNN), a novel visual navigation approach employing a modular learning framework comprising perception and policy modules. Within the perception module, we estimate an auxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric features from it. These features, along with user-defined goals, are utilized by the policy module to predict navigation actions. Through extensive empirical evaluation, we demonstrate that SNN outperforms baseline approaches in terms of success rates, success weighted by path length, and navigation error. Furthermore, SNN exhibits better generalizability, characterized by maintaining leading performance when navigating across previously unseen environments.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2403.08144.pdf' target='_blank'>https://arxiv.org/pdf/2403.08144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elaheh Sanoubari, Atil Iscen, Leila Takayama, Stefano Saliceti, Corbin Cunningham, Ken Caluwaerts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08144">Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the use of 'prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces. Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation. We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction. A human operator, serving as the robot's sensory and processing proxy, translated human communication into a basic set of navigation commands, effectively simulating an intuitive interface. During our analysis of interaction videos, when lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues. Qualitative evidence suggests that participants intuitively relied on prosody to control robot navigation. We highlight specific distinct prosodic constructs that emerged from this preliminary exploration and discuss their pragmatic functions. This work contributes a discussion on the broader potential of prosody as a multifunctional communicative signal for designing future intuitive robotic interfaces, enabling lifelong learning and personalization in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2403.01977.pdf' target='_blank'>https://arxiv.org/pdf/2403.01977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01977">TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our "plug-and-play" method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model. Then, it feeds the reconstruction of a corrupted image back to the pre-trained model. Finally, the pre-trained model does forward pass again to output action. Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation. The pre-trained navigation model with our top-down decoder significantly enhances navigation performance across almost all visual corruptions in our benchmarks. Our method improves the success rate of point-goal navigation from the state-of-the-art result of 46% to 94% on the most severe corruption. This suggests its potential for broader application in robotic visual navigation. Project page: https://sites.google.com/view/tta-nav
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2402.04206.pdf' target='_blank'>https://arxiv.org/pdf/2402.04206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David SobrÃ­n-Hidalgo, Miguel A. GonzÃ¡lez-Santamarta, Ãngel M. Guerrero-Higueras, Francisco J. RodrÃ­guez-Lera, Vicente MatellÃ¡n-Olivera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04206">Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI). Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area. The work described in this paper aims to take advantage of the capabilities of Large Language Models (LLMs) in performing natural language processing tasks. This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems. In addition, this work also presents a formalization of the proposed explanation system. It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition. Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users. The results obtained during the experiment highlight the potential utility of LLMs in achieving explanatory capabilities in robots.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2401.09132.pdf' target='_blank'>https://arxiv.org/pdf/2401.09132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose L. Pulloquinga, Rafael J. Escarabajal, Marina Valles, Miguel Diaz-Rodriguez, Vicente Mata, Angel Valera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09132">Admittance Controller Complemented with Real-time Singularity Avoidance for Rehabilitation Parallel Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rehabilitation tasks demand robust and accurate trajectory-tracking performance, mainly achieved with parallel robots. In this field, limiting the value of the force exerted on the patient is crucial, especially when an injured limb is involved. In human-robot interaction studies, the admittance controller modifies the location of the robot according to the user efforts driving the end-effector to an arbitrary location within the workspace. However, a parallel robot has singularities within the workspace, making implementing a conventional admittance controller unsafe. Thus, this study proposes an admittance controller that overcomes the limitations of singular configurations by using a real-time singularity avoidance algorithm. The singularity avoidance algorithm modifies the original trajectory based on the actual location of the parallel robot. The complemented admittance controller is applied to a 4 degrees of freedom parallel robot for knee rehabilitation. In this case, the actual location is measured by a 3D tracking system because the location calculated by the forward kinematics is inaccurate in the vicinity of a singularity. The experimental results verify the effectiveness of the proposed admittance controller for safe knee rehabilitation exercises
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2312.07540.pdf' target='_blank'>https://arxiv.org/pdf/2312.07540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07540">diff History for Neural Language Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-of-the-art performance for neural agents while needing 1800x fewer training examples compared to prior work. Even on the simpler BabyAI-Text environment with concise text observations, we find that although diff history increases the length of prompts, the representation it provides offers a 25% improvement in the efficiency of low-sample instruction tuning. Further, we show that diff history scales favorably across different tuning dataset sizes. We open-source our code and data to https://diffhistory.github.io.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2312.05473.pdf' target='_blank'>https://arxiv.org/pdf/2312.05473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Zuo, Kaibo He, Jing Shao, Yanan Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05473">Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and control of the human musculoskeletal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current human musculoskeletal models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model (MS-Human-700) with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algorithm, will be made available to the research community to promote a deeper understanding of human motion control and better design of interactive robots.
  Project page: https://lnsgroup.cc/research/MS-Human-700
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2312.02711.pdf' target='_blank'>https://arxiv.org/pdf/2312.02711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Rozlivek, Alessandro Roncone, Ugo Pattacini, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02711">HARMONIOUS -- Human-like reactive motion control and multimodal perception for humanoid robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For safe and effective operation of humanoid robots in human-populated environments, the problem of commanding a large number of Degrees of Freedom (DoF) while simultaneously considering dynamic obstacles and human proximity has still not been solved. We present a new reactive motion controller that commands two arms of a humanoid robot and three torso joints (17 DoF in total). We formulate a quadratic program that seeks joint velocity commands respecting multiple constraints while minimizing the magnitude of the velocities. We introduce a new unified treatment of obstacles that dynamically maps visual and proximity (pre-collision) and tactile (post-collision) obstacles as additional constraints to the motion controller, in a distributed fashion over the surface of the upper body of the iCub robot (with 2000 pressure-sensitive receptors). This results in a bio-inspired controller that: (i) gives rise to a robot with whole-body visuo-tactile awareness, resembling peripersonal space representations, and (ii) produces human-like minimum jerk movement profiles. The controller was extensively experimentally validated, including a physical human-robot interaction scenario.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2312.00265.pdf' target='_blank'>https://arxiv.org/pdf/2312.00265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Tang, Yijing Feng, Yue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00265">RoboSync: Efficient Real-Time Operating System for Social Robots with Customizable Behaviour</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robotic systems require complex implementations that are not always accessible or easy to use for Human-Robot Interaction (HRI) application developers. With the aim of simplifying the implementation of HRI applications, this paper introduces a novel real-time operating system (RTOS) designed for customizable HRI - RoboSync. By creating multi-level abstraction layers, the system enables users to define complex emotional and behavioral models without needing deep technical expertise. The system's modular architecture comprises a behavior modeling layer, a machine learning plugin configuration layer, a sensor checks customization layer, a scheduler that fits the need of HRI, and a communication and synchronization layer. This approach not only promotes ease of use without highly specialized skills but also ensures real-time responsiveness and adaptability. The primary functionality of the RTOS has been implemented for proof of concept and was tested on a CortexM4 microcontroller, demonstrating its potential for a wide range of lightweight simple-to-implement social robotics applications.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2311.12992.pdf' target='_blank'>https://arxiv.org/pdf/2311.12992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Rollo, Andrea Zunino, Gennaro Raiola, Fabio Amadio, Arash Ajoudani, Nikolaos Tsagarakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12992">FollowMe: a Robust Person Following Framework Based on Re-Identification and Gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) has become a crucial enabler in houses and industries for facilitating operational flexibility. When it comes to mobile collaborative robots, this flexibility can be further increased due to the autonomous mobility and navigation capacity of the robotic agents, expanding their workspace and consequently, the personalizable assistance they can provide to the human operators. This however requires that the robot is capable of detecting and identifying the human counterpart in all stages of the collaborative task, and in particular while following a human in crowded workplaces. To respond to this need, we developed a unified perception and navigation framework, which enables the robot to identify and follow a target person using a combination of visual Re-Identification (Re-ID), hand gestures detection, and collision-free navigation. The Re-ID module can autonomously learn the features of a target person and use the acquired knowledge to visually re-identify the target. The navigation stack is used to follow the target avoiding obstacles and other individuals in the environment. Experiments are conducted with few subjects in a laboratory setting where some unknown dynamic obstacles are introduced.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2310.19413.pdf' target='_blank'>https://arxiv.org/pdf/2310.19413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19413">CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendency exists to assume that the robot shall cooperate with the closest individual or that the scene involves merely a singular human actor. However, in realistic scenarios, such as shop floor operations, such an assumption may not hold and personalized target recognition by the robot in crowded environments is required. To fulfil this requirement, in this work, we propose a person re-identification module based on continual visual adaptation techniques that ensure the robot's seamless cooperation with the appropriate individual even subject to varying visual appearances or partial or complete occlusions. We test the framework singularly using recorded videos in a laboratory environment and an HRI scenario, i.e., a person-following task by a mobile robot. The targets are asked to change their appearance during tracking and to disappear from the camera field of view to test the challenging cases of occlusion and outfit variations. We compare our framework with one of the state-of-the-art Multi-Object Tracking (MOT) methods and the results show that the CARPE-ID can accurately track each selected target throughout the experiments in all the cases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of 4 tracking errors for each video.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2310.15862.pdf' target='_blank'>https://arxiv.org/pdf/2310.15862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Barbara Sienkiewicz, Bipin Indurkhya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15862">Is a humorous robot more trustworthy?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As more and more social robots are being used for collaborative activities with humans, it is crucial to investigate mechanisms to facilitate trust in the human-robot interaction. One such mechanism is humour: it has been shown to increase creativity and productivity in human-human interaction, which has an indirect influence on trust. In this study, we investigate if humour can increase trust in human-robot interaction. We conducted a between-subjects experiment with 40 participants to see if the participants are more likely to accept the robot's suggestion in the Three-card Monte game, as a trust check task. Though we were unable to find a significant effect of humour, we discuss the effect of possible confounding variables, and also report some interesting qualitative observations from our study: for instance, the participants interacted effectively with the robot as a team member, regardless of the humour or no-humour condition.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2310.15020.pdf' target='_blank'>https://arxiv.org/pdf/2310.15020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Ai, Zhanxin Wu, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15020">Invariance is Key to Generalization: Examining the Role of Representation in Sim-to-Real Transfer for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The data-driven approach to robot control has been gathering pace rapidly, yet generalization to unseen task domains remains a critical challenge. We argue that the key to generalization is representations that are (i) rich enough to capture all task-relevant information and (ii) invariant to superfluous variability between the training and the test domains. We experimentally study such a representation -- containing both depth and semantic information -- for visual navigation and show that it enables a control policy trained entirely in simulated indoor scenes to generalize to diverse real-world environments, both indoors and outdoors. Further, we show that our representation reduces the A-distance between the training and test domains, improving the generalization error bound as a result. Our proposed approach is scalable: the learned policy improves continuously, as the foundation models that it exploits absorb more diverse data during pre-training.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2310.05865.pdf' target='_blank'>https://arxiv.org/pdf/2310.05865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil C. Janwani, Ersin DaÅ, Thomas Touma, Skylar X. Wei, Tamas G. Molnar, Joel W. Burdick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05865">A Learning-Based Framework for Safe Human-Robot Collaboration with Multiple Backup Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring robot safety in complex environments is a difficult task due to actuation limits, such as torque bounds. This paper presents a safety-critical control framework that leverages learning-based switching between multiple backup controllers to formally guarantee safety under bounded control inputs while satisfying driver intention. By leveraging backup controllers designed to uphold safety and input constraints, backup control barrier functions (BCBFs) construct implicitly defined control invariance sets via a feasible quadratic program (QP). However, BCBF performance largely depends on the design and conservativeness of the chosen backup controller, especially in our setting of human-driven vehicles in complex, e.g, off-road, conditions. While conservativeness can be reduced by using multiple backup controllers, determining when to switch is an open problem. Consequently, we develop a broadcast scheme that estimates driver intention and integrates BCBFs with multiple backup strategies for human-robot interaction. An LSTM classifier uses data inputs from the robot, human, and safety algorithms to continually choose a backup controller in real-time. We demonstrate our method's efficacy on a dual-track robot in obstacle avoidance scenarios. Our framework guarantees robot safety while adhering to driver intention.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2309.17257.pdf' target='_blank'>https://arxiv.org/pdf/2309.17257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17257">A Survey on Deep Learning Techniques for Action Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to anticipate possible future human actions is essential for a wide range of applications, including autonomous driving and human-robot interaction. Consequently, numerous methods have been introduced for action anticipation in recent years, with deep learning-based approaches being particularly popular. In this work, we review the recent advances of action anticipation algorithms with a particular focus on daily-living scenarios. Additionally, we classify these methods according to their primary contributions and summarize them in tabular form, allowing readers to grasp the details at a glance. Furthermore, we delve into the common evaluation metrics and datasets used for action anticipation and provide future directions with systematical discussions.
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2309.16898.pdf' target='_blank'>https://arxiv.org/pdf/2309.16898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JongYoon Lim, Inkyu Sa, Bruce MacDonald, Ho Seok Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16898">A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research explores using lightweight deep neural network architectures to enable the humanoid robot Pepper to understand American Sign Language (ASL) and facilitate non-verbal human-robot interaction. First, we introduce a lightweight and efficient model for ASL understanding optimized for embedded systems, ensuring rapid sign recognition while conserving computational resources. Building upon this, we employ large language models (LLMs) for intelligent robot interactions. Through intricate prompt engineering, we tailor interactions to allow the Pepper Robot to generate natural Co-Speech Gesture responses, laying the foundation for more organic and intuitive humanoid-robot dialogues. Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model. Leveraging the Pepper Robot's capabilities, we demonstrate the practicality and effectiveness of our approach in real-world scenarios. The results highlight a profound potential for enhancing human-robot interaction through non-verbal interactions, bridging communication gaps, and making technology more accessible and understandable.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2309.16219.pdf' target='_blank'>https://arxiv.org/pdf/2309.16219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilin Shan, Quang-Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16219">Sensorless Estimation of Contact Using Deep-Learning for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction has been an area of interest for decades. Collaborative tasks, such as joint compliance, demand high-quality joint torque sensing. While external torque sensors are reliable, they come with the drawbacks of being expensive and vulnerable to impacts. To address these issues, studies have been conducted to estimate external torques using only internal signals, such as joint states and current measurements. However, insufficient attention has been given to friction hysteresis approximation, which is crucial for tasks involving extensive dynamic to static state transitions. In this paper, we propose a deep-learning-based method that leverages a novel long-term memory scheme to achieve dynamics identification, accurately approximating the static hysteresis. We also introduce modifications to the well-known Residual Learning architecture, retaining high accuracy while reducing inference time. The robustness of the proposed method is illustrated through a joint compliance and task compliance experiment.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2309.12001.pdf' target='_blank'>https://arxiv.org/pdf/2309.12001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahya Ramezani, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12001">Exploring Human's Gender Perception and Bias toward Non-Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we investigate the human perception of gender and bias toward non-humanoid robots. As robots increasingly integrate into various sectors beyond industry, it is essential to understand how humans engage with non-humanoid robotic forms. This research focuses on the role of anthropomorphic cues, including gender signals, in influencing human robot interaction and user acceptance of non-humanoid robots. Through three surveys, we analyze how design elements such as physical appearance, voice modulation, and behavioral attributes affect gender perception and task suitability. Our findings demonstrate that even non-humanoid robots like Spot, Mini-Cheetah, and drones are subject to gender attribution based on anthropomorphic features, affecting their perceived roles and operational trustworthiness. The results underscore the importance of balancing design elements to optimize both functional efficiency and user relatability, particularly in critical contexts.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2309.05404.pdf' target='_blank'>https://arxiv.org/pdf/2309.05404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nat Wannawas, A. Aldo Faisal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05404">Physics-informed reinforcement learning via probabilistic co-adjustment functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning of real-world tasks is very data inefficient, and extensive simulation-based modelling has become the dominant approach for training systems. However, in human-robot interaction and many other real-world settings, there is no appropriate one-model-for-all due to differences in individual instances of the system (e.g. different people) or necessary oversimplifications in the simulation models. This requires two approaches: 1. either learning the individual system's dynamics approximately from data which requires data-intensive training or 2. using a complete digital twin of the instances, which may not be realisable in many cases. We introduce two approaches: co-kriging adjustments (CKA) and ridge regression adjustment (RRA) as novel ways to combine the advantages of both approaches. Our adjustment methods are based on an auto-regressive AR1 co-kriging model that we integrate with GP priors. This yield a data- and simulation-efficient way of using simplistic simulation models (e.g., simple two-link model) and rapidly adapting them to individual instances (e.g., biomechanics of individual people). Using CKA and RRA, we obtain more accurate uncertainty quantification of the entire system's dynamics than pure GP-based and AR1 methods. We demonstrate the efficiency of co-kriging adjustment with an interpretable reinforcement learning control example, learning to control a biomechanical human arm using only a two-link arm simulation model (offline part) and CKA derived from a small amount of interaction data (on-the-fly online). Our method unlocks an efficient and uncertainty-aware way to implement reinforcement learning methods in real world complex systems for which only imperfect simulation models exist.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2309.04687.pdf' target='_blank'>https://arxiv.org/pdf/2309.04687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxu Zhang, Parham M. Kebria, Shady Mohamed, Samson Yu, Saeid Nahavandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04687">A Review on Robot Manipulation Methods in Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot manipulation is an important part of human-robot interaction technology. However, traditional pre-programmed methods can only accomplish simple and repetitive tasks. To enable effective communication between robots and humans, and to predict and adapt to uncertain environments, this paper reviews recent autonomous and adaptive learning in robotic manipulation algorithms. It includes typical applications and challenges of human-robot interaction, fundamental tasks of robot manipulation and one of the most widely used formulations of robot manipulation, Markov Decision Process. Recent research focusing on robot manipulation is mainly based on Reinforcement Learning and Imitation Learning. This review paper shows the importance of Deep Reinforcement Learning, which plays an important role in manipulating robots to complete complex tasks in disturbed and unfamiliar environments. With the introduction of Imitation Learning, it is possible for robot manipulation to get rid of reward function design and achieve a simple, stable and supervised learning process. This paper reviews and compares the main features and popular algorithms for both Reinforcement Learning and Imitation Learning.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2309.01269.pdf' target='_blank'>https://arxiv.org/pdf/2309.01269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Naiseh, Mohammad D. Soorati, Sarvapali Ramchurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01269">Outlining the design space of eXplainable swarm (xSwarm): experts perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, agents interact through local roles to solve complex tasks beyond an individual's ability. Even though swarms are capable of carrying out some operations without the need for human intervention, many safety-critical applications still call for human operators to control and monitor the swarm. There are novel challenges to effective Human-Swarm Interaction (HSI) that are only beginning to be addressed. Explainability is one factor that can facilitate effective and trustworthy HSI and improve the overall performance of Human-Swarm team. Explainability was studied across various Human-AI domains, such as Human-Robot Interaction and Human-Centered ML. However, it is still ambiguous whether explanations studied in Human-AI literature would be beneficial in Human-Swarm research and development. Furthermore, the literature lacks foundational research on the prerequisites for explainability requirements in swarm robotics, i.e., what kind of questions an explainable swarm is expected to answer, and what types of explanations a swarm is expected to generate. By surveying 26 swarm experts, we seek to answer these questions and identify challenges experts faced to generate explanations in Human-Swarm environments. Our work contributes insights into defining a new area of research of eXplainable Swarm (xSwarm) which looks at how explainability can be implemented and developed in swarm systems. This paper opens the discussion on xSwarm and paves the way for more research in the field.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2308.14269.pdf' target='_blank'>https://arxiv.org/pdf/2308.14269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elad Liebman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14269">Utilizing Mood-Inducing Background Music in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Past research has clearly established that music can affect mood and that mood affects emotional and cognitive processing, and thus decision-making. It follows that if a robot interacting with a person needs to predict the person's behavior, knowledge of the music the person is listening to when acting is a potentially relevant feature. To date, however, there has not been any concrete evidence that a robot can improve its human-interactive decision-making by taking into account what the person is listening to. This research fills this gap by reporting the results of an experiment in which human participants were required to complete a task in the presence of an autonomous agent while listening to background music. Specifically, the participants drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results clearly indicate that such background information can be effectively incorporated in an agent's world representation in order to better predict people's behavior. We subsequently analyze how knowledge of music impacted both participant behavior and the resulting learned policy.\setcounter{footnote}{2}\footnote{An earlier version of part of the material in this paper appeared originally in the first author's Ph.D. Dissertation~\cite{liebman2020sequential} but it has not appeared in any pear-reviewed conference or journal.}
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2308.01466.pdf' target='_blank'>https://arxiv.org/pdf/2308.01466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierce Howell, Jack Kolb, Yifan Liu, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01466">The Effects of Robot Motion on Comfort Dynamics of Novice Users in Close-Proximity Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective and fluent close-proximity human-robot interaction requires understanding how humans get habituated to robots and how robot motion affects human comfort. While prior work has identified humans' preferences over robot motion characteristics and studied their influence on comfort, we are yet to understand how novice first-time robot users get habituated to robots and how robot motion impacts the dynamics of comfort over repeated interactions. To take the first step towards such understanding, we carry out a user study to investigate the connections between robot motion and user comfort and habituation. Specifically, we study the influence of workspace overlap, end-effector speed, and robot motion legibility on overall comfort and its evolution over repeated interactions. Our analyses reveal that workspace overlap, in contrast to speed and legibility, has a significant impact on users' perceived comfort and habituation. In particular, lower workspace overlap leads to users reporting significantly higher overall comfort, lower variations in comfort, and fewer fluctuations in comfort levels during habituation.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2307.10897.pdf' target='_blank'>https://arxiv.org/pdf/2307.10897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Merle M. Reimann, Florian A. Kunneman, Catharine Oertel, Koen V. Hindriks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10897">A Survey on Dialogue Management in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As social robots see increasing deployment within the general public, improving the interaction with those robots is essential. Spoken language offers an intuitive interface for the human-robot interaction (HRI), with dialogue management (DM) being a key component in those interactive systems. Yet, to overcome current challenges and manage smooth, informative and engaging interaction a more structural approach to combining HRI and DM is needed. In this systematic review, we analyse the current use of DM in HRI and focus on the type of dialogue manager used, its capabilities, evaluation methods and the challenges specific to DM in HRI. We identify the challenges and current scientific frontier related to the DM approach, interaction domain, robot appearance, physical situatedness and multimodality.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2306.14589.pdf' target='_blank'>https://arxiv.org/pdf/2306.14589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maciej K. Wozniak, Rebecca Stower, Patric Jensfelt, Andre Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14589">Happily Error After: Framework Development and User Study for Correcting Robot Perception Errors in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While we can see robots in more areas of our lives, they still make errors. One common cause of failure stems from the robot perception module when detecting objects. Allowing users to correct such errors can help improve the interaction and prevent the same errors in the future. Consequently, we investigate the effectiveness of a virtual reality (VR) framework for correcting perception errors of a Franka Panda robot. We conducted a user study with 56 participants who interacted with the robot using both VR and screen interfaces. Participants learned to collaborate with the robot faster in the VR interface compared to the screen interface. Additionally, participants found the VR interface more immersive, enjoyable, and expressed a preference for using it again. These findings suggest that VR interfaces may offer advantages over screen interfaces for human-robot interaction in erroneous environments.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2305.11978.pdf' target='_blank'>https://arxiv.org/pdf/2305.11978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Scheele, Pierce Howell, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11978">Fast Anticipatory Motion Planning for Close-Proximity Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective close-proximity human-robot interaction (CP-HRI) requires robots to be able to both efficiently perform tasks as well as adapt to human behavior and preferences. However, this ability is mediated by many, sometimes competing, aspects of interaction. We propose a real-time motion-planning framework for robotic manipulators that can simultaneously optimize a set of both task- and human-centric cost functions. To this end, we formulate a Nonlinear Model-Predictive Control (NMPC) problem with kino-dynamic constraints and efficiently solve it by leveraging recent advances in nonlinear trajectory optimization. We employ stochastic predictions of the human partner's trajectories in order to adapt the robot's nominal behavior in anticipation of its human partner. Our framework explicitly models and allows balancing of different task- and human-centric cost functions. While previous approaches to trajectory optimization for CP-HRI take anywhere from several seconds to a full minute to compute a trajectory, our approach is capable of computing one in 318 ms on average, enabling real-time implementation. We illustrate the effectiveness of our framework by simultaneously optimizing for separation distance, end-effector visibility, legibility, smoothness, and deviation from nominal behavior. We also demonstrate that our approach performs comparably to prior work in terms of the chosen cost functions, while significantly improving computational efficiency.
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2303.15453.pdf' target='_blank'>https://arxiv.org/pdf/2303.15453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenny Zhang, Samson Yu, Jiafei Duan, Cheston Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15453">Robustness of Utilizing Feedback in Embodied Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a framework for training an agent to actively request help in object-goal navigation tasks, with feedback indicating the location of the target object in its field of view. To make the agent more robust in scenarios where a teacher may not always be available, the proposed training curriculum includes a mix of episodes with and without feedback. The results show that this approach improves the agent's performance, even in the absence of feedback.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2303.15042.pdf' target='_blank'>https://arxiv.org/pdf/2303.15042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huajian Fang, Niklas Wittmer, Johannes Twiefel, Stefan Wermter, Timo Gerkmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15042">Partially Adaptive Multichannel Joint Reduction of Ego-noise and Environmental Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction relies on a noise-robust audio processing module capable of estimating target speech from audio recordings impacted by environmental noise, as well as self-induced noise, so-called ego-noise. While external ambient noise sources vary from environment to environment, ego-noise is mainly caused by the internal motors and joints of a robot. Ego-noise and environmental noise reduction are often decoupled, i.e., ego-noise reduction is performed without considering environmental noise. Recently, a variational autoencoder (VAE)-based speech model has been combined with a fully adaptive non-negative matrix factorization (NMF) noise model to recover clean speech under different environmental noise disturbances. However, its enhancement performance is limited in adverse acoustic scenarios involving, e.g. ego-noise. In this paper, we propose a multichannel partially adaptive scheme to jointly model ego-noise and environmental noise utilizing the VAE-NMF framework, where we take advantage of spatially and spectrally structured characteristics of ego-noise by pre-training the ego-noise model, while retaining the ability to adapt to unknown environmental noise. Experimental results show that our proposed approach outperforms the methods based on a completely fixed scheme and a fully adaptive scheme when ego-noise and environmental noise are present simultaneously.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2303.03560.pdf' target='_blank'>https://arxiv.org/pdf/2303.03560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dandan Zhang, Jin Zheng, Jialin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03560">IoHRT: An Open-Source Unified Framework Towards the Internet of Humans and Robotic Things with Cloud Computing for Home-Care Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accelerating aging population has led to an increasing demand for domestic robotics to ease caregivers' burden. The integration of Internet of Things (IoT), robotics, and human-robot interaction (HRI) technologies is essential for home-care applications. Although the concept of the Internet of Robotic Things (IoRT) has been utilized in various fields, most existing IoRT frameworks lack ergonomic HRI interfaces and are limited to specific tasks.
  This paper presents an open-source unified Internet of Humans and Robotic Things (IoHRT) framework with cloud computing, which combines personalized HRI interfaces with intelligent robotics and IoT techniques. This proposed open-source framework demonstrates characteristics of high security, compatibility, and modularity, allowing unlimited user access. Two case studies were conducted to evaluate the proposed framework's functionalities, evaluating its effectiveness in home-care scenarios. Users' feedback was collected via questionnaires, which indicates the IoHRT framework's high potential for home-care applications.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2303.02367.pdf' target='_blank'>https://arxiv.org/pdf/2303.02367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Rozlivek, Petr Svarny, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02367">Perirobot space representation for HRI: measuring and designing collaborative workspace coverage by diverse sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two regimes permitting safe physical human-robot interaction, speed and separation monitoring and safety-rated monitored stop, depend on reliable perception of the space surrounding the robot. This can be accomplished by visual sensors (like cameras, RGB-D cameras, LIDARs), proximity sensors, or dedicated devices used in industrial settings like pads that are activated by the presence of the operator. The deployment of a particular solution is often ad hoc and no unified representation of the interaction space or its coverage by the different sensors exists. In this work, we make first steps in this direction by defining the spaces to be monitored, representing all sensor data as information about occupancy and using occupancy-based metrics to calculate how a particular sensor covers the workspace. We demonstrate our approach in two (multi-)sensor-placement experiments in three static scenes and one experiment in a dynamic scene. The occupancy representation allow to compare the effectiveness of various sensor setups. Therefore, this approach can serve as a prototyping tool to establish the sensor setup that provides the most efficient coverage for the given metrics and sensor representations.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2301.09800.pdf' target='_blank'>https://arxiv.org/pdf/2301.09800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Boateng, Wenlong Zhang, Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09800">Improving Responsiveness to Robots for Tacit Human-Robot Interaction via Implicit and Naturalistic Team Status Projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fluent human-human teaming is often characterized by tacit interaction without explicit communication. This is because explicit communication, such as language utterances and gestures, are inherently interruptive. On the other hand, tacit interaction requires team situation awareness (TSA) to facilitate, which often relies on explicit communication to maintain, creating a paradox. In this paper, we consider implicit and naturalistic team status projection for tacit human-robot interaction. Implicitness minimizes interruption while naturalness reduces cognitive demand, and they together improve responsiveness to robots. We introduce a novel process for such Team status Projection via virtual Shadows, or TPS. We compare our method with two baselines that use explicit projection for maintaining TSA. Results via human factors studies demonstrate that TPS provides a more fluent human-robot interaction experience by significantly improving human responsiveness to robots in tacit teaming scenarios, which suggests better TSA. Participants acknowledged robots implementing TPS as more acceptable as a teammate and favorable. Simultaneously, we demonstrate that TPS is comparable to, and sometimes better than, the best-performing baseline in maintaining accurate TSA
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2301.03867.pdf' target='_blank'>https://arxiv.org/pdf/2301.03867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thorsten Hempel, Laslo Dinges, Ayoub Al-Hamadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03867">Sentiment-based Engagement Strategies for intuitive Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion expressions serve as important communicative signals and are crucial cues in intuitive interactions between humans. Hence, it is essential to include these fundamentals in robotic behavior strategies when interacting with humans to promote mutual understanding and to reduce misjudgements. We tackle this challenge by detecting and using the emotional state and attention for a sentiment analysis of potential human interaction partners to select well-adjusted engagement strategies. This way, we pave the way for more intuitive human-robot interactions, as the robot's action conforms to the person's mood and expectation. We propose four different engagement strategies with implicit and explicit communication techniques that we implement on a mobile robot platform for initial experiments.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2207.07742.pdf' target='_blank'>https://arxiv.org/pdf/2207.07742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Docekal, Jakub Rozlivek, Jiri Matas, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07742">Human keypoint detection for close proximity human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the performance of state-of-the-art human keypoint detectors in the context of close proximity human-robot interaction. The detection in this scenario is specific in that only a subset of body parts such as hands and torso are in the field of view. In particular, (i) we survey existing datasets with human pose annotation from the perspective of close proximity images and prepare and make publicly available a new Human in Close Proximity (HiCP) dataset; (ii) we quantitatively and qualitatively compare state-of-the-art human whole-body 2D keypoint detection methods (OpenPose, MMPose, AlphaPose, Detectron2) on this dataset; (iii) since accurate detection of hands and fingers is critical in applications with handovers, we evaluate the performance of the MediaPipe hand detector; (iv) we deploy the algorithms on a humanoid robot with an RGB-D camera on its head and evaluate the performance in 3D human keypoint detection. A motion capture system is used as reference.
  The best performing whole-body keypoint detectors in close proximity were MMPose and AlphaPose, but both had difficulty with finger detection. Thus, we propose a combination of MMPose or AlphaPose for the body and MediaPipe for the hands in a single framework providing the most accurate and robust detection. We also analyse the failure modes of individual detectors -- for example, to what extent the absence of the head of the person in the image degrades performance. Finally, we demonstrate the framework in a scenario where a humanoid robot interacting with a person uses the detected 3D keypoints for whole-body avoidance maneuvers.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2206.10606.pdf' target='_blank'>https://arxiv.org/pdf/2206.10606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenny Zhang, Samson Yu, Jiafei Duan, Cheston Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.10606">Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In reality, it is often more efficient to ask for help than to search the entire space to find an object with an unknown location. We present a learning framework that enables an agent to actively ask for help in such embodied visual navigation tasks, where the feedback informs the agent of where the goal is in its view. To emulate the real-world scenario that a teacher may not always be present, we propose a training curriculum where feedback is not always available. We formulate an uncertainty measure of where the goal is and use empirical results to show that through this approach, the agent learns to ask for help effectively while remaining robust when feedback is not available.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2107.06011.pdf' target='_blank'>https://arxiv.org/pdf/2107.06011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.06011">Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. Recent work introduces learnable policies parametrized by deep neural networks and trained with Reinforcement Learning (RL). In classical RL setups, the capacity to map and reason spatially is learned end-to-end, from reward alone. In this setting, we introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input. A learning-based agent from the literature trained with the proposed auxiliary losses was the winning entry to the Multi-Object Navigation Challenge, part of the CVPR 2021 Embodied AI Workshop.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2103.15990.pdf' target='_blank'>https://arxiv.org/pdf/2103.15990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rex Liu, Albara Ah Ramli, Huanle Zhang, Erik Henricson, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.15990">An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of the internet of things (IoT) and artificial intelligence (AI) technologies, human activity recognition (HAR) has been applied in a variety of domains such as security and surveillance, human-robot interaction, and entertainment. Even though a number of surveys and review papers have been published, there is a lack of HAR overview papers focusing on healthcare applications that use wearable sensors. Therefore, we fill in the gap by presenting this overview paper. In particular, we present our projects to illustrate the system design of HAR applications for healthcare. Our projects include early mobility identification of human activities for intensive care unit (ICU) patients and gait analysis of Duchenne muscular dystrophy (DMD) patients. We cover essential components of designing HAR systems including sensor factors (e.g., type, number, and placement location), AI model selection (e.g., classical machine learning models versus deep learning models), and feature engineering. In addition, we highlight the challenges of such healthcare-oriented HAR systems and propose several research opportunities for both the medical and the computer science community.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2509.18576.pdf' target='_blank'>https://arxiv.org/pdf/2509.18576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Kang, Liang He, Yanxin Zhang, Zuheng Ming, Kaixing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18576">LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2509.16176.pdf' target='_blank'>https://arxiv.org/pdf/2509.16176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Lin, Sophie Ziyu Liu, Ran Qi, George Z. Xue, Xinping Song, Chao Qin, Hugh H. -T. Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16176">Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories (ACDC), an autonomous drone cinematography system driven by natural language communication between human directors and drones. The main limitation of previous drone cinematography workflows is that they require manual selection of waypoints and view angles based on predefined human intent, which is labor-intensive and yields inconsistent performance. In this paper, we propose employing large language models (LLMs) and vision foundation models (VFMs) to convert free-form natural language prompts directly into executable indoor UAV video tours. Specifically, our method comprises a vision-language retrieval pipeline for initial waypoint selection, a preference-based Bayesian optimization framework that refines poses using aesthetic feedback, and a motion planner that generates safe quadrotor trajectories. We validate ACDC through both simulation and hardware-in-the-loop experiments, demonstrating that it robustly produces professional-quality footage across diverse indoor scenes without requiring expertise in robotics or cinematography. These results highlight the potential of embodied AI agents to close the loop from open-vocabulary dialogue to real-world autonomous aerial cinematography.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2509.15404.pdf' target='_blank'>https://arxiv.org/pdf/2509.15404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoting Peng, Katherine Driggs-Campbell, Roy Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15404">Trust-Aware Embodied Bayesian Persuasion for Mixed-Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and efficient interaction between autonomous vehicles (AVs) and human-driven vehicles (HVs) is a critical challenge for future transportation systems. While game-theoretic models capture how AVs influence HVs, they often suffer from a long-term decay of influence and can be perceived as manipulative, eroding the human's trust. This can paradoxically lead to riskier human driving behavior over repeated interactions. In this paper, we address this challenge by proposing the Trust-Aware Embodied Bayesian Persuasion (TA-EBP) framework. Our work makes three key contributions: First, we apply Bayesian persuasion to model communication at traffic intersections, offering a transparent alternative to traditional game-theoretic models. Second, we introduce a trust parameter to the persuasion framework, deriving a theorem for the minimum trust level required for influence. Finally, we ground the abstract signals of Bayesian persuasion theory into a continuous, physically meaningful action space, deriving a second theorem for the optimal signal magnitude, realized as an AV's forward nudge. Additionally, we validate our framework in a mixed-autonomy traffic simulation, demonstrating that TA-EBP successfully persuades HVs to drive more cautiously, eliminating collisions and improving traffic flow compared to baselines that either ignore trust or lack communication. Our work provides a transparent and non-strategic framework for influence in human-robot interaction, enhancing both safety and efficiency.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2509.05433.pdf' target='_blank'>https://arxiv.org/pdf/2509.05433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Chen, Domenico Chiaradia, Antonio Frisoli, Daniele Leonardis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05433">HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Haptic interfaces that can simultaneously modulate multiple physical properties remain a fundamental challenge in human-robot interaction. Existing systems typically allow the rendering of either geometric features or mechanical properties, but rarely both, within wearable form factors. Here, we introduce HapMorph, a pneumatic framework that enables continuous, simultaneous modulation of object size and stiffness through antagonistic fabric-based pneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for hands interaction achieving size variation from 50 to 104 mm, stiffness modulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through systematic characterization, we demonstrate decoupled control of size and stiffness properties via dual-chamber pressure regulation. Human perception studies with 10 participants reveal that users can distinguish nine discrete states across three size categories and three stiffness levels with 89.4% accuracy and 6.7 s average response time. We further demonstrate extended architectures that combine AFPAs with complementary pneumatic structures to enable shape or geometry morphing with concurrent stiffness control. Our results establish antagonistic pneumatic principle as a pathway toward next-generation haptic interfaces, capable of multi-dimensiona rendering properties within practical wearable constraints.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2509.04356.pdf' target='_blank'>https://arxiv.org/pdf/2509.04356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04356">SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2509.02983.pdf' target='_blank'>https://arxiv.org/pdf/2509.02983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02983">DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2509.00660.pdf' target='_blank'>https://arxiv.org/pdf/2509.00660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe Arias-Russi, Yuanchen Bai, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00660">CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz (WoZ) controlled robots to explore navigation, conversational dynamics, human-in-the-loop interactions, and more to explore appropriate robot behaviors in everyday settings. However, existing WoZ tools are often limited to one context, making them less adaptable across different settings, users, and robotic platforms. To mitigate these issues, we introduce a Context-Adaptable Robot Interface System (CARIS) that combines advanced robotic capabilities such teleoperation, human perception, human-robot dialogue, and multimodal data recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ control a robot in two contexts: 1) mental health companion and as a 2) tour guide. Furthermore, we identified areas of improvement for CARIS, including smoother integration between movement and communication, clearer functionality separation, recommended prompts, and one-click communication options to enhance the usability wizard control of CARIS. This project offers a publicly available, context-adaptable tool for the HRI community, enabling researchers to streamline data-driven approaches to intelligent robot behavior.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2509.00210.pdf' target='_blank'>https://arxiv.org/pdf/2509.00210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00210">Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2508.13982.pdf' target='_blank'>https://arxiv.org/pdf/2508.13982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Thompson, Kate Candon, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13982">The Social Context of Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term "social context" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term "social context". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2508.11988.pdf' target='_blank'>https://arxiv.org/pdf/2508.11988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Mastropasqua, Ignacio Bugueno-Cordova, Rodrigo Verschae, Daniel Acevedo, Pablo Negri, Maria E. Buemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11988">Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-expression analysis has applications in domains such as Human-Robot Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast facial movements remains difficult when relying solely on RGB cameras, due to limitations in temporal resolution and sensitivity to motion blur. Event cameras offer an alternative, with microsecond-level precision, high dynamic range, and low latency. However, public datasets featuring event-based recordings of Action Units are still scarce. In this work, we introduce a novel, preliminary multi-resolution and multi-modal micro-expression dataset recorded with synchronized RGB and event cameras under variable lighting conditions. Two baseline tasks are evaluated to explore the spatial-temporal dynamics of micro-expressions: Action Unit classification using Spiking Neural Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame reconstruction using Conditional Variational Autoencoders, achieving SSIM = 0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising results show that event-based data can be used for micro-expression recognition and frame reconstruction.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2508.01651.pdf' target='_blank'>https://arxiv.org/pdf/2508.01651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Wang, Zhenhao Zhang, Kaiyang Ji, Mingyu Liu, Wenti Yin, Yuchao Chen, Zhirui Liu, Xiangyu Zeng, Tianxiang Gui, Hangxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01651">DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object affordance grounding aims to predict the touchable regions on a 3d object, which is crucial for human-object interaction, human-robot interaction, embodied perception, and robot learning. Recent advances tackle this problem via learning from demonstration images. However, these methods fail to capture the general affordance knowledge within the image, leading to poor generalization. To address this issue, we propose to use text-to-image diffusion models to extract the general affordance knowledge because we find that such models can generate semantically valid HOI images, which demonstrate that their internal representation space is highly correlated with real-world affordance concepts. Specifically, we introduce the DAG, a diffusion-based 3d affordance grounding framework, which leverages the frozen internal representations of the text-to-image diffusion model and unlocks affordance knowledge within the diffusion model to perform 3D affordance grounding. We further introduce an affordance block and a multi-source affordance decoder to endow 3D dense affordance prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2507.23544.pdf' target='_blank'>https://arxiv.org/pdf/2507.23544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Miyoshi, Yuki Okafuji, Takuya Iwamoto, Junya Nakanishi, Jun Baba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23544">User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the demand for social robots has grown, requiring them to adapt their behaviors based on users' states. Accurately assessing user experience (UX) in human-robot interaction (HRI) is crucial for achieving this adaptability. UX is a multi-faceted measure encompassing aspects such as sentiment and engagement, yet existing methods often focus on these individually. This study proposes a UX estimation method for HRI by leveraging multimodal social signals. We construct a UX dataset and develop a Transformer-based model that utilizes facial expressions and voice for estimation. Unlike conventional models that rely on momentary observations, our approach captures both short- and long-term interaction patterns using a multi-instance learning framework. This enables the model to capture temporal dynamics in UX, providing a more holistic representation. Experimental results demonstrate that our method outperforms third-party human evaluators in UX estimation.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2507.17383.pdf' target='_blank'>https://arxiv.org/pdf/2507.17383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas P Zollo, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17383">Confidence Calibration in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2507.06700.pdf' target='_blank'>https://arxiv.org/pdf/2507.06700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Pandey, Ramviyas Parasuraman, Prashant Doshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06700">Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety in human-robot interaction (HRI) is essential to foster user trust and enable the broader adoption of robotic systems. Traditional safety models primarily rely on sensor-based measures, such as relative distance and velocity, to assess physical safety. However, these models often fail to capture subjective safety perceptions, which are shaped by individual traits and contextual factors. In this paper, we introduce and analyze a parameterized general safety model that bridges the gap between physical and perceived safety by incorporating a personalization parameter, $Ï$, into the safety measurement framework to account for individual differences in safety perception. Through a series of hypothesis-driven human-subject studies in a simulated rescue scenario, we investigate how emotional state, trust, and robot behavior influence perceived safety. Our results show that $Ï$ effectively captures meaningful individual differences, driven by affective responses, trust in task consistency, and clustering into distinct user types. Specifically, our findings confirm that predictable and consistent robot behavior as well as the elicitation of positive emotional states, significantly enhance perceived safety. Moreover, responses cluster into a small number of user types, supporting adaptive personalization based on shared safety models. Notably, participant role significantly shapes safety perception, and repeated exposure reduces perceived safety for participants in the casualty role, emphasizing the impact of physical interaction and experiential change. These findings highlight the importance of adaptive, human-centered safety models that integrate both psychological and behavioral dimensions, offering a pathway toward more trustworthy and effective HRI in safety-critical domains.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2507.05419.pdf' target='_blank'>https://arxiv.org/pdf/2507.05419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aliasghar Khani, Arianna Rampini, Bruno Roy, Larasika Nadela, Noa Kaplan, Evan Atherton, Derek Cheung, Jacky Bibliowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05419">Motion Generation: A Survey of Generative Approaches and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion generation, the task of synthesizing realistic motion sequences from various conditioning inputs, has become a central problem in computer vision, computer graphics, and robotics, with applications ranging from animation and virtual agents to human-robot interaction. As the field has rapidly progressed with the introduction of diverse modeling paradigms including GANs, autoencoders, autoregressive models, and diffusion-based techniques, each approach brings its own advantages and limitations. This growing diversity has created a need for a comprehensive and structured review that specifically examines recent developments from the perspective of the generative approach employed.
  In this survey, we provide an in-depth categorization of motion generation methods based on their underlying generative strategies. Our main focus is on papers published in top-tier venues since 2023, reflecting the most recent advancements in the field. In addition, we analyze architectural principles, conditioning mechanisms, and generation settings, and compile a detailed overview of the evaluation metrics and datasets used across the literature. Our objective is to enable clearer comparisons and identify open challenges, thereby offering a timely and foundational reference for researchers and practitioners navigating the rapidly evolving landscape of motion generation.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2506.20268.pdf' target='_blank'>https://arxiv.org/pdf/2506.20268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruben Janssens, Jens De Bock, Sofie Labat, Eva Verhelst, Veronique Hoste, Tony Belpaeme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20268">Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting miscommunication in human-robot interaction is a critical function for maintaining user engagement and trust. While humans effortlessly detect communication errors in conversations through both verbal and non-verbal cues, robots face significant challenges in interpreting non-verbal feedback, despite advances in computer vision for recognizing affective expressions. This research evaluates the effectiveness of machine learning models in detecting miscommunications in robot dialogue. Using a multi-modal dataset of 240 human-robot conversations, where four distinct types of conversational failures were systematically introduced, we assess the performance of state-of-the-art computer vision models. After each conversational turn, users provided feedback on whether they perceived an error, enabling an analysis of the models' ability to accurately detect robot mistakes. Despite using state-of-the-art models, the performance barely exceeds random chance in identifying miscommunication, while on a dataset with more expressive emotional content, they successfully identified confused states. To explore the underlying cause, we asked human raters to do the same. They could also only identify around half of the induced miscommunications, similarly to our model. These results uncover a fundamental limitation in identifying robot miscommunications in dialogue: even when users perceive the induced miscommunication as such, they often do not communicate this to their robotic conversation partner. This knowledge can shape expectations of the performance of computer vision models and can help researchers to design better human-robot conversations by deliberately eliciting feedback where needed.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2506.20212.pdf' target='_blank'>https://arxiv.org/pdf/2506.20212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Bussolan, Oliver Avram, Andrea Pignata, Gianvito Urgese, Stefano Baraldo, Anna Valente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20212">Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of Industry 5.0, manufacturers are increasingly prioritizing worker well-being alongside mass customization. Stress-aware Human-Robot Collaboration (HRC) plays a crucial role in this paradigm, where robots must adapt their behavior to human mental states to improve collaboration fluency and safety. This paper presents a novel framework that integrates Federated Learning (FL) to enable personalized mental state evaluation while preserving user privacy. By leveraging physiological signals, including EEG, ECG, EDA, EMG, and respiration, a multimodal model predicts an operator's stress level, facilitating real-time robot adaptation. The FL-based approach allows distributed on-device training, ensuring data confidentiality while improving model generalization and individual customization. Results demonstrate that the deployment of an FL approach results in a global model with performance in stress prediction accuracy comparable to a centralized training approach. Moreover, FL allows for enhancing personalization, thereby optimizing human-robot interaction in industrial settings, while preserving data privacy. The proposed framework advances privacy-preserving, adaptive robotics to enhance workforce well-being in smart manufacturing.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2506.17991.pdf' target='_blank'>https://arxiv.org/pdf/2506.17991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thien Tran, Jonathan Kua, Minh Tran, Honghao Lyu, Thuong Hoang, Jiong Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17991">CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Telerobotics is a key foundation in autonomous Industrial Cyber-Physical Systems (ICPS), enabling remote operations across various domains. However, conventional cloud-based telerobotics suffers from latency, reliability, scalability, and resilience issues, hindering real-time performance in critical applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation (CFA) paradigm to address these limitations by leveraging a distributed Cloud-Edge-Robotics computing architecture, enabling deterministic connectivity, deterministic connected intelligence, and deterministic networked computing. This paper synthesizes recent advancements in CFTel, aiming to highlight its role in facilitating scalable, low-latency, autonomous, and AI-driven telerobotics. We analyze architectural frameworks and technologies that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel has the potential to enhance real-time control, scalability, and autonomy while supporting service-oriented solutions. We also discuss practical challenges, including latency constraints, cybersecurity risks, interoperability issues, and standardization efforts. This work serves as a foundational reference for researchers, stakeholders, and industry practitioners in future telerobotics research.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2506.15150.pdf' target='_blank'>https://arxiv.org/pdf/2506.15150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanlong Ji, Xingbang Yang, Ruoqi Zhao, Qihan Ye, Quan Zheng, Yubo Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15150">Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait phase estimation based on inertial measurement unit (IMU) signals facilitates precise adaptation of exoskeletons to individual gait variations. However, challenges remain in achieving high accuracy and robustness, particularly during periods of terrain changes. To address this, we develop a gait phase estimation neural network based on implicit modeling of human locomotion, which combines temporal convolution for feature extraction with transformer layers for multi-channel information fusion. A channel-wise masked reconstruction pre-training strategy is proposed, which first treats gait phase state vectors and IMU signals as joint observations of human locomotion, thus enhancing model generalization. Experimental results demonstrate that the proposed method outperforms existing baseline approaches, achieving a gait phase RMSE of $2.729 \pm 1.071%$ and phase rate MAE of $0.037 \pm 0.016%$ under stable terrain conditions with a look-back window of 2 seconds, and a phase RMSE of $3.215 \pm 1.303%$ and rate MAE of $0.050 \pm 0.023%$ under terrain transitions. Hardware validation on a hip exoskeleton further confirms that the algorithm can reliably identify gait cycles and key events, adapting to various continuous motion scenarios. This research paves the way for more intelligent and adaptive exoskeleton systems, enabling safer and more efficient human-robot interaction across diverse real-world environments.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2506.13937.pdf' target='_blank'>https://arxiv.org/pdf/2506.13937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caio C. G. Ribeiro, Douglas G. Macharet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13937">Beyond the Plane: A 3D Representation of Human Personal Space for Socially-Aware Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing presence of robots in human environments requires them to exhibit socially appropriate behavior, adhering to social norms. A critical aspect in this context is the concept of personal space, a psychological boundary around an individual that influences their comfort based on proximity. This concept extends to human-robot interaction, where robots must respect personal space to avoid causing discomfort. While much research has focused on modeling personal space in two dimensions, almost none have considered the vertical dimension. In this work, we propose a novel three-dimensional personal space model that integrates both height (introducing a discomfort function along the Z-axis) and horizontal proximity (via a classic XY-plane formulation) to quantify discomfort. To the best of our knowledge, this is the first work to compute discomfort in 3D space at any robot component's position, considering the person's configuration and height.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2506.12678.pdf' target='_blank'>https://arxiv.org/pdf/2506.12678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranay Gupta, Henny Admoni, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12678">Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end visuomotor policies trained using behavior cloning have shown a remarkable ability to generate complex, multi-modal low-level robot behaviors. However, at deployment time, these policies still struggle to act reliably when faced with out-of-distribution (OOD) visuals induced by objects, backgrounds, or environment changes. Prior works in interactive imitation learning solicit corrective expert demonstrations under the OOD conditions -- but this can be costly and inefficient. We observe that task success under OOD conditions does not always warrant novel robot behaviors. In-distribution (ID) behaviors can directly be transferred to OOD conditions that share functional similarities with ID conditions. For example, behaviors trained to interact with in-distribution (ID) pens can apply to interacting with a visually-OOD pencil. The key challenge lies in disambiguating which ID observations functionally correspond to the OOD observation for the task at hand. We propose that an expert can provide this OOD-to-ID functional correspondence. Thus, instead of collecting new demonstrations and re-training at every OOD encounter, our method: (1) detects the need for feedback by first checking if current observations are OOD and then identifying whether the most similar training observations show divergent behaviors, (2) solicits functional correspondence feedback to disambiguate between those behaviors, and (3) intervenes on the OOD observations with the functionally corresponding ID observations to perform deployment-time generalization. We validate our method across diverse real-world robotic manipulation tasks with a Franka Panda robotic manipulator. Our results show that test-time functional correspondences can improve the generalization of a vision-based diffusion policy to OOD objects and environment conditions with low feedback.
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2506.09697.pdf' target='_blank'>https://arxiv.org/pdf/2506.09697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Franceschi, Andrea Bussolan, Vincenzo Pomponi, Oliver Avram, Stefano Baraldo, Anna Valente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09697">Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, industries are showing a growing interest in human-robot collaboration, particularly for shared tasks. This requires intelligent strategies to plan a robot's motions, considering both task constraints and human-specific factors such as height and movement preferences. This work introduces a novel approach to generate personalized trajectories using Dynamic Movement Primitives (DMPs), enhanced with real-time velocity scaling based on human feedback. The method was rigorously tested in industrial-grade experiments, focusing on the collaborative transport of an engine cowl lip section. Comparative analysis between DMP-generated trajectories and a state-of-the-art motion planner (BiTRRT) highlights their adaptability combined with velocity scaling. Subjective user feedback further demonstrates a clear preference for DMP- based interactions. Objective evaluations, including physiological measurements from brain and skin activity, reinforce these findings, showcasing the advantages of DMPs in enhancing human-robot interaction and improving user experience.
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2506.09172.pdf' target='_blank'>https://arxiv.org/pdf/2506.09172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09172">An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent innovations in multimodal action models represent a promising direction for developing general-purpose agentic systems, combining visual understanding, language comprehension, and action generation. We introduce MultiNet - a novel, fully open-source benchmark and surrounding software ecosystem designed to rigorously evaluate and adapt models across vision, language, and action domains. We establish standardized evaluation protocols for assessing vision-language models (VLMs) and vision-language-action models (VLAs), and provide open source software to download relevant data, models, and evaluations. Additionally, we provide a composite dataset with over 1.3 trillion tokens of image captioning, visual question answering, commonsense reasoning, robotic control, digital game-play, simulated locomotion/manipulation, and many more tasks. The MultiNet benchmark, framework, toolkit, and evaluation harness have been used in downstream research on the limitations of VLA generalization.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2506.03516.pdf' target='_blank'>https://arxiv.org/pdf/2506.03516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Debnath, Gregory J. Stein, Jana Kosecka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03516">SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object goal navigation is a fundamental task in embodied AI, where an agent is instructed to locate a target object in an unexplored environment. Traditional learning-based methods rely heavily on large-scale annotated data or require extensive interaction with the environment in a reinforcement learning setting, often failing to generalize to novel environments and limiting scalability. To overcome these challenges, we explore a zero-shot setting where the agent operates without task-specific training, enabling more scalable and adaptable solution. Recent advances in Vision Foundation Models (VFMs) offer powerful capabilities for visual understanding and reasoning, making them ideal for agents to comprehend scenes, identify relevant regions, and infer the likely locations of objects. In this work, we present a zero-shot object goal navigation framework that integrates the perceptual strength of VFMs with a model-based planner that is capable of long-horizon decision making through frontier exploration. We evaluate our approach on the HM3D dataset using the Habitat simulator and demonstrate that our method achieves state-of-the-art performance in terms of success weighted by path length for zero-shot object goal navigation.
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2506.02265.pdf' target='_blank'>https://arxiv.org/pdf/2506.02265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Li, Pujith Kachana, Prajwal Chidananda, Saurabh Nair, Yasutaka Furukawa, Matthew Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02265">Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating agent pose and 3D scene structure from multi-camera rigs is a central task in embodied AI applications such as autonomous driving. Recent learned approaches such as DUSt3R have shown impressive performance in multiview settings. However, these models treat images as unstructured collections, limiting effectiveness in scenarios where frames are captured from synchronized rigs with known or inferable structure.
  To this end, we introduce Rig3R, a generalization of prior multiview reconstruction models that incorporates rig structure when available, and learns to infer it when not. Rig3R conditions on optional rig metadata including camera ID, time, and rig poses to develop a rig-aware latent space that remains robust to missing information. It jointly predicts pointmaps and two types of raymaps: a pose raymap relative to a global frame, and a rig raymap relative to a rig-centric frame consistent across time. Rig raymaps allow the model to infer rig structure directly from input images when metadata is missing.
  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose estimation, and rig discovery, outperforming both traditional and learned methods by 17-45% mAA across diverse real-world rig datasets, all in a single forward pass without post-processing or iterative refinement.
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2506.00075.pdf' target='_blank'>https://arxiv.org/pdf/2506.00075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Pollini, Bruna V. Guterres, Rodrigo S. Guerra, Ricardo B. Grando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00075">Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2505.23153.pdf' target='_blank'>https://arxiv.org/pdf/2505.23153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Wang, Shaoshan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23153">Conceptual Framework Toward Embodied Collective Adaptive Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2505.10359.pdf' target='_blank'>https://arxiv.org/pdf/2505.10359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Le Shi, Yifei Shi, Xin Xu, Tenglong Liu, Junhua Xi, Chengyuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10359">NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep generative models demonstrate unprecedented zero-shot generalization capabilities, offering great potential for robot manipulation in unstructured environments. Given a partial observation of a scene, deep generative models could generate the unseen regions and therefore provide more context, which enhances the capability of robots to generalize across unseen environments. However, due to the visual artifacts in generated images and inefficient integration of multi-modal features in policy learning, this direction remains an open challenge. We introduce NVSPolicy, a generalizable language-conditioned policy learning method that couples an adaptive novel-view synthesis module with a hierarchical policy network. Given an input image, NVSPolicy dynamically selects an informative viewpoint and synthesizes an adaptive novel-view image to enrich the visual context. To mitigate the impact of the imperfect synthesized images, we adopt a cycle-consistent VAE mechanism that disentangles the visual features into the semantic feature and the remaining feature. The two features are then fed into the hierarchical policy network respectively: the semantic feature informs the high-level meta-skill selection, and the remaining feature guides low-level action estimation. Moreover, we propose several practical mechanisms to make the proposed method efficient. Extensive experiments on CALVIN demonstrate the state-of-the-art performance of our method. Specifically, it achieves an average success rate of 90.4\% across all tasks, greatly outperforming the recent methods. Ablation studies confirm the significance of our adaptive novel-view synthesis paradigm. In addition, we evaluate NVSPolicy on a real-world robotic platform to demonstrate its practical applicability.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2505.05540.pdf' target='_blank'>https://arxiv.org/pdf/2505.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Harshvardhan Sikka, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05540">Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLMs and VLAs - including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST - on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity; (2) VLAs generally outperforms other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering. We release our benchmark, evaluation framework, and findings to enable the assessment of future VLA models and identify critical areas for improvement in their application to out-of-distribution digital tasks.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2505.03929.pdf' target='_blank'>https://arxiv.org/pdf/2505.03929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael R. Baptista, Nina R. Gerszberg, Ricardo V. Godoy, Gustavo J. G. Lahr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03929">MIHRaGe: A Mixed-Reality Interface for Human-Robot Interaction via Gaze-Oriented Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Individuals with upper limb mobility impairments often require assistive technologies to perform activities of daily living. While gaze-tracking has emerged as a promising method for robotic assistance, existing solutions lack sufficient feedback mechanisms, leading to uncertainty in user intent recognition and reduced adaptability. This paper presents the MIHRAGe interface, an integrated system that combines gaze-tracking, robotic assistance, and a mixed-reality to create an immersive environment for controlling the robot using only eye movements. The system was evaluated through an experimental protocol involving four participants, assessing gaze accuracy, robotic positioning precision, and the overall success of a pick and place task. Results showed an average gaze fixation error of 1.46 cm, with individual variations ranging from 1.28 cm to 2.14 cm. The robotic arm demonstrated an average positioning error of +-1.53 cm, with discrepancies attributed to interface resolution and calibration constraints. In a pick and place task, the system achieved a success rate of 80%, highlighting its potential for improving accessibility in human-robot interaction with visual feedback to the user.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2505.02414.pdf' target='_blank'>https://arxiv.org/pdf/2505.02414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Hafner, Chaoran Liu, Carlos Ishi, Hiroshi Ishiguro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02414">Quadrupedal Spine Control Strategies: Exploring Correlations Between System Dynamic Responses and Human Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike their biological cousins, the majority of existing quadrupedal robots are constructed with rigid chassis. This results in motion that is either beetle-like or distinctly robotic, lacking the natural fluidity characteristic of mammalian movements. Existing literature on quadrupedal robots with spinal configurations primarily focuses on energy efficiency and does not consider the effects in human-robot interaction scenarios. Our contributions include an initial investigation into various trajectory generation strategies for a quadrupedal robot with a four degree of freedom spine, and an analysis on the effect that such methods have on human perception of gait naturalness compared to a fixed spine baseline. The strategies were evaluated using videos of walking, trotting and turning simulations. Among the four different strategies developed, the optimised time varying and the foot-tracking strategies were perceived to be more natural than the baseline in a randomised trial with 50 participants. Although none of the strategies demonstrated any energy efficiency improvements over the no-spine baseline, some showed greater footfall consistency at higher speeds. Given the greater likeability drawn from the more natural locomotion patterns, this type of robot displays potential for applications in social robot scenarios such as elderly care, where energy efficiency is not a primary concern.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2505.00693.pdf' target='_blank'>https://arxiv.org/pdf/2505.00693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbang Li, Ziyang Gong, Haoyang Li, Xiaoqi Huang, Haolan Kang, Guangping Bai, Xianzheng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00693">Robotic Visual Instruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision introduces challenges for robotic task definition such as ambiguity and verbosity. Moreover, in some public settings where quiet is required, such as libraries or hospitals, verbal communication with robots is inappropriate. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment,enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Project website: https://robotic-visual-instruction.github.io/
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2504.16649.pdf' target='_blank'>https://arxiv.org/pdf/2504.16649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Lin, Yuzhe Huang, Wanlin Li, Jianpeng Ma, Chenxi Xiao, Ziyuan Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16649">PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly envisioned as human companions, assisting with everyday tasks that often involve manipulating deformable objects. Although recent advances in robotic hardware and embodied AI have expanded their capabilities, current systems still struggle with handling thin, flat, and deformable objects such as paper and fabric. This limitation arises from the lack of suitable perception techniques for robust state estimation under diverse object appearances, as well as the absence of planning techniques for generating appropriate grasp motions. To bridge these gaps, this paper introduces PP-Tac, a robotic system for picking up paper-like objects. PP-Tac features a multi-fingered robotic hand with high-resolution omnidirectional tactile sensors \sensorname. This hardware configuration enables real-time slip detection and online frictional force control that mitigates such slips. Furthermore, grasp motion generation is achieved through a trajectory synthesis pipeline, which first constructs a dataset of finger's pinching motions. Based on this dataset, a diffusion-based policy is trained to control the hand-arm robotic system. Experiments demonstrate that PP-Tac can effectively grasp paper-like objects of varying material, thickness, and stiffness, achieving an overall success rate of 87.5\%. To our knowledge, this work is the first attempt to grasp paper-like deformable objects using a tactile dexterous hand. Our project webpage can be found at: https://peilin-666.github.io/projects/PP-Tac/
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2504.09927.pdf' target='_blank'>https://arxiv.org/pdf/2504.09927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyong Yu, Yanqiong Jin, Yonghao He, Wei Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09927">Efficient Task-specific Conditional Diffusion Policies: Shortcut Model Acceleration and SO(3) Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning, particularly Diffusion Policies based methods, has recently gained significant traction in embodied AI as a powerful approach to action policy generation. These models efficiently generate action policies by learning to predict noise. However, conventional Diffusion Policy methods rely on iterative denoising, leading to inefficient inference and slow response times, which hinder real-time robot control. To address these limitations, we propose a Classifier-Free Shortcut Diffusion Policy (CF-SDP) that integrates classifier-free guidance with shortcut-based acceleration, enabling efficient task-specific action generation while significantly improving inference speed. Furthermore, we extend diffusion modeling to the SO(3) manifold in shortcut model, defining the forward and reverse processes in its tangent space with an isotropic Gaussian distribution. This ensures stable and accurate rotational estimation, enhancing the effectiveness of diffusion-based control. Our approach achieves nearly 5x acceleration in diffusion inference compared to DDIM-based Diffusion Policy while maintaining task performance. Evaluations both on the RoboTwin simulation platform and real-world scenarios across various tasks demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2504.05291.pdf' target='_blank'>https://arxiv.org/pdf/2504.05291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haley N. Green, Tariq Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05291">Using Physiological Measures, Gaze, and Facial Expressions to Model Human Trust in a Robot Partner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With robots becoming increasingly prevalent in various domains, it has become crucial to equip them with tools to achieve greater fluency in interactions with humans. One of the promising areas for further exploration lies in human trust. A real-time, objective model of human trust could be used to maximize productivity, preserve safety, and mitigate failure. In this work, we attempt to use physiological measures, gaze, and facial expressions to model human trust in a robot partner. We are the first to design an in-person, human-robot supervisory interaction study to create a dedicated trust dataset. Using this dataset, we train machine learning algorithms to identify the objective measures that are most indicative of trust in a robot partner, advancing trust prediction in human-robot interactions. Our findings indicate that a combination of sensor modalities (blood volume pulse, electrodermal activity, skin temperature, and gaze) can enhance the accuracy of detecting human trust in a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision Trees classifiers exhibit consistently better performance in measuring the person's trust in the robot partner. These results lay the groundwork for constructing a real-time trust model for human-robot interaction, which could foster more efficient interactions between humans and robots.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2503.19941.pdf' target='_blank'>https://arxiv.org/pdf/2503.19941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Sun, Pengfei Tian, Xiaozhu Hu, Xiaoyu Zhao, Huiying Li, Zhenliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19941">Body Discovery of Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the pursuit of realizing artificial general intelligence (AGI), the importance of embodied artificial intelligence (AI) becomes increasingly apparent. Following this trend, research integrating robots with AGI has become prominent. As various kinds of embodiments have been designed, adaptability to diverse embodiments will become important to AGI. We introduce a new challenge, termed "Body Discovery of Embodied AI", focusing on tasks of recognizing embodiments and summarizing neural signal functionality. The challenge encompasses the precise definition of an AI body and the intricate task of identifying embodiments in dynamic environments, where conventional approaches often prove inadequate. To address these challenges, we apply causal inference method and evaluate it by developing a simulator tailored for testing algorithms with virtual environments. Finally, we validate the efficacy of our algorithms through empirical testing, demonstrating their robust performance in various scenarios based on virtual environments.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2503.17046.pdf' target='_blank'>https://arxiv.org/pdf/2503.17046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsheng Yang, Qianying Liu, Wataru Sato, Takashi Minato, Chaoran Liu, Shin'ya Nishida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17046">HAPI: A Model for Learning Robot Facial Expressions from Human Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic robotic facial expression generation is crucial for human-robot interaction, as handcrafted methods based on fixed joint configurations often yield rigid and unnatural behaviors. Although recent automated techniques reduce the need for manual tuning, they tend to fall short by not adequately bridging the gap between human preferences and model predictions-resulting in a deficiency of nuanced and realistic expressions due to limited degrees of freedom and insufficient perceptual integration. In this work, we propose a novel learning-to-rank framework that leverages human feedback to address this discrepancy and enhanced the expressiveness of robotic faces. Specifically, we conduct pairwise comparison annotations to collect human preference data and develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese RankNet-based approach that refines expression evaluation. Results obtained via Bayesian Optimization and online expression survey on a 35-DOF android platform demonstrate that our approach produces significantly more realistic and socially resonant expressions of Anger, Happiness, and Surprise than those generated by baseline and expert-designed methods. This confirms that our framework effectively bridges the gap between human preferences and model predictions while robustly aligning robotic expression generation with human affective responses.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2503.12034.pdf' target='_blank'>https://arxiv.org/pdf/2503.12034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enes Erdogan, Eren Erdal Aksoy, Sanem Sariel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12034">Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognition of human manipulation actions in real-time is essential for safe and effective human-robot interaction and collaboration. The challenge lies in developing a model that is both lightweight enough for real-time execution and capable of generalization. While some existing methods in the literature can run in real-time, they struggle with temporal scalability, i.e., they fail to adapt to long-duration manipulations effectively. To address this, leveraging the generalizable scene graph representations, we propose a new Factorized Graph Sequence Encoder network that not only runs in real-time but also scales effectively in the temporal dimension, thanks to its factorized encoder architecture. Additionally, we introduce Hand Pooling operation, a simple pooling operation for more focused extraction of the graph-level embeddings. Our model outperforms the previous state-of-the-art real-time approach, achieving a 14.3\% and 5.6\% improvement in F1-macro score on the KIT Bimanual Action (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively. Moreover, we conduct an extensive ablation study to validate our network design choices. Finally, we compare our model with its architecturally similar RGB-based model on the Bimacs dataset and show the limitations of this model in contrast to ours on such an object-centric manipulation dataset.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2503.10928.pdf' target='_blank'>https://arxiv.org/pdf/2503.10928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Widhalm, Cory Ohnsted, Corey Knutson, Demetrious Kutzke, Sakshi Singh, Rishi Mukherjee, Grant Schwidder, Ying-Kun Wu, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10928">Design and Development of the MeCO Open-Source Autonomous Underwater Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MeCO, the Medium Cost Open-source autonomous underwater vehicle (AUV), a versatile autonomous vehicle designed to support research and development in underwater human-robot interaction (UHRI) and marine robotics in general. An inexpensive platform to build compared to similarly-capable AUVs, the MeCO design and software are released under open-source licenses, making it a cost effective, extensible, and open platform. It is equipped with UHRI-focused systems, such as front and side facing displays, light-based communication devices, a transducer for acoustic interaction, and stereo vision, in addition to typical AUV sensing and actuation components. Additionally, MeCO is capable of real-time deep learning inference using the latest edge computing devices, while maintaining low-latency, closed-loop control through high-performance microcontrollers. MeCO is designed from the ground up for modularity in internal electronics, external payloads, and software architecture, exploiting open-source robotics and containerarization tools. We demonstrate the diverse capabilities of MeCO through simulated, closed-water, and open-water experiments. All resources necessary to build and run MeCO, including software and hardware design, have been made publicly available.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2503.09959.pdf' target='_blank'>https://arxiv.org/pdf/2503.09959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiansheng Li, Haotian Song, Jinni Zhou, Qiang Nie, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09959">RMG: Real-Time Expressive Motion Generation with Self-collision Avoidance for 6-DOF Companion Robotic Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The six-degree-of-freedom (6-DOF) robotic arm has gained widespread application in human-coexisting environments. While previous research has predominantly focused on functional motion generation, the critical aspect of expressive motion in human-robot interaction remains largely unexplored. This paper presents a novel real-time motion generation planner that enhances interactivity by creating expressive robotic motions between arbitrary start and end states within predefined time constraints. Our approach involves three key contributions: first, we develop a mapping algorithm to construct an expressive motion dataset derived from human dance movements; second, we train motion generation models in both Cartesian and joint spaces using this dataset; third, we introduce an optimization algorithm that guarantees smooth, collision-free motion while maintaining the intended expressive style. Experimental results demonstrate the effectiveness of our method, which can generate expressive and generalized motions in under 0.5 seconds while satisfying all specified constraints.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2503.05825.pdf' target='_blank'>https://arxiv.org/pdf/2503.05825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Sherwin Stephen Chan, Mingyuan Lei, Lek Syn Lim, Henry Johan, Bingran Zuo, Wei Tech Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05825">A Human-In-The-Loop Simulation Framework for Evaluating Control Strategies in Gait Assistive Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the global population ages, effective rehabilitation and mobility aids will become increasingly critical. Gait assistive robots are promising solutions, but designing adaptable controllers for various impairments poses a significant challenge. This paper presented a Human-In-The-Loop (HITL) simulation framework tailored specifically for gait assistive robots, addressing unique challenges posed by passive support systems. We incorporated a realistic physical human-robot interaction (pHRI) model to enable a quantitative evaluation of robot control strategies, highlighting the performance of a speed-adaptive controller compared to a conventional PID controller in maintaining compliance and reducing gait distortion. We assessed the accuracy of the simulated interactions against that of the real-world data and revealed discrepancies in the adaptation strategies taken by the human and their effect on the human's gait. This work underscored the potential of HITL simulation as a versatile tool for developing and fine-tuning personalized control policies for various users.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2503.04414.pdf' target='_blank'>https://arxiv.org/pdf/2503.04414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Tebaldi, Dario Onfiani, Luigi Biagiotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04414">On the Analysis of Stability, Sensitivity and Transparency in Variable Admittance Control for pHRI Enhanced by Virtual Fixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interest in Physical Human-Robot Interaction (pHRI) has significantly increased over the last two decades thanks to the availability of collaborative robots that guarantee user safety during force exchanges. For this reason, stability concerns have been addressed extensively in the literature while proposing new control schemes for pHRI applications. Because of the nonlinear nature of robots, stability analyses generally leverage passivity concepts. On the other hand, the proposed algorithms generally consider ideal models of robot manipulators. For this reason, the primary objective of this paper is to conduct a detailed analysis of the sources of instability for a class of pHRI control schemes, namely proxy-based constrained admittance controllers, by considering parasitic effects such as transmission elasticity, motor velocity saturation, and actuation delay. Next, a sensitivity analysis supported by experimental results is carried out, in order to identify how the control parameters affect the stability of the overall system. Finally, an adaptation technique for the proxy parameters is proposed with the goal of maximizing transparency in pHRI. The proposed adaptation method is validated through both simulations and experimental tests.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2503.03911.pdf' target='_blank'>https://arxiv.org/pdf/2503.03911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Hafez, Alireza Naderi Akhormeh, Amr Hegazy, Amr Alanwar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03911">Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of Large Language Models (LLMs) in robotic systems presents unique safety challenges, particularly in unpredictable environments. Although LLMs, leveraging zero-shot learning, enhance human-robot interaction and decision-making capabilities, their inherent probabilistic nature and lack of formal guarantees raise significant concerns for safety-critical applications. Traditional model-based verification approaches often rely on precise system models, which are difficult to obtain for real-world robotic systems and may not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or environmental uncertainties. To address these challenges, this paper introduces a safety assurance framework for LLM-controlled robots based on data-driven reachability analysis, a formal verification technique that ensures all possible system trajectories remain within safe operational limits. Our framework specifically investigates the problem of instructing an LLM to navigate the robot to a specified goal and assesses its ability to generate low-level control actions that successfully guide the robot safely toward that goal. By leveraging historical data to construct reachable sets of states for the robot-LLM system, our approach provides rigorous safety guarantees against unsafe behaviors without relying on explicit analytical models. We validate the framework through experimental case studies in autonomous navigation and task planning, demonstrating its effectiveness in mitigating risks associated with LLM-generated commands. This work advances the integration of formal methods into LLM-based robotics, offering a principled and practical approach to ensuring safety in next-generation autonomous systems.
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2502.18749.pdf' target='_blank'>https://arxiv.org/pdf/2502.18749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Heng San, Vasanthamaran Ravichandram, J-Anne Yow, Sherwin Stephen Chan, Yifan Wang, Wei Tech Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18749">Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe and comfortable bite transfer during robot-assisted feeding is challenging due to the close physical human-robot interaction required. This paper presents a novel approach to modeling physical human-robot interaction in a physics-based simulator (MuJoCo) using soft-body dynamics. We integrate a flexible head model with a rigid skeleton while accounting for internal dynamics, enabling the flexible model to be actuated by the skeleton. Incorporating realistic soft-skin contact dynamics in simulation allows for systematically evaluating bite transfer parameters, such as insertion depth and entry angle, and their impact on user safety and comfort. Our findings suggest that a straight-in-straight-out strategy minimizes forces and enhances user comfort in robot-assisted feeding, assuming a static head. This simulation-based approach offers a safer and more controlled alternative to real-world experimentation. Supplementary videos can be found at: https://tinyurl.com/224yh2kx.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2502.13498.pdf' target='_blank'>https://arxiv.org/pdf/2502.13498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwei Lian, Feitian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13498">Improving Collision-Free Success Rate For Object Goal Visual Navigation Via Two-Stage Training With Collision Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The object goal visual navigation is the task of navigating to a specific target object using egocentric visual observations. Recent end-to-end navigation models based on deep reinforcement learning have achieved remarkable performance in finding and reaching target objects. However, the collision problem of these models during navigation remains unresolved, since the collision is typically neglected when evaluating the success. Although incorporating a negative reward for collision during training appears straightforward, it results in a more conservative policy, thereby limiting the agent's ability to reach targets. In addition, many of these models utilize only RGB observations, further increasing the difficulty of collision avoidance without depth information. To address these limitations, a new concept -- collision-free success is introduced to evaluate the ability of navigation models to find a collision-free path towards the target object. A two-stage training method with collision prediction is proposed to improve the collision-free success rate of the existing navigation models using RGB observations. In the first training stage, the collision prediction module supervises the agent's collision states during exploration to learn to predict the possible collision. In the second stage, leveraging the trained collision prediction, the agent learns to navigate to the target without collision. The experimental results in the AI2-THOR environment demonstrate that the proposed method greatly improves the collision-free success rate of different navigation models and outperforms other comparable collision-avoidance methods.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2502.02772.pdf' target='_blank'>https://arxiv.org/pdf/2502.02772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Tejwani, Karl Velazquez, John Payne, Paolo Bonato, Harry Asada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02772">Cross-modality Force and Language Embeddings for Natural Human-Robot Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A method for cross-modality embedding of force profile and words is presented for synergistic coordination of verbal and haptic communication. When two people carry a large, heavy object together, they coordinate through verbal communication about the intended movements and physical forces applied to the object. This natural integration of verbal and physical cues enables effective coordination. Similarly, human-robot interaction could achieve this level of coordination by integrating verbal and haptic communication modalities. This paper presents a framework for embedding words and force profiles in a unified manner, so that the two communication modalities can be integrated and coordinated in a way that is effective and synergistic. Here, it will be shown that, although language and physical force profiles are deemed completely different, the two can be embedded in a unified latent space and proximity between the two can be quantified. In this latent space, a force profile and words can a) supplement each other, b) integrate the individual effects, and c) substitute in an exchangeable manner. First, the need for cross-modality embedding is addressed, and the basic architecture and key building block technologies are presented. Methods for data collection and implementation challenges will be addressed, followed by experimental results and discussions.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2502.01256.pdf' target='_blank'>https://arxiv.org/pdf/2502.01256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajashekhar V S, Gowdham Prabhakar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01256">Soft is Safe: Human-Robot Interaction for Soft Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the presence of robots increasing in the society, the need for interacting with robots is becoming necessary. The field of Human-Robot Interaction (HRI) has emerged important since more repetitive and tiresome jobs are being done by robots. In the recent times, the field of soft robotics has seen a boom in the field of research and commercialization. The Industry 5.0 focuses on human robot collaboration which also spurs the field of soft robotics. However the HRI for soft robotics is still in the nascent stage. In this work we review and then discuss how HRI is done for soft robots. We first discuss the control, design, materials and manufacturing of soft robots. This will provide an understanding of what is being interacted with. Then we discuss about the various input and output modalities that are used in HRI. The applications where the HRI for soft robots are found in the literature are discussed in detail. Then the limitations of HRI for soft robots and various research opportunities that exist in this field are discussed in detail. It is concluded that there is a huge scope for development for HRI for soft robots.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2501.11887.pdf' target='_blank'>https://arxiv.org/pdf/2501.11887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting-Han Lin, Hannah Dinner, Tsz Long Leung, Bilge Mutlu, J. Gregory Trafton, Sarah Sebo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11887">Connection-Coordination Rapport (CCR) Scale: A Dual-Factor Scale to Measure Human-Robot Rapport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots, particularly in service and companionship roles, must develop positive relationships with people they interact with regularly to be successful. These positive human-robot relationships can be characterized as establishing "rapport," which indicates mutual understanding and interpersonal connection that form the groundwork for successful long-term human-robot interaction. However, the human-robot interaction research literature lacks scale instruments to assess human-robot rapport in a variety of situations. In this work, we developed the 18-item Connection-Coordination Rapport (CCR) Scale to measure human-robot rapport. We first ran Study 1 (N = 288) where online participants rated videos of human-robot interactions using a set of candidate items. Our Study 1 results showed the discovery of two factors in our scale, which we named "Connection" and "Coordination." We then evaluated this scale by running Study 2 (N = 201) where online participants rated a new set of human-robot interaction videos with our scale and an existing rapport scale from virtual agents research for comparison. We also validated our scale by replicating a prior in-person human-robot interaction study, Study 3 (N = 44), and found that rapport is rated significantly greater when participants interacted with a responsive robot (responsive condition) as opposed to an unresponsive robot (unresponsive condition). Results from these studies demonstrate high reliability and validity for the CCR scale, which can be used to measure rapport in both first-person and third-person perspectives. We encourage the adoption of this scale in future studies to measure rapport in a variety of human-robot interactions.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2501.04929.pdf' target='_blank'>https://arxiv.org/pdf/2501.04929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amy Koike, Yuki Okafuji, Kenya Hoshimure, Jun Baba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04929">What Drives You to Interact?: The Role of User Motivation for a Robot in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we aim to understand how user motivation shapes human-robot interaction (HRI) in the wild. To explore this, we conducted a field study by deploying a fully autonomous conversational robot in a shopping mall over two days. Through sequential video analysis, we identified five patterns of interaction fluency (Smooth, Awkward, Active, Messy, and Quiet), four types of user motivation for interacting with the robot (Function, Experiment, Curiosity, and Education), and user positioning towards the robot. We further analyzed how these motivations and positioning influence interaction fluency. Our findings suggest that incorporating users' motivation types into the design of robot behavior can enhance interaction fluency, engagement, and user satisfaction in real-world HRI scenarios.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2501.03304.pdf' target='_blank'>https://arxiv.org/pdf/2501.03304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgenii Kruzhkov, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03304">LiLMaps: Learnable Implicit Language Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the current trends in robotics is to employ large language models (LLMs) to provide non-predefined command execution and natural human-robot interaction. It is useful to have an environment map together with its language representation, which can be further utilized by LLMs. Such a comprehensive scene representation enables numerous ways of interaction with the map for autonomously operating robots. In this work, we present an approach that enhances incremental implicit mapping through the integration of vision-language features. Specifically, we (i) propose a decoder optimization technique for implicit language maps which can be used when new objects appear on the scene, and (ii) address the problem of inconsistent vision-language predictions between different viewing positions. Our experiments demonstrate the effectiveness of LiLMaps and solid improvements in performance.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2412.16093.pdf' target='_blank'>https://arxiv.org/pdf/2412.16093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massimiliano Nigro, Emmanuel Akinrintoyo, Nicole Salomons, Micol Spitale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16093">Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Group interactions are a natural part of our daily life, and as robots become more integrated into society, they must be able to socially interact with multiple people at the same time. However, group human-robot interaction (HRI) poses unique computational challenges often overlooked in the current HRI literature. We conducted a scoping review including 44 group HRI papers from the last decade (2015-2024). From these papers, we extracted variables related to perception and behaviour generation challenges, as well as factors related to the environment, group, and robot capabilities that influence these challenges. Our findings show that key computational challenges in perception included detection of groups, engagement, and conversation information, while challenges in behaviour generation involved developing approaching and conversational behaviours. We also identified research gaps, such as improving detection of subgroups and interpersonal relationships, and recommended future work in group HRI to help researchers address these computational challenges
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2412.13569.pdf' target='_blank'>https://arxiv.org/pdf/2412.13569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sithu Aung, Min-Cheol Sagong, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13569">Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address an advanced challenge of predicting pedestrian occupancy as an extension of multi-view pedestrian detection in urban traffic. To support this, we have created a new synthetic dataset called MVP-Occ, designed for dense pedestrian scenarios in large-scale scenes. Our dataset provides detailed representations of pedestrians using voxel structures, accompanied by rich semantic scene understanding labels, facilitating visual navigation and insights into pedestrian spatial information. Furthermore, we present a robust baseline model, termed OmniOcc, capable of predicting both the voxel occupancy state and panoptic labels for the entire scene from multi-view images. Through in-depth analysis, we identify and evaluate the key elements of our proposed model, highlighting their specific contributions and importance.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2412.06808.pdf' target='_blank'>https://arxiv.org/pdf/2412.06808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Liu, FNU Shrutika, Boshen Zhang, Zhehui Huang, Gaurav Sukhatme, Feifei Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06808">Effect of Adaptive Communication Support on LLM-powered Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot collaboration requires robot to adopt their roles and levels of support based on human needs, task requirements, and complexity. Traditional human-robot teaming often relies on a pre-determined robot communication scheme, restricting teamwork adaptability in complex tasks. Leveraging strong communication capabilities of Large Language Models (LLMs), we propose a Human-Robot Teaming Framework with Multi-Modal Language feedback (HRT-ML), a framework designed to enhance human-robot interaction by adjusting the frequency and content of language-based feedback. HRT-ML framework includes two core modules: a Coordinator for high-level, low-frequency strategic guidance, and a Manager for subtask-specific, high-frequency instructions, enabling passive and active interactions with human teammates. To assess the impact of language feedback in collaborative scenarios, we conducted experiments in an enhanced Overcooked environment with varying levels of task complexity (easy, medium, hard) and feedback frequency (inactive, passive, active, superactive). Our results show that as task complexity increases relative to human capabilities, human teammates exhibited a stronger preference towards robotic agents that can offer frequent, proactive support. However, when task complexities exceed the LLM's capacity, noisy and inaccurate feedback from superactive robotic agents can instead hinder team performance, as it requires human teammates to increase their effort to interpret and respond to a large number of communications, with limited performance return. Our results offer a general principle for robotic agents to dynamically adjust their levels and frequencies of communications to work seamlessly with humans and achieve improved teaming performance.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2412.06227.pdf' target='_blank'>https://arxiv.org/pdf/2412.06227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marsha Mariya Kappan, Eduardo Benitez Sandoval, Erik Meijering, Francisco Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06227">Attention-Enhanced Lightweight Hourglass Network for Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation is a critical task in computer vision with a wide range of applications from activity monitoring to human-robot interaction. However,most of the existing methods are computationally expensive or have complex architecture. Here we propose a lightweight attention based pose estimation network that utilizes depthwise separable convolution and Convolutional Block Attention Module on an hourglass backbone. The network significantly reduces the computational complexity (floating point operations) and the model size (number of parameters) containing only about 10% of parameters of original eight stack Hourglass network. Experiments were conducted on COCO and MPII datasets using a two stack hourglass backbone. The results showed that our model performs well in comparison to six other lightweight pose estimation models with an average precision of 72.07. The model achieves this performance with only 2.3M parameters and 3.7G FLOPs.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2412.04908.pdf' target='_blank'>https://arxiv.org/pdf/2412.04908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Althubyani, Zhijin Meng, Shengyuan Xie, Cha Seung, Imran Razzak, Eduardo B. Sandoval, Baki Kocaballi, Francisco Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04908">MERCI: Multimodal Emotional and peRsonal Conversational Interactions Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of conversational agents into our daily lives has become increasingly common, yet many of these agents cannot engage in deep interactions with humans. Despite this, there is a noticeable shortage of datasets that capture multimodal information from human-robot interaction dialogues. To address this gap, we have recorded a novel multimodal dataset (MERCI) that encompasses rich embodied interaction data. The process involved asking participants to complete a questionnaire and gathering their profiles on ten topics, such as hobbies and favorite music. Subsequently, we initiated conversations between the robot and the participants, leveraging GPT-4 to generate contextually appropriate responses based on the participant's profile and emotional state, as determined by facial expression recognition and sentiment analysis. Automatic and user evaluations were conducted to assess the overall quality of the collected data. The results of both evaluations indicated a high level of naturalness, engagement, fluency, consistency, and relevance in the conversation, as well as the robot's ability to provide empathetic responses. It is worth noting that the dataset is derived from genuine interactions with the robot, involving participants who provided personal information and conveyed actual emotions.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2411.15033.pdf' target='_blank'>https://arxiv.org/pdf/2411.15033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Colombani, Dimitri Ognibene, Giuseppe Boccignone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15033">One to rule them all: natural language to bind communication, perception and action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2411.15027.pdf' target='_blank'>https://arxiv.org/pdf/2411.15027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Colombani, Luca Brini, Dimitri Ognibene, Giuseppe Boccignone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15027">Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly being used in dynamic environments like workplaces, hospitals, and homes. As a result, interactions with robots must be simple and intuitive, with robots perception adapting efficiently to human-induced changes. This paper presents a robot control architecture that addresses key challenges in human-robot interaction, with a particular focus on the dynamic creation and continuous update of the robot state representation. The architecture uses Large Language Models to integrate diverse information sources, including natural language commands, robotic skills representation, real-time dynamic semantic mapping of the perceived scene. This enables flexible and adaptive robotic behavior in complex, dynamic environments. Traditional robotic systems often rely on static, pre-programmed instructions and settings, limiting their adaptability to dynamic environments and real-time collaboration. In contrast, this architecture uses LLMs to interpret complex, high-level instructions and generate actionable plans that enhance human-robot collaboration. At its core, the system Perception Module generates and continuously updates a semantic scene graph using RGB-D sensor data, providing a detailed and structured representation of the environment. A particle filter is employed to ensure accurate object localization in dynamic, real-world settings. The Planner Module leverages this up-to-date semantic map to break down high-level tasks into sub-tasks and link them to robotic skills such as navigation, object manipulation (e.g., PICK and PLACE), and movement (e.g., GOTO). By combining real-time perception, state tracking, and LLM-driven communication and task planning, the architecture enhances adaptability, task efficiency, and human-robot collaboration in dynamic environments.
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2411.05821.pdf' target='_blank'>https://arxiv.org/pdf/2411.05821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Guruprasad, Harshvardhan Sikka, Jaewoo Song, Yangyue Wang, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05821">Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models represent a promising direction for developing general-purpose robotic systems, demonstrating the ability to combine visual understanding, language comprehension, and action generation. However, systematic evaluation of these models across diverse robotic tasks remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite for assessing VLA models. We profile three state-of-the-art VLM and VLAs - GPT-4o, OpenVLA, and JAT - across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their performance on various manipulation tasks. Our analysis reveals several key insights: 1. current VLA models show significant variation in performance across different tasks and robot platforms, with GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering, 2. all models struggle with complex manipulation tasks requiring multi-step planning, and 3. model performance is notably sensitive to action space characteristics and environmental factors. We release our evaluation framework and findings to facilitate systematic assessment of future VLA models and identify critical areas for improvement in the development of general purpose robotic systems.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2411.04499.pdf' target='_blank'>https://arxiv.org/pdf/2411.04499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Han, Yu Zhou, Qiongyan Chen, David Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04499">Memory Remedy: An AI-Enhanced Interactive Story Exploring Human-Robot Interaction and Companionship</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our approach to using AI-generated content (AIGC) and multiple media to develop an immersive, game-based, interactive story experience. The narrative of the story, "Memory Remedy", unfolds through flashbacks, allowing the audience to gradually uncover the story and the complex relationship between the robot protagonist and the older adults. This exploration explores important themes such as the journey of life, the profound influence of memories, and the concept of post-human emotional care. By engaging with this AIGC-based interactive story, audiences are encouraged to reflect on the potential role of robotic companionship in the lives of older adults in the future; and to encourage deeper reflection on the complex relationship between artificial intelligence and humanity.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2411.03555.pdf' target='_blank'>https://arxiv.org/pdf/2411.03555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael BÃ¼ttner, Jonathan Francis, Helge Rhodin, Andrew Melnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03555">Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2410.23234.pdf' target='_blank'>https://arxiv.org/pdf/2410.23234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peide Huang, Yuhan Hu, Nataliya Nechyporenko, Daehwa Kim, Walter Talbott, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23234">EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in humanlike non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of human non-verbal communication. To address this gap, our approach leverages the in-context learning capability of large language models (LLMs) to dynamically generate socially appropriate gesture motion sequences for human-robot interaction. We use this framework to generate 10 different expressive gestures and conduct online user studies comparing the naturalness and understandability of the motions generated by EMOTION and its human-feedback version, EMOTION++, against those by human operators. The results demonstrate that our approach either matches or surpasses human performance in generating understandable and natural robot motions under certain scenarios. We also provide design implications for future research to consider a set of variables when generating expressive robotic gestures.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2410.21197.pdf' target='_blank'>https://arxiv.org/pdf/2410.21197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritam Ghosh, Nibraas Khan, Miroslava Migovich, Judith A. Tate, Cathy Maxwell, Emily Latshaw, Paul Newhouse, Douglas W. Scharre, Alai Tan, Kelley Colopietro, Lorraine C. Mion, Nilanjan Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21197">User-Centered Design of Socially Assistive Robotic Combined with Non-Immersive Virtual Reality-based Dyadic Activities for Older Adults Residing in Long Term Care Facilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Apathy impairs the quality of life for older adults and their care providers. While few pharmacological remedies exist, current non-pharmacologic approaches are resource intensive. To address these concerns, this study utilizes a user-centered design (UCD) process to develop and test a set of dyadic activities that provide physical, cognitive, and social stimuli to older adults residing in long-term care (LTC) communities. Within the design, a novel framework that combines socially assistive robots and non-immersive virtual reality (SAR-VR) emphasizing human-robot interaction (HRI) and human-computer interaction (HCI) is utilized with the roles of the robots being coach and entertainer. An interdisciplinary team of engineers, nurses, and physicians collaborated with an advisory panel comprising LTC activity coordinators, staff, and residents to prototype the activities. The study resulted in four virtual activities: three with the humanoid robot, Nao, and one with the animal robot, Aibo. Fourteen participants tested the acceptability of the different components of the system and provided feedback at different stages of development. Participant approval increased significantly over successive iterations of the system highlighting the importance of stakeholder feedback. Five LTC staff members successfully set up the system with minimal help from the researchers, demonstrating the usability of the system for caregivers. Rationale for activity selection, design changes, and both quantitative and qualitative results on the acceptability and usability of the system have been presented. The paper discusses the challenges encountered in developing activities for older adults in LTCs and underscores the necessity of the UCD process to address them.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2410.20496.pdf' target='_blank'>https://arxiv.org/pdf/2410.20496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Hae Mangalindan, Ericka Rovira, Vaibhav Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20496">Trust-Aware Assistance Seeking in Human-Supervised Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to model and experimentally assess trust evolution to predict future beliefs and behaviors of human-robot teams in dynamic environments. Research suggests that maintaining trust among team members in a human-robot team is vital for successful team performance. Research suggests that trust is a multi-dimensional and latent entity that relates to past experiences and future actions in a complex manner. Employing a human-robot collaborative task, we design an optimal assistance-seeking strategy for the robot using a POMDP framework. In the task, the human supervises an autonomous mobile manipulator collecting objects in an environment. The supervisor's task is to ensure that the robot safely executes its task. The robot can either choose to attempt to collect the object or seek human assistance. The human supervisor actively monitors the robot's activities, offering assistance upon request, and intervening if they perceive the robot may fail. In this setting, human trust is the hidden state, and the primary objective is to optimize team performance. We execute two sets of human-robot interaction experiments. The data from the first experiment are used to estimate POMDP parameters, which are used to compute an optimal assistance-seeking policy evaluated in the second experiment. The estimated POMDP parameters reveal that, for most participants, human intervention is more probable when trust is low, particularly in high-complexity tasks. Our estimates suggest that the robot's action of asking for assistance in high-complexity tasks can positively impact human trust. Our experimental results show that the proposed trust-aware policy is better than an optimal trust-agnostic policy. By comparing model estimates of human trust, obtained using only behavioral data, with the collected self-reported trust values, we show that model estimates are isomorphic to self-reported responses.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2410.19072.pdf' target='_blank'>https://arxiv.org/pdf/2410.19072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyson Jordan, Pranav Pandey, Prashant Doshi, Ramviyas Parasuraman, Adam Goodie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19072">Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation Scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is promising, but existing implementations overlook key insights from human-robot interaction (HRI) research. Studies on human-machine teams demonstrate that human perceptions of a machine teammate are critical in governing the machine's performance. Here, we present a mixed factorial design to assess human perceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants were assigned to the role of casualty (CAS) or bystander (BYS) and subjected to three within-subjects conditions based on the MEDEVAC robot's operating mode: autonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each trial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and transporting them to an ambulance exchange point while avoiding an idle bystander. Following each trial, subjects completed a questionnaire measuring their emotional states, perceived safety, and social compatibility with the robot. Results indicate a consistent main effect of operating mode on reported emotional states and perceived safety. Pairwise analyses suggest that the employment of the AF operating mode negatively impacted perceptions along these dimensions. There were no persistent differences between casualty and bystander responses.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2410.18633.pdf' target='_blank'>https://arxiv.org/pdf/2410.18633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kieran Gilday, Chapa Sirithunge, Fumiya Iida, Josie Hughes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18633">Embodied Manipulation with Past and Future Morphologies through an Open Parametric Hand Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A human-shaped robotic hand offers unparalleled versatility and fine motor skills, enabling it to perform a broad spectrum of tasks with precision, power and robustness. Across the paleontological record and animal kingdom we see a wide range of alternative hand and actuation designs. Understanding the morphological design space and the resulting emergent behaviors can not only aid our understanding of dexterous manipulation and its evolution, but also assist design optimization, achieving, and eventually surpassing human capabilities. Exploration of hand embodiment has to date been limited by inaccessibility of customizable hands in the real-world, and by the reality gap in simulation of complex interactions. We introduce an open parametric design which integrates techniques for simplified customization, fabrication, and control with design features to maximize behavioral diversity. Non-linear rolling joints, anatomical tendon routing, and a low degree-of-freedom, modulating, actuation system, enable rapid production of single-piece 3D printable hands without compromising dexterous behaviors. To demonstrate this, we evaluated the design's low-level behavior range and stability, showing variable stiffness over two orders of magnitude. Additionally, we fabricated three hand designs: human, mirrored human with two thumbs, and aye-aye hands. Manipulation tests evaluate the variation in each hand's proficiency at handling diverse objects, and demonstrate emergent behaviors unique to each design. Overall, we shed light on new possible designs for robotic hands, provide a design space to compare and contrast different hand morphologies and structures, and share a practical and open-source design for exploring embodied manipulation.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2410.16919.pdf' target='_blank'>https://arxiv.org/pdf/2410.16919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoyuki Kagaya, Yuxuan Lou, Thong Jing Yuan, Subramanian Lakshmi, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Koki Oguri, Felix Wick, Yang You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16919">EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities. However, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation. To address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks. We validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2410.11085.pdf' target='_blank'>https://arxiv.org/pdf/2410.11085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahira Ali, Haley N. Green, Tariq Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11085">What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot's perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot. The results suggest that while language fluency may play a more significant role than task competency in the perception of the verbal competency of a robot, both language fluency and task competency contribute to the perception of the intelligence and reliability of the robot. The results also indicate that task competency may play a more significant role than language fluency in the perception of meeting expectations and being a good teammate. The findings of this study highlight the relationship between language fluency and task competency in the context of social HRI and will enable the development of more intelligent robots in the future.
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2410.09081.pdf' target='_blank'>https://arxiv.org/pdf/2410.09081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nuri Kim, Jeongho Park, Mineui Hong, Songhwai Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09081">Semantic Environment Atlas for Object-Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Semantic Environment Atlas (SEA), a novel mapping approach designed to enhance visual navigation capabilities of embodied agents. The SEA utilizes semantic graph maps that intricately delineate the relationships between places and objects, thereby enriching the navigational context. These maps are constructed from image observations and capture visual landmarks as sparsely encoded nodes within the environment. The SEA integrates multiple semantic maps from various environments, retaining a memory of place-object relationships, which proves invaluable for tasks such as visual localization and navigation. We developed navigation frameworks that effectively leverage the SEA, and we evaluated these frameworks through visual localization and object-goal navigation tasks. Our SEA-based localization framework significantly outperforms existing methods, accurately identifying locations from single query images. Experimental results in Habitat scenarios show that our method not only achieves a success rate of 39.0%, an improvement of 12.4% over the current state-of-the-art, but also maintains robustness under noisy odometry and actuation conditions, all while keeping computational costs low.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2410.06472.pdf' target='_blank'>https://arxiv.org/pdf/2410.06472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rob Royce, Marcel Kaufmann, Jonathan Becktor, Sangwoo Moon, Kalind Carpenter, Kai Pak, Amanda Towler, Rohan Thakker, Shehryar Khattak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06472">Enabling Novel Mission Operations and Interactions with ROSA: The Robot Operating System Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of robotic systems has revolutionized numerous industries, yet their operation often demands specialized technical knowledge, limiting accessibility for non-expert users. This paper introduces ROSA (Robot Operating System Agent), an AI-powered agent that bridges the gap between the Robot Operating System (ROS) and natural language interfaces. By leveraging state-of-the-art language models and integrating open-source frameworks, ROSA enables operators to interact with robots using natural language, translating commands into actions and interfacing with ROS through well-defined tools. ROSA's design is modular and extensible, offering seamless integration with both ROS1 and ROS2, along with safety mechanisms like parameter validation and constraint enforcement to ensure secure, reliable operations. While ROSA is originally designed for ROS, it can be extended to work with other robotics middle-wares to maximize compatibility across missions. ROSA enhances human-robot interaction by democratizing access to complex robotic systems, empowering users of all expertise levels with multi-modal capabilities such as speech integration and visual perception. Ethical considerations are thoroughly addressed, guided by foundational principles like Asimov's Three Laws of Robotics, ensuring that AI integration promotes safety, transparency, privacy, and accountability. By making robotic technology more user-friendly and accessible, ROSA not only improves operational efficiency but also sets a new standard for responsible AI use in robotics and potentially future mission operations. This paper introduces ROSA's architecture and showcases initial mock-up operations in JPL's Mars Yard, a laboratory, and a simulation using three different robots. The core ROSA library is available as open-source.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2410.05756.pdf' target='_blank'>https://arxiv.org/pdf/2410.05756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetao Li, Fang Gao, Jun Yu, Shaodong Li, Feng Shuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05756">Learning the Generalizable Manipulation Skills on Soft-body Tasks via Guided Self-attention Behavior Cloning Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI represents a paradigm in AI research where artificial agents are situated within and interact with physical or virtual environments. Despite the recent progress in Embodied AI, it is still very challenging to learn the generalizable manipulation skills that can handle large deformation and topological changes on soft-body objects, such as clay, water, and soil. In this work, we proposed an effective policy, namely GP2E behavior cloning policy, which can guide the agent to learn the generalizable manipulation skills from soft-body tasks, including pouring, filling, hanging, excavating, pinching, and writing. Concretely, we build our policy from three insights:(1) Extracting intricate semantic features from point cloud data and seamlessly integrating them into the robot's end-effector frame; (2) Capturing long-distance interactions in long-horizon tasks through the incorporation of our guided self-attention module; (3) Mitigating overfitting concerns and facilitating model convergence to higher accuracy levels via the introduction of our two-stage fine-tuning strategy. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in the soft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied AI workshop. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their practical applications in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2410.01071.pdf' target='_blank'>https://arxiv.org/pdf/2410.01071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Leusmann, Steeven Villa, Thomas Liang, Chao Wang, Albrecht Schmidt, Sven Mayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01071">An Approach to Elicit Human-Understandable Robot Expressions to Support Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the intentions of robots is essential for natural and seamless human-robot collaboration. Ensuring that robots have means for non-verbal communication is a basis for intuitive and implicit interaction. For this, we contribute an approach to elicit and design human-understandable robot expressions. We outline the approach in the context of non-humanoid robots. We paired human mimicking and enactment with research from gesture elicitation in two phases: first, to elicit expressions, and second, to ensure they are understandable. We present an example application through two studies (N=16 \& N=260) of our approach to elicit expressions for a simple 6-DoF robotic arm. We show that it enabled us to design robot expressions that signal curiosity and interest in getting attention. Our main contribution is an approach to generate and validate understandable expressions for robots, enabling more natural human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2410.00349.pdf' target='_blank'>https://arxiv.org/pdf/2410.00349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Arzate Cruz, Yotam Sechayk, Takeo Igarashi, Randy Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00349">Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans use multiple communication channels to interact with each other. For instance, body gestures or facial expressions are commonly used to convey an intent. The use of such non-verbal cues has motivated the development of prediction models. One such approach is predicting arousal and valence (AV) from facial expressions. However, making these models accurate for human-robot interaction (HRI) settings is challenging as it requires handling multiple subjects, challenging conditions, and a wide range of facial expressions. In this paper, we propose a data augmentation (DA) technique to improve the performance of AV predictors using 3D morphable models (3DMM). We then utilize this approach in an HRI setting with a mediator robot and a group of three humans. Our augmentation method creates synthetic sequences for underrepresented values in the AV space of the SEWA dataset, which is the most comprehensive dataset with continuous AV labels. Results show that using our DA method improves the accuracy and robustness of AV prediction in real-time applications. The accuracy of our models on the SEWA dataset is 0.793 for arousal and valence.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2409.10525.pdf' target='_blank'>https://arxiv.org/pdf/2409.10525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Bohus, Sean Andrist, Yuwei Bao, Eric Horvitz, Ann Paradiso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10525">"Is This It?": Towards Ecologically Valid Benchmarks for Situated Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We report initial work towards constructing ecologically valid benchmarks to assess the capabilities of large multimodal models for engaging in situated collaboration. In contrast to existing benchmarks, in which question-answer pairs are generated post hoc over preexisting or synthetic datasets via templates, human annotators, or large language models (LLMs), we propose and investigate an interactive system-driven approach, where the questions are generated by users in context, during their interactions with an end-to-end situated AI system. We illustrate how the questions that arise are different in form and content from questions typically found in existing embodied question answering (EQA) benchmarks and discuss new real-world challenge problems brought to the fore.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2409.07560.pdf' target='_blank'>https://arxiv.org/pdf/2409.07560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Houston Claure, Kate Candon, Inyoung Shin, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07560">Dynamic Fairness Perceptions in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People deeply care about how fairly they are treated by robots. The established paradigm for probing fairness in Human-Robot Interaction (HRI) involves measuring the perception of the fairness of a robot at the conclusion of an interaction. However, such an approach is limited as interactions vary over time, potentially causing changes in fairness perceptions as well. To validate this idea, we conducted a 2x2 user study with a mixed design (N=40) where we investigated two factors: the timing of unfair robot actions (early or late in an interaction) and the beneficiary of those actions (either another robot or the participant). Our results show that fairness judgments are not static. They can shift based on the timing of unfair robot actions. Further, we explored using perceptions of three key factors (reduced welfare, conduct, and moral transgression) proposed by a Fairness Theory from Organizational Justice to predict momentary perceptions of fairness in our study. Interestingly, we found that the reduced welfare and moral transgression factors were better predictors than all factors together. Our findings reinforce the idea that unfair robot behavior can shape perceptions of group dynamics and trust towards a robot and pave the path to future research directions on moment-to-moment fairness perceptions
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2409.06503.pdf' target='_blank'>https://arxiv.org/pdf/2409.06503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sajjad Hussain, Khizer Saeed, Almas Baimagambetov, Shanay Rab, Md Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06503">Advancements in Gesture Recognition Techniques and Machine Learning for Enhanced Human-Robot Interaction: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years robots have become an important part of our day-to-day lives with various applications. Human-robot interaction creates a positive impact in the field of robotics to interact and communicate with the robots. Gesture recognition techniques combined with machine learning algorithms have shown remarkable progress in recent years, particularly in human-robot interaction (HRI). This paper comprehensively reviews the latest advancements in gesture recognition methods and their integration with machine learning approaches to enhance HRI. Furthermore, this paper represents the vision-based gesture recognition for safe and reliable human-robot-interaction with a depth-sensing system, analyses the role of machine learning algorithms such as deep learning, reinforcement learning, and transfer learning in improving the accuracy and robustness of gesture recognition systems for effective communication between humans and robots.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2409.02522.pdf' target='_blank'>https://arxiv.org/pdf/2409.02522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Li, Yanfeng Lu, Yao Mu, Hong Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02522">Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2408.16865.pdf' target='_blank'>https://arxiv.org/pdf/2408.16865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Angelopoulos, Dimitri Lacroix, Ricarda Wullenkord, Alessandra Rossi, Silvia Rossi, Friederike Eyssel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16865">Measuring Transparency in Intelligent Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly integrated into our daily lives, the need to make them transparent has never been more critical. Yet, despite its importance in human-robot interaction, a standardized measure of robot transparency has been missing until now. This paper addresses this gap by presenting the first comprehensive scale to measure perceived transparency in robotic systems, available in English, German, and Italian languages. Our approach conceptualizes transparency as a multidimensional construct, encompassing explainability, legibility, predictability, and meta-understanding. The proposed scale was a product of a rigorous three-stage process involving 1,223 participants. Firstly, we generated the items of our scale, secondly, we conducted an exploratory factor analysis, and thirdly, a confirmatory factor analysis served to validate the factor structure of the newly developed TOROS scale. The final scale encompasses 26 items and comprises three factors: Illegibility, Explainability, and Predictability. TOROS demonstrates high cross-linguistic reliability, inter-factor correlation, model fit, internal consistency, and convergent validity across the three cross-national samples. This empirically validated tool enables the assessment of robot transparency and contributes to the theoretical understanding of this complex construct. By offering a standardized measure, we facilitate consistent and comparable research in human-robot interaction in which TOROS can serve as a benchmark.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2408.06175.pdf' target='_blank'>https://arxiv.org/pdf/2408.06175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Jeanne Kirschner, Jinyu Yang, Edonis Elshani, Carina M. Micheler, Tobias Leibbrand, Dirk MÃ¼ller, Claudio Glowalla, Nader Rajaei, Rainer Burgkart, Sami Haddadin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06175">Towards Unconstrained Collision Injury Protection Data Sets: Initial Surrogate Experiments for the Human Hand</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety for physical human-robot interaction (pHRI) is a major concern for all application domains. While current standardization for industrial robot applications provide safety constraints that address the onset of pain in blunt impacts, these impact thresholds are difficult to use on edged or pointed impactors. The most severe injuries occur in constrained contact scenarios, where crushing is possible. Nevertheless, situations potentially resulting in constrained contact only occur in certain areas of a workspace and design or organisational approaches can be used to avoid them. What remains are risks to the human physical integrity caused by unconstrained accidental contacts, which are difficult to avoid while maintaining robot motion efficiency. Nevertheless, the probability and severity of injuries occurring with edged or pointed impacting objects in unconstrained collisions is hardly researched. In this paper, we propose an experimental setup and procedure using two pendulums modeling human hands and arms and robots to understand the injury potential of unconstrained collisions of human hands with edged objects. Pig feet are used as ex vivo surrogate samples - as these closely resemble the physiological characteristics of human hands - to create an initial injury database on the severity of injuries caused by unconstrained edged or pointed impacts. For the effective mass range of typical lightweight robots, the data obtained show low probabilities of injuries such as skin cuts or bone/tendon injuries in unconstrained collisions when the velocity is reduced to < 0.5 m/s. The proposed experimental setups and procedures should be complemented by sufficient human modeling and will eventually lead to a complete understanding of the biomechanical injury potential in pHRI.
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2408.00238.pdf' target='_blank'>https://arxiv.org/pdf/2408.00238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Dekarske, Gregory Bales, Zhaodan Kong, Sanjay Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00238">Anytime Trust Rating Dynamics in a Human-Robot Interaction Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective We model factors contributing to rating timing for a single-dimensional, any-time trust in robotics measure.
  Background Many studies view trust as a slow-changing value after subjects complete a trial or at regular intervals. Trust is a multifaceted concept that can be measured simultaneously with a human-robot interaction.
  Method 65 subjects commanded a remote robot arm in a simulated space station. The robot picked and placed stowage commanded by the subject, but the robot's performance varied from trial to trial. Subjects rated their trust on a non-obtrusive trust slider at any time throughout the experiment.
  Results A Cox Proportional Hazards Model described the time it took subjects to rate their trust in the robot. A retrospective survey indicated that subjects based their trust on the robot's performance or outcome of the task. Strong covariates representing the task's state reflected this in the model.
  Conclusion Trust and robot task performance contributed little to the timing of the trust rating. The subjects' exit survey responses aligned with the assumption that the robot's task progress was the main reason for the timing of their trust rating.
  Application Measuring trust in a human-robot interaction task should take as little attention away from the task as possible. This trust rating technique lays the groundwork for single-dimensional trust queries that probe estimated human action.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2407.16533.pdf' target='_blank'>https://arxiv.org/pdf/2407.16533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujin Jeon, Suyeon Shin, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16533">HAPFI: History-Aware Planning based on Fused Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as "Rinse a slice of lettuce and place on the white table next to the fork". To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information (HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2406.16093.pdf' target='_blank'>https://arxiv.org/pdf/2406.16093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omkar Joglekar, Tal Lancewicki, Shir Kozlovsky, Vladimir Tchuiev, Zohar Feldman, Dotan Di Castro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16093">Towards Natural Language-Driven Assembly Using Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) and strong vision models have enabled rapid research and development in the field of Vision-Language-Action models that enable robotic control. The main objective of these methods is to develop a generalist policy that can control robots with various embodiments. However, in industrial robotic applications such as automated assembly and disassembly, some tasks, such as insertion, demand greater accuracy and involve intricate factors like contact engagement, friction handling, and refined motor skills. Implementing these skills using a generalist policy is challenging because these policies might integrate further sensory data, including force or torque measurements, for enhanced precision. In our method, we present a global control policy based on LLMs that can transfer the control policy to a finite set of skills that are specifically trained to perform high-precision tasks through dynamic context switching. The integration of LLMs into this framework underscores their significance in not only interpreting and processing language inputs but also in enriching the control mechanisms for diverse and intricate robotic operations.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2406.12473.pdf' target='_blank'>https://arxiv.org/pdf/2406.12473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara Aldhaheri, Federico Renda, Giulia De Masi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12473">Underwater Human-Robot and Human-Swarm Interaction: A Review and Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a growing interest in extending the capabilities of autonomous underwater vehicles (AUVs) in subsea missions, particularly in integrating underwater human-robot interaction (UHRI) for control. UHRI and its subfield,underwater gesture recognition (UGR), play a significant role in enhancing diver-robot communication for marine research. This review explores the latest developments in UHRI and examines its promising applications for multi-robot systems. With the developments in UGR, opportunities are presented for underwater robots to work alongside human divers to increase their functionality. Human gestures creates a seamless and safe collaborative environment where divers and robots can interact more efficiently. By highlighting the state-of-the-art in this field, we can potentially encourage advancements in underwater multi-robot system (UMRS) blending the natural communication channels of human-robot interaction with the multi-faceted coordination capabilities of underwater swarms,thus enhancing robustness in complex aquatic environments.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2406.05951.pdf' target='_blank'>https://arxiv.org/pdf/2406.05951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tjeard van Oort, Dimity Miller, Will N. Browne, Nicolas Marticorena, Jesse Haviland, Niko Suenderhauf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05951">Open-Vocabulary Part-Based Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many robotic tasks require grasping objects at specific object parts instead of arbitrarily, a crucial capability for interactions beyond simple pick-and-place, such as human-robot interaction, handovers, or tool use. Prior work has focused either on generic grasp prediction or task-conditioned grasping, but not on directly targeting object parts in an open-vocabulary way. We propose AnyPart, a modular framework that unifies open-vocabulary object detection, part segmentation, and 6-DoF grasp prediction to enable robots to grasp user-specified parts of arbitrary objects based on natural language prompts. We evaluate 16 model combinations, and demonstrate that the best-performing combination achieves 60.8% grasp success in cluttered real-world scenes at 60 times faster inference than existing approaches. To support this study, we introduce a new dataset for part-based grasping and conduct a detailed failure analysis. Our core insight is that modularly combining existing foundation models unlocks surprisingly strong and efficient capabilities for open-vocabulary part-based grasping without requiring additional training.
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2405.03473.pdf' target='_blank'>https://arxiv.org/pdf/2405.03473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni Braglia, Sylvain Calinon, Luigi Biagiotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03473">A Minimum-Jerk Approach to Handle Singularities in Virtual Fixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Implementing virtual fixtures in guiding tasks constrains the movement of the robot's end effector to specific curves within its workspace. However, incorporating guiding frameworks may encounter discontinuities when optimizing the reference target position to the nearest point relative to the current robot position. This article aims to give a geometric interpretation of such discontinuities, with specific reference to the commonly adopted Gauss-Newton algorithm. The effect of such discontinuities, defined as Euclidean Distance Singularities, is experimentally proved. We then propose a solution that is based on a Linear Quadratic Tracking problem with minimum jerk command, then compare and validate the performances of the proposed framework in two different human-robot interaction scenarios.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2405.03155.pdf' target='_blank'>https://arxiv.org/pdf/2405.03155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boxin Xu, Luoyan Zhong, Grace Zhang, Xiaoyu Liang, Diego Virtue, Rishabh Madan, Tapomayukh Bhattacharjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03155">CushSense: Soft, Stretchable, and Comfortable Tactile-Sensing Skin for Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-arm tactile feedback is crucial for robots to ensure safe physical interaction with their surroundings. This paper introduces CushSense, a fabric-based soft and stretchable tactile-sensing skin designed for physical human-robot interaction (pHRI) tasks such as robotic caregiving. Using stretchable fabric and hyper-elastic polymer, CushSense identifies contacts by monitoring capacitive changes due to skin deformation. CushSense is cost-effective ($\sim$US\$7 per taxel) and easy to fabricate. We detail the sensor design and fabrication process and perform characterization, highlighting its high sensing accuracy (relative error of 0.58%) and durability (0.054% accuracy drop after 1000 interactions). We also present a user study underscoring its perceived safety and comfort for the assistive task of limb manipulation. We open source all sensor-related resources on https://emprise.cs.cornell.edu/cushsense.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2404.15131.pdf' target='_blank'>https://arxiv.org/pdf/2404.15131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Ying Su, Yuchen Wu, Chengtao Wen, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15131">Optimizing Multi-Touch Textile and Tactile Skin Sensing Through Circuit Parameter Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments. Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing. This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays. Utilizing a Regularized Least Squares objective function which estimates the resistance distribution of the skin. We enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered. Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance. Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2404.08353.pdf' target='_blank'>https://arxiv.org/pdf/2404.08353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwei Lian, Feitian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08353">TDANet: Target-Directed Attention Network For Object-Goal Visual Navigation With Zero-Shot Ability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of the end-to-end deep reinforcement learning (DRL) for object-goal visual navigation is a long-standing challenge since object classes and placements vary in new test environments. Learning domain-independent visual representation is critical for enabling the trained DRL agent with the ability to generalize to unseen scenes and objects. In this letter, a target-directed attention network (TDANet) is proposed to learn the end-to-end object-goal visual navigation policy with zero-shot ability. TDANet features a novel target attention (TA) module that learns both the spatial and semantic relationships among objects to help TDANet focus on the most relevant observed objects to the target. With the Siamese architecture (SA) design, TDANet distinguishes the difference between the current and target states and generates the domain-independent visual representation. To evaluate the navigation performance of TDANet, extensive experiments are conducted in the AI2-THOR embodied AI environment. The simulation results demonstrate a strong generalization ability of TDANet to unseen scenes and target objects, with higher navigation success rate (SR) and success weighted by length (SPL) than other state-of-the-art models. TDANet is finally deployed on a wheeled robot in real scenes, demonstrating satisfactory generalization of TDANet to the real world.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2404.07883.pdf' target='_blank'>https://arxiv.org/pdf/2404.07883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Glen Smith, Adit Gupta, Christopher MacLellan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07883">Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent tutoring systems (ITS) are effective for improving students' learning outcomes. However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization. We present the Apprentice Tutor Builder (ATB) , a platform that simplifies tutor creation and personalization. Instructors can utilize ATB's drag-and-drop tool to build tutor interfaces. Instructors can then interactively train the tutors' underlying AI agent to produce expert models that can solve problems. Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels. We conducted a user study with 14 instructors to evaluate the effectiveness of ATB's design with end users. We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features. With these insights, we identified a set of design recommendations for our platform and others that utilize interactive AI agents for tutor creation and customization.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2404.07774.pdf' target='_blank'>https://arxiv.org/pdf/2404.07774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Namasivayam Kalithasan, Sachit Sachdeva, Himanshu Gaurav Singh, Vishal Bindal, Arnav Tuli, Gurarmaan Singh Panjeta, Harsh Himanshu Vora, Divyanshu Aggarwal, Rohan Paul, Parag Singla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07774">Sketch-Plan-Generalize: Learning and Planning with Neuro-Symbolic Programmatic Representations for Inductive Spatial Concepts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human-robot collaboration requires the ability to learn personalized concepts from a limited number of demonstrations, while exhibiting inductive generalization, hierarchical composition, and adaptability to novel constraints. Existing approaches that use code generation capabilities of pre-trained large (vision) language models as well as purely neural models show poor generalization to \emph{a-priori} unseen complex concepts. Neuro-symbolic methods (Grand et al., 2023) offer a promising alternative by searching in program space, but face challenges in large program spaces due to the inability to effectively guide the search using demonstrations. Our key insight is to factor inductive concept learning as: (i) {\it Sketch:} detecting and inferring a coarse signature of a new concept (ii) {\it Plan:} performing an MCTS search over grounded action sequences guided by human demonstrations (iii) {\it Generalize:} abstracting out grounded plans as inductive programs. Our pipeline facilitates generalization and modular re-use, enabling continual concept learning. Our approach combines the benefits of code generation ability of large language models (LLMs) along with grounded neural representations, resulting in neuro-symbolic programs that show stronger inductive generalization on the task of constructing complex structures vis-Ã¡-vis LLM-only and purely neural approaches. Further, we demonstrate reasoning and planning capabilities with learned concepts for embodied instruction following.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2404.07672.pdf' target='_blank'>https://arxiv.org/pdf/2404.07672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Pagliara, Enrico Ferrentino, Andrea Chiacchio, Giovanni Russo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07672">Safe haptic teleoperations of admittance controlled robots with virtualization of the force feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Haptic teleoperations play a key role in extending human capabilities to perform complex tasks remotely, employing a robotic system. The impact of haptics is far-reaching and can improve the sensory awareness and motor accuracy of the operator. In this context, a key challenge is attaining a natural, stable and safe haptic human-robot interaction. Achieving these conflicting requirements is particularly crucial for complex procedures, e.g. medical ones. To address this challenge, in this work we develop a novel haptic bilateral teleoperation system (HBTS), featuring a virtualized force feedback, based on the motion error generated by an admittance controlled robot. This approach allows decoupling the force rendering system from the control of the interaction: the rendered force is assigned with the desired dynamics, while the admittance control parameters are separately tuned to maximize interaction performance. Furthermore, recognizing the necessity to limit the forces exerted by the robot on the environment, to ensure a safe interaction, we embed a saturation strategy of the motion references provided by the haptic device to admittance control. We validate the different aspects of the proposed HBTS, through a teleoperated blackboard writing experiment, against two other architectures. The results indicate that the proposed HBTS improves the naturalness of teleoperation, as well as safety and accuracy of the interaction.
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2403.13801.pdf' target='_blank'>https://arxiv.org/pdf/2403.13801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusuke Mikami, Andrew Melnik, Jun Miura, Ville HautamÃ¤ki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13801">Natural Language as Policies: Reasoning for Coordinate-Level Embodied Control with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We demonstrate experimental results with LLMs that address robotics task planning problems. Recently, LLMs have been applied in robotics task planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates task planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies with pre-defined APIs. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks. The project website: https://natural-language-as-policies.github.io/
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2403.03746.pdf' target='_blank'>https://arxiv.org/pdf/2403.03746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Kaduk, Friederike Weilbeer, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03746">Emotional Tandem Robots: How Different Robot Behaviors Affect Human Perception While Controlling a Mobile Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being. Especially, the influence on humans by tandem robots with one human controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood. Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot. We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy and sad. We analyzed how participants maneuvered the leader robot along a set path while experiencing each follower behavior in a randomized order. We identified a significant shift in attention toward the follower with emotionally expressive behaviors compared to the neutral condition. For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior. The happy behavior was the most preferred and associated with increased excitement by the participants. Integrating the proposed behaviors in robots can profoundly influence the human operator's attention, emotional state, and overall experience. These insights are valuable for future HRI tandem robot designs.
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2402.11569.pdf' target='_blank'>https://arxiv.org/pdf/2402.11569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MatouÅ¡ JelÃ­nek, Eric Nichols, Randy Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11569">Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality. Additionally, our study contributes to the broader understanding of dialogue design in social robotics, offering practical insights for future developments in the field.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2402.08837.pdf' target='_blank'>https://arxiv.org/pdf/2402.08837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maneesh Bilalpur, Mert Inan, Dorsa Zeinali, Jeffrey F. Cohn, Malihe Alikhani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08837">Learning to Generate Context-Sensitive Backchannel Smiles for Embodied AI Agents with Applications in Mental Health Dialogues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods. Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles. Using cues from speech prosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors of the intensity of backchannel smiles. Based on our findings, we introduce backchannel smile production in embodied agents as a generation problem. Our attention-based generative model suggests that listener information offers performance improvements over the baseline speaker-centric generation approach. Conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality. Our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more human-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2402.08212.pdf' target='_blank'>https://arxiv.org/pdf/2402.08212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhe Yang, Qian Luo, Anumpam Pani, Yanchao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08212">BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents capable of complex physical skills can improve productivity, elevate life quality, and reshape human-machine collaboration. We aim at autonomous training of embodied agents for various tasks involving mainly large foundation models. It is believed that these models could act as a brain for embodied agents; however, existing methods heavily rely on humans for task proposal and scene customization, limiting the learning autonomy, training efficiency, and generalization of the learned policies. In contrast, we introduce a brain-body synchronization ({\it BBSEA}) scheme to promote embodied learning in unknown environments without human involvement. The proposed combines the wisdom of foundation models (``brain'') with the physical capabilities of embodied agents (``body''). Specifically, it leverages the ``brain'' to propose learnable physical tasks and success metrics, enabling the ``body'' to automatically acquire various skills by continuously interacting with the scene. We carry out an exploration of the proposed autonomous learning scheme in a table-top setting, and we demonstrate that the proposed synchronization can generate diverse tasks and develop multi-task policies with promising adaptability to new tasks and configurations. We will release our data, code, and trained models to facilitate future studies in building autonomously learning agents with large foundation models in more complex scenarios. More visualizations are available at \href{https://bbsea-embodied-ai.github.io}{https://bbsea-embodied-ai.github.io}
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2402.05703.pdf' target='_blank'>https://arxiv.org/pdf/2402.05703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgio Angelotti, Caroline P. C. Chanel, Adam H. M. Pinto, Christophe Lounis, Corentin Chauffaut, Nicolas Drougard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05703">Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human's state. Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods. In the present work, we not only highlight the potential of partially observable representations and physiological measurements to improve human operator state estimation and performance, but also enhance the overall mission effectiveness of a human-robot team. Importantly, as the fixed data set may not contain enough information to fully represent complex stochastic processes, we propose a method to incorporate model uncertainty, thus enabling risk-sensitive sequential decision-making. Experiments were conducted with a group of twenty-six human participants within a simulated robot teleoperation environment, yielding empirical evidence of the method's efficacy. The obtained adaptive task allocation policy led to statistically significant higher scores than the one that was used to collect the data set, allowing for generalization across diverse participants also taking into account risk-sensitive metrics.
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2402.04598.pdf' target='_blank'>https://arxiv.org/pdf/2402.04598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarah SchÃ¶mbs, Jorge Goncalves, Wafa Johal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04598">Exploring Data Agency and Autonomous Agents as Embodied Data Visualizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the light of recent advances in embodied data visualizations, we aim to shed light on agency in the context of data visualization. To do so, we introduce Data Agency and Data-Agent Interplay as potential terms and research focus. Furthermore, we exemplify the former in the context of human-robot interaction, and identify future challenges and research questions.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2402.00190.pdf' target='_blank'>https://arxiv.org/pdf/2402.00190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kate Candon, Nicholas C. Georgiou, Helen Zhou, Sidney Richardson, Qiping Zhang, Brian Scassellati, Marynel VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00190">REACT: Two Datasets for Analyzing Both Human Reactions and Evaluative Feedback to Robots Over Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work in Human-Robot Interaction (HRI) has shown that robots can leverage implicit communicative signals from users to understand how they are being perceived during interactions. For example, these signals can be gaze patterns, facial expressions, or body motions that reflect internal human states. To facilitate future research in this direction, we contribute the REACT database, a collection of two datasets of human-robot interactions that display users' natural reactions to robots during a collaborative game and a photography scenario. Further, we analyze the datasets to show that interaction history is an important factor that can influence human reactions to robots. As a result, we believe that future models for interpreting implicit feedback in HRI should explicitly account for this history. REACT opens up doors to this possibility in the future.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2401.15159.pdf' target='_blank'>https://arxiv.org/pdf/2401.15159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Madan, Skyler Valdez, David Kim, Sujie Fang, Luoyan Zhong, Diego Virtue, Tapomayukh Bhattacharjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15159">RABBIT: A Robot-Assisted Bed Bathing System with Multimodal Perception and Integrated Compliance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces RABBIT, a novel robot-assisted bed bathing system designed to address the growing need for assistive technologies in personal hygiene tasks. It combines multimodal perception and dual (software and hardware) compliance to perform safe and comfortable physical human-robot interaction. Using RGB and thermal imaging to segment dry, soapy, and wet skin regions accurately, RABBIT can effectively execute washing, rinsing, and drying tasks in line with expert caregiving practices. Our system includes custom-designed motion primitives inspired by human caregiving techniques, and a novel compliant end-effector called Scrubby, optimized for gentle and effective interactions. We conducted a user study with 12 participants, including one participant with severe mobility limitations, demonstrating the system's effectiveness and perceived comfort. Supplementary material and videos can be found on our website https://emprise.cs.cornell.edu/rabbit.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2401.10946.pdf' target='_blank'>https://arxiv.org/pdf/2401.10946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Lin, Francisco Cruz, Eduardo Benitez Sandoval
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10946">Self context-aware emotion perception on human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion recognition plays a crucial role in various domains of human-robot interaction. In long-term interactions with humans, robots need to respond continuously and accurately, however, the mainstream emotion recognition methods mostly focus on short-term emotion recognition, disregarding the context in which emotions are perceived. Humans consider that contextual information and different contexts can lead to completely different emotional expressions. In this paper, we introduce self context-aware model (SCAM) that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. Simultaneously, it incorporates its distinctive information retention structure and contextual loss. This approach has yielded significant improvements across audio, video, and multimodal. In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing from 77.03% to 80.82%. In the multimodal, accuracy has experienced an elevation from 77.48% to 78.93%. In the future, we will validate the reliability and usability of SCAM on robots through psychology experiments.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2401.04545.pdf' target='_blank'>https://arxiv.org/pdf/2401.04545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandeep Reddy Sabbella, Sara Kaszuba, Francesco Leotta, Pascal Serrarens, Daniele Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04545">Evaluating Gesture Recognition in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-Robot Interaction (HRI) has become increasingly important as robots are being integrated into various aspects of daily life. One key aspect of HRI is gesture recognition, which allows robots to interpret and respond to human gestures in real-time. Gesture recognition plays an important role in non-verbal communication in HRI. To this aim, there is ongoing research on how such non-verbal communication can strengthen verbal communication and improve the system's overall efficiency, thereby enhancing the user experience with the robot. However, several challenges need to be addressed in gesture recognition systems, which include data generation, transferability, scalability, generalizability, standardization, and lack of benchmarking of the gestural systems. In this preliminary paper, we want to address the challenges of data generation using virtual reality simulations and standardization issues by presenting gestures to some commands that can be used as a standard in ground robots.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2401.04534.pdf' target='_blank'>https://arxiv.org/pdf/2401.04534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara Kaszuba, Sandeep Reddy Sabbella, Francesco Leotta, Pascal Serrarens, Daniele Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04534">Testing Human-Robot Interaction in Virtual Reality: Experience from a Study on Speech Act Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, an increasing number of Human-Robot Interaction (HRI) approaches have been implemented and evaluated in Virtual Reality (VR), as it allows to speed-up design iterations and makes it safer for the final user to evaluate and master the HRI primitives. However, identifying the most suitable VR experience is not straightforward. In this work, we evaluate how, in a smart agriculture scenario, immersive and non-immersive VR are perceived by users with respect to a speech act understanding task. In particular, we collect opinions and suggestions from the 81 participants involved in both experiments to highlight the strengths and weaknesses of these different experiences.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2312.12473.pdf' target='_blank'>https://arxiv.org/pdf/2312.12473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tung Nguyen, Eric Nichols, Randy Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12473">A Study on Social Robot Behavior in Group Conversation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, research in human-robot interaction began to consider a robot's influence at the group level. Despite the recent growth in research investigating the effects of robots within groups of people, our overall understanding of what happens when robots are placed within groups or teams of people is still limited. This paper investigates several key problems for social robots that manage conversations in a group setting, where the number of participants is more than two. In a group setting, the conversation dynamics are a lot more complicated than the conventional one-to-one conversation, thus, there are more challenges need to be solved.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2312.04670.pdf' target='_blank'>https://arxiv.org/pdf/2312.04670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Liang, Kevin Ellis, JoÃ£o Henriques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04670">Rapid Motor Adaptation for Robotic Manipulator Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing generalizable manipulation skills is a core challenge in embodied AI. This includes generalization across diverse task configurations, encompassing variations in object shape, density, friction coefficient, and external disturbances such as forces applied to the robot. Rapid Motor Adaptation (RMA) offers a promising solution to this challenge. It posits that essential hidden variables influencing an agent's task performance, such as object mass and shape, can be effectively inferred from the agent's action and proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand rotation, we use depth perception to develop agents tailored for rapid motor adaptation in a variety of manipulation tasks. We evaluated our agents on four challenging tasks from the Maniskill2 benchmark, namely pick-and-place operations with hundreds of objects from the YCB and EGAD datasets, peg insertion with precise position and orientation, and operating a variety of faucets and handles, with customized environment variations. Empirical results demonstrate that our agents surpass state-of-the-art methods like automatic domain randomization and vision-based policies, obtaining better generalization performance and sample efficiency.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2312.00948.pdf' target='_blank'>https://arxiv.org/pdf/2312.00948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soheil Habibian, Antonio Alvarez Valdivia, Laura H. Blumenschein, Dylan P. Losey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00948">A Review of Communicating Robot Learning during Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For robots to seamlessly interact with humans, we first need to make sure that humans and robots understand one another. Diverse algorithms have been developed to enable robots to learn from humans (i.e., transferring information from humans to robots). In parallel, visual, haptic, and auditory communication interfaces have been designed to convey the robot's internal state to the human (i.e., transferring information from robots to humans). Prior research often separates these two directions of information transfer, and focuses primarily on either learning algorithms or communication interfaces. By contrast, in this review we take an interdisciplinary approach to identify common themes and emerging trends that close the loop between learning and communication. Specifically, we survey state-of-the-art methods and outcomes for communicating a robot's learning back to the human teacher during human-robot interaction. This discussion connects human-in-the-loop learning methods and explainable robot learning with multi-modal feedback systems and measures of human-robot interaction. We find that -- when learning and communication are developed together -- the resulting closed-loop system can lead to improved human teaching, increased human trust, and human-robot co-adaptation. The paper includes a perspective on several of the interdisciplinary research themes and open questions that could advance how future robots communicate their learning to everyday operators. Finally, we implement a selection of the reviewed methods in a case study where participants kinesthetically teach a robot arm. This case study documents and tests an integrated approach for learning in ways that can be communicated, conveying this learning across multi-modal interfaces, and measuring the resulting changes in human and robot behavior. See videos of our case study here: https://youtu.be/EXfQctqFzWs
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2311.17938.pdf' target='_blank'>https://arxiv.org/pdf/2311.17938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Fan, Jianxiong Zhou, Xiaoying Xing, Ying Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17938">Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active recognition, which allows intelligent agents to explore observations for better recognition performance, serves as a prerequisite for various embodied AI tasks, such as grasping, navigation and room arrangements. Given the evolving environment and the multitude of object classes, it is impractical to include all possible classes during the training stage. In this paper, we aim at advancing active open-vocabulary recognition, empowering embodied agents to actively perceive and classify arbitrary objects. However, directly adopting recent open-vocabulary classification models, like Contrastive Language Image Pretraining (CLIP), poses its unique challenges. Specifically, we observe that CLIP's performance is heavily affected by the viewpoint and occlusions, compromising its reliability in unconstrained embodied perception scenarios. Further, the sequential nature of observations in agent-environment interactions necessitates an effective method for integrating features that maintains discriminative strength for open-vocabulary classification. To address these issues, we introduce a novel agent for active open-vocabulary recognition. The proposed method leverages inter-frame and inter-concept similarities to navigate agent movements and to fuse features, without relying on class-specific knowledge. Compared to baseline CLIP model with 29.6% accuracy on ShapeNet dataset, the proposed agent could achieve 53.3% accuracy for open-vocabulary recognition, without any fine-tuning to the equipped CLIP model. Additional experiments conducted with the Habitat simulator further affirm the efficacy of our method.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2311.16838.pdf' target='_blank'>https://arxiv.org/pdf/2311.16838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Angelopoulos, Luigi Mangiacapra, Alessandra Rossi, Claudia Di Napoli, Silvia Rossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16838">Increasing Transparency of Reinforcement Learning using Shielding for Human Preferences and Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The adoption of Reinforcement Learning (RL) in several human-centred applications provides robots with autonomous decision-making capabilities and adaptability based on the observations of the operating environment. In such scenarios, however, the learning process can make robots' behaviours unclear and unpredictable to humans, thus preventing a smooth and effective Human-Robot Interaction (HRI). As a consequence, it becomes crucial to avoid robots performing actions that are unclear to the user. In this work, we investigate whether including human preferences in RL (concerning the actions the robot performs during learning) improves the transparency of a robot's behaviours. For this purpose, a shielding mechanism is included in the RL algorithm to include human preferences and to monitor the learning agent's decisions. We carried out a within-subjects study involving 26 participants to evaluate the robot's transparency in terms of Legibility, Predictability, and Expectability in different settings. Results indicate that considering human preferences during learning improves Legibility with respect to providing only Explanations, and combining human preferences with explanations elucidating the rationale behind the robot's decisions further amplifies transparency. Results also confirm that an increase in transparency leads to an increase in the safety, comfort, and reliability of the robot. These findings show the importance of transparency during learning and suggest a paradigm for robotic applications with human in the loop.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2311.14454.pdf' target='_blank'>https://arxiv.org/pdf/2311.14454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan R. Suissa, Shikhar Kumar, Yael Edan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14454">Can Robotic Experimenters help improve HRI Experiments? An Experimental Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To evaluate the design and skills of a robot or an algorithm for robotics, human-robot interaction user studies need to be performed. Classically, these studies are conducted by human experimenters, requiring considerable effort, and introducing variability and potential human error. In this paper, we investigate the use of robots in support of HRI experiments. Robots can perform repeated tasks accurately, thereby reducing human effort and improving validity through reduction of error and variability between participants. To assess the potential for robot led HRI experiments, we ran an HRI experiment with two participant groups, one led by a human experimenter and another led mostly by a robot experimenter.We show that the replacement of several repetitive experiment tasks through robots is not only possible but beneficial: Trials performed by the robot experimenter had fewer errors and were more fluent. There was no statistically significant difference in participants' perception w.r.t. cognitive load, comfortability, enjoyment, safety, trust and understandability between both groups. To the best of our knowledge, this is the first comparison between robot-led and human-led HRI experiments. It suggests that using robot experimenters can be beneficial and should be considered.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2311.13814.pdf' target='_blank'>https://arxiv.org/pdf/2311.13814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armin Ghanbarzadeh, Esmaeil Najafi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13814">Safe Physical Human-Robot Interaction through Variable Impedance Control based on ISO/TS 15066</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The successful implementation of Physical Human-Robot Interaction in industrial environments depends on ensuring safe collaboration between human operators and robotic devices. This necessitates the adoption of measures that guarantee the safety of human operators in close proximity to robots, without constraining the speed and motion of the robotic systems. This paper proposes a novel variable impedance-based controller for cobots that ensures safe collaboration by adhering to the ISO/TS 15066 safety standard, namely power and force limiting mode, while achieving higher operational speeds. The effectiveness of the proposed controller has been compared with conventional methods and implemented on two different robotic platforms. The results demonstrate the designed controller achieves higher speeds, while maintaining compliance with safety standards. The proposed variable impedance holds significant potential for enabling efficient and safe collaboration between humans and robots in industrial settings.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2311.00988.pdf' target='_blank'>https://arxiv.org/pdf/2311.00988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank Regal, Steven Swanbeck, Fabian Parra, Jared Rosenbaum, Mitch Pryor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00988">Using Augmented Reality to Assess and Modify Mobile Manipulator Surface Repair Plans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial robotics are redefining inspection and maintenance routines across multiple sectors, enhancing safety, efficiency, and environmental sustainability. In outdoor industrial facilities, it is crucial to inspect and repair complex surfaces affected by corrosion. To address this challenge, mobile manipulators have been developed to navigate these facilities, identify corroded areas, and apply protective coatings. However, given that this technology is still in its infancy and the consequences of improperly coating essential equipment can be significant, human oversight is necessary to review the robot's corrosion identification and repair plan. We present a practical and scalable Augmented Reality (AR)-based system designed to empower non-experts to visualize, modify, and approve robot-generated surface corrosion repair plans in real-time. Built upon an AR-based human-robot interaction framework, Augmented Robot Environment (AugRE), we developed a comprehensive AR application module called Situational Task Accept and Repair (STAR). STAR allows users to examine identified corrosion images, point cloud data, and robot navigation objectives overlaid on the physical environment within these industrial environments. Users are able to additionally make adjustments to the robot repair plan in real-time using interactive holographic volumes, excluding critical nearby equipment that might be at risk of coating overspray. We demonstrate the entire system using a Microsoft HoloLens 2 and a dual-arm mobile manipulator. Our future research will focus on evaluating user experience, system robustness, and real-world validation.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2310.20468.pdf' target='_blank'>https://arxiv.org/pdf/2310.20468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaron J. R. Lee, Gopika Ajaykumar, Ilya Shpitser, Chien-Ming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20468">An Introduction to Causal Inference Methods for Observational Human-Robot Interaction Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative methods in Human-Robot Interaction (HRI) research have primarily relied upon randomized, controlled experiments in laboratory settings. However, such experiments are not always feasible when external validity, ethical constraints, and ease of data collection are of concern. Furthermore, as consumer robots become increasingly available, increasing amounts of real-world data will be available to HRI researchers, which prompts the need for quantative approaches tailored to the analysis of observational data. In this article, we present an alternate approach towards quantitative research for HRI researchers using methods from causal inference that can enable researchers to identify causal relationships in observational settings where randomized, controlled experiments cannot be run. We highlight different scenarios that HRI research with consumer household robots may involve to contextualize how methods from causal inference can be applied to observational HRI research.
  We then provide a tutorial summarizing key concepts from causal inference using a graphical model perspective and link to code examples throughout the article, which are available at https://gitlab.com/causal/causal_hri. Our work paves the way for further discussion on new approaches towards observational HRI research while providing a starting point for HRI researchers to add causal inference techniques to their analytical toolbox.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2310.12887.pdf' target='_blank'>https://arxiv.org/pdf/2310.12887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthik Subramanian, Saurav Singh, Justin Namba, Jamison Heard, Christopher Kanan, Ferat Sahin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12887">Spatial and Temporal Attention-based emotion estimation on HRI-AVC dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many attempts have been made at estimating discrete emotions (calmness, anxiety, boredom, surprise, anger) and continuous emotional measures commonly used in psychology, namely `valence' (The pleasantness of the emotion being displayed) and `arousal' (The intensity of the emotion being displayed). Existing methods to estimate arousal and valence rely on learning from data sets, where an expert annotator labels every image frame. Access to an expert annotator is not always possible, and the annotation can also be tedious. Hence it is more practical to obtain self-reported arousal and valence values directly from the human in a real-time Human-Robot collaborative setting. Hence this paper provides an emotion data set (HRI-AVC) obtained while conducting a human-robot interaction (HRI) task. The self-reported pair of labels in this data set is associated with a set of image frames. This paper also proposes a spatial and temporal attention-based network to estimate arousal and valence from this set of image frames. The results show that an attention-based network can estimate valence and arousal on the HRI-AVC data set even when Arousal and Valence values are unavailable per frame.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2310.05904.pdf' target='_blank'>https://arxiv.org/pdf/2310.05904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Lau, Vaibhav Srivastava, Shaunak D. Bopardikar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05904">On Multi-Fidelity Impedance Tuning for Human-Robot Cooperative Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We examine how a human-robot interaction (HRI) system may be designed when input-output data from previous experiments are available. In particular, we consider how to select an optimal impedance in the assistance design for a cooperative manipulation task with a new operator. Due to the variability between individuals, the design parameters that best suit one operator of the robot may not be the best parameters for another one. However, by incorporating historical data using a linear auto-regressive (AR-1) Gaussian process, the search for a new operator's optimal parameters can be accelerated. We lay out a framework for optimizing the human-robot cooperative manipulation that only requires input-output data. We establish how the AR-1 model improves the bound on the regret and numerically simulate a human-robot cooperative manipulation task to show the regret improvement. Further, we show how our approach's input-output nature provides robustness against modeling error through an additional numerical study.
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2310.03932.pdf' target='_blank'>https://arxiv.org/pdf/2310.03932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Jiang, Martin Jagersand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03932">Bridging Low-level Geometry to High-level Concepts in Visual Servoing of Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a framework of building knowledgeable robot control in the scope of smart human-robot interaction, by empowering a basic uncalibrated visual servoing controller with contextual knowledge through the joint usage of event knowledge graphs (EKGs) and large-scale pretrained vision-language models (VLMs). The framework is expanded in twofold: first, we interpret low-level image geometry as high-level concepts, allowing us to prompt VLMs and to select geometric features of points and lines for motor control skills; then, we create an event knowledge graph (EKG) to conceptualize a robot manipulation task of interest, where the main body of the EKG is characterized by an executable behavior tree, and the leaves by semantic concepts relevant to the manipulation context. We demonstrate, in an uncalibrated environment with real robot trials, that our method lowers the reliance of human annotation during task interfacing, allows the robot to perform activities of daily living more easily by treating low-level geometric-based motor control skills as high-level concepts, and is beneficial in building cognitive thinking for smart robot applications.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2309.02979.pdf' target='_blank'>https://arxiv.org/pdf/2309.02979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meriam Moujahid, David A. Robb, Christian Dondrup, Helen Hastie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02979">Come Closer: The Effects of Robot Personality on Human Proxemics Behaviours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social Robots in human environments need to be able to reason about their physical surroundings while interacting with people. Furthermore, human proxemics behaviours around robots can indicate how people perceive the robots and can inform robot personality and interaction design. Here, we introduce Charlie, a situated robot receptionist that can interact with people using verbal and non-verbal communication in a dynamic environment, where users might enter or leave the scene at any time. The robot receptionist is stationary and cannot navigate. Therefore, people have full control over their personal space as they are the ones approaching the robot. We investigated the influence of different apparent robot personalities on the proxemics behaviours of the humans. The results indicate that different types of robot personalities, specifically introversion and extroversion, can influence human proxemics behaviours. Participants maintained shorter distances with the introvert robot receptionist, compared to the extrovert robot. Interestingly, we observed that human-robot proxemics were not the same as typical human-human interpersonal distances, as defined in the literature. We therefore propose new proxemics zones for human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2309.02942.pdf' target='_blank'>https://arxiv.org/pdf/2309.02942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mei Yii Lim, David A. Robb, Bruce W. Wilson, Helen Hastie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02942">Feeding the Coffee Habit: A Longitudinal Study of a Robo-Barista</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Studying Human-Robot Interaction over time can provide insights into what really happens when a robot becomes part of people's everyday lives. "In the Wild" studies inform the design of social robots, such as for the service industry, to enable them to remain engaging and useful beyond the novelty effect and initial adoption. This paper presents an "In the Wild" experiment where we explored the evolution of interaction between users and a Robo-Barista. We show that perceived trust and prior attitudes are both important factors associated with the usefulness, adaptability and likeability of the Robo-Barista. A combination of interaction features and user attributes are used to predict user satisfaction. Qualitative insights illuminated users' Robo-Barista experience and contribute to a number of lessons learned for future long-term studies.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2308.06498.pdf' target='_blank'>https://arxiv.org/pdf/2308.06498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiqi Chen, Jing Yu Lim, Kingsley Kuan, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06498">Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and beliefs on three partially-observable HRI tasks. Experiments show that our method significantly outperforms existing baselines and is able to infer visual observations available to other agent and their internal beliefs.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2308.05386.pdf' target='_blank'>https://arxiv.org/pdf/2308.05386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuqiang Zhao, Bidan Huang, Mingchang Li, Mengde Li, Zhongtao Fu, Ziwei Lei, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05386">A novel tactile palm for robotic object manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing is of great importance during human hand usage such as object exploration, grasping and manipulation. Different types of tactile sensors have been designed during the past decades, which are mainly focused on either the fingertips for grasping or the upper-body for human-robot interaction. In this paper, a novel soft tactile sensor has been designed to mimic the functionality of human palm that can estimate the contact state of different objects. The tactile palm mainly consists of three parts including an electrode array, a soft cover skin and the conductive sponge. The design principle are described in details, with a number of experiments showcasing the effectiveness of the proposed design.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2308.03072.pdf' target='_blank'>https://arxiv.org/pdf/2308.03072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Ying Su, Zhongqi Wei, James McCann, Wenzhen Yuan, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03072">Customizing Textile and Tactile Skins for Interactive Industrial Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile skins made from textiles enhance robot-human interaction by localizing contact points and measuring contact forces. This paper presents a solution for rapidly fabricating, calibrating, and deploying these skins on industrial robot arms. The novel automated skin calibration procedure maps skin locations to robot geometry and calibrates contact force. Through experiments on a FANUC LR Mate 200id/7L industrial robot, we demonstrate that tactile skins made from textiles can be effectively used for human-robot interaction in industrial environments, and can provide unique opportunities in robot control and learning, making them a promising technology for enhancing robot perception and interaction.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2307.15568.pdf' target='_blank'>https://arxiv.org/pdf/2307.15568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mei Yii Lim, JosÃ© David Aguas Lopes, David A. Robb, Bruce W. Wilson, Meriam Moujahid, Emanuele De Pellegrin, Helen Hastie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15568">We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots take on roles in our society, it is important that their appearance, behaviour and personality are appropriate for the job they are given and are perceived favourably by the people with whom they interact. Here, we provide an extensive quantitative and qualitative study exploring robot personality but, importantly, with respect to individual human traits. Firstly, we show that we can accurately portray personality in a social robot, in terms of extroversion-introversion using vocal cues and linguistic features. Secondly, through garnering preferences and trust ratings for these different robot personalities, we establish that, for a Robo-Barista, an extrovert robot is preferred and trusted more than an introvert robot, regardless of the subject's own personality. Thirdly, we find that individual attitudes and predispositions towards robots do impact trust in the Robo-Baristas, and are therefore important considerations in addition to robot personality, roles and interaction context when designing any human-robot interaction study.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2304.13676.pdf' target='_blank'>https://arxiv.org/pdf/2304.13676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Selma Wanna, Fabian Parra, Robert Valner, Karl KruusamÃ¤e, Mitch Pryor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13676">Multimodal Grounding for Embodied AI via Augmented Reality Headsets for Natural Language Driven Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative modeling have spurred a resurgence in the field of Embodied Artificial Intelligence (EAI). EAI systems typically deploy large language models to physical systems capable of interacting with their environment. In our exploration of EAI for industrial domains, we successfully demonstrate the feasibility of co-located, human-robot teaming. Specifically, we construct an experiment where an Augmented Reality (AR) headset mediates information exchange between an EAI agent and human operator for a variety of inspection tasks. To our knowledge the use of an AR headset for multimodal grounding and the application of EAI to industrial tasks are novel contributions within Embodied AI research. In addition, we highlight potential pitfalls in EAI's construction by providing quantitative and qualitative analysis on prompt robustness.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2304.11293.pdf' target='_blank'>https://arxiv.org/pdf/2304.11293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline Urakami, Katie Seaborn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11293">Nonverbal Cues in Human-Robot Interaction: A Communication Studies Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication between people is characterized by a broad range of nonverbal cues. Transferring these cues into the design of robots and other artificial agents that interact with people may foster more natural, inviting, and accessible experiences. In this position paper, we offer a series of definitive nonverbal codes for human-robot interaction (HRI) that address the five human sensory systems (visual, auditory, haptic, olfactory, gustatory) drawn from the field of communication studies. We discuss how these codes can be translated into design patterns for HRI using a curated sample of the communication studies and HRI literatures. As nonverbal codes are an essential mode in human communication, we argue that integrating robotic nonverbal codes in HRI will afford robots a feeling of "aliveness" or "social agency" that would otherwise be missing. We end with suggestions for research directions to stimulate work on nonverbal communication within the field of HRI and improve communication between human and robots.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2304.08750.pdf' target='_blank'>https://arxiv.org/pdf/2304.08750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katie Seaborn, Giulia Barbareschi, Shruti Chandra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08750">Not Only WEIRD but "Uncanny"? A Systematic Review of Diversity in Human-Robot Interaction Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Critical voices within and beyond the scientific community have pointed to a grave matter of concern regarding who is included in research and who is not. Subsequent investigations have revealed an extensive form of sampling bias across a broad range of disciplines that conduct human subjects research called "WEIRD": Western, Educated, Industrial, Rich, and Democratic. Recent work has indicated that this pattern exists within human-computer interaction (HCI) research, as well. How then does human-robot interaction (HRI) fare? And could there be other patterns of sampling bias at play, perhaps those especially relevant to this field of study? We conducted a systematic review of the premier ACM/IEEE International Conference on Human-Robot Interaction (2006-2022) to discover whether and how WEIRD HRI research is. Importantly, we expanded our purview to other factors of representation highlighted by critical work on inclusion and intersectionality as potentially underreported, overlooked, and even marginalized factors of human diversity. Findings from 827 studies across 749 papers confirm that participants in HRI research also tend to be drawn from WEIRD populations. Moreover, we find evidence of limited, obscured, and possible misrepresentation in participant sampling and reporting along key axes of diversity: sex and gender, race and ethnicity, age, sexuality and family configuration, disability, body type, ideology, and domain expertise. We discuss methodological and ethical implications for recruitment, analysis, and reporting, as well as the significance for HRI as a base of knowledge.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2303.05539.pdf' target='_blank'>https://arxiv.org/pdf/2303.05539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chrisantus Eze, Christopher Crick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05539">Enhancing Human-robot Collaboration by Exploring Intuitive Augmented Reality Design Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the use of Augmented Reality (AR) to enhance interactions between human agents and robotic systems in a work environment continues to grow, robots must communicate their intents in informative yet straightforward ways. This improves the human agent's feeling of trust and safety in the work environment while also reducing task completion time. To this end, we discuss a set of guidelines for the systematic design of AR interfaces for Human-Robot Interaction (HRI) systems. Furthermore, we develop design frameworks that would ride on these guidelines and serve as a base for researchers seeking to explore this direction further. We develop a series of designs for visually representing the robot's planned path and reactions, which we evaluate by conducting a user survey involving 14 participants. Subjects were given different design representations to review and rate based on their intuitiveness and informativeness. The collated results showed that our design representations significantly improved the participants' ease of understanding the robot's intents over the baselines for the robot's proposed navigation path, planned arm trajectory, and reactions.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2303.02858.pdf' target='_blank'>https://arxiv.org/pdf/2303.02858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Si, Tianhong Catherine Yu, Katrene Morozov, James McCann, Wenzhen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02858">RobotSweater: Scalable, Generalizable, and Customizable Machine-Knitted Tactile Skins for Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing is essential for robots to perceive and react to the environment. However, it remains a challenge to make large-scale and flexible tactile skins on robots. Industrial machine knitting provides solutions to manufacture customizable fabrics. Along with functional yarns, it can produce highly customizable circuits that can be made into tactile skins for robots. In this work, we present RobotSweater, a machine-knitted pressure-sensitive tactile skin that can be easily applied on robots. We design and fabricate a parameterized multi-layer tactile skin using off-the-shelf yarns, and characterize our sensor on both a flat testbed and a curved surface to show its robust contact detection, multi-contact localization, and pressure sensing capabilities. The sensor is fabricated using a well-established textile manufacturing process with a programmable industrial knitting machine, which makes it highly customizable and low-cost. The textile nature of the sensor also makes it easily fit curved surfaces of different robots and have a friendly appearance. Using our tactile skins, we conduct closed-loop control with tactile feedback for two applications: (1) human lead-through control of a robot arm, and (2) human-robot interaction with a mobile robot.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2303.00304.pdf' target='_blank'>https://arxiv.org/pdf/2303.00304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Obin Kwon, Jeongho Park, Songhwai Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00304">Renderable Neural Radiance Map for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel type of map for visual navigation, a renderable neural radiance map (RNR-Map), which is designed to contain the overall visual information of a 3D environment. The RNR-Map has a grid form and consists of latent codes at each pixel. These latent codes are embedded from image observations, and can be converted to the neural radiance field which enables image rendering given a camera pose. The recorded latent codes implicitly contain visual information about the environment, which makes the RNR-Map visually descriptive. This visual information in RNR-Map can be a useful guideline for visual localization and navigation. We develop localization and navigation frameworks that can effectively utilize the RNR-Map. We evaluate the proposed frameworks on camera tracking, visual localization, and image-goal navigation. Experimental results show that the RNR-Map-based localization framework can find the target location based on a single query image with fast speed and competitive accuracy compared to other baselines. Also, this localization framework is robust to environmental changes, and even finds the most visually similar places when a query image from a different environment is given. The proposed navigation framework outperforms the existing image-goal navigation methods in difficult scenarios, under odometry and actuation noises. The navigation framework shows 65.7% success rate in curved scenarios of the NRNS dataset, which is an improvement of 18.6% over the current state-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2302.11933.pdf' target='_blank'>https://arxiv.org/pdf/2302.11933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Rezayati, Grammatiki Zanni, Ying Zaoshi, Davide Scaramuzza, Hans Wernher van de Venn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11933">Improving safety in physical human-robot collaboration via deep metric learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Direct physical interaction with robots is becoming increasingly important in flexible production scenarios, but robots without protective fences also pose a greater risk to the operator. In order to keep the risk potential low, relatively simple measures are prescribed for operation, such as stopping the robot if there is physical contact or if a safety distance is violated. Although human injuries can be largely avoided in this way, all such solutions have in common that real cooperation between humans and robots is hardly possible and therefore the advantages of working with such systems cannot develop its full potential. In human-robot collaboration scenarios, more sophisticated solutions are required that make it possible to adapt the robot's behavior to the operator and/or the current situation. Most importantly, during free robot movement, physical contact must be allowed for meaningful interaction and not recognized as a collision. However, here lies a key challenge for future systems: detecting human contact by using robot proprioception and machine learning algorithms. This work uses the Deep Metric Learning (DML) approach to distinguish between non-contact robot movement, intentional contact aimed at physical human-robot interaction, and collision situations. The achieved results are promising and show show that DML achieves 98.6\% accuracy, which is 4\% higher than the existing standards (i.e. a deep learning network trained without DML). It also indicates a promising generalization capability for easy portability to other robots (target robots) by detecting contact (distinguishing between contactless and intentional or accidental contact) without having to retrain the model with target robot data.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2302.08064.pdf' target='_blank'>https://arxiv.org/pdf/2302.08064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akihiro Maehigashi, Seiji Yamada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08064">Modeling Trust and Reliance with Wait Time in a Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigated how wait time influences trust in and reliance on a robot. Experiment 1 was conducted as an online experiment manipulating the wait time for the task partner's action from 1 to 20 seconds and the anthropomorphism of the partner. As a result, the anthropomorphism influenced trust in the partner and did not influence reliance on the partner. However, the wait time negatively influenced trust in and reliance on the partner. Moreover, a mediation effect of trust from the wait time on reliance on the partner was confirmed. Experiment 2 was conducted to confirm the effects of wait time on trust and reliance in a human-robot face-to-face situation. As a result, the same effects of wait time found in Experiment 1 were confirmed. This study revealed that wait time is a strong and controllable factor that influences trust in and reliance on a robot.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2302.06456.pdf' target='_blank'>https://arxiv.org/pdf/2302.06456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhosein Alian, George Mylonas, James Avery
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06456">Soft Continuum Actuator Tip Position and Contact Force Prediction, Using Electrical Impedance Tomography and Recurrent Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling dexterous manipulation and safe human-robot interaction, soft robots are widely used in numerous surgical applications. One of the complications associated with using soft robots in surgical applications is reconstructing their shape and the external force exerted on them. Several sensor-based and model-based approaches have been proposed to address the issue. In this paper, a shape sensing technique based on Electrical Impedance Tomography (EIT) is proposed. The performance of this sensing technique in predicting the tip position and contact force of a soft bending actuator is highlighted by conducting a series of empirical tests. The predictions were performed based on a data-driven approach using a Long Short-Term Memory (LSTM) recurrent neural network. The tip position predictions indicate the importance of using EIT data along with pressure inputs. Changing the number of EIT channels, we evaluated the effect of the number of EIT inputs on the accuracy of the predictions. The least RMSE values for the tip position are 3.6 and 4.6 mm in Y and Z coordinates, respectively, which are 7.36% and 6.07% of the actuator's total range of motion. Contact force predictions were conducted in three different bending angles and by varying the number of EIT channels. The results of the predictions illustrated that increasing the number of channels contributes to higher accuracy of the force estimation. The mean errors of using 8 channels are 7.69%, 2.13%, and 2.96% of the total force range in three different bending angles.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2302.04865.pdf' target='_blank'>https://arxiv.org/pdf/2302.04865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Shen, Daniel Bis, Cynthia Lu, Ismini Lourentzou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04865">ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The research community has shown increasing interest in designing intelligent embodied agents that can assist humans in accomplishing tasks. Although there have been significant advancements in related vision-language benchmarks, most prior work has focused on building agents that follow instructions rather than endowing agents the ability to ask questions to actively resolve ambiguities arising naturally in embodied environments. To address this gap, we propose an Embodied Learning-By-Asking (ELBA) model that learns when and what questions to ask to dynamically acquire additional information for completing the task. We evaluate ELBA on the TEACh vision-dialog navigation and task completion dataset. Experimental results show that the proposed method achieves improved task performance compared to baseline models without question-answering capabilities.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2301.05620.pdf' target='_blank'>https://arxiv.org/pdf/2301.05620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsheng Yang, Wataru Sato, Qianying Liu, Takashi Minato, Shushi Namba, Shin'ya Nishida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05620">Optimizing Facial Expressions of an Android Robot Effectively: a Bayesian Optimization Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Expressing various facial emotions is an important social ability for efficient communication between humans. A key challenge in human-robot interaction research is providing androids with the ability to make various human-like facial expressions for efficient communication with humans. The android Nikola, we have developed, is equipped with many actuators for facial muscle control. While this enables Nikola to simulate various human expressions, it also complicates identification of the optimal parameters for producing desired expressions. Here, we propose a novel method that automatically optimizes the facial expressions of our android. We use a machine vision algorithm to evaluate the magnitudes of seven basic emotions, and employ the Bayesian Optimization algorithm to identify the parameters that produce the most convincing facial expressions. Evaluations by naive human participants demonstrate that our method improves the rated strength of the android's facial expressions of anger, disgust, sadness, and surprise compared with the previous method that relied on Ekman's theory and parameter adjustments by a human expert.
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2301.05575.pdf' target='_blank'>https://arxiv.org/pdf/2301.05575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carolina GonÃ§alves, JoÃ£o M. Lopes, Sara Moccia, Daniele Berardini, Lucia Migliorelli, Cristina P. Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05575">Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait disabilities are among the most frequent worldwide. Their treatment relies on rehabilitation therapies, in which smart walkers are being introduced to empower the user's recovery and autonomy, while reducing the clinicians effort. For that, these should be able to decode human motion and needs, as early as possible. Current walkers decode motion intention using information of wearable or embedded sensors, namely inertial units, force and hall sensors, and lasers, whose main limitations imply an expensive solution or hinder the perception of human movement. Smart walkers commonly lack a seamless human-robot interaction, which intuitively understands human motions. A contactless approach is proposed in this work, addressing human motion decoding as an early action recognition/detection problematic, using RGB-D cameras. We studied different deep learning-based algorithms, organised in three different approaches, to process lower body RGB-D video sequences, recorded from an embedded camera of a smart walker, and classify them into 4 classes (stop, walk, turn right/left). A custom dataset involving 15 healthy participants walking with the device was acquired and prepared, resulting in 28800 balanced RGB-D frames, to train and evaluate the deep networks. The best results were attained by a convolutional neural network with a channel attention mechanism, reaching accuracy values of 99.61% and above 93%, for offline early detection/recognition and trial simulations, respectively. Following the hypothesis that human lower body features encode prominent information, fostering a more robust prediction towards real-time applications, the algorithm focus was also evaluated using Dice metric, leading to values slightly higher than 30%. Promising results were attained for early action detection as a human motion decoding strategy, with enhancements in the focus of the proposed architectures.
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2212.01507.pdf' target='_blank'>https://arxiv.org/pdf/2212.01507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Drolet, Joseph Campbell, Heni Ben Amor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01507">Learning and Blending Robot Hugging Behaviors in Time and Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an imitation learning-based physical human-robot interaction algorithm capable of predicting appropriate robot responses in complex interactions involving a superposition of multiple interactions. Our proposed algorithm, Blending Bayesian Interaction Primitives (B-BIP) allows us to achieve responsive interactions in complex hugging scenarios, capable of reciprocating and adapting to a hugs motion and timing. We show that this algorithm is a generalization of prior work, for which the original formulation reduces to the particular case of a single interaction, and evaluate our method through both an extensive user study and empirical experiments. Our algorithm yields significantly better quantitative prediction error and more-favorable participant responses with respect to accuracy, responsiveness, and timing, when compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2210.02709.pdf' target='_blank'>https://arxiv.org/pdf/2210.02709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qie Sima, Sinan Tan, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.02709">Embodied Referring Expression for Manipulation Question Answering in Interactive Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents are expected to perform more complicated tasks in an interactive environment, with the progress of Embodied AI in recent years. Existing embodied tasks including Embodied Referring Expression (ERE) and other QA-form tasks mainly focuses on interaction in term of linguistic instruction. Therefore, enabling the agent to manipulate objects in the environment for exploration actively has become a challenging problem for the community. To solve this problem, We introduce a new embodied task: Remote Embodied Manipulation Question Answering (REMQA) to combine ERE with manipulation tasks. In the REMQA task, the agent needs to navigate to a remote position and perform manipulation with the target object to answer the question. We build a benchmark dataset for the REMQA task in the AI2-THOR simulator. To this end, a framework with 3D semantic reconstruction and modular network paradigms is proposed. The evaluation of the proposed framework on the REMQA dataset is presented to validate its effectiveness.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2207.12267.pdf' target='_blank'>https://arxiv.org/pdf/2207.12267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Su Kyoung Kim, Michael Maurus, Mathias Trampler, Marc Tabie, Elsa Andrea Kirchner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.12267">Continuous ErrP detections during multimodal human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-in-the-loop approaches are of great importance for robot applications. In the presented study, we implemented a multimodal human-robot interaction (HRI) scenario, in which a simulated robot communicates with its human partner through speech and gestures. The robot announces its intention verbally and selects the appropriate action using pointing gestures. The human partner, in turn, evaluates whether the robot's verbal announcement (intention) matches the action (pointing gesture) chosen by the robot. For cases where the verbal announcement of the robot does not match the corresponding action choice of the robot, we expect error-related potentials (ErrPs) in the human electroencephalogram (EEG). These intrinsic evaluations of robot actions by humans, evident in the EEG, were recorded in real time, continuously segmented online and classified asynchronously. For feature selection, we propose an approach that allows the combinations of forward and backward sliding windows to train a classifier. We achieved an average classification performance of 91% across 9 subjects. As expected, we also observed a relatively high variability between the subjects. In the future, the proposed feature selection approach will be extended to allow for customization of feature selection. To this end, the best combinations of forward and backward sliding windows will be automatically selected to account for inter-subject variability in classification performance. In addition, we plan to use the intrinsic human error evaluation evident in the error case by the ErrP in interactive reinforcement learning to improve multimodal human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2206.11350.pdf' target='_blank'>https://arxiv.org/pdf/2206.11350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Yee Wong, Lucas Vergez, Wael Suleiman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.11350">Vision- and tactile-based continuous multimodal intention and attention recognition for safer physical human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Employing skin-like tactile sensors on robots enhances both the safety and usability of collaborative robots by adding the capability to detect human contact. Unfortunately, simple binary tactile sensors alone cannot determine the context of the human contact -- whether it is a deliberate interaction or an unintended collision that requires safety manoeuvres. Many published methods classify discrete interactions using more advanced tactile sensors or by analysing joint torques. Instead, we propose to augment the intention recognition capabilities of simple binary tactile sensors by adding a robot-mounted camera for human posture analysis. Different interaction characteristics, including touch location, human pose, and gaze direction, are used to train a supervised machine learning algorithm to classify whether a touch is intentional or not with an F1-score of 86%. We demonstrate that multimodal intention recognition is significantly more accurate than monomodal analyses with the collaborative robot Baxter. Furthermore, our method can also continuously monitor interactions that fluidly change between intentional or unintentional by gauging the user's attention through gaze. If a user stops paying attention mid-task, the proposed intention and attention recognition algorithm can activate safety features to prevent unsafe interactions. We also employ a feature reduction technique that reduces the number of inputs to five to achieve a more generalized low-dimensional classifier. This simplification both reduces the amount of training data required and improves real-world classification accuracy. It also renders the method potentially agnostic to the robot and touch sensor architectures while achieving a high degree of task adaptability.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2108.00677.pdf' target='_blank'>https://arxiv.org/pdf/2108.00677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio Stroppa, Mario Selvaggio, Nathaniel Agharese, MingLuo, Laura H. Blumenschein, Elliot W. Hawkes, Allison M. Okamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.00677">Shared-Control Teleoperation Paradigms on a Soft Growing Robot Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-autonomous telerobotic systems allow both humans and robots to exploit their strengths, while enabling personalized execution of a task. However, for new soft robots with degrees of freedom dissimilar to those of human operators, it is unknown how the control of a task should be divided between the human and robot. This work presents a set of interaction paradigms between a human and a soft growing robot manipulator, and demonstrates them in both real and simulated scenarios. The robot can grow and retract by eversion and inversion of its tubular body, a property we exploit to implement interaction paradigms. We implemented and tested six different paradigms of human-robot interaction, beginning with full teleoperation and gradually adding automation to various aspects of the task execution. All paradigms were demonstrated by two expert and two naive operators. Results show that humans and the soft robot manipulator can split control along degrees of freedom while acting simultaneously. In the simple pick-and-place task studied in this work, performance improves as the control is gradually given to the robot, because the robot can correct certain human errors. However, human engagement and enjoyment may be maximized when the task is at least partially shared. Finally, when the human operator is assisted by haptic feedback based on soft robot position errors, we observed that the improvement in performance is highly dependent on the expertise of the human operator.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2107.02792.pdf' target='_blank'>https://arxiv.org/pdf/2107.02792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arun Narenthiran Sivakumar, Sahil Modi, Mateus Valverde Gasparino, Che Ellis, Andres Eduardo Baquero Velasquez, Girish Chowdhary, Saurabh Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.02792">Learned Visual Navigation for Under-Canopy Agricultural Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe a system for visually guided autonomous navigation of under-canopy farm robots. Low-cost under-canopy robots can drive between crop rows under the plant canopy and accomplish tasks that are infeasible for over-the-canopy drones or larger agricultural equipment. However, autonomously navigating them under the canopy presents a number of challenges: unreliable GPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to leaves and weeds, and large variability in appearance over the season and across crop types. We address these challenges by building a modular system that leverages machine learning for robust and generalizable perception from monocular RGB images from low-cost cameras, and model predictive control for accurate control in challenging terrain. Our system, CropFollow, is able to autonomously drive 485 meters per intervention on average, outperforming a state-of-the-art LiDAR based system (286 meters per intervention) in extensive field testing spanning over 25 km.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2509.11791.pdf' target='_blank'>https://arxiv.org/pdf/2509.11791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauri Suomela, Sasanka Kuruppu Arachchige, German F. Torres, Harry Edelman, Joni-Kristian Kämäräinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11791">Synthetic vs. Real Training Data for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates how the performance of visual navigation policies trained in simulation compares to policies trained with real-world data. Performance degradation of simulator-trained policies is often significant when they are evaluated in the real world. However, despite this well-known sim-to-real gap, we demonstrate that simulator-trained policies can match the performance of their real-world-trained counterparts. Central to our approach is a navigation policy architecture that bridges the sim-to-real appearance gap by leveraging pretrained visual representations and runs real-time on robot hardware. Evaluations on a wheeled mobile robot show that the proposed policy, when trained in simulation, outperforms its real-world-trained version by 31% and the prior state-of-the-art methods by 50% in navigation success rate. Policy generalization is verified by deploying the same model onboard a drone. Our results highlight the importance of diverse image encoder pretraining for sim-to-real generalization, and identify on-policy learning as a key advantage of simulated training over training with real data.
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2509.08157.pdf' target='_blank'>https://arxiv.org/pdf/2509.08157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viraj Parimi, Brian C. Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08157">Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe navigation is essential for autonomous systems operating in hazardous environments, especially when multiple agents must coordinate using just visual inputs over extended time horizons. Traditional planning methods excel at solving long-horizon tasks but rely on predefined distance metrics, while safe Reinforcement Learning (RL) can learn complex behaviors using high-dimensional inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an intermediate graph from replay buffer states, pruning unsafe edges, and using Conflict-Based Search (CBS) for multi-agent path planning. Although effective, this graph-pruning approach can be overly conservative, limiting mission efficiency by precluding missions that must traverse high-risk regions. To address this limitation, we propose RB-CBS, a novel extension to CBS that dynamically allocates and adjusts user-specified risk bound ($Δ$) across agents to flexibly trade off safety and speed. Our improved planner ensures that each agent receives a local risk budget ($δ$) enabling more efficient navigation while still respecting overall safety constraints. Experimental results demonstrate that this iterative risk-allocation framework yields superior performance in complex environments, allowing multiple agents to find collision-free paths within the user-specified $Δ$.
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2509.07488.pdf' target='_blank'>https://arxiv.org/pdf/2509.07488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Li, Bharat Gandhi, Ming Zhan, Mohit Nehra, Zhicheng Zhang, Yuchen Sun, Meijia Song, Naisheng Zhang, Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07488">Fine-Tuning Vision-Language Models for Visual Navigation Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2509.06597.pdf' target='_blank'>https://arxiv.org/pdf/2509.06597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frederik Plahl, Georgios Katranis, Ilshat Mamaev, Andrey Morozov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06597">LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LiHRA, a novel dataset designed to facilitate the development of automated, learning-based, or classical risk monitoring (RM) methods for Human-Robot Interaction (HRI) scenarios. The growing prevalence of collaborative robots in industrial environments has increased the need for reliable safety systems. However, the lack of high-quality datasets that capture realistic human-robot interactions, including potentially dangerous events, slows development. LiHRA addresses this challenge by providing a comprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body keypoints, and robot joint states, capturing the complete spatial and dynamic context of human-robot collaboration. This combination of modalities allows for precise tracking of human movement, robot actions, and environmental conditions, enabling accurate RM during collaborative tasks. The LiHRA dataset covers six representative HRI scenarios involving collaborative and coexistent tasks, object handovers, and surface polishing, with safe and hazardous versions of each scenario. In total, the data set includes 4,431 labeled point clouds recorded at 10 Hz, providing a rich resource for training and benchmarking classical and AI-driven RM algorithms. Finally, to demonstrate LiHRA's utility, we introduce an RM method that quantifies the risk level in each scenario over time. This method leverages contextual information, including robot states and the dynamic model of the robot. With its combination of high-resolution LiDAR data, precise human tracking, robot state data, and realistic collision events, LiHRA offers an essential foundation for future research into real-time RM and adaptive safety strategies in human-robot workspaces.
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2509.03842.pdf' target='_blank'>https://arxiv.org/pdf/2509.03842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanglu Jia, Ceng Zhang, Gregory S. Chirikjian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03842">INGRID: Intelligent Generative Robotic Design Using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of large language models (LLMs) into robotic systems has accelerated progress in embodied artificial intelligence, yet current approaches remain constrained by existing robotic architectures, particularly serial mechanisms. This hardware dependency fundamentally limits the scope of robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic Design), a framework that enables the automated design of parallel robotic mechanisms through deep integration with reciprocal screw theory and kinematic synthesis methods. We decompose the design challenge into four progressive tasks: constraint analysis, kinematic joint generation, chain construction, and complete mechanism design. INGRID demonstrates the ability to generate novel parallel mechanisms with both fixed and variable mobility, discovering kinematic configurations not previously documented in the literature. We validate our approach through three case studies demonstrating how INGRID assists users in designing task-specific parallel robots based on desired mobility requirements. By bridging the gap between mechanism theory and machine learning, INGRID enables researchers without specialized robotics training to create custom parallel mechanisms, thereby decoupling advances in robotic intelligence from hardware constraints. This work establishes a foundation for mechanism intelligence, where AI systems actively design robotic hardware, potentially transforming the development of embodied AI systems.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2509.02164.pdf' target='_blank'>https://arxiv.org/pdf/2509.02164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshen Zhang, Tongxi Fu, Xu Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02164">Omnidirectional Spatial Modeling from Correlated Panoramas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional scene understanding is vital for various downstream applications, such as embodied AI, autonomous driving, and immersive environments, yet remains challenging due to geometric distortion and complex spatial relations in 360° imagery. Existing omnidirectional methods achieve scene understanding within a single frame while neglecting cross-frame correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the \textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas visual question answering in the holistic 360° scenes. CFpano consists of over 2700 images together with over 8000 question-answer pairs, and the question types include both multiple choice and open-ended VQA. Building upon our CFpano, we further present \methodname, a multi-modal large language model (MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of tailored reward functions for robust and consistent reasoning with cross-frame correlated panoramas. Benchmark experiments with existing MLLMs are conducted with our CFpano. The experimental results demonstrate that \methodname achieves state-of-the-art performance across both multiple-choice and open-ended VQA tasks, outperforming strong baselines on all major reasoning categories (\textbf{+5.37\%} in overall performance). Our analyses validate the effectiveness of GRPO and establish a new benchmark for panoramic scene understanding.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2509.01113.pdf' target='_blank'>https://arxiv.org/pdf/2509.01113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyun Zhang, Kelvin HoLam Heung, Gabrielle J. Naquila, Ashwin Hingwe, Ashish D. Deshpande
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01113">A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement in physical human-robot interaction (HRI) has accelerated the development of soft robot designs and controllers. Controlling soft robots, especially soft hand grasping, is challenging due to their continuous deformation, motivating the use of reduced model-based controllers for real-time dynamic performance. Most existing models, however, suffer from computational inefficiency and complex parameter identification, limiting their real-time applicability. To address this, we propose a paradigm coupling Pseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter estimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate PRBM plus LDM for predicting position and force output from pressure input and benchmark its performance. We then implement PRBM plus LDM as the basis for closed-loop position and force controllers. Compared to a simple PID controller, the PRBM plus LDM position controller achieves lower error (average maximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force control, PRBM plus LDM outperforms constant pressure grasping in pinching tasks on delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70, brass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a computationally efficient and accurate modeling technique for soft actuators, enabling stable and flexible grasping with precise force regulation.
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2509.01106.pdf' target='_blank'>https://arxiv.org/pdf/2509.01106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01106">Robix: A Unified Model for Robot Interaction, Reasoning and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2508.19257.pdf' target='_blank'>https://arxiv.org/pdf/2508.19257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19257">TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2508.16622.pdf' target='_blank'>https://arxiv.org/pdf/2508.16622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Blair, Peggy Gregory, Mary Ellen Foster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16622">Observations of atypical users from a pilot deployment of a public-space social robot in a church</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though a goal of HRI is the natural integration of social robots into everyday public spaces, real-world studies still occur mostly within controlled environments with predetermined participants. True public spaces present an environment which is largely unconstrained and unpredictable, frequented by a diverse range of people whose goals can often conflict with those of the robot. When combined with the general unfamiliarity most people have with social robots, this leads to unexpected human-robot interactions in these public spaces that are rarely discussed or detected in other contexts. In this paper, we describe atypical users we observed interacting with our robot, and those who did not, during a three-day pilot deployment within a large working church and visitor attraction. We then discuss theoretical future advances in the field that could address these challenges, as well as immediate practical mitigations and strategies to help improve public space human-robot interactions in the present. This work contributes empirical insights into the dynamics of human-robot interaction in public environments and offers actionable guidance for more effective future deployments for social robot designers.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2508.15119.pdf' target='_blank'>https://arxiv.org/pdf/2508.15119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15119">Open-Universe Assistance Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI agents must infer and act in an interpretable way on diverse human goals and preferences that are not predefined. To formalize this setting, we introduce Open-Universe Assistance Games (OU-AGs), a framework where the agent must reason over an unbounded and evolving space of possible goals. In this context, we introduce GOOD (GOals from Open-ended Dialogue), a data-efficient, online method that extracts goals in the form of natural language during an interaction with a human, and infers a distribution over natural language goals. GOOD prompts an LLM to simulate users with different complex intents, using its responses to perform probabilistic inference over candidate goals. This approach enables rich goal representations and uncertainty estimation without requiring large offline datasets. We evaluate GOOD in a text-based grocery shopping domain and in a text-operated simulated household robotics environment (AI2Thor), using synthetic user profiles. Our method outperforms a baseline without explicit goal tracking, as confirmed by both LLM-based and human evaluations.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2508.13976.pdf' target='_blank'>https://arxiv.org/pdf/2508.13976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlo Mazzola, Hassan Ali, Kristína Malinovská, Igor Farkaš
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13976">Toward an Interaction-Centered Approach to Robot Trustworthiness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots get more integrated into human environments, fostering trustworthiness in embodied robotic agents becomes paramount for an effective and safe human-robot interaction (HRI). To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. In this position paper, we outline an interaction-based framework for building trust through mutual understanding between humans and robots. We emphasize two main pillars: human awareness and transparency, referring to the robot ability to interpret human actions accurately and to clearly communicate its intentions and goals, respectively. By integrating these two pillars, robots can behave in a manner that aligns with human expectations and needs while providing their human partners with both comprehension and control over their actions. We also introduce four components that we think are important for bridging the gap between a human-perceived sense of trust and a robot true capabilities.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2508.10399.pdf' target='_blank'>https://arxiv.org/pdf/2508.10399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, Ping Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10399">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2508.05104.pdf' target='_blank'>https://arxiv.org/pdf/2508.05104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrej LÃºÄny, Matilde Antonj, Carlo Mazzola, Hana HornÃ¡ÄkovÃ¡, Ana FariÄ, KristÃ­na MalinovskÃ¡, Michal Vavrecka, Igor FarkaÅ¡
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05104">Examining the legibility of humanoid robot arm movements in a pointing task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human--robot interaction requires robots whose actions are legible, allowing humans to interpret, predict, and feel safe around them. This study investigates the legibility of humanoid robot arm movements in a pointing task, aiming to understand how humans predict robot intentions from truncated movements and bodily cues. We designed an experiment using the NICO humanoid robot, where participants observed its arm movements towards targets on a touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or 80\% of their full length, and participants predicted the final target. We tested the multimodal superiority and ocular primacy hypotheses, both of which were supported by the experiment.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2508.01736.pdf' target='_blank'>https://arxiv.org/pdf/2508.01736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyrone Justin Sta Maria, Faith Griffin, Jordan Aiko Deja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01736">Set the Stage: Enabling Storytelling with Multiple Robots through Roleplaying Metaphors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are an expressive input modality for controlling multiple robots, but their use is often limited by rigid mappings and recognition constraints. To move beyond these limitations, we propose roleplaying metaphors as a scaffold for designing richer interactions. By introducing three roles: Director, Puppeteer, and Wizard, we demonstrate how narrative framing can guide the creation of diverse gesture sets and interaction styles. These roles enable a variety of scenarios, showing how roleplay can unlock new possibilities for multi-robot systems. Our approach emphasizes creativity, expressiveness, and intuitiveness as key elements for future human-robot interaction design.
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2507.21431.pdf' target='_blank'>https://arxiv.org/pdf/2507.21431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Liu, Timothy Du, Jordy Sehn, Jack Collier, FranÃ§ois Grondin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21431">Sound Source Localization for Human-Robot Interaction in Outdoor Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a sound source localization strategy that relies on a microphone array embedded in an unmanned ground vehicle and an asynchronous close-talking microphone near the operator. A signal coarse alignment strategy is combined with a time-domain acoustic echo cancellation algorithm to estimate a time-frequency ideal ratio mask to isolate the target speech from interferences and environmental noise. This allows selective sound source localization, and provides the robot with the direction of arrival of sound from the active operator, which enables rich interaction in noisy scenarios. Results demonstrate an average angle error of 4 degrees and an accuracy within 5 degrees of 95\% at a signal-to-noise ratio of 1dB, which is significantly superior to the state-of-the-art localization methods.
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2507.21225.pdf' target='_blank'>https://arxiv.org/pdf/2507.21225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Annan Zhang, Miguel Flores-Acton, Andy Yu, Anshul Gupta, Maggie Yao, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21225">Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing plays a fundamental role in enabling robots to navigate dynamic and unstructured environments, particularly in applications such as delicate object manipulation, surface exploration, and human-robot interaction. In this paper, we introduce a passive soft robotic fingertip with integrated tactile sensing, fabricated using a 3D-printed elastomer lattice with embedded air channels. This sensorization approach, termed fluidic innervation, transforms the lattice into a tactile sensor by detecting pressure changes within sealed air channels, providing a simple yet robust solution to tactile sensing in robotics. Unlike conventional methods that rely on complex materials or designs, fluidic innervation offers a simple, scalable, single-material fabrication process. We characterize the sensors' response, develop a geometric model to estimate tip displacement, and train a neural network to accurately predict contact location and contact force. Additionally, we integrate the fingertip with an admittance controller to emulate spring-like behavior, demonstrate its capability for environment exploration through tactile feedback, and validate its durability under high impact and cyclic loading conditions. This tactile sensing technique offers advantages in terms of simplicity, adaptability, and durability and opens up new opportunities for versatile robotic manipulation.
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2507.13041.pdf' target='_blank'>https://arxiv.org/pdf/2507.13041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Wacquez, Elisabetta Zibetti, Joffrey Becker, Lorenzo Aloe, Fabio Amadio, Salvatore Anzalone, Lola CaÃ±amero, Serena Ivaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13041">What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots find their way into more and more aspects of everyday life, questions around trust are becoming increasingly important. What does it mean to trust a robot? And how should we think about trust in relationships that involve both humans and non-human agents? While the field of Human-Robot Interaction (HRI) has made trust a central topic, the concept is often approached in fragmented ways. At the same time, established work in sociology, where trust has long been a key theme, is rarely brought into conversation with developments in robotics. This article argues that we need a more interdisciplinary approach. By drawing on insights from both social sciences and social robotics, we explore how trust is shaped, tested and made visible. Our goal is to open up a dialogue between disciplines and help build a more grounded and adaptable framework for understanding trust in the evolving world of human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2507.10960.pdf' target='_blank'>https://arxiv.org/pdf/2507.10960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>He Zhu, Ryo Miyoshi, Yuki Okafuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10960">Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior human-robot interaction (HRI) research has primarily focused on single-user interactions, where robots do not need to consider the timing or recipient of their responses. However, in multi-party interactions, such as at malls and hospitals, social robots must understand the context and decide both when and to whom they should respond. In this paper, we propose a Transformer-based multi-task learning framework to improve the decision-making process of social robots, particularly in multi-user environments. Considering the characteristics of HRI, we propose two novel loss functions: one that enforces constraints on active speakers to improve scene modeling, and another that guides response selection towards utterances specifically directed at the robot. Additionally, we construct a novel multi-party HRI dataset that captures real-world complexities, such as gaze misalignment. Experimental results demonstrate that our model achieves state-of-the-art performance in respond decisions, outperforming existing heuristic-based and single-task approaches. Our findings contribute to the development of socially intelligent social robots capable of engaging in natural and context-aware multi-party interactions.
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2507.05555.pdf' target='_blank'>https://arxiv.org/pdf/2507.05555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Obin Kwon, Sankalp Yamsani, Noboru Myers, Sean Taylor, Jooyoung Hong, Kyungseo Park, Alex Alspach, Joohyung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05555">PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular ecosystem that enables flexible placement and control of robotic limbs. With PAPRLE, a user can change the arrangement of the robotic limbs, and control them using a variety of input devices, including puppeteers, gaming controllers, and VR-based interfaces. This versatility supports a wide range of teleoperation scenarios and promotes adaptability to different task requirements. To further enhance configurability, we introduce a pluggable puppeteer device that can be easily mounted and adapted to match the target robot configurations. PAPRLE supports bilateral teleoperation through these puppeteer devices, agnostic to the type or configuration of the follower robot. By supporting both joint-space and task-space control, the system provides real-time force feedback, improving user fidelity and physical interaction awareness. The modular design of PAPRLE facilitates novel spatial arrangements of the limbs and enables scalable data collection, thereby advancing research in embodied AI and learning-based control. We validate PAPRLE in various real-world settings, demonstrating its versatility across diverse combinations of leader devices and follower robots. The system will be released as open source, including both hardware and software components, to support broader adoption and community-driven extension. Additional resources and demonstrations are available at the project website: https://uiuckimlab.github.io/paprle-pages
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2507.04095.pdf' target='_blank'>https://arxiv.org/pdf/2507.04095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Mortezapour, Giuliana Vitiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04095">Human-centered AI with focus on Human-robot interaction (Book chapter)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern social robots can be considered the descendants of steam engines from the First Industrial Revolution (IR 1.0) and industrial robotic arms from the Third Industrial Revolution (IR 3.0). As some time has passed since the introduction of these robots during the Fourth Industrial Revolution (IR 4.0), challenges and issues in their interaction with humans have emerged, leading researchers to conclude that, like any other AI-based technology, these robots must also be human-centered to meet the needs of their users. This chapter aims to introduce humans and their needs in interactions with robots, ranging from short-term, one-on-one interactions (micro-level) to long-term, macro-level needs at the societal scale. Building upon the principles of human-centered AI, this chapter presents, for the first time, a new framework of human needs called the Dual Pyramid. This framework encompasses a comprehensive list of human needs in robot interactions, from the most fundamental, robot effectiveness to macro level requirements, such as the collaboration with robots in achieving the United Nations 17 Sustainable Development Goals.
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2507.01206.pdf' target='_blank'>https://arxiv.org/pdf/2507.01206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathy Zhuang, Zixun Huang, Yukun Song, Rui Li, Yinuo Zhou, Allen Y. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01206">2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As modern computing advances, new interaction paradigms have emerged, particularly in Augmented Reality (AR), which overlays virtual interfaces onto physical objects. This evolution poses challenges in machine perception, especially for tasks like 3D object pose estimation in complex, dynamic environments. Our project addresses critical issues in human-robot interaction within mobile AR, focusing on non-intrusive, spatially aware interfaces. We present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024 SUITS challenge, targeting future spaceflight needs such as the Artemis missions. URSA integrates three core technologies: a head-mounted AR device (e.g., HoloLens) for intuitive visual feedback, voice control powered by large language models for hands-free interaction, and robot tracking algorithms that enable accurate 3D localization in dynamic settings. To enhance precision, we leverage digital twin localization technologies, using datasets like DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world tracking under noise and occlusion. Our system enables real-time robot control and monitoring via an AR interface, even in the absence of ground-truth sensors--vital for hazardous or remote operations. Key contributions include: (1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5) end-to-end integration for astronaut mission support. This work advances digital twin applications in robotics, offering scalable solutions for both aerospace and industrial domains.
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2506.20134.pdf' target='_blank'>https://arxiv.org/pdf/2506.20134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ningwei Xie, Zizi Tian, Lei Yang, Xiao-Ping Zhang, Meng Guo, Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20134">From 2D to 3D Cognition: A Brief Survey of General World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have garnered increasing attention in the development of artificial general intelligence (AGI), serving as computational frameworks for learning representations of the external world and forecasting future states. While early efforts focused on 2D visual perception and simulation, recent 3D-aware generative world models have demonstrated the ability to synthesize geometrically consistent, interactive 3D environments, marking a shift toward 3D spatial cognition. Despite rapid progress, the field lacks systematic analysis to categorize emerging techniques and clarify their roles in advancing 3D cognitive world models. This survey addresses this need by introducing a conceptual framework, providing a structured and forward-looking review of world models transitioning from 2D perception to 3D cognition. Within this framework, we highlight two key technological drivers, particularly advances in 3D representations and the incorporation of world knowledge, as fundamental pillars. Building on these, we dissect three core cognitive capabilities that underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction. We further examine the deployment of these capabilities in real-world applications, including embodied AI, autonomous driving, digital twin, and gaming/VR. Finally, we identify challenges across data, modeling, and deployment, and outline future directions for advancing more robust and generalizable 3D world models.
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2506.17639.pdf' target='_blank'>https://arxiv.org/pdf/2506.17639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Chen, Xiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17639">RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLA) have demonstrated remarkable capabilities and promising potential in solving complex robotic manipulation tasks. However, their substantial parameter sizes and high inference latency pose significant challenges for real-world deployment, particularly on resource-constrained robotic platforms. To address this issue, we begin by conducting an extensive empirical study to explore the effectiveness of model compression techniques when applied to VLAs. Building on the insights gained from these preliminary experiments, we propose RLRC, a three-stage recovery method for compressed VLAs, including structured pruning, performance recovery based on SFT and RL, and further quantization. RLRC achieves up to an 8x reduction in memory usage and a 2.3x improvement in inference throughput, while maintaining or even surpassing the original VLA's task success rate. Extensive experiments show that RLRC consistently outperforms existing compression baselines, demonstrating strong potential for on-device deployment of VLAs. Project website: https://rlrc-vla.github.io
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2506.04982.pdf' target='_blank'>https://arxiv.org/pdf/2506.04982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunlong Dong, Xing Liu, Jun Wan, Zelin Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04982">GEX: Democratizing Dexterity with Fully-Actuated Dexterous Hand and Exoskeleton Glove</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces GEX, an innovative low-cost dexterous manipulation system that combines the GX11 tri-finger anthropomorphic hand (11 DoF) with the EX12 tri-finger exoskeleton glove (12 DoF), forming a closed-loop teleoperation framework through kinematic retargeting for high-fidelity control. Both components employ modular 3D-printed finger designs, achieving ultra-low manufacturing costs while maintaining full actuation capabilities. Departing from conventional tendon-driven or underactuated approaches, our electromechanical system integrates independent joint motors across all 23 DoF, ensuring complete state observability and accurate kinematic modeling. This full-actuation architecture enables precise bidirectional kinematic calculations, substantially enhancing kinematic retargeting fidelity between the exoskeleton and robotic hand. The proposed system bridges the cost-performance gap in dexterous manipulation research, providing an accessible platform for acquiring high-quality demonstration data to advance embodied AI and dexterous robotic skill transfer learning.
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2506.00220.pdf' target='_blank'>https://arxiv.org/pdf/2506.00220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingru Zhou, Sadanand Modak, Yao-Cheng Chan, Zhiyun Deng, Luis Sentis, Maria Esteva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00220">Curate, Connect, Inquire: A System for Findable Accessible Interoperable and Reusable (FAIR) Human-Robot Centered Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of AI in robotics has amplified the need for high-quality, reusable datasets, particularly in human-robot interaction (HRI) and AI-embedded robotics. While more robotics datasets are being created, the landscape of open data in the field is uneven. This is due to a lack of curation standards and consistent publication practices, which makes it difficult to discover, access, and reuse robotics data. To address these challenges, this paper presents a curation and access system with two main contributions: (1) a structured methodology to curate, publish, and integrate FAIR (Findable, Accessible, Interoperable, Reusable) human-centered robotics datasets; and (2) a ChatGPT-powered conversational interface trained with the curated datasets metadata and documentation to enable exploration, comparison robotics datasets and data retrieval using natural language. Developed based on practical experience curating datasets from robotics labs within Texas Robotics at the University of Texas at Austin, the system demonstrates the value of standardized curation and persistent publication of robotics data. The system's evaluation suggests that access and understandability of human-robotics data are significantly improved. This work directly aligns with the goals of the HCRL @ ICRA 2025 workshop and represents a step towards more human-centered access to data for embodied AI.
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2505.22088.pdf' target='_blank'>https://arxiv.org/pdf/2505.22088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam O'Connor Russell, Naomi Harte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22088">Visual Cues Support Robust Turn-taking Prediction in Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate predictive turn-taking models (PTTMs) are essential for naturalistic human-robot interaction. However, little is known about their performance in noise. This study therefore explores PTTM performance in types of noise likely to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10 dB music noise. Training with noisy data enables a multimodal PTTM, which includes visual features to better exploit visual cues, with 72% accuracy in 10 dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all noise types and SNRs, highlighting its ability to exploit visual cues; however, this does not always generalise to new types of noise. Analysis also reveals that successful training relies on accurate transcription, limiting the use of ASR-derived transcriptions to clean conditions. We make code publicly available for future research.
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2505.21043.pdf' target='_blank'>https://arxiv.org/pdf/2505.21043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam O'Connor Russell, Naomi Harte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21043">Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2505.20537.pdf' target='_blank'>https://arxiv.org/pdf/2505.20537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxiang Wang, Emek BarÄ±Å KÃ¼Ã§Ã¼ktabak, Rana Soltani Zarrin, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20537">CoRI: Communication of Robot Intent for Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot's upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot's image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot's high-level intentions but also crucial details about its motion and any collaborative user action needed. Video and code of our project can be found on our project website: https://cori-phri.github.io/
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2505.20537.pdf' target='_blank'>https://arxiv.org/pdf/2505.20537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxiang Wang, Emek Barış Küçüktabak, Rana Soltani Zarrin, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20537">CoRI: Communication of Robot Intent for Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot's upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot's image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot's high-level intentions but also crucial details about its motion and any collaborative user action needed. Video and code of our project can be found on our project website: https://cori-phri.github.io/
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2505.14366.pdf' target='_blank'>https://arxiv.org/pdf/2505.14366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14366">Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2505.14197.pdf' target='_blank'>https://arxiv.org/pdf/2505.14197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshen Zhang, Zhen Ye, Xu Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14197">Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional images (ODIs), with their 360Â° field of view, provide unparalleled spatial awareness for immersive applications like augmented reality and embodied AI. However, the capability of existing multi-modal large language models (MLLMs) to comprehend and reason about such panoramic scenes remains underexplored. This paper addresses this gap by introducing OmniVQA, the first dataset and conducting the first benchmark for omnidirectional visual question answering. Our evaluation of state-of-the-art MLLMs reveals significant limitations in handling omnidirectional visual question answering, highlighting persistent challenges in object localization, feature extraction, and hallucination suppression within panoramic contexts. These results underscore the disconnect between current MLLM capabilities and the demands of omnidirectional visual understanding, which calls for dedicated architectural or training innovations tailored to 360Â° imagery. Building on the OmniVQA dataset and benchmark, we further introduce a rule-based reinforcement learning method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group relative policy optimization (GRPO) by proposing three novel reward functions: (1) reasoning process similarity reward, (2) answer semantic accuracy reward, and (3) structured format compliance reward. Extensive experiments on our OmniVQA demonstrate the superiority of our proposed method in omnidirectional space (+6% improvement).
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2505.14129.pdf' target='_blank'>https://arxiv.org/pdf/2505.14129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jed Muff, Keiichi Ito, Elijah H. W. Ang, Karine Miras, A. E. Eiben
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14129">Unconventional Hexacopters via Evolution and Learning: Performance Gains and New Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolution and learning have historically been interrelated topics, and their interplay is attracting increased interest lately. The emerging new factor in this trend is morphological evolution, the evolution of physical forms within embodied AI systems such as robots. In this study, we investigate a system of hexacopter-type drones with evolvable morphologies and learnable controllers and make contributions to two fields. For aerial robotics, we demonstrate that the combination of evolution and learning can deliver non-conventional drones that significantly outperform the traditional hexacopter on several tasks that are more complex than previously considered in the literature. For the field of Evolutionary Computing, we introduce novel metrics and perform new analyses into the interaction of morphological evolution and learning, uncovering hitherto unidentified effects. Our analysis tools are domain-agnostic, making a methodological contribution towards building solid foundations for embodied AI systems that integrate evolution and learning.
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2505.12707.pdf' target='_blank'>https://arxiv.org/pdf/2505.12707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingchen He, Christian D. Weilbach, Martyna E. Wojciechowska, Yuxuan Zhang, Frank Wood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12707">PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\footnote{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2505.09737.pdf' target='_blank'>https://arxiv.org/pdf/2505.09737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Osher Elhadad, Reuth Mirsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09737">General Dynamic Goal Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding an agent's intent through its behavior is essential in human-robot interaction, interactive AI systems, and multi-agent collaborations. This task, known as Goal Recognition (GR), poses significant challenges in dynamic environments where goals are numerous and constantly evolving. Traditional GR methods, designed for a predefined set of goals, often struggle to adapt to these dynamic scenarios. To address this limitation, we introduce the General Dynamic GR problem - a broader definition of GR - aimed at enabling real-time GR systems and fostering further research in this area. Expanding on this foundation, this paper employs a model-free goal-conditioned RL approach to enable fast adaptation for GR across various changing tasks.
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2505.07710.pdf' target='_blank'>https://arxiv.org/pdf/2505.07710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasmin Rafiq, Baslin A. James, Ke Xu, Robert M. Hierons, Sanja Dogramadzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07710">Hybrid Control Strategies for Safe and Adaptive Robot-Assisted Dressing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety, reliability, and user trust are crucial in human-robot interaction (HRI) where the robots must address hazards in real-time. This study presents hazard driven low-level control strategies implemented in robot-assisted dressing (RAD) scenarios where hazards like garment snags and user discomfort in real-time can affect task performance and user safety. The proposed control mechanisms include: (1) Garment Snagging Control Strategy, which detects excessive forces and either seeks user intervention via a chatbot or autonomously adjusts its trajectory, and (2) User Discomfort/Pain Mitigation Strategy, which dynamically reduces velocity based on user feedback and aborts the task if necessary. We used physical dressing trials in order to evaluate these control strategies. Results confirm that integrating force monitoring with user feedback improves safety and task continuity. The findings emphasise the need for hybrid approaches that balance autonomous intervention, user involvement, and controlled task termination, supported by bi-directional interaction and real-time user-driven adaptability, paving the way for more responsive and personalised HRI systems.
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2505.06832.pdf' target='_blank'>https://arxiv.org/pdf/2505.06832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyang Guo, Hongwei Hu, Chengye Song, Jiale Chen, Zilin Zhao, Yu Fu, Bowen Guan, Zhenze Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06832">UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary, task-oriented grasping of specific functional parts, particularly with dual arms, remains a key challenge, as current Vision-Language Models (VLMs), while enhancing task understanding, often struggle with precise grasp generation within defined constraints and effective dual-arm coordination. We innovatively propose UniDiffGrasp, a unified framework integrating VLM reasoning with guided part diffusion to address these limitations. UniDiffGrasp leverages a VLM to interpret user input and identify semantic targets (object, part(s), mode), which are then grounded via open-vocabulary segmentation. Critically, the identified parts directly provide geometric constraints for a Constrained Grasp Diffusion Field (CGDF) using its Part-Guided Diffusion, enabling efficient, high-quality 6-DoF grasps without retraining. For dual-arm tasks, UniDiffGrasp defines distinct target regions, applies part-guided diffusion per arm, and selects stable cooperative grasps. Through extensive real-world deployment, UniDiffGrasp achieves grasp success rates of 0.876 in single-arm and 0.767 in dual-arm scenarios, significantly surpassing existing state-of-the-art methods, demonstrating its capability to enable precise and coordinated open-vocabulary grasping in complex real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2505.06278.pdf' target='_blank'>https://arxiv.org/pdf/2505.06278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongfei Bian, Mathieu Chollet, Tanaya Guha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06278">Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for social robots and agents to interact and assist humans is growing steadily. To be able to successfully interact with humans, they need to understand and analyse socially interactive scenes from their (robot's) perspective. Works that model social situations between humans and agents are few; and even those existing ones are often too computationally intensive to be suitable for deployment in real time or on real world scenarios with limited available information. We propose a robust knowledge distillation framework that models social interactions through various multimodal cues, yet is robust against incomplete and noisy information during inference. Our teacher model is trained with multimodal input (body, face and hand gestures, gaze, raw images) that transfers knowledge to a student model that relies solely on body pose. Extensive experiments on two publicly available human-robot interaction datasets demonstrate that the our student model achieves an average accuracy gain of 14.75\% over relevant baselines on multiple downstream social understanding task even with up to 51\% of its input being corrupted. The student model is highly efficient: it is $<1$\% in size of the teacher model in terms of parameters and uses $\sim 0.5$\textperthousand~FLOPs of that in the teacher model. Our code will be made public during publication.
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2505.00055.pdf' target='_blank'>https://arxiv.org/pdf/2505.00055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoqi Zeng, Yuxiang Wei, Jiawen Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00055">TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (EAI) addresses autonomous driving challenges in Vehicular Embodied AI Networks (VEANETs) through multi-modal perception, adaptive decision-making, and hardware-software co-scheduling. However, the computational demands of virtual services and the inherent mobility of autonomous vehicles (AVs) necessitate real-time migration of Vehicular Embodied Agent AI Twins (VEAATs) between resource-constrained Roadside Units (RSUs). This paper proposes a novel framework for efficient VEAAT migration in VEANETs, combining a multi-leader multi-follower (MLMF) Stackelberg game-theoretic incentive mechanism with a tiny multi-agent deep reinforcement learning (MADRL) algorithm. First, We propose an virtual immersive experience-driven utility model that captures AV-RSU dynamic interactions by integrating AVs' social influence, service complementarity and substitutability, and RSUs' resource allocation strategies to optimize VEAAT migration decisions. Second, to enhance training efficiency and enable efficient deployment on computation-constrained AVs while preserving exploration-exploitation performance, we propose TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance based on agents' exploration incentives. Numerical results demonstrate that our approach achieves convergence comparable to baseline models and closely approximates the Stackelberg equilibrium.
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2504.17128.pdf' target='_blank'>https://arxiv.org/pdf/2504.17128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Yousef Soltanian, Wenlong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17128">PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of a two-player linear quadratic differential game with incomplete information, a scenario commonly encountered in multi-agent control, human-robot interaction (HRI), and approximation methods for solving general-sum differential games. While solutions to such linear differential games are typically obtained through coupled Riccati equations, the complexity increases when agents have incomplete information, particularly when neither is aware of the other's cost function. To tackle this challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework for learning the cost parameters of the other agent. In PACE, each agent treats its peer as a learning agent rather than a stationary optimal agent, models their learning dynamics, and leverages this dynamic to infer the cost function parameters of the other agent. This approach enables agents to infer each other's objective function in real time based solely on their previous state observations and dynamically adapt their control policies. Furthermore, we provide a theoretical guarantee for the convergence of parameter estimation and the stability of system states in PACE. Additionally, in our numerical studies, we demonstrate how modeling the learning dynamics of the other agent benefits PACE, compared to approaches that approximate the other agent as having complete information, particularly in terms of stability and convergence speed.
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2504.16516.pdf' target='_blank'>https://arxiv.org/pdf/2504.16516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16516">Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2504.13370.pdf' target='_blank'>https://arxiv.org/pdf/2504.13370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Jin, Bo Xiao, Huijiang Wang, Wendong Wang, Zhenhua Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13370">Multi-Sensor Fusion-Based Mobile Manipulator Remote Control for Intelligent Smart Home Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a wearable-controlled mobile manipulator system for intelligent smart home assistance, integrating MEMS capacitive microphones, IMU sensors, vibration motors, and pressure feedback to enhance human-robot interaction. The wearable device captures forearm muscle activity and converts it into real-time control signals for mobile manipulation. The wearable device achieves an offline classification accuracy of 88.33\%\ across six distinct movement-force classes for hand gestures by using a CNN-LSTM model, while real-world experiments involving five participants yield a practical accuracy of 83.33\%\ with an average system response time of 1.2 seconds. In Human-Robot synergy in navigation and grasping tasks, the robot achieved a 98\%\ task success rate with an average trajectory deviation of only 3.6 cm. Finally, the wearable-controlled mobile manipulator system achieved a 93.3\%\ gripping success rate, a transfer success of 95.6\%\, and a full-task success rate of 91.1\%\ during object grasping and transfer tests, in which a total of 9 object-texture combinations were evaluated. These three experiments' results validate the effectiveness of MEMS-based wearable sensing combined with multi-sensor fusion for reliable and intuitive control of assistive robots in smart home scenarios.
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2504.01888.pdf' target='_blank'>https://arxiv.org/pdf/2504.01888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Qiu, Zhongcai Pei, Chen Wang, Jing Zhang, Zhiyong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01888">A novel gesture interaction control method for rehabilitation lower extremity exoskeleton</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of Rehabilitation Lower Extremity Robotic Exoskeletons (RLEEX) technology, significant advancements have been made in Human-Robot Interaction (HRI) methods. These include traditional physical HRI methods that are easily recognizable and various bio-electrical signal-based HRI methods that can visualize and predict actions. However, most of these HRI methods are contact-based, facing challenges such as operational complexity, sensitivity to interference, risks associated with implantable devices, and, most importantly, limitations in comfort. These challenges render the interaction less intuitive and natural, which can negatively impact patient motivation for rehabilitation. To address these issues, this paper proposes a novel non-contact gesture interaction control method for RLEEX, based on RGB monocular camera depth estimation. This method integrates three key steps: detecting keypoints, recognizing gestures, and assessing distance, thereby applying gesture information and augmented reality triggering technology to control gait movements of RLEEX. Results indicate that this approach provides a feasible solution to the problems of poor comfort, low reliability, and high latency in HRI for RLEEX platforms. Specifically, it achieves a gesture-controlled exoskeleton motion accuracy of 94.11\% and an average system response time of 0.615 seconds through non-contact HRI. The proposed non-contact HRI method represents a pioneering advancement in control interactions for RLEEX, paving the way for further exploration and development in this field.
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2504.00167.pdf' target='_blank'>https://arxiv.org/pdf/2504.00167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teresa Sinico, Giovanni Boschetti, Pedro Neto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00167">Enhancing Physical Human-Robot Interaction: Recognizing Digits via Intrinsic Robot Tactile Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interaction (pHRI) remains a key challenge for achieving intuitive and safe interaction with robots. Current advancements often rely on external tactile sensors as interface, which increase the complexity of robotic systems. In this study, we leverage the intrinsic tactile sensing capabilities of collaborative robots to recognize digits drawn by humans on an uninstrumented touchpad mounted to the robot's flange. We propose a dataset of robot joint torque signals along with corresponding end-effector (EEF) forces and moments, captured from the robot's integrated torque sensors in each joint, as users draw handwritten digits (0-9) on the touchpad. The pHRI-DIGI-TACT dataset was collected from different users to capture natural variations in handwriting. To enhance classification robustness, we developed a data augmentation technique to account for reversed and rotated digits inputs. A Bidirectional Long Short-Term Memory (Bi-LSTM) network, leveraging the spatiotemporal nature of the data, performs online digit classification with an overall accuracy of 94\% across various test scenarios, including those involving users who did not participate in training the system. This methodology is implemented on a real robot in a fruit delivery task, demonstrating its potential to assist individuals in everyday life. Dataset and video demonstrations are available at: https://TS-Robotics.github.io/pHRI-DIGI/.
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2503.16449.pdf' target='_blank'>https://arxiv.org/pdf/2503.16449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangyeol Kang, Thiago Freitas dos Santos, Maher Ben Moussa, Nadia Magnenat-Thalmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16449">Mitigating the Uncanny Valley Effect in Hyper-Realistic Robots: A Student-Centered Study on LLM-Driven Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The uncanny valley effect poses a significant challenge in the development and acceptance of hyper-realistic social robots. This study investigates whether advanced conversational capabilities powered by large language models (LLMs) can mitigate this effect in highly anthropomorphic robots. We conducted a user study with 80 participants interacting with Nadine, a hyper-realistic humanoid robot equipped with LLM-driven communication skills. Through pre- and post-interaction surveys, we assessed changes in perceptions of uncanniness, conversational quality, and overall user experience. Our findings reveal that LLM-enhanced interactions significantly reduce feelings of eeriness while fostering more natural and engaging conversations. Additionally, we identify key factors influencing user acceptance, including conversational naturalness, human-likeness, and interestingness. Based on these insights, we propose design recommendations to enhance the appeal and acceptability of hyper-realistic robots in social contexts. This research contributes to the growing field of human-robot interaction by offering empirical evidence on the potential of LLMs to bridge the uncanny valley, with implications for the future development of social robots.
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2503.15781.pdf' target='_blank'>https://arxiv.org/pdf/2503.15781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuci Han, Charles Toth, Alper Yilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15781">UAS Visual Navigation in Large and Unseen Environments via a Meta Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of this work is to develop an approach that enables Unmanned Aerial System (UAS) to efficiently learn to navigate in large-scale urban environments and transfer their acquired expertise to novel environments. To achieve this, we propose a meta-curriculum training scheme. First, meta-training allows the agent to learn a master policy to generalize across tasks. The resulting model is then fine-tuned on the downstream tasks. We organize the training curriculum in a hierarchical manner such that the agent is guided from coarse to fine towards the target task. In addition, we introduce Incremental Self-Adaptive Reinforcement learning (ISAR), an algorithm that combines the ideas of incremental learning and meta-reinforcement learning (MRL). In contrast to traditional reinforcement learning (RL), which focuses on acquiring a policy for a specific task, MRL aims to learn a policy with fast transfer ability to novel tasks. However, the MRL training process is time consuming, whereas our proposed ISAR algorithm achieves faster convergence than the conventional MRL algorithm. We evaluate the proposed methodologies in simulated environments and demonstrate that using this training philosophy in conjunction with the ISAR algorithm significantly improves the convergence speed for navigation in large-scale cities and the adaptation proficiency in novel environments.
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2503.15522.pdf' target='_blank'>https://arxiv.org/pdf/2503.15522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Tanevska, Katie Winkle, Ginevra Castellano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15522">"I don't like things where I do not have control": Participants' Experience of Trustworthy Interaction with Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of autonomous vehicle (AV) technology, AVs are progressively seen as interactive agents with some level of autonomy, as well as some context-dependent social features.
  This introduces new challenges and questions, already relevant in other areas of human-robot interaction (HRI) - namely, if an AV is perceived as a social agent by the human with whom it is interacting, how are the various facets of its design and behaviour impacting its human partner? And how can we foster a successful human-agent interaction (HAI) between the AV and the human, maximizing the human's comfort, acceptance, and trust in the AV?
  In this work, we attempt to understand the various factors that could influence naÃ¯ve participants' acceptance and trust when interacting with an AV in the role of a driver. Through a large-scale online study, we investigate the effect of the AV's autonomy on the human driver, as well as explore which parameters of the interaction have the highest impact on the user's sense of trust in the AV. Finally, we analyze our preliminary findings from the user study within existing guidelines on Trustworthy HAI/HRI.
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2503.13250.pdf' target='_blank'>https://arxiv.org/pdf/2503.13250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zejia Zhang, Bo Yang, Xinxing Chen, Weizhuang Shi, Haoyuan Wang, Wei Luo, Jian Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13250">MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System for Implicit Intention Recognition and Task Execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A promising effective human-robot interaction in assistive robotic systems is gaze-based control. However, current gaze-based assistive systems mainly help users with basic grasping actions, offering limited support. Moreover, the restricted intent recognition capability constrains the assistive system's ability to provide diverse assistance functions. In this paper, we propose an open implicit intention recognition framework powered by Large Language Model (LLM) and Vision Foundation Model (VFM), which can process gaze input and recognize user intents that are not confined to predefined or specific scenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot system (MindEye-OmniAssist) that recognizes user's intentions through gaze and assists in completing task. To achieve this, the system utilizes open vocabulary object detector, intention recognition network and LLM to infer their full intentions. By integrating eye movement feedback and LLM, it generates action sequences to assist the user in completing tasks. Real-world experiments have been conducted for assistive tasks, and the system achieved an overall success rate of 41/55 across various undefined tasks. Preliminary results show that the proposed method holds the potential to provide a more user-friendly human-computer interaction interface and significantly enhance the versatility and effectiveness of assistive systems by supporting more complex and diverse task.
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2503.13048.pdf' target='_blank'>https://arxiv.org/pdf/2503.13048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haofeng Chen, Bedrich Himmel, Bin Li, Xiaojie Wang, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13048">Multi-Touch and Bending Perception Using Electrical Impedance Tomography for Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electrical Impedance Tomography (EIT) offers a promising solution for distributed tactile sensing with minimal wiring and full-surface coverage in robotic applications. However, EIT-based tactile sensors face significant challenges during surface bending. Deformation alters the baseline impedance distribution and couples with touch-induced conductivity variations, complicating signal interpretation. To address this challenge, we present a novel sensing framework that integrates a deep neural network for interaction state classification with a dynamic adaptive reference strategy to decouple touch and deformation signals, while a data-driven regression model translates EIT voltage changes into continuous bending angles. The framework is validated using a magnetic hydrogel composite sensor that conforms to bendable surfaces. Experimental evaluations demonstrate that the proposed framework achieves precise and robust bending angle estimation, high accuracy in distinguishing touch, bending, and idle states, and significantly improves touch localization quality under bending deformation compared to conventional fixed-reference methods. Real-time experiments confirm the system's capability to reliably detect multi-touch interactions and track bending angles across varying deformation conditions. This work paves the way for flexible EIT-based robotic skins capable of rich multimodal sensing in robotics and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2503.05152.pdf' target='_blank'>https://arxiv.org/pdf/2503.05152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonetani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05152">GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to image-goal navigation by integrating 3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal navigation by guiding a robot through a sequence of point-of-view images without requiring metrical localization or environment-specific training. However, constructing a dense and traversable sequence of target viewpoints from start to goal remains a central challenge, particularly when the available image database is sparse. To address these challenges, we propose a 3DGS-based viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints to seamlessly bridge gaps in sparse data while significantly reducing storage overhead. Experimental results in a photorealistic simulator demonstrate that our approach not only enhances navigation efficiency but also exhibits robustness under varying levels of image database sparsity.
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2503.01363.pdf' target='_blank'>https://arxiv.org/pdf/2503.01363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanghai Zhang, Changyi Liu, Keting Fu, Wenbin Zhou, Qingdu Li, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01363">FABG : End-to-end Imitation Learning for Embodied Affective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes FABG (Facial Affective Behavior Generation), an end-to-end imitation learning system for human-robot interaction, designed to generate natural and fluid facial affective behaviors. In interaction, effectively obtaining high-quality demonstrations remains a challenge. In this work, we develop an immersive virtual reality (VR) demonstration system that allows operators to perceive stereoscopic environments. This system ensures "the operator's visual perception matches the robot's sensory input" and "the operator's actions directly determine the robot's behaviors" - as if the operator replaces the robot in human interaction engagements. We propose a prediction-driven latency compensation strategy to reduce robotic reaction delays and enhance interaction fluency. FABG naturally acquires human interactive behaviors and subconscious motions driven by intuition, eliminating manual behavior scripting. We deploy FABG on a real-world 25-degree-of-freedom (DoF) humanoid robot, validating its effectiveness through four fundamental interaction tasks: expression response, dynamic gaze, foveated attention, and gesture recognition, supported by data collection and policy training. Project website: https://cybergenies.github.io
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2503.01271.pdf' target='_blank'>https://arxiv.org/pdf/2503.01271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An-Chi He, Jungsoo Park, Benjamin Beiter, Bhaben Kalita, Alexander Leonessa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01271">Design and Development of a Locomotion Interface for Virtual Reality Lower-Body Haptic Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents the design, build, control, and preliminary user data of a locomotion interface called ForceBot. It delivers lower-body haptic interaction in virtual reality (VR), enabling users to walk in VR while interacting with various simulated terrains. It utilizes two planar gantries to give each foot two degrees of freedom and passive heel-lifting motion. The design used motion capture data with dynamic simulation for ergonomic human-robot workspace and hardware selection. Its system framework uses open-source robotic software and pairs with a custom-built power delivery system that offers EtherCAT communication with a 1,000 Hz soft real-time computation rate. This system features an admittance controller to regulate physical human-robot interaction (pHRI) alongside a walking algorithm to generate walking motion and simulate virtual terrains. The system's performance is explored through three measurements that evaluate the relationship between user input force and output pHRI motion. Overall, this platform presents a unique approach by utilizing planar gantries to realize VR terrain interaction with an extensive workspace, reasonably compact footprint, and preliminary user data.
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2503.00576.pdf' target='_blank'>https://arxiv.org/pdf/2503.00576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerard GÃ³mez-Izquierdo, Javier Laplaza, Alberto Sanfeliu, AnaÃ­s Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00576">Enhancing Context-Aware Human Motion Prediction for Efficient Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction (HMP) is critical for seamless human-robot collaboration, particularly in handover tasks that require real-time adaptability. Despite the high accuracy of state-of-the-art models, their computational complexity limits practical deployment in real-world robotic applications. In this work, we enhance human motion forecasting for handover tasks by leveraging siMLPe [1], a lightweight yet powerful architecture, and introducing key improvements. Our approach, named IntentMotion incorporates intention-aware conditioning, task-specific loss functions, and a novel intention classifier, significantly improving motion prediction accuracy while maintaining efficiency. Experimental results demonstrate that our method reduces body loss error by over 50%, achieves 200x faster inference, and requires only 3% of the parameters compared to existing state-of-the-art HMP models. These advancements establish our framework as a highly efficient and scalable solution for real-time human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2502.06851.pdf' target='_blank'>https://arxiv.org/pdf/2502.06851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yerkebulan Massalim, Yermakhan Kassym, Zerde Nurbayeva, Zhanat Kappassov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06851">Survey on Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2502.02051.pdf' target='_blank'>https://arxiv.org/pdf/2502.02051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aimee Allen, Tom Drummond, Dana KuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02051">Sound Judgment: Properties of Consequential Sounds Affecting Human-Perception of Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positive human-perception of robots is critical to achieving sustained use of robots in shared environments. One key factor affecting human-perception of robots are their sounds, especially the consequential sounds which robots (as machines) must produce as they operate. This paper explores qualitative responses from 182 participants to gain insight into human-perception of robot consequential sounds. Participants viewed videos of different robots performing their typical movements, and responded to an online survey regarding their perceptions of robots and the sounds they produce. Topic analysis was used to identify common properties of robot consequential sounds that participants expressed liking, disliking, wanting or wanting to avoid being produced by robots. Alongside expected reports of disliking high pitched and loud sounds, many participants preferred informative and audible sounds (over no sound) to provide predictability of purpose and trajectory of the robot. Rhythmic sounds were preferred over acute or continuous sounds, and many participants wanted more natural sounds (such as wind or cat purrs) in-place of machine-like noise. The results presented in this paper support future research on methods to improve consequential sounds produced by robots by highlighting features of sounds that cause negative perceptions, and providing insights into sound profile changes for improvement of human-perception of robots, thus enhancing human robot interaction.
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2501.19318.pdf' target='_blank'>https://arxiv.org/pdf/2501.19318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19318">MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2501.16513.pdf' target='_blank'>https://arxiv.org/pdf/2501.16513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16513">Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.
  Our study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2501.15332.pdf' target='_blank'>https://arxiv.org/pdf/2501.15332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Qin, Richard T. Lee, Paul Sajda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15332">Perception of an AI Teammate in an Embodied Control Task Affects Team Performance, Reflected in Human Teammates' Behaviors and Physiological Responses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of artificial intelligence (AI) into human teams is widely expected to enhance performance and collaboration. However, our study reveals a striking and counterintuitive result: human-AI teams performed worse than human-only teams, especially when task difficulty increased. Using a virtual reality-based sensorimotor task, we observed that the inclusion of an active human-like AI teammate disrupted team dynamics, leading to elevated arousal, reduced engagement, and diminished communication intensity among human participants. These effects persisted even as the human teammates' perception of the AI teammate improved over time. These findings challenge prevailing assumptions about the benefits of AI in team settings and highlight the critical need for human-centered AI design to mitigate adverse physiological and behavioral impacts, ensuring more effective human-AI collaboration.
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2501.14099.pdf' target='_blank'>https://arxiv.org/pdf/2501.14099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaclyn Molan, Laura Saad, Eileen Roesler, J. Malcolm McCurry, Nathaniel Gyory, J. Gregory Trafton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14099">The Perceived Danger (PD) Scale: Development and Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are currently no psychometrically valid tools to measure the perceived danger of robots. To fill this gap, we provided a definition of perceived danger and developed and validated a 12-item bifactor scale through four studies. An exploratory factor analysis revealed four subdimensions of perceived danger: affective states, physical vulnerability, ominousness, and cognitive readiness. A confirmatory factor analysis confirmed the bifactor model. We then compared the perceived danger scale to the Godspeed perceived safety scale and found that the perceived danger scale is a better predictor of empirical data. We also validated the scale in an in-person setting and found that the perceived danger scale is sensitive to robot speed manipulations, consistent with previous empirical findings. Results across experiments suggest that the perceived danger scale is reliable, valid, and an adequate predictor of both perceived safety and perceived danger in human-robot interaction contexts.
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2501.13996.pdf' target='_blank'>https://arxiv.org/pdf/2501.13996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Farshian Abbasi, Aghil Yousefi-Koma, Soheil Dehghani Firouzabadi, Parisa Rashidi, Alireza Naeini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13996">Integrating Persian Lip Reading in Surena-V Humanoid Robot for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lip reading is vital for robots in social settings, improving their ability to understand human communication. This skill allows them to communicate more easily in crowded environments, especially in caregiving and customer service roles. Generating a Persian Lip-reading dataset, this study integrates Persian lip-reading technology into the Surena-V humanoid robot to improve its speech recognition capabilities. Two complementary methods are explored, an indirect method using facial landmark tracking and a direct method leveraging convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. The indirect method focuses on tracking key facial landmarks, especially around the lips, to infer movements, while the direct method processes raw video data for action and speech recognition. The best-performing model, LSTM, achieved 89\% accuracy and has been successfully implemented into the Surena-V robot for real-time human-robot interaction. The study highlights the effectiveness of these methods, particularly in environments where verbal communication is limited.
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2501.13203.pdf' target='_blank'>https://arxiv.org/pdf/2501.13203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Amiri, Mehdi Hosseinzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13203">Safe and Efficient Robot Action Planning in the Presence of Unconcerned Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a robot action planning scheme that provides an efficient and probabilistically safe plan for a robot interacting with an unconcerned human -- someone who is either unaware of the robot's presence or unwilling to engage in ensuring safety. The proposed scheme is predictive, meaning that the robot is required to predict human actions over a finite future horizon; such predictions are often inaccurate in real-world scenarios. One possible approach to reduce the uncertainties is to provide the robot with the capability of reasoning about the human's awareness of potential dangers. This paper discusses that by using a binary variable, so-called danger awareness coefficient, it is possible to differentiate between concerned and unconcerned humans, and provides a learning algorithm to determine this coefficient by observing human actions. Moreover, this paper argues how humans rely on predictions of other agents' future actions (including those of robots in human-robot interaction) in their decision-making. It also shows that ignoring this aspect in predicting human's future actions can significantly degrade the efficiency of the interaction, causing agents to deviate from their optimal paths. The proposed robot action planning scheme is verified and validated via extensive simulation and experimental studies on a LoCoBot WidowX-250.
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2501.08946.pdf' target='_blank'>https://arxiv.org/pdf/2501.08946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Skantze, Bahar Irfan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08946">Applying General Turn-taking Models to Conversational Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2501.07507.pdf' target='_blank'>https://arxiv.org/pdf/2501.07507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniele Meli, Paolo Fiorini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07507">Inductive Learning of Robot Task Knowledge from Raw Data and Online Expert Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios.
  In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2501.07213.pdf' target='_blank'>https://arxiv.org/pdf/2501.07213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Ala Yahyaoui, Mouaad Oujabour, Leila Ben Letaifa, Amine Bohi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07213">Multi-face emotion detection for effective Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of dialogue interfaces in mobile devices has become ubiquitous, providing a wide array of services. As technology progresses, humanoid robots designed with human-like features to interact effectively with people are gaining prominence, and the use of advanced human-robot dialogue interfaces is continually expanding. In this context, emotion recognition plays a crucial role in enhancing human-robot interaction by enabling robots to understand human intentions. This research proposes a facial emotion detection interface integrated into a mobile humanoid robot, capable of displaying real-time emotions from multiple individuals on a user interface. To this end, various deep neural network models for facial expression recognition were developed and evaluated under consistent computer-based conditions, yielding promising results. Afterwards, a trade-off between accuracy and memory footprint was carefully considered to effectively implement this application on a mobile humanoid robot.
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2501.06867.pdf' target='_blank'>https://arxiv.org/pdf/2501.06867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alice Nardelli, Lorenzo Landolfi, Dario Pasquali, Antonio Sgorbissa, Francesco Rea, Carmine Recchiuto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06867">Toward a Universal Concept of Artificial Personality: Implementing Robotic Personality in a Kinova Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fundamental role of personality in shaping interactions is increasingly being exploited in robotics. A carefully designed robotic personality has been shown to improve several key aspects of Human-Robot Interaction (HRI). However, the fragmentation and rigidity of existing approaches reveal even greater challenges when applied to non-humanoid robots. On one hand, the state of the art is very dispersed; on the other hand, Industry 4.0 is moving towards a future where humans and industrial robots are going to coexist. In this context, the proper design of a robotic personality can lead to more successful interactions. This research takes a first step in that direction by integrating a comprehensive cognitive architecture built upon the definition of robotic personality - validated on humanoid robots - into a robotic Kinova Jaco2 arm. The robot personality is defined through the cognitive architecture as a vector in the three-dimensional space encompassing Conscientiousness, Extroversion, and Agreeableness, affecting how actions are executed, the action selection process, and the internal reaction to environmental stimuli. Our main objective is to determine whether users perceive distinct personalities in the robot, regardless of its shape, and to understand the role language plays in shaping these perceptions. To achieve this, we conducted a user study comprising 144 sessions of a collaborative game between a Kinova Jaco2 arm and participants, where the robot's behavior was influenced by its assigned personality. Furthermore, we compared two conditions: in the first, the robot communicated solely through gestures and action choices, while in the second, it also utilized verbal interaction.
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2501.04860.pdf' target='_blank'>https://arxiv.org/pdf/2501.04860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael F. Xu, Bilge Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04860">Exploring the Use of Robots for Diary Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2501.04755.pdf' target='_blank'>https://arxiv.org/pdf/2501.04755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phillip Richter, Heiko Wersing, Anna-Lisa Vollmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04755">Improving Human-Robot Teaching by Quantifying and Reducing Mental Model Mismatch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of artificial intelligence and robotics has had a significant impact on our lives, with intelligent systems increasingly performing tasks traditionally performed by humans. Efficient knowledge transfer requires matching the mental model of the human teacher with the capabilities of the robot learner. This paper introduces the Mental Model Mismatch (MMM) Score, a feedback mechanism designed to quantify and reduce mismatches by aligning human teaching behavior with robot learning behavior. Using Large Language Models (LLMs), we analyze teacher intentions in natural language to generate adaptive feedback. A study with 150 participants teaching a virtual robot to solve a puzzle game shows that intention-based feedback significantly outperforms traditional performance-based feedback or no feedback. The results suggest that intention-based feedback improves instructional outcomes, improves understanding of the robot's learning process and reduces misconceptions. This research addresses a critical gap in human-robot interaction (HRI) by providing a method to quantify and mitigate discrepancies between human mental models and robot capabilities, with the goal of improving robot learning and human teaching effectiveness.
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2501.00953.pdf' target='_blank'>https://arxiv.org/pdf/2501.00953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Casey Kennington, Pierre Lison, David Schlangen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00953">Prior Lessons of Incremental Dialogue and Robot Action Management for the Age of Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efforts towards endowing robots with the ability to speak have benefited from recent advancements in natural language processing, in particular large language models. However, current language models are not fully incremental, as their processing is inherently monotonic and thus lack the ability to revise their interpretations or output in light of newer observations. This monotonicity has important implications for the development of dialogue systems for human--robot interaction. In this paper, we review the literature on interactive systems that operate incrementally (i.e., at the word level or below it). We motivate the need for incremental systems, survey incremental modeling of important aspects of dialogue like speech recognition and language generation. Primary focus is on the part of the system that makes decisions, known as the dialogue manager. We find that there is very little research on incremental dialogue management, offer some requirements for practical incremental dialogue management, and the implications of incremental dialogue for embodied, robotic platforms in the age of large language models.
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2412.19595.pdf' target='_blank'>https://arxiv.org/pdf/2412.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Rao Marpally, Pranav Goyal, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19595">SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current social navigation methods and benchmarks primarily focus on proxemics and task efficiency. While these factors are important, qualitative aspects such as perceptions of a robot's social competence are equally crucial for successful adoption and integration into human environments. We propose a more comprehensive evaluation of social navigation through scenario-based testing, where specific human-robot interaction scenarios can reveal key robot behaviors. However, creating such scenarios is often labor-intensive and complex. In this work, we address this challenge by introducing a pipeline that automates the generation of context-, and location-appropriate social navigation scenarios, ready for simulation. Our pipeline transforms simple scenario metadata into detailed textual scenarios, infers pedestrian and robot trajectories, and simulates pedestrian behaviors, which enables more controlled evaluation. We leverage the social reasoning and code-generation capabilities of Large Language Models (LLMs) to streamline scenario generation and translation. Our experiments show that our pipeline produces realistic scenarios and significantly improves scenario translation over naive LLM prompting. Additionally, we present initial feedback from a usability study with social navigation experts and a case-study demonstrating a scenario-based evaluation of three navigation algorithms.
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2412.12990.pdf' target='_blank'>https://arxiv.org/pdf/2412.12990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonios Gasteratos, Stavros N. Moutsis, Konstantinos A. Tsintotas, Yiannis Aloimonos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12990">Future Aspects in Human Action Recognition: Exploring Emerging Techniques and Ethical Influences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-based human action recognition can be found in various application fields, e.g., surveillance systems, sports analytics, medical assistive technologies, or human-robot interaction frameworks, and it concerns the identification and classification of individuals' activities within a video. Since actions typically occur over a sequence of consecutive images, it is particularly challenging due to the inclusion of temporal analysis, which introduces an extra layer of complexity. However, although multiple approaches try to handle temporal analysis, there are still difficulties because of their computational cost and lack of adaptability. Therefore, different types of vision data, containing transition information between consecutive images, provided by next-generation hardware sensors will guide the robotics community in tackling the problem of human action recognition. On the other hand, while there is a plethora of still-image datasets, that researchers can adopt to train new artificial intelligence models, videos representing human activities are of limited capabilities, e.g., small and unbalanced datasets or selected without control from multiple sources. To this end, generating new and realistic synthetic videos is possible since labeling is performed throughout the data creation process, while reinforcement learning techniques can permit the avoidance of considerable dataset dependence. At the same time, human factors' involvement raises ethical issues for the research community, as doubts and concerns about new technologies already exist.
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2412.07057.pdf' target='_blank'>https://arxiv.org/pdf/2412.07057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Li, Chicheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07057">A Note on Sample Complexity of Interactive Imitation Learning with Log Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning (IL) is a general paradigm for learning from experts in sequential decision-making problems. Recent advancements in IL have shown that offline imitation learning, specifically Behavior Cloning (BC) with log loss, is minimax optimal. Meanwhile, its interactive counterpart, DAgger, is shown to suffer from suboptimal sample complexity. In this note, we focus on realizable deterministic expert and revisit interactive imitation learning, particularly DAgger with log loss. We demonstrate: 1. A one-sample-per-round DAgger variant that outperforms BC in state-wise annotation. 2. Without recoverability assumption, DAgger with first-step mixture policies matches the performance of BC. Along the analysis, we introduce a new notion of decoupled Hellinger distance that separates state and action sequences, which can be of independent interest.
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2412.05024.pdf' target='_blank'>https://arxiv.org/pdf/2412.05024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Sievers, Nele Russwinkel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05024">Talking Like One of Us: Effects of Using Regional Language in a Humanoid Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are becoming more and more perceptible in public service settings. For engaging people in a natural environment a smooth social interaction as well as acceptance by the users are important issues for future successful Human-Robot Interaction (HRI). The type of verbal communication has a special significance here. In this paper we investigate the effects of spoken language varieties of a non-standard/regional language compared to standard language. More precisely we compare a human dialog with a humanoid social robot Pepper where the robot on the one hand is answering in High German and on the other hand in Low German, a regional language that is understood and partly still spoken in the northern parts of Germany. The content of what the robot says remains the same in both variants. We are interested in the effects that these two different ways of robot talk have on human interlocutors who are more or less familiar with Low German in terms of perceived warmth, competence and possible discomfort in conversation against a background of cultural identity. To measure these factors we use the Robotic Social Attributes Scale (RoSAS) on 17 participants with an age ranging from 19 to 61. Our results show that significantly higher warmth is perceived in the Low German version of the conversation.
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2412.00435.pdf' target='_blank'>https://arxiv.org/pdf/2412.00435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Liu, Boshen Zhang, Zhehui Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00435">Benchmark Real-time Adaptation and Communication Capabilities of Embodied Agent in Collaborative Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in Large Language Models (LLMs) have opened transformative possibilities for human-robot interaction, especially in collaborative environments. However, Real-time human-AI collaboration requires agents to adapt to unseen human behaviors while maintaining effective communication dynamically. Existing benchmarks fall short in evaluating such adaptability for embodied agents, focusing mostly on the task performance of the agent itself. To address this gap, we propose a novel benchmark that assesses agents' reactive adaptability and instantaneous communication capabilities at every step. Based on this benchmark, we propose a Monitor-then-Adapt framework (MonTA), combining strong adaptability and communication with real-time execution. MonTA contains three key LLM modules, a lightweight \textit{Monitor} for monitoring the need for adaptation in high frequency, and two proficient \textit{Adapters} for subtask and path adaptation reasoning in low frequency. Our results demonstrate that MonTA outperforms other baseline agents on our proposed benchmark. Further user studies confirm the high reasonability adaptation plan and consistent language instruction provided by our framework.
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2411.13851.pdf' target='_blank'>https://arxiv.org/pdf/2411.13851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyou Pei, Alexander Chen, Ronak Kaoshik, Ruofei Du, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13851">Arm Robot: AR-Enhanced Embodied Control and Visualization for Intuitive Robot Arm Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied interaction has been introduced to human-robot interaction (HRI) as a type of teleoperation, in which users control robot arms with bodily action via handheld controllers or haptic gloves. Embodied teleoperation has made robot control intuitive to non-technical users, but differences between humans' and robots' capabilities \eg ranges of motion and response time, remain challenging. In response, we present Arm Robot, an embodied robot arm teleoperation system that helps users tackle human-robot discrepancies. Specifically, Arm Robot (1) includes AR visualization as real-time feedback on temporal and spatial discrepancies, and (2) allows users to change observing perspectives and expand action space. We conducted a user study (N=18) to investigate the usability of the Arm Robot and learn how users perceive the embodiment. Our results show users could use Arm Robot's features to effectively control the robot arm, providing insights for continued work in embodied HRI.
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2411.04796.pdf' target='_blank'>https://arxiv.org/pdf/2411.04796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayan Paul, Ruddra dev Roychoudhury, Brojeshwar Bhowmick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04796">MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2411.03873.pdf' target='_blank'>https://arxiv.org/pdf/2411.03873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Italo Belli, Florian van Melis, J. Micah Prendergast, Ajay Seth, Luka Peternel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03873">Biomechanics-Aware Trajectory Optimization for Online Navigation during Robotic Physiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic devices provide a great opportunity to assist in delivering physical therapy and rehabilitation movements, yet current robot-assisted methods struggle to incorporate biomechanical metrics essential for safe and effective therapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization approach to online robotic Navigation of human musculoskeletal loads for rotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the human shoulder into an optimal control framework, generating strain-minimizing trajectories for real-time control of therapeutic movements. \addedText{Its core strength lies in the ability to adapt biomechanics-informed trajectories online to unpredictable volitional human actions or reflexive reactions during physical human-robot interaction based on robot-sensed motion and forces. BATON's adaptability is enabled by a real-time, model-based estimator that infers changes in muscle activity via a rapid redundancy solver driven by robot pose and force/torque sensor data. We validated BATON through physical human-robot interaction experiments, assessing response speed, motion smoothness, and interaction forces.
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2410.24036.pdf' target='_blank'>https://arxiv.org/pdf/2410.24036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niti Parikh, Yiran Zhao, Maria Alinea-Bravo, Tapan Parikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24036">The Communal Loom: Integrating Tangible Interaction and Participatory Data Collection for Assessing Well-Being</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For most health or well-being interventions, the process of evaluation is distinct from the activity itself, both in terms of who is involved, and how the actual data is collected and analyzed. Tangible interaction affords the opportunity to combine direct and embodied collaboration with a holistic approach to data collection and evaluation. We demonstrate this potential by describing our experiences designing and using the Communal Loom, an artifact for art therapy that translates quantitative data to collectively woven artifacts.
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2410.20423.pdf' target='_blank'>https://arxiv.org/pdf/2410.20423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Gao, Cheng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20423">A Deconfounding Framework for Human Behavior Prediction: Enhancing Robotic Systems in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of human behavior is crucial for effective human-robot interaction (HRI) systems, especially in dynamic environments where real-time decisions are essential. This paper addresses the challenge of forecasting future human behavior using multivariate time series data from wearable sensors, which capture various aspects of human movement. The presence of hidden confounding factors in this data often leads to biased predictions, limiting the reliability of traditional models. To overcome this, we propose a robust predictive model that integrates deconfounding techniques with advanced time series prediction methods, enhancing the model's ability to isolate true causal relationships and improve prediction accuracy. Evaluation on real-world datasets demonstrates that our approach significantly outperforms traditional methods, providing a more reliable foundation for responsive and adaptive HRI systems.
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2410.13847.pdf' target='_blank'>https://arxiv.org/pdf/2410.13847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariel Slepyan, Dian Li, Aidan Aug, Sriramana Sankar, Trac Tran, Nitish Thakor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13847">Adaptive Compressive Tactile Subsampling: Enabling High Spatiotemporal Resolution in Scalable Robotic Skin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots, like humans, require full-body, high-resolution tactile sensing to operate safely and effectively in unstructured environments, enabling reflexive responses and closed-loop control. However, the high pixel counts necessary for dense, large-area coverage limit readout rates of most tactile arrays to below 100 Hz, hindering their use in high-speed tasks. We introduce Adaptive Compressive Tactile Subsampling (ACTS), a scalable and data-driven method that dramatically enhances the performance of traditional tactile matrices by leveraging sparse recovery and a learned tactile dictionary. Tested on a 1024-pixel tactile sensor array (32X32), ACTS achieved frame rates up to 1,000 Hz, an 18X improvement over conventional raster scanning, with minimal reconstruction error. For the first time, ACTS enables wearable, large-area, high-density tactile sensing systems that can deliver high-speed results. We demonstrate rapid object classification within 20 ms of contact, high-speed projectile detection, ricochet angle estimation, and soft deformation tracking, in tactile and robotics applications, all using flexible, high-density tactile arrays. These include high-resolution tactile gloves, pressure insoles, and full-body configurations covering robotic arms and human-sized mannequins. ACTS transforms standard, low-cost, and robust tactile sensors into high-speed systems, supporting applications from object manipulation to human-robot interaction. By enabling comprehensive, scalable, and efficient tactile coverage for robots and wearables, ACTS advances robotics toward lifelike, responsive, and adaptable operation in dynamic environments.
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2410.11564.pdf' target='_blank'>https://arxiv.org/pdf/2410.11564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shang-Ching Liu, Van Nhiem Tran, Wenkai Chen, Wei-Lun Cheng, Yen-Lin Huang, I-Bin Liao, Yung-Hui Li, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11564">PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance understanding, the task of identifying actionable regions on 3D objects, plays a vital role in allowing robotic systems to engage with and operate within the physical world. Although Visual Language Models (VLMs) have excelled in high-level reasoning and long-horizon planning for robotic manipulation, they still fall short in grasping the nuanced physical properties required for effective human-robot interaction. In this paper, we introduce PAVLM (Point cloud Affordance Vision-Language Model), an innovative framework that utilizes the extensive multimodal knowledge embedded in pre-trained language models to enhance 3D affordance understanding of point cloud. PAVLM integrates a geometric-guided propagation module with hidden embeddings from large language models (LLMs) to enrich visual semantics. On the language side, we prompt Llama-3.1 models to generate refined context-aware text, augmenting the instructional input with deeper semantic cues. Experimental results on the 3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods for both full and partial point clouds, particularly excelling in its generalization to novel open-world affordance tasks of 3D objects. For more information, visit our project site: pavlm-source.github.io.
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2410.09874.pdf' target='_blank'>https://arxiv.org/pdf/2410.09874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinxin Zhao, Wenzhe Cai, Likun Tang, Teng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09874">ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2410.06355.pdf' target='_blank'>https://arxiv.org/pdf/2410.06355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Gajewski, Antonio Galiza Cerdeira Gonzalez, Bipin Indurkhya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06355">Context-Aware Command Understanding for Tabletop Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel hybrid algorithm designed to interpret natural human commands in tabletop scenarios. By integrating multiple sources of information, including speech, gestures, and scene context, the system extracts actionable instructions for a robot, identifying relevant objects and actions. The system operates in a zero-shot fashion, without reliance on predefined object models, enabling flexible and adaptive use in various environments. We assess the integration of multiple deep learning models, evaluating their suitability for deployment in real-world robotic setups. Our algorithm performs robustly across different tasks, combining language processing with visual grounding. In addition, we release a small dataset of video recordings used to evaluate the system. This dataset captures real-world interactions in which a human provides instructions in natural language to a robot, a contribution to future research on human-robot interaction. We discuss the strengths and limitations of the system, with particular focus on how it handles multimodal command interpretation, and its ability to be integrated into symbolic robotic frameworks for safe and explainable decision-making.
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2410.03086.pdf' target='_blank'>https://arxiv.org/pdf/2410.03086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danyi Chen, Ravi Prakash, Zacharias Chen, Sarah Dias, Vincent Wang, Leila Bridgeman, Siobhan Oca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03086">Design and Evaluation of a Compliant Quasi Direct Drive End-effector for Safe Robotic Ultrasound Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot-assisted ultrasound scanning promises to advance autonomous and accessible medical imaging. However, ensuring patient safety and compliant human-robot interaction (HRI) during probe contact poses a significant challenge. Most existing systems either have high mechanical stiffness or are compliant but lack sufficient force and precision. This paper presents a novel single-degree-of-freedom end-effector for safe and accurate robotic ultrasound imaging, using a quasi-direct drive actuator to achieve both passive mechanical compliance and precise active force regulation, even during motion. The end-effector demonstrates an effective force control bandwidth of 100 Hz and can apply forces ranging from 2.5N to 15N. To validate the end-effector's performance, we developed a novel ex vivo actuating platform, enabling compliance testing of the end-effector on simulated abdominal breathing and sudden patient movements. Experiments demonstrate that the end-effector can maintain consistent probe contact during simulated respiratory motion at 2.5N, 5N, 10N, and 15N, with an average force tracking RMS error of 0.83N compared to 4.70N on a UR3e robot arm using conventional force control. This system represents the first compliant ultrasound end-effector tested on a tissue platform simulating dynamic movement. The proposed solution provides a novel approach for designing and evaluating compliant robotic ultrasound systems, advancing the path for more compliant and patient-friendly robotic ultrasound systems in clinical settings.
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2409.16465.pdf' target='_blank'>https://arxiv.org/pdf/2409.16465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan-Diego Florez, Mehregan Dor, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16465">Initialization of Monocular Visual Navigation for Autonomous Agents Using Modified Structure from Small Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a standalone monocular visual Simultaneous Localization and Mapping (vSLAM) initialization pipeline for autonomous space robots. Our method, a state-of-the-art factor graph optimization pipeline, extends Structure from Small Motion (SfSM) to robustly initialize a monocular agent in spacecraft inspection trajectories, addressing visual estimation challenges such as weak-perspective projection and center-pointing motion, which exacerbates the bas-relief ambiguity, dominant planar geometry, which causes motion estimation degeneracies in classical Structure from Motion, and dynamic illumination conditions, which reduce the survivability of visual information. We validate our approach on realistic, simulated satellite inspection image sequences with a tumbling spacecraft and demonstrate the method's effectiveness over existing monocular initialization procedures.
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2409.15658.pdf' target='_blank'>https://arxiv.org/pdf/2409.15658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Liu, Jiawei Du, Sicheng Xiang, Zibo Wang, Dingsheng Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15658">Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon embodied planning underpins embodied AI. To accomplish long-horizon tasks, one of the most feasible ways is to decompose abstract instructions into a sequence of actionable steps. Foundation models still face logical errors and hallucinations in long-horizon planning, unless provided with highly relevant examples to the tasks. However, providing highly relevant examples for any random task is unpractical. Therefore, we present ReLEP, a novel framework for Real-time Long-horizon Embodied Planning. ReLEP can complete a wide range of long-horizon tasks without in-context examples by learning implicit logical inference through fine-tuning. The fine-tuned large vision-language model formulates plans as sequences of skill functions. These functions are selected from a carefully designed skill library. ReLEP is also equipped with a Memory module for plan and status recall, and a Robot Configuration module for versatility across robot types. In addition, we propose a data generation pipeline to tackle dataset scarcity. When constructing the dataset, we considered the implicit logical relationships, enabling the model to learn implicit logical relationships and dispel hallucinations. Through comprehensive evaluations across various long-horizon tasks, ReLEP demonstrates high success rates and compliance to execution even on unseen tasks and outperforms state-of-the-art baseline methods.
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2409.01036.pdf' target='_blank'>https://arxiv.org/pdf/2409.01036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Magri, Javad Amirian, Mohamed Chetouani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01036">Upgrading Pepper Robot s Social Interaction with Advanced Hardware and Perception Enhancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose hardware and software enhancements for the Pepper robot to improve its human-robot interaction capabilities. This includes the integration of an NVIDIA Jetson GPU to enhance computational capabilities and execute real time algorithms, and a RealSense D435i camera to capture depth images, as well as the computer vision algorithms to detect and localize the humans around the robot and estimate their body orientation and gaze direction. The new stack is implemented on ROS and is running on the extended Pepper hardware, and the communication with the robot s firmware is done through the NAOqi ROS driver API. We have also collected a MoCap dataset of human activities in a controlled environment, together with the corresponding RGB-D data, to validate the proposed perception algorithms.
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2408.15761.pdf' target='_blank'>https://arxiv.org/pdf/2408.15761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NicolÃ¡s Soncini, Javier Civera, TaihÃº Pire
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15761">Addressing the challenges of loop detection in agricultural environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While visual SLAM systems are well studied and achieve impressive results in indoor and urban settings, natural, outdoor and open-field environments are much less explored and still present relevant research challenges. Visual navigation and local mapping have shown a relatively good performance in open-field environments. However, globally consistent mapping and long-term localization still depend on the robustness of loop detection and closure, for which the literature is scarce. In this work we propose a novel method to pave the way towards robust loop detection in open fields, particularly in agricultural settings, based on local feature search and stereo geometric refinement, with a final stage of relative pose estimation. Our method consistently achieves good loop detections, with a median error of 15cm. We aim to characterize open fields as a novel environment for loop detection, understanding the limitations and problems that arise when dealing with them.
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2408.08734.pdf' target='_blank'>https://arxiv.org/pdf/2408.08734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristian Camardella, Vittorio Lippi, Francesco Porcini, Giulia Bassani, Lucia Lencioni, Christoph Mauer, Christian Haverkamp, Carlo Alberto Avizzano, Antonio Frisoli, Alessandro Filippeschi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08734">User-centered evaluation of the Wearable Walker lower limb exoskeleton, preliminary assessment based on the Experience protocol</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using lower-limbs exoskeletons provides potential advantages in terms of productivity and safety associated with reduced stress. However, complex issues in human-robot interaction are still open, such as the physiological effects of exoskeletons and the impact on the user's subjective experience. In this work, an innovative exoskeleton, the Wearable Walker, is assessed using the EXPERIENCE benchmarking protocol from the EUROBENCH project. The Wearable Walker is a lower-limb exoskeleton that enhances human abilities, such as carrying loads. The device uses a unique control approach called Blend Control that provides smooth assistance torques. It operates two models simultaneously, one in the case in which the left foot is grounded and another for the grounded right foot. These models generate assistive torques combined to provide continuous and smooth overall assistance, preventing any abrupt changes in torque due to model switching. The EXPERIENCE protocol consists of walking on flat ground while gathering physiological signals such as heart rate, its variability, respiration rate, and galvanic skin response and completing a questionnaire. The test was performed with five healthy subjects. The scope of the present study is twofold: to evaluate the specific exoskeleton and its current control system to gain insight into possible improvements and to present a case study for a formal and replicable benchmarking of wearable robots.
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2408.02535.pdf' target='_blank'>https://arxiv.org/pdf/2408.02535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Kaichen, Song Yaoxian, Zhao Haiquan, Liu Haoyu, Li Tiefeng, Li Zhixu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02535">Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input. Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time. Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\%$ improvement in success rate. Our project is available at https://sites.google.com/view/vln-eventkg
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2407.18335.pdf' target='_blank'>https://arxiv.org/pdf/2407.18335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalini Sushri, Rahul Dass, Rhea Basappa, Hong Lu, Ashok Goel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18335">Combining Cognitive and Generative AI for Self-explanation in Interactive AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Virtual Experimental Research Assistant (VERA) is an inquiry-based learning environment that empowers a learner to build conceptual models of complex ecological systems and experiment with agent-based simulations of the models. This study investigates the convergence of cognitive AI and generative AI for self-explanation in interactive AI agents such as VERA. From a cognitive AI viewpoint, we endow VERA with a functional model of its own design, knowledge, and reasoning represented in the Task--Method--Knowledge (TMK) language. From the perspective of generative AI, we use ChatGPT, LangChain, and Chain-of-Thought to answer user questions based on the VERA TMK model. Thus, we combine cognitive and generative AI to generate explanations about how VERA works and produces its answers. The preliminary evaluation of the generation of explanations in VERA on a bank of 66 questions derived from earlier work appears promising.
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2407.01280.pdf' target='_blank'>https://arxiv.org/pdf/2407.01280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emilia Heikkinen, Elsa Silvennoinen, Imran Khan, Zakaria Lemhaouri, Laura Cohen, Lola CaÃ±amero, Robert Lowe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01280">Human-Robot Mutual Learning through Affective-Linguistic Interaction and Differential Outcomes Training [Pre-Print]</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Owing to the recent success of Large Language Models, Modern A.I has been much focused on linguistic interactions with humans but less focused on non-linguistic forms of communication between man and machine. In the present paper, we test how affective-linguistic communication, in combination with differential outcomes training, affects mutual learning in a human-robot context. Taking inspiration from child-caregiver dynamics, our human-robot interaction setup consists of a (simulated) robot attempting to learn how best to communicate internal, homeostatically-controlled needs; while a human "caregiver" attempts to learn the correct object to satisfy the robot's present communicated need. We studied the effects of i) human training type, and ii) robot reinforcement learning type, to assess mutual learning terminal accuracy and rate of learning (as measured by the average reward achieved by the robot). Our results find mutual learning between a human and a robot is significantly improved with Differential Outcomes Training (DOT) compared to Non-DOT (control) conditions. We find further improvements when the robot uses an exploration-exploitation policy selection, compared to purely exploitation policy selection. These findings have implications for utilizing socially assistive robots (SAR) in therapeutic contexts, e.g. for cognitive interventions, and educational applications.
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2406.12253.pdf' target='_blank'>https://arxiv.org/pdf/2406.12253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Jiang, Elizabeth A. Croft, Michael G. Burke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12253">Influence-Based Reward Modulation for Implicit Communication in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is essential for successful interaction. In human-robot interaction, implicit communication holds the potential to enhance robots' understanding of human needs, emotions, and intentions. This paper introduces a method to foster implicit communication in HRI without explicitly modelling human intentions or relying on pre-existing knowledge. Leveraging Transfer Entropy, we modulate influence between agents in social interactions in scenarios involving either collaboration or competition. By integrating influence into agents' rewards within a partially observable Markov decision process, we demonstrate that boosting influence enhances collaboration, while resisting influence diminishes performance. Our findings are validated through simulations and real-world experiments with human participants in social navigation settings.
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2406.00869.pdf' target='_blank'>https://arxiv.org/pdf/2406.00869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarthak Arora, Karthik Subramanian, Odysseus Adamides, Ferat Sahin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00869">Using 3-D LiDAR Data for Safe Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the use of 3D lidar in a physical Human-Robot Interaction (pHRI) scenario. To achieve the aforementioned, experiments were conducted to mimic a modern shop-floor environment. Data was collected from a pool of seventeen participants while performing pre-determined tasks in a shared workspace with the robot. To demonstrate an end-to-end case; a perception pipeline was developed that leverages reflectivity, signal, near-infrared, and point-cloud data from a 3-D lidar. This data is then used to perform safety based control whilst satisfying the speed and separation monitoring (SSM) criteria. In order to support the perception pipeline, a state-of-the-art object detection network was leveraged and fine-tuned by transfer learning. An analysis is provided along with results of the perception and the safety based controller. Additionally, this system is compared with the previous work.
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2405.20189.pdf' target='_blank'>https://arxiv.org/pdf/2405.20189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangyeol Kang, Maher Ben Moussa, Nadia Magnenat-Thalmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20189">Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we describe our approach to developing an intelligent and robust social robotic system for the Nadine social robot platform. We achieve this by integrating Large Language Models (LLMs) and skilfully leveraging the powerful reasoning and instruction-following capabilities of these types of models to achieve advanced human-like affective and cognitive capabilities. This approach is novel compared to the current state-of-the-art LLM-based agents which do not implement human-like long-term memory or sophisticated emotional appraisal. The naturalness of social robots, consisting of multiple modules, highly depends on the performance and capabilities of each component of the system and the seamless integration of the components. We built a social robot system that enables generating appropriate behaviours through multimodal input processing, bringing episodic memories accordingly to the recognised user, and simulating the emotional states of the robot induced by the interaction with the human partner. In particular, we introduce an LLM-agent frame for social robots, SoR-ReAct, serving as a core component for the interaction module in our system. This design has brought forth the advancement of social robots and aims to increase the quality of human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2405.17670.pdf' target='_blank'>https://arxiv.org/pdf/2405.17670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Sikorski, Leendert Schrader, Kaleb Yu, Lucy Billadeau, Jinka Meenakshi, Naveena Mutharasan, Flavio Esposito, Hadi AliAkbarpour, Madi Babaiasl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17670">Deployment of Large Language Models to Control Mobile Robots at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the possibility of intuitive human-robot interaction through the application of Natural Language Processing (NLP) and Large Language Models (LLMs) in mobile robotics. This work aims to explore the feasibility of using these technologies for edge-based deployment, where traditional cloud dependencies are eliminated. The study specifically contrasts the performance of GPT-4-Turbo, which requires cloud connectivity, with an offline-capable, quantized version of LLaMA 2 (LLaMA 2-7B.Q5 K M). These results show that GPT-4-Turbo delivers superior performance in interpreting and executing complex commands accurately, whereas LLaMA 2 exhibits significant limitations in consistency and reliability of command execution. Communication between the control computer and the mobile robot is established via a Raspberry Pi Pico W, which wirelessly receives commands from the computer without internet dependency and transmits them through a wired connection to the robot's Arduino controller. This study highlights the potential and challenges of implementing LLMs and NLP at the edge, providing groundwork for future research into fully autonomous and network-independent robotic systems. For video demonstrations and source code, please refer to: https://tinyurl.com/MobileRobotGPT4LLaMA2024.
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2405.16344.pdf' target='_blank'>https://arxiv.org/pdf/2405.16344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emily Jensen, Sriram Sankaranarayanan, Bradley Hayes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16344">Large Language Models Enable Automated Formative Feedback in Human-Robot Interaction Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We claim that LLMs can be paired with formal analysis methods to provide accessible, relevant feedback for HRI tasks. While logic specifications are useful for defining and assessing a task, these representations are not easily interpreted by non-experts. Luckily, LLMs are adept at generating easy-to-understand text that explains difficult concepts. By integrating task assessment outcomes and other contextual information into an LLM prompt, we can effectively synthesize a useful set of recommendations for the learner to improve their performance.
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2405.15023.pdf' target='_blank'>https://arxiv.org/pdf/2405.15023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leia Stirling, Joseph Montgomery, Mark Draelos, Christoforos Mavrogiannis, Lionel P. Robert, Odest Chadwicke Jenkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15023">ROB 204: Introduction to Human-Robot Systems at the University of Michigan, Ann Arbor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The University of Michigan Robotics program focuses on the study of embodied intelligence that must sense, reason, act, and work with people to improve quality of life and productivity equitably across society. ROB 204, part of the core curriculum towards the undergraduate degree in Robotics, introduces students to topics that enable conceptually designing a robotic system to address users' needs from a sociotechnical context. Students are introduced to human-robot interaction (HRI) concepts and the process for socially-engaged design with a Learn-Reinforce-Integrate approach. In this paper, we discuss the course topics and our teaching methodology, and provide recommendations for delivering this material. Overall, students leave the course with a new understanding and appreciation for how human capabilities can inform requirements for a robotics system, how humans can interact with a robot, and how to assess the usability of robotic systems.
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2405.14154.pdf' target='_blank'>https://arxiv.org/pdf/2405.14154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaotian Liu, Yu Cao, Jeff Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14154">Skip-SCAR: Hardware-Friendly High-Quality Embodied Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In ObjectNav, agents must locate specific objects within unseen environments, requiring effective perception, prediction, localization and planning capabilities. This study finds that state-of-the-art embodied AI agents compete for higher navigation quality, but often compromise the computational efficiency. To address this issue, we introduce "Skip-SCAR," an optimization framework that builds computationally and memory-efficient embodied AI agents to accomplish high-quality visual navigation tasks. Skip-SCAR opportunistically skips the redundant step computations during semantic segmentation and local re-planning without hurting the navigation quality. Skip-SCAR also adopts a novel hybrid sparse and dense network for object prediction, optimizing both the computation and memory footprint. Tested on the HM3D ObjectNav datasets and real-world physical hardware systems, Skip-SCAR not only minimizes hardware resources but also sets new performance benchmarks, demonstrating the benefits of optimizing both navigation quality and computational efficiency for robotics.
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2405.01354.pdf' target='_blank'>https://arxiv.org/pdf/2405.01354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bahar Irfan, Jura Miniota, Sofia Thunberg, Erik Lagerstedt, Sanna KuoppamÃ¤ki, Gabriel Skantze, AndrÃ© Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01354">Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding user enjoyment is crucial in human-robot interaction (HRI), as it can impact interaction quality and influence user acceptance and long-term engagement with robots, particularly in the context of conversations with social robots. However, current assessment methods rely solely on self-reported questionnaires, failing to capture interaction dynamics. This work introduces the Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES), a novel 5-point scale to assess user enjoyment from an external perspective (e.g. by an annotator) for conversations with a robot. The scale was developed through rigorous evaluations and discussions among three annotators with relevant expertise, using open-domain conversations with a companion robot that was powered by a large language model, and was applied to each conversation exchange (i.e. a robot-participant turn pair) alongside overall interaction. It was evaluated on 25 older adults' interactions with the companion robot, corresponding to 174 minutes of data, showing moderate to good alignment between annotators. Although the scale was developed and tested in the context of older adult interactions with a robot, its basis in general and non-task-specific indicators of enjoyment supports its broader applicability. The study further offers insights into understanding the nuances and challenges of assessing user enjoyment in robot interactions, and provides guidelines on applying the scale to other domains and populations. The dataset is available online.
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2404.18687.pdf' target='_blank'>https://arxiv.org/pdf/2404.18687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Wang, Yuqi Kong, Wenzheng Chi, Lining Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18687">Socially Adaptive Path Planning Based on Generative Adversarial Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians. Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments. However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated. In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm. Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios. Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments. Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths. In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path. In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments. Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths.
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2404.10718.pdf' target='_blank'>https://arxiv.org/pdf/2404.10718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10718">GazeHTA: End-to-end Gaze Target Detection with Head-Target Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precisely detecting which object a person is paying attention to is critical for human-robot interaction since it provides important cues for the next action from the human user. We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets.
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2404.00938.pdf' target='_blank'>https://arxiv.org/pdf/2404.00938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghao Shi, Ellen Landrum, Amy O' Connell, Mina Kian, Leticia Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J MatariÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00938">How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of LLMs in SAR technologies, and discuss the potentials and risks of applying LLMs to the following three major technical challenges of SAR: 1) natural language dialog; 2) multimodal understanding; 3) LLMs as robot policies.
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2403.12538.pdf' target='_blank'>https://arxiv.org/pdf/2403.12538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanjiong Ying, Xian Huang, Wei Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12538">Multi-View Active Sensing for Human-Robot Interaction via Hierarchically Connected Tree</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensive perception of human beings is the prerequisite to ensure the safety of human-robot interaction. Currently, prevailing visual sensing approach typically involves a single static camera, resulting in a restricted and occluded field of view. In our work, we develop an active vision system using multiple cameras to dynamically capture multi-source RGB-D data. An integrated human sensing strategy based on a hierarchically connected tree structure is proposed to fuse localized visual information. Constituting the tree model are the nodes representing keypoints and the edges representing keyparts, which are consistently interconnected to preserve the structural constraints during multi-source fusion. Utilizing RGB-D data and HRNet, the 3D positions of keypoints are analytically estimated, and their presence is inferred through a sliding widow of confidence scores. Subsequently, the point clouds of reliable keyparts are extracted by drawing occlusion-resistant masks, enabling fine registration between data clouds and cylindrical model following the hierarchical order. Experimental results demonstrate that our method enhances keypart recognition recall from 69.20% to 90.10%, compared to employing a single static camera. Furthermore, in overcoming challenges related to localized and occluded perception, the robotic arm's obstacle avoidance capabilities are effectively improved.
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2403.10762.pdf' target='_blank'>https://arxiv.org/pdf/2403.10762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seif Ismail, Antonio Arbues, Ryan Cotterell, RenÃ© ZurbrÃ¼gg, Carmen Amo Alonso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10762">NARRATE: Versatile Language Architecture for Optimal Control in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impressive capabilities of Large Language Models (LLMs) have led to various efforts to enable robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by LLMs to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an LLM in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the LLM to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these benchmarks and effectively transfers to the real world on two different embodiments. Videos, Code and Prompts at narrate-mpc.github.io
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2403.10565.pdf' target='_blank'>https://arxiv.org/pdf/2403.10565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10565">PTSD-MDNN : Fusion tardive de rÃ©seaux de neurones profonds multimodaux pour la dÃ©tection du trouble de stress post-traumatique</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2403.01766.pdf' target='_blank'>https://arxiv.org/pdf/2403.01766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangjie Zhong, Leimin Tian, Duy Tho Le, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01766">Improving Visual Perception of a Social Robot for Controlled and In-the-wild Human-robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots often rely on visual perception to understand their users and the environment. Recent advancements in data-driven approaches for computer vision have demonstrated great potentials for applying deep-learning models to enhance a social robot's visual perception. However, the high computational demands of deep-learning methods, as opposed to the more resource-efficient shallow-learning models, bring up important questions regarding their effects on real-world interaction and user experience. It is unclear how will the objective interaction performance and subjective user experience be influenced when a social robot adopts a deep-learning based visual perception model. We employed state-of-the-art human perception and tracking models to improve the visual perception function of the Pepper robot and conducted a controlled lab study and an in-the-wild human-robot interaction study to evaluate this novel perception function for following a specific user with other people present in the scene.
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2402.14299.pdf' target='_blank'>https://arxiv.org/pdf/2402.14299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Xin, Zhongrui You, Zihan Zhang, Taoran Jiang, Tingjia Xu, Haotian Liang, Guojing Ge, Yuchen Ji, Shentong Mo, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14299">We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SpaceAgents-1, a system for learning human and multi-robot collaboration (HMRC) strategies under microgravity conditions. Future space exploration requires humans to work together with robots. However, acquiring proficient robot skills and adept collaboration under microgravity conditions poses significant challenges within ground laboratories. To address this issue, we develop a microgravity simulation environment and present three typical configurations of intra-cabin robots. We propose a hierarchical heterogeneous multi-agent collaboration architecture: guided by foundation models, a Decision-Making Agent serves as a task planner for human-robot collaboration, while individual Skill-Expert Agents manage the embodied control of robots. This mechanism empowers the SpaceAgents-1 system to execute a range of intricate long-horizon HMRC tasks.
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2402.13466.pdf' target='_blank'>https://arxiv.org/pdf/2402.13466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanbit Oh, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13466">Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive imitation learning is an efficient, model-free method through which a robot can learn a task by repetitively iterating an execution of a learning policy and a data collection by querying human demonstrations. However, deploying unmatured policies for clearance-limited tasks, like industrial insertion, poses significant collision risks. For such tasks, a robot should detect the collision risks and request intervention by ceding control to a human when collisions are imminent. The former requires an accurate model of the environment, a need that significantly limits the scope of IIL applications. In contrast, humans implicitly demonstrate environmental precision by adjusting their behavior to avoid collisions when performing tasks. Inspired by human behavior, this paper presents a novel interactive learning method that uses demonstrator-perceived precision as a criterion for human intervention called Demonstrator-perceived Precision-aware Interactive Imitation Learning (DPIIL). DPIIL captures precision by observing the speed-accuracy trade-off exhibited in human demonstrations and cedes control to a human to avoid collisions in states where high precision is estimated. DPIIL improves the safety of interactive policy learning and ensures efficiency without explicitly providing precise information of the environment. We assessed DPIIL's effectiveness through simulations and real-robot experiments that trained a UR5e 6-DOF robotic arm to perform assembly tasks. Our results significantly improved training safety, and our best performance compared favorably with other learning methods.
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2401.11776.pdf' target='_blank'>https://arxiv.org/pdf/2401.11776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyu Yang, Camille Vindolet, Julio Rogelio Guadarrama Olvera, Gordon Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11776">On the impact of robot personalization on human-robot interaction: A review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study reviews the impact of personalization on human-robot interaction. Firstly, the various strategies used to achieve personalization are briefly described. Secondly, the effects of personalization known to date are discussed. They are presented along with the personalized parameters, personalized features, used technology, and use case they relate to. It is observed that various positive effects have been discussed in the literature while possible negative effects seem to require further investigation.
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2401.04320.pdf' target='_blank'>https://arxiv.org/pdf/2401.04320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Demetrious T. Kutzke, Ashwin Wariar, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04320">Autonomous robotic re-alignment for face-to-face underwater human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of autonomous underwater vehicles (AUVs) to accomplish traditionally challenging and dangerous tasks has proliferated thanks to advances in sensing, navigation, manipulation, and on-board computing technologies. Utilizing AUVs in underwater human-robot interaction (UHRI) has witnessed comparatively smaller levels of growth due to limitations in bi-directional communication and significant technical hurdles to bridge the gap between analogies with terrestrial interaction strategies and those that are possible in the underwater domain. A necessary component to support UHRI is establishing a system for safe robotic-diver approach to establish face-to-face communication that considers non-standard human body pose. In this work, we introduce a stereo vision system for enhancing UHRI that utilizes three-dimensional reconstruction from stereo image pairs and machine learning for localizing human joint estimates. We then establish a convention for a coordinate system that encodes the direction the human is facing with respect to the camera coordinate frame. This allows automatic setpoint computation that preserves human body scale and can be used as input to an image-based visual servo control scheme. We show that our setpoint computations tend to agree both quantitatively and qualitatively with experimental setpoint baselines. The methodology introduced shows promise for enhancing UHRI by improving robotic perception of human orientation underwater.
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2401.04108.pdf' target='_blank'>https://arxiv.org/pdf/2401.04108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank FÃ¶rster, Marta Romeo, Patrick Holthaus, Maria Jose Galvez Trigo, Joel E. Fischer, Birthe Nesset, Christian Dondrup, Christine Murad, Cosmin Munteanu, Benjamin R. Cowan, Leigh Clark, Martin Porcheron, Heloisa Candello, Raina Langevin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04108">Working with Trouble and Failures in Conversation between Humans and Robots (WTF 2023) & Is CUI Design Ready Yet?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Workshop proceedings of two co-located workshops "Working with Troubles and Failures in Conversation with Humans and Robots" (WTF 2023) and "Is CUI Design Ready Yet?", both of which were part of the ACM conference on conversational user interfaces 2023.
  WTF 23 aimed at bringing together researchers from human-robot interaction, dialogue systems, human-computer interaction, and conversation analysis. Despite all progress, robotic speech interfaces continue to be brittle in a number of ways and the experience of failure of such interfaces is commonplace amongst roboticists. However, the technical literature is positively skewed toward their good performance. The workshop aims to provide a platform for discussing communicative troubles and failures in human-robot interactions and related failures in non-robotic speech interfaces. Aims include a scrupulous investigation into communicative failures, to begin working on a taxonomy of such failures, and enable a preliminary discussion on possible mitigating strategies. Workshop website: https://sites.google.com/view/wtf2023/overview
  Is CUI Design Ready Yet? As CUIs become more prevalent in both academic research and the commercial market, it becomes more essential to design usable and adoptable CUIs. While research has been growing on the methods for designing CUIs for commercial use, there has been little discussion on the overall community practice of developing design resources to aid in practical CUI design. The aim of this workshop, therefore, is to bring the CUI community together to discuss the current practices for developing tools and resources for practical CUI design, the adoption (or non-adoption) of these tools and resources, and how these resources are utilized in the training and education of new CUI designers entering the field. Workshop website: https://speech-interaction.org/cui2023_design_workshop/index.html
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2401.03398.pdf' target='_blank'>https://arxiv.org/pdf/2401.03398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Li, Kang Li, Dewei Han, Jian Xu, Zhaoyuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03398">Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI and robotics technologies have witnessed remarkable advancements in the past decade, revolutionizing work patterns and opportunities in various domains. The application of these technologies has propelled society towards an era of symbiosis between humans and machines. To facilitate efficient communication between humans and intelligent robots, we propose the "Avatar" system, an immersive low-latency panoramic human-robot interaction platform. We have designed and tested a prototype of a rugged mobile platform integrated with edge computing units, panoramic video capture devices, power batteries, robot arms, and network communication equipment. Under favorable network conditions, we achieved a low-latency high-definition panoramic visual experience with a delay of 357ms. Operators can utilize VR headsets and controllers for real-time immersive control of robots and devices. The system enables remote control over vast physical distances, spanning campuses, provinces, countries, and even continents (New York to Shenzhen). Additionally, the system incorporates visual SLAM technology for map and trajectory recording, providing autonomous navigation capabilities. We believe that this intuitive system platform can enhance efficiency and situational experience in human-robot collaboration, and with further advancements in related technologies, it will become a versatile tool for efficient and symbiotic cooperation between AI and humans.
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2401.03217.pdf' target='_blank'>https://arxiv.org/pdf/2401.03217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Callie Y. Kim, Christine P. Lee, Bilge Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03217">Understanding Large-Language Model (LLM)-powered Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2312.16860.pdf' target='_blank'>https://arxiv.org/pdf/2312.16860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Li, Chicheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16860">Agnostic Interactive Imitation Learning: New Theory and Practical Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study interactive imitation learning, where a learner interactively queries a demonstrating expert for action annotations, aiming to learn a policy that has performance competitive with the expert, using as few annotations as possible. We focus on the general agnostic setting where the expert demonstration policy may not be contained in the policy class used by the learner. We propose a new oracle-efficient algorithm MFTPL-P (abbreviation for Mixed Follow the Perturbed Leader with Poisson perturbations) with provable finite-sample guarantees, under the assumption that the learner is given access to samples from some ``explorative'' distribution over states. Our guarantees hold for any policy class, which is considerably broader than prior state of the art. We further propose Bootstrap-Dagger, a more practical variant that does not require additional sample access. Empirically, MFTPL-P and Bootstrap-Dagger notably surpass online and offline imitation learning baselines in continuous control tasks.
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2312.12262.pdf' target='_blank'>https://arxiv.org/pdf/2312.12262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luke Meyer, Gloria Araiza-Illan, Laura Rachman, Etienne Gaudrain, Deniz Baskent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12262">Evaluating Speech-in-Speech Perception via a Humanoid Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underlying mechanisms of speech perception masked by background speakers, a common daily listening condition, are often investigated using various and lengthy psychophysical tests. The presence of a social agent, such as an interactive humanoid NAO robot, may help maintain engagement and attention. However, such robots potentially have limited sound quality or processing speed. As a first step towards the use of NAO in psychophysical testing of speech-in-speech perception, we compared normal-hearing young adults' performance when using the standard computer interface to that when using a NAO robot to introduce the test and present all corresponding stimuli. Target sentences were presented with colour and number keywords in the presence of competing masker speech at varying target-to-masker ratios. Sentences were produced by the same speaker, but voice differences between the target and masker were introduced using speech synthesis methods. To assess test performance, speech intelligibility and data collection duration were compared between the computer and NAO setups. Human-robot interaction was assessed using the Negative Attitude Towards Robot Scale (NARS) and quantification of behavioural cues (backchannels). Speech intelligibility results showed functional similarity between the computer and NAO setups. Data collection durations were longer when using NAO. NARS results showed participants had a more positive attitude toward robots prior to their interaction with NAO. The presence of more positive backchannels when using NAO suggest higher engagement with the robot in comparison to the computer. Overall, the study presents the potential of the NAO for presentingspeech materials and collecting psychophysical measurements for speech-in-speech perception.
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2312.09056.pdf' target='_blank'>https://arxiv.org/pdf/2312.09056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rudra P. K. Poudel, Harit Pandya, Stephan Liwicki, Roberto Cipolla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09056">ReCoRe: Regularized Contrastive Representation Learning of World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the naÃ¯ve integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overcome this issue, we propose an intervention-invariant regularizer in the form of an auxiliary task such as depth prediction, image denoising, image segmentation, etc., that explicitly enforces invariance to style interventions. Our method outperforms current state-of-the-art model-based and model-free RL methods and significantly improves on out-of-distribution point navigation tasks evaluated on the iGibson benchmark. With only visual observations, we further demonstrate that our approach outperforms recent language-guided foundation models for point navigation, which is essential for deployment on robots with limited computation capabilities. Finally, we demonstrate that our proposed model excels at the sim-to-real transfer of its perception module on the Gibson benchmark.
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2312.06825.pdf' target='_blank'>https://arxiv.org/pdf/2312.06825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahand Shaghaghi, Pourya Aliasghari, Bryan Tripp, Kerstin Dautenhahn, Chrystopher Nehaniv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06825">Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This abstract explores classroom Human-Robot Interaction (HRI) scenarios with an emphasis on the adaptation of human-inspired social gaze models in robot cognitive architecture to facilitate a more seamless social interaction. First, we detail the HRI scenarios explored by us in our studies followed by a description of the social gaze model utilized for our research. We highlight the advantages of utilizing such an attentional model in classroom HRI scenarios. We also detail the intended goals of our upcoming study involving this social gaze model.
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2312.05023.pdf' target='_blank'>https://arxiv.org/pdf/2312.05023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hirakjyoti Basumatary, Daksh Adhar, Atharva Shrawge, Prathamesh Kanbaskar, Shyamanta M. Hazarika
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05023">Reinforcement Learning-Based Bionic Reflex Control for Anthropomorphic Robotic Grasping exploiting Domain Randomization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving human-level dexterity in robotic grasping remains a challenging endeavor. Robotic hands frequently encounter slippage and deformation during object manipulation, issues rarely encountered by humans due to their sensory receptors, experiential learning, and motor memory. The emulation of the human grasping reflex within robotic hands is referred to as the ``bionic reflex". Past endeavors in the realm of bionic reflex control predominantly relied on model-based and supervised learning approaches, necessitating human intervention during thresholding and labeling tasks. In this study, we introduce an innovative bionic reflex control pipeline, leveraging reinforcement learning (RL); thereby eliminating the need for human intervention during control design. Our proposed bionic reflex controller has been designed and tested on an anthropomorphic hand, manipulating deformable objects in the PyBullet physics simulator, incorporating domain randomization (DR) for enhanced Sim2Real transferability. Our findings underscore the promise of RL as a potent tool for advancing bionic reflex control within anthropomorphic robotic hands. We anticipate that this autonomous, RL-based bionic reflex controller will catalyze the development of dependable and highly efficient robotic and prosthetic hands, revolutionizing human-robot interaction and assistive technologies.
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2311.17593.pdf' target='_blank'>https://arxiv.org/pdf/2311.17593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rudra P. K. Poudel, Harit Pandya, Chao Zhang, Roberto Cipolla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17593">LanGWM: Language Grounded World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep reinforcement learning have showcased its potential in tackling complex tasks. However, experiments on visual control tasks have revealed that state-of-the-art reinforcement learning models struggle with out-of-distribution generalization. Conversely, expressing higher-level concepts and global contexts is relatively easy using language.
  Building upon recent success of the large language models, our main objective is to improve the state abstraction technique in reinforcement learning by leveraging language for robust action selection. Specifically, we focus on learning language-grounded visual features to enhance the world model learning, a model-based reinforcement learning technique.
  To enforce our hypothesis explicitly, we mask out the bounding boxes of a few objects in the image observation and provide the text prompt as descriptions for these masked objects. Subsequently, we predict the masked objects along with the surrounding regions as pixel reconstruction, similar to the transformer-based masked autoencoder approach.
  Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art performance in out-of-distribution test at the 100K interaction steps benchmarks of iGibson point navigation tasks. Furthermore, our proposed technique of explicit language-grounded visual representation learning has the potential to improve models for human-robot interaction because our extracted visual features are language grounded.
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2311.16137.pdf' target='_blank'>https://arxiv.org/pdf/2311.16137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Thomas Walker, Stefan Ultes, Pierre Lison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16137">A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge graphs are often used to represent structured information in a flexible and efficient manner, but their use in situated dialogue remains under-explored. This paper presents a novel conversational model for human--robot interaction that rests upon a graph-based representation of the dialogue state. The knowledge graph representing the dialogue state is continuously updated with new observations from the robot sensors, including linguistic, situated and multimodal inputs, and is further enriched by other modules, in particular for spatial understanding. The neural conversational model employed to respond to user utterances relies on a simple but effective graph-to-text mechanism that traverses the dialogue state graph and converts the traversals into a natural language form. This conversion of the state graph into text is performed using a set of parameterized functions, and the values for those parameters are optimized based on a small set of Wizard-of-Oz interactions. After this conversion, the text representation of the dialogue state graph is included as part of the prompt of a large language model used to decode the agent response. The proposed approach is empirically evaluated through a user study with a humanoid robot that acts as conversation partner to evaluate the impact of the graph-to-text mechanism on the response generation. After moving a robot along a tour of an indoor environment, participants interacted with the robot using spoken dialogue and evaluated how well the robot was able to answer questions about what the robot observed during the tour. User scores show a statistically significant improvement in the perceived factuality of the robot responses when the graph-to-text approach is employed, compared to a baseline using inputs structured as semantic triples.
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2311.14848.pdf' target='_blank'>https://arxiv.org/pdf/2311.14848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Demetrious T. Kutzke, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14848">Robotic Detection and Estimation of Single Scuba Diver Respiration Rate from Underwater Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human respiration rate (HRR) is an important physiological metric for diagnosing a variety of health conditions from stress levels to heart conditions. Estimation of HRR is well-studied in controlled terrestrial environments, yet robotic estimation of HRR as an indicator of diver stress in underwater for underwater human robot interaction (UHRI) scenarios is to our knowledge unexplored. We introduce a novel system for robotic estimation of HRR from underwater visual data by utilizing bubbles from exhalation cycles in scuba diving to time respiration rate. We introduce a fuzzy labeling system that utilizes audio information to label a diverse dataset of diver breathing data on which we compare four different methods for characterizing the presence of bubbles in images. Ultimately we show that our method is effective at estimating HRR by comparing the respiration rate output with human analysts.
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2311.10653.pdf' target='_blank'>https://arxiv.org/pdf/2311.10653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shafagh Keyvanian, Michelle J. Johnson, Nadia Figueroa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10653">Learning Realistic Joint Space Boundaries for Range of Motion Analysis of Healthy and Impaired Human Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A realistic human kinematic model that satisfies anatomical constraints is essential for human-robot interaction, biomechanics and robot-assisted rehabilitation. Modeling realistic joint constraints, however, is challenging as human arm motion is constrained by joint limits, inter- and intra-joint dependencies, self-collisions, individual capabilities and muscular or neurological constraints which are difficult to represent. Hence, physicians and researchers have relied on simple box-constraints, ignoring important anatomical factors. In this paper, we propose a data-driven method to learn realistic anatomically constrained upper-limb range of motion (RoM) boundaries from motion capture data. This is achieved by fitting a one-class support vector machine to a dataset of upper-limb joint space exploration motions with an efficient hyper-parameter tuning scheme. Our approach outperforms similar works focused on valid RoM learning. Further, we propose an impairment index (II) metric that offers a quantitative assessment of capability/impairment when comparing healthy and impaired arms. We validate the metric on healthy subjects physically constrained to emulate hemiplegia and different disability levels as stroke patients. [https://sites.google.com/seas.upenn.edu/learning-rom]
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2311.06391.pdf' target='_blank'>https://arxiv.org/pdf/2311.06391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Schiffer, Astrid Rosenthal-von der PÃ¼tten, Bastian Leibe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06391">BUSSARD -- Better Understanding Social Situations for Autonomous Robot Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We report on our effort to create a corpus dataset of different social context situations in an office setting for further disciplinary and interdisciplinary research in computer vision, psychology, and human-robot-interaction. For social robots to be able to behave appropriately, they need to be aware of the social context they act in. Consider, for example, a robot with the task to deliver a personal message to a person. If the person is arguing with an office mate at the time of message delivery, it might be more appropriate to delay playing the message as to respect the recipient's privacy and not to interfere with the current situation. This can only be done if the situation is classified correctly and in a second step if an appropriate behavior is chosen that fits the social situation. Our work aims to enable robots accomplishing the task of classifying social situations by creating a dataset composed of semantically annotated video scenes of office situations from television soap operas. The dataset can then serve as a basis for conducting research in both computer vision and human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2311.06250.pdf' target='_blank'>https://arxiv.org/pdf/2311.06250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieting Luo, Mehdi Dastani, Thomas Studer, Beishui Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06250">What Do You Care About: Inferring Values from Emotions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Observers can glean information from others' emotional expressions through the act of drawing inferences from another individual's emotional expressions. It is important for socially aware artificial systems to be capable of doing that as it can facilitate social interaction among agents, and is particularly important in human-robot interaction for supporting a more personalized treatment of users. In this short paper, we propose a methodology for developing a formal model that allows agents to infer another agent's values from her emotion expressions.
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2311.03783.pdf' target='_blank'>https://arxiv.org/pdf/2311.03783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Yaoxian, Sun Penglei, Liu Haoyu, Li Zhixu, Song Wei, Xiao Yanghua, Zhou Xiaofang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03783">Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities (Manipulation and Mobility), named ManipMob-MMKG. Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly. Our project can be found at https://sites.google.com/view/manipmob-mmkg
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2311.03357.pdf' target='_blank'>https://arxiv.org/pdf/2311.03357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03357">Exploitation-Guided Exploration for Semantic Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XGX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XGX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XGX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XGX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2310.13377.pdf' target='_blank'>https://arxiv.org/pdf/2310.13377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alva Markelius, Sofia SjÃ¶berg, Zakaria Lemhauori, Laura Cohen, Martin BergstrÃ¶m, Robert Lowe, Lola CaÃ±amero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13377">A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel human-robot interaction setup for robot and human learning of symbolic language for identifying robot homeostatic needs. The robot and human learn to use and respond to the same language symbols that convey homeostatic needs and the stimuli that satisfy the homeostatic needs, respectively. We adopted a differential outcomes training (DOT) protocol whereby the robot provides feedback specific (differential) to its internal needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We found evidence that DOT can enhance the human's learning efficiency, which in turn enables more efficient robot language acquisition. The robot used in the study has a vocabulary similar to that of a human infant in the linguistic ``babbling'' phase. The robot software architecture is built upon a model for affect-grounded language acquisition where the robot associates vocabulary with internal needs (hunger, thirst, curiosity) through interactions with the human. The paper presents the results of an initial pilot study conducted with the interactive setup, which reveal that the robot's language acquisition achieves higher convergence rate in the DOT condition compared to the non-DOT control condition. Additionally, participants reported positive affective experiences, feeling of being in control, and an empathetic connection with the robot. This mutual learning (teacher-student learning) approach offers a potential contribution of facilitating cognitive interventions with DOT (e.g. for people with dementia) through increased therapy adherence as a result of engaging humans more in training tasks by taking an active teaching-learning role. The homeostatic motivational grounding of the robot's language acquisition has potential to contribute to more ecologically valid and social (collaborative/nurturing) interactions with robots.
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2310.09757.pdf' target='_blank'>https://arxiv.org/pdf/2310.09757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David C. Jeong, Tianma Shen, Hongji Liu, Raghav Kapoor, Casey Nguyen, Song Liu, Christopher A. Kitts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09757">MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion detection presents challenges to intelligent human-robot interaction (HRI). Foundational deep learning techniques used in emotion detection are limited by information-constrained datasets or models that lack the necessary complexity to learn interactions between input data elements, such as the the variance of human emotions across different contexts. In the current effort, we introduce 1) MoEmo (Motion to Emotion), a cross-attention vision transformer (ViT) for human emotion detection within robotics systems based on 3D human pose estimations across various contexts, and 2) a data set that offers full-body videos of human movement and corresponding emotion labels based on human gestures and environmental contexts. Compared to existing approaches, our method effectively leverages the subtle connections between movement vectors of gestures and environmental contexts through the use of cross-attention on the extracted movement vectors of full-body human gestures/poses and feature maps of environmental contexts. We implement a cross-attention fusion model to combine movement vectors and environment contexts into a joint representation to derive emotion estimation. Leveraging our Naturalistic Motion Database, we train the MoEmo system to jointly analyze motion and context, yielding emotion detection that outperforms the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2310.01421.pdf' target='_blank'>https://arxiv.org/pdf/2310.01421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Erle, Lara Timm, Carolin StraÃmann, Sabrina C. Eimler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01421">Using Focus Group Interviews to Examine Biased Experiences in Human-Robot-Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When deploying interactive agents like (social) robots in public spaces they need to be able to interact with a diverse audience, with members each having individual diversity characteristics and prior experiences with interactive systems. To cater for these various predispositions, it is important to examine what experiences citizens have made with interactive systems and how these experiences might create a bias towards such systems. To analyze these bias-inducing experiences, focus group interviews have been conducted to learn of citizens individual discrimination experiences, their attitudes towards and arguments for and against the deployment of social robots in public spaces. This extended abstract focuses especially on the method and measurement of diversity.
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2310.00146.pdf' target='_blank'>https://arxiv.org/pdf/2310.00146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungseok Hong, Sadman Sakib Enan, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00146">Diver Identification Using Anthropometric Data Ratios for Underwater Multi-Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in efficient design, perception algorithms, and computing hardware have made it possible to create improved human-robot interaction (HRI) capabilities for autonomous underwater vehicles (AUVs). To conduct secure missions as underwater human-robot teams, AUVs require the ability to accurately identify divers. However, this remains an open problem due to divers' challenging visual features, mainly caused by similar-looking scuba gear. In this paper, we present a novel algorithm that can perform diver identification using either pre-trained models or models trained during deployment. We exploit anthropometric data obtained from diver pose estimates to generate robust features that are invariant to changes in distance and photometric conditions. We also propose an embedding network that maximizes inter-class distances in the feature space and minimizes those for the intra-class features, which significantly improves classification performance. Furthermore, we present an end-to-end diver identification framework that operates on an AUV and evaluate the accuracy of the proposed algorithm. Quantitative results in controlled-water experiments show that our algorithm achieves a high level of accuracy in diver identification.
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2310.00010.pdf' target='_blank'>https://arxiv.org/pdf/2310.00010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharjeel Tahir, Syed Afaq Shah, Jumana Abu-Khalaf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00010">Artificial Empathy Classification: A Survey of Deep Learning Techniques, Datasets, and Evaluation Scales</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>From the last decade, researchers in the field of machine learning (ML) and assistive developmental robotics (ADR) have taken an interest in artificial empathy (AE) as a possible future paradigm for human-robot interaction (HRI). Humans learn empathy since birth, therefore, it is challenging to instill this sense in robots and intelligent machines. Nevertheless, by training over a vast amount of data and time, imitating empathy, to a certain extent, can be possible for robots. Training techniques for AE, along with findings from the field of empathetic AI research, are ever-evolving. The standard workflow for artificial empathy consists of three stages: 1) Emotion Recognition (ER) using the retrieved features from video or textual data, 2) analyzing the perceived emotion or degree of empathy to choose the best course of action, and 3) carrying out a response action. Recent studies that show AE being used with virtual agents or robots often include Deep Learning (DL) techniques. For instance, models like VGGFace are used to conduct ER. Semi-supervised models like Autoencoders generate the corresponding emotional states and behavioral responses. However, there has not been any study that presents an independent approach for evaluating AE, or the degree to which a reaction was empathetic. This paper aims to investigate and evaluate existing works for measuring and evaluating empathy, as well as the datasets that have been collected and used so far. Our goal is to highlight and facilitate the use of state-of-the-art methods in the area of AE by comparing their performance. This will aid researchers in the area of AE in selecting their approaches with precision.
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2309.14937.pdf' target='_blank'>https://arxiv.org/pdf/2309.14937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AndrÃ© Helgert, Sabrina C. Eimler, Carolin StraÃmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14937">Virtual Reality as a Tool for Studying Diversity and Inclusion in Human-Robot Interaction: Advantages and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the potential of Virtual Reality (VR) as a research tool for studying diversity and inclusion characteristics in the context of human-robot interactions (HRI). Some exclusive advantages of using VR in HRI are discussed, such as a controllable environment, the possibility to manipulate the variables related to the robot and the human-robot interaction, flexibility in the design of the robot and the environment, and advanced measurement methods related e.g. to eye tracking and physiological data. At the same time, the challenges of researching diversity and inclusion in HRI are described, especially in accessibility, cyber sickness and bias when developing VR-environments. Furthermore, solutions to these challenges are being discussed to fully harness the benefits of VR for the studying of diversity and inclusion.
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2308.14843.pdf' target='_blank'>https://arxiv.org/pdf/2308.14843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farid Shahnavaz, Riley Tavassoli, Reza Akhavian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14843">Robust Activity Recognition for Adaptive Worker-Robot Interaction using Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) using machine learning has shown tremendous promise in detecting construction workers' activities. HAR has many applications in human-robot interaction research to enable robots' understanding of human counterparts' activities. However, many existing HAR approaches lack robustness, generalizability, and adaptability. This paper proposes a transfer learning methodology for activity recognition of construction workers that requires orders of magnitude less data and compute time for comparable or better classification accuracy. The developed algorithm transfers features from a model pre-trained by the original authors and fine-tunes them for the downstream task of activity recognition in construction. The model was pre-trained on Kinetics-400, a large-scale video-based human activity recognition dataset with 400 distinct classes. The model was fine-tuned and tested using videos captured from manual material handling (MMH) activities found on YouTube. Results indicate that the fine-tuned model can recognize distinct MMH tasks in a robust and adaptive manner which is crucial for the widespread deployment of collaborative robots in construction.
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2308.05627.pdf' target='_blank'>https://arxiv.org/pdf/2308.05627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Lubitz, Lisa Gutzeit, Frank Kirchner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05627">CoBaIR: A Python Library for Context-Based Intention Recognition in Human-Robot-Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-Robot Interaction (HRI) becomes more and more important in a world where robots integrate fast in all aspects of our lives but HRI applications depend massively on the utilized robotic system as well as the deployment environment and cultural differences. Because of these variable dependencies it is often not feasible to use a data-driven approach to train a model for human intent recognition. Expert systems have been proven to close this gap very efficiently. Furthermore, it is important to support understandability in HRI systems to establish trust in the system. To address the above-mentioned challenges in HRI we present an adaptable python library in which current state-of-the-art Models for context recognition can be integrated. For Context-Based Intention Recognition a two-layer Bayesian Network (BN) is used. The bayesian approach offers explainability and clarity in the creation of scenarios and is easily extendable with more modalities. Additionally, it can be used as an expert system if no data is available but can as well be fine-tuned when data becomes available.
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2307.13977.pdf' target='_blank'>https://arxiv.org/pdf/2307.13977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chencheng Tang, Matthias Althoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13977">Formal Verification of Robotic Contact Tasks via Reachability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verifying the correct behavior of robots in contact tasks is challenging due to model uncertainties associated with contacts. Standard methods for testing often fall short since all (uncountable many) solutions cannot be obtained. Instead, we propose to formally and efficiently verify robot behaviors in contact tasks using reachability analysis, which enables checking all the reachable states against user-provided specifications. To this end, we extend the state of the art in reachability analysis for hybrid (mixed discrete and continuous) dynamics subject to discrete-time input trajectories. In particular, we present a novel and scalable guard intersection approach to reliably compute the complex behavior caused by contacts. We model robots subject to contacts as hybrid automata in which crucial time delays are included. The usefulness of our approach is demonstrated by verifying safe human-robot interaction in the presence of constrained collisions, which was out of reach for existing methods.
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2307.11954.pdf' target='_blank'>https://arxiv.org/pdf/2307.11954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11954">On-Robot Bayesian Reinforcement Learning for POMDPs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2307.06007.pdf' target='_blank'>https://arxiv.org/pdf/2307.06007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojgan Hashemian, Marta Couto, Samuel Mascarenhas, Ana Paiva, Pedro A. Santos, Rui Prada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06007">Building Persuasive Robots with Social Power Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can social power endow social robots with the capacity to persuade? This paper represents our recent endeavor to design persuasive social robots. We have designed and run three different user studies to investigate the effectiveness of different bases of social power (inspired by French and Raven's theory) on peoples' compliance to the requests of social robots. The results show that robotic persuaders that exert social power (specifically from expert, reward, and coercion bases) demonstrate increased ability to influence humans. The first study provides a positive answer and shows that under the same circumstances, people with different personalities prefer robots using a specific social power base. In addition, social rewards can be useful in persuading individuals. The second study suggests that by employing social power, social robots are capable of persuading people objectively to select a less desirable choice among others. Finally, the third study shows that the effect of power on persuasion does not decay over time and might strengthen under specific circumstances. Moreover, exerting stronger social power does not necessarily lead to higher persuasion. Overall, we argue that the results of these studies are relevant for designing human--robot-interaction scenarios especially the ones aiming at behavioral change.
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2306.05582.pdf' target='_blank'>https://arxiv.org/pdf/2306.05582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denizhan Pak, Donsuk Lee, Samantha M. W. Wood, Justin N. Wood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05582">A newborn embodied Turing test for view-invariant object recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in artificial intelligence has renewed interest in building machines that learn like animals. Almost all of the work comparing learning across biological and artificial systems comes from studies where animals and machines received different training data, obscuring whether differences between animals and machines emerged from differences in learning mechanisms versus training data. We present an experimental approach-a "newborn embodied Turing Test"-that allows newborn animals and machines to be raised in the same environments and tested with the same tasks, permitting direct comparison of their learning abilities. To make this platform, we first collected controlled-rearing data from newborn chicks, then performed "digital twin" experiments in which machines were raised in virtual environments that mimicked the rearing conditions of the chicks. We found that (1) machines (deep reinforcement learning agents with intrinsic motivation) can spontaneously develop visually guided preference behavior, akin to imprinting in newborn chicks, and (2) machines are still far from newborn-level performance on object recognition tasks. Almost all of the chicks developed view-invariant object recognition, whereas the machines tended to develop view-dependent recognition. The learning outcomes were also far more constrained in the chicks versus machines. Ultimately, we anticipate that this approach will help researchers develop embodied AI systems that learn like newborn animals.
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2306.04047.pdf' target='_blank'>https://arxiv.org/pdf/2306.04047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiulong Liu, Sudipta Paul, Moitreya Chatterjee, Anoop Cherian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04047">CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual navigation of an agent towards locating an audio goal is a challenging task especially when the audio is sporadic or the environment is noisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual Embodied Navigation framework in which the agent may interact with a human/oracle for solving the task of navigating to an audio goal. Specifically, CAVEN is modeled as a budget-aware partially observable semi-Markov decision process that implicitly learns the uncertainty in the audio-based navigation policy to decide when and how the agent may interact with the oracle. Our CAVEN agent can engage in fully-bidirectional natural language conversations by producing relevant questions and interpret free-form, potentially noisy responses from the oracle based on the audio-visual context. To enable such a capability, CAVEN is equipped with: (i) a trajectory forecasting network that is grounded in audio-visual cues to produce a potential trajectory to the estimated goal, and (ii) a natural language based question generation and reasoning network to pose an interactive question to the oracle or interpret the oracle's response to produce navigation instructions. To train the interactive modules, we present a large scale dataset: AVN-Instruct, based on the Landmark-RxR dataset. To substantiate the usefulness of conversations, we present experiments on the benchmark audio-goal task using the SoundSpaces simulator under various noisy settings. Our results reveal that our fully-conversational approach leads to nearly an order-of-magnitude improvement in success rate, especially in localizing new sound sources and against methods that only use uni-directional interaction.
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2306.01295.pdf' target='_blank'>https://arxiv.org/pdf/2306.01295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotian Liu, Hector Palacios, Christian Muise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01295">Egocentric Planning for Scalable Embodied Task Achievement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types reveal information about others. It is capable of naturally scaling to solve new tasks beyond ALFRED, as long as they can be solved using the available skills. This work offers a solid baseline for studying end-to-end and hybrid methods that aim to generalize to new tasks, including recent approaches relying on LLMs, but often struggle to scale to long sequences of actions or produce robust plans for novel tasks.
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2305.15657.pdf' target='_blank'>https://arxiv.org/pdf/2305.15657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Q. Tram, Joseph M. Cloud, William J. Beksi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15657">Intuitive Robot Integration via Virtual Reality Workspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly prominent in diverse industrial settings, the desire for an accessible and reliable system has correspondingly increased. Yet, the task of meaningfully assessing the feasibility of introducing a new robotic component, or adding more robots into an existing infrastructure, remains a challenge. This is due to both the logistics of acquiring a robot and the need for expert knowledge in setting it up. In this paper, we address these concerns by developing a purely virtual simulation of a robotic system. Our proposed framework enables natural human-robot interaction through a visually immersive representation of the workspace. The main advantages of our approach are the following: (i) independence from a physical system, (ii) flexibility in defining the workspace and robotic tasks, and (iii) an intuitive interaction between the operator and the simulated environment. Not only does our system provide an enhanced understanding of 3D space to the operator, but it also encourages a hands-on way to perform robot programming. We evaluate the effectiveness of our method in applying novel automation assignments by training a robot in virtual reality and then executing the task on a real robot.
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2304.14502.pdf' target='_blank'>https://arxiv.org/pdf/2304.14502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brenda Elizabeth Olivas-Padilla, Alina Glushkova, Sotiris Manitsaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14502">Deep state-space modeling for explainable representation, analysis, and generation of professional human poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of human movements has been extensively studied due to its wide variety of practical applications, such as human-robot interaction, human learning applications, or clinical diagnosis. Nevertheless, the state-of-the-art still faces scientific challenges when modeling human movements. To begin, new models must account for the stochasticity of human movement and the physical structure of the human body in order to accurately predict the evolution of full-body motion descriptors over time. Second, while utilizing deep learning algorithms, their explainability in terms of body posture predictions needs to be improved as they lack comprehensible representations of human movement. This paper addresses these challenges by introducing three novel methods for creating explainable representations of human movement. In this study, human body movement is formulated as a state-space model adhering to the structure of the Gesture Operational Model (GOM), whose parameters are estimated through the application of deep learning and statistical algorithms. The trained models are used for the full-body dexterity analysis of expert professionals, in which dynamic associations between body joints are identified, and for generating artificially professional movements.
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2304.11817.pdf' target='_blank'>https://arxiv.org/pdf/2304.11817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangge Wang, Yiwei Lyu, John M. Dolan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11817">Active Probing and Influencing Human Behaviors Via Autonomous Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents (robots) face tremendous challenges while interacting with heterogeneous human agents in close proximity. One of these challenges is that the autonomous agent does not have an accurate model tailored to the specific human that the autonomous agent is interacting with, which could sometimes result in inefficient human-robot interaction and suboptimal system dynamics. Developing an online method to enable the autonomous agent to learn information about the human model is therefore an ongoing research goal. Existing approaches position the robot as a passive learner in the environment to observe the physical states and the associated human response. This passive design, however, only allows the robot to obtain information that the human chooses to exhibit, which sometimes doesn't capture the human's full intention. In this work, we present an online optimization-based probing procedure for the autonomous agent to clarify its belief about the human model in an active manner. By optimizing an information radius, the autonomous agent chooses the action that most challenges its current conviction. This procedure allows the autonomous agent to actively probe the human agents to reveal information that's previously unavailable to the autonomous agent. With this gathered information, the autonomous agent can interactively influence the human agent for some designated objectives. Our main contributions include a coherent theoretical framework that unifies the probing and influence procedures and two case studies in autonomous driving that show how active probing can help to create better participant experience during influence, like higher efficiency or less perturbations.
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2304.10990.pdf' target='_blank'>https://arxiv.org/pdf/2304.10990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iris Andrussow, Huanbo Sun, Katherine J. Kuchenbecker, Georg Martius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10990">Minsight: A Fingertip-Sized Vision-Based Tactile Sensor for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent interaction with the physical world requires perceptual abilities beyond vision and hearing; vibrant tactile sensing is essential for autonomous robots to dexterously manipulate unfamiliar objects or safely contact humans. Therefore, robotic manipulators need high-resolution touch sensors that are compact, robust, inexpensive, and efficient. The soft vision-based haptic sensor presented herein is a miniaturized and optimized version of the previously published sensor Insight. Minsight has the size and shape of a human fingertip and uses machine learning methods to output high-resolution maps of 3D contact force vectors at 60 Hz. Experiments confirm its excellent sensing performance, with a mean absolute force error of 0.07 N and contact location error of 0.6 mm across its surface area. Minsight's utility is shown in two robotic tasks on a 3-DoF manipulator. First, closed-loop force control enables the robot to track the movements of a human finger based only on tactile data. Second, the informative value of the sensor output is shown by detecting whether a hard lump is embedded within a soft elastomer with an accuracy of 98%. These findings indicate that Minsight can give robots the detailed fingertip touch sensing needed for dexterous manipulation and physical human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2304.10588.pdf' target='_blank'>https://arxiv.org/pdf/2304.10588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuangzhuang Dai, Jinha Park, Aleksandra Kaszowska, Chen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10588">Detecting Worker Attention Lapses in Human-Robot Interaction: An Eye Tracking and Multimodal Sensing Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of industrial robotics and autonomous systems endow human-robot collaboration in a massive scale. However, current industrial robots are restrained in co-working with human in close proximity due to inability of interpreting human agents' attention. Human attention study is non-trivial since it involves multiple aspects of the mind: perception, memory, problem solving, and consciousness. Human attention lapses are particularly problematic and potentially catastrophic in industrial workplace, from assembling electronics to operating machines. Attention is indeed complex and cannot be easily measured with single-modality sensors. Eye state, head pose, posture, and manifold environment stimulus could all play a part in attention lapses. To this end, we propose a pipeline to annotate multimodal dataset of human attention tracking, including eye tracking, fixation detection, third-person surveillance camera, and sound. We produce a pilot dataset containing two fully annotated phone assembly sequences in a realistic manufacturing environment. We evaluate existing fatigue and drowsiness prediction methods for attention lapse detection. Experimental results show that human attention lapses in production scenarios are more subtle and imperceptible than well-studied fatigue and drowsiness.
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2304.01701.pdf' target='_blank'>https://arxiv.org/pdf/2304.01701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebecka Winqvist, InÃªs Lourenco, Francesco Quinzan, Cristian R. Rojas, Bo Wahlberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01701">Optimal Transport for Correctional Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The contribution of this paper is a generalized formulation of correctional learning using optimal transport, which is about how to optimally transport one mass distribution to another. Correctional learning is a framework developed to enhance the accuracy of parameter estimation processes by means of a teacher-student approach. In this framework, an expert agent, referred to as the teacher, modifies the data used by a learning agent, known as the student, to improve its estimation process. The objective of the teacher is to alter the data such that the student's estimation error is minimized, subject to a fixed intervention budget. Compared to existing formulations of correctional learning, our novel optimal transport approach provides several benefits. It allows for the estimation of more complex characteristics as well as the consideration of multiple intervention policies for the teacher. We evaluate our approach on two theoretical examples, and on a human-robot interaction application in which the teacher's role is to improve the robots performance in an inverse reinforcement learning setting.
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2303.14055.pdf' target='_blank'>https://arxiv.org/pdf/2303.14055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emilyann Nault, Ronnie Smith, Lynne Baillie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14055">Addressing Potential Pitfalls of SAR Assistance on the Aging Population</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of Human Robot Interaction (HRI), socially assistive robots are being investigated to see if they can help combat challenges that can come with aging by providing different forms of support to older adults. As a result, it is imperative that the HRI community are aware of the potential pitfalls that can occur such as over-attachment, over-reliance, and increased isolation. This position paper argues designers should (a) avoid pitfalls that can lead to a negative impact on decline, and (b) leverage SAR decision making to avoid the pitfalls while attaining the benefits of this technology. Finally, we describe the concept for a framework as a starting point for addressing the concerns raised in this paper.
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2303.08470.pdf' target='_blank'>https://arxiv.org/pdf/2303.08470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Lison, Casey Kennington
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08470">Who's in Charge? Roles and Responsibilities of Decision-Making Components in Conversational Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software architectures for conversational robots typically consist of multiple modules, each designed for a particular processing task or functionality. Some of these modules are developed for the purpose of making decisions about the next action that the robot ought to perform in the current context. Those actions may relate to physical movements, such as driving forward or grasping an object, but may also correspond to communicative acts, such as asking a question to the human user. In this position paper, we reflect on the organization of those decision modules in human-robot interaction platforms. We discuss the relative benefits and limitations of modular vs. end-to-end architectures, and argue that, despite the increasing popularity of end-to-end approaches, modular architectures remain preferable when developing conversational robots designed to execute complex tasks in collaboration with human users. We also show that most practical HRI architectures tend to be either robot-centric or dialogue-centric, depending on where developers wish to place the ``command center'' of their system. While those design choices may be justified in some application domains, they also limit the robot's ability to flexibly interleave physical movements and conversational behaviours. We contend that architectures placing ``action managers'' and ``interaction managers'' on an equal footing may provide the best path forward for future human-robot interaction systems.
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2303.03548.pdf' target='_blank'>https://arxiv.org/pdf/2303.03548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03548">Large Language Models as Zero-Shot Human Models for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large-language models (LLMs) -- which have consumed vast amounts of human-generated text data -- to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment (n = 65) where preliminary results show that planning with a LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2303.01995.pdf' target='_blank'>https://arxiv.org/pdf/2303.01995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongrong Liu, John M. Wandeto, Florent Nageotte, Philippe Zanne, Michel de Mathelin, Birgitta Dresp-Langley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01995">Spatiotemporal modeling of grip forces captures proficiency in manual robot control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper builds on our previous work by exploiting Artificial Intelligence to predict individual grip force variability in manual robot control. Grip forces were recorded from various loci in the dominant and non dominant hands of individuals by means of wearable wireless sensor technology. Statistical analyses bring to the fore skill specific temporal variations in thousands of grip forces of a complete novice and a highly proficient expert in manual robot control. A brain inspired neural network model that uses the output metric of a Self Organizing Map with unsupervised winner take all learning was run on the sensor output from both hands of each user. The neural network metric expresses the difference between an input representation and its model representation at any given moment in time t and reliably captures the differences between novice and expert performance in terms of grip force variability.Functionally motivated spatiotemporal analysis of individual average grip forces, computed for time windows of constant size in the output of a restricted amount of task-relevant sensors in the dominant (preferred) hand, reveal finger-specific synergies reflecting robotic task skill. The analyses lead the way towards grip force monitoring in real time to permit tracking task skill evolution in trainees, or identify individual proficiency levels in human robot interaction in environmental contexts of high sensory uncertainty. Parsimonious Artificial Intelligence (AI) assistance will contribute to the outcome of new types of surgery, in particular single-port approaches such as NOTES (Natural Orifice Transluminal Endoscopic Surgery) and SILS (Single Incision Laparoscopic Surgery).
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2302.04329.pdf' target='_blank'>https://arxiv.org/pdf/2302.04329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kassandra Friebe, Kristina Malinovska, Sabina Samporova, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04329">Gaze Cueing and the Role of Presence in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze cueing is a fundamental part of social interactions, and broadly studied using Posner task based gaze cueing paradigms. While studies using human stimuli consistently yield a gaze cueing effect, results from studies using robotic stimuli are inconsistent. Typically, these studies use virtual agents or pictures of robots. As previous research has pointed to the significance of physical presence in human-robot interaction, it is of fundamental importance to understand its yet unexplored role in interactions with gaze cues. This paper investigates whether the physical presence of the iCub humanoid robot affects the strength of the gaze cueing effect in human-robot interaction. We exposed 42 participants to a gaze cueing task. We asked participants to react as quickly and accurately as possible to the appearance of a target stimulus that was either congruently or incongruently cued by the gaze of a copresent iCub robot or a virtual version of the same robot. Analysis of the reaction time measurements showed that participants were consistently affected by their robot interaction partner's gaze, independently on the way the robot was presented. Additional analyses of participants' ratings of the robot's anthropomorphism, animacy and likeability further add to the impression that presence does not play a significant role in simple gaze based interactions. Together our findings open up interesting discussions about the possibility to generalize results from studies using virtual agents to real life interactions with copresent robots.
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2210.01199.pdf' target='_blank'>https://arxiv.org/pdf/2210.01199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kensuke Nakamura, Somil Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.01199">Online Update of Safety Assurances Using Confidence-Based Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots such as autonomous vehicles and assistive manipulators are increasingly operating in dynamic environments and close physical proximity to people. In such scenarios, the robot can leverage a human motion predictor to predict their future states and plan safe and efficient trajectories. However, no model is ever perfect -- when the observed human behavior deviates from the model predictions, the robot might plan unsafe maneuvers. Recent works have explored maintaining a confidence parameter in the human model to overcome this challenge, wherein the predicted human actions are tempered online based on the likelihood of the observed human action under the prediction model. This has opened up a new research challenge, i.e., \textit{how to compute the future human states online as the confidence parameter changes?} In this work, we propose a Hamilton-Jacobi (HJ) reachability-based approach to overcome this challenge. Treating the confidence parameter as a virtual state in the system, we compute a parameter-conditioned forward reachable tube (FRT) that provides the future human states as a function of the confidence parameter. Online, as the confidence parameter changes, we can simply query the corresponding FRT, and use it to update the robot plan. Computing parameter-conditioned FRT corresponds to an (offline) high-dimensional reachability problem, which we solve by leveraging recent advances in data-driven reachability analysis. Overall, our framework enables online maintenance and updates of safety assurances in human-robot interaction scenarios, even when the human prediction model is incorrect. We demonstrate our approach in several safety-critical autonomous driving scenarios, involving a state-of-the-art deep learning-based prediction model.
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2209.14447.pdf' target='_blank'>https://arxiv.org/pdf/2209.14447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadman Sakib Enan, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14447">A Diver Attention Estimation Framework for Effective Underwater Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many underwater tasks, such as cable-and-wreckage inspection and search-and-rescue, can benefit from robust Human-Robot Interaction (HRI) capabilities. With the recent advancements in vision-based underwater HRI methods, Autonomous Underwater Vehicles (AUVs) have the capability to interact with their human partners without requiring assistance from a topside operator. However, in these methods, the AUV assumes that the diver is ready for interaction, while in reality, the diver may be distracted. In this paper, we attempt to address this problem by presenting a diver attention estimation framework for AUVs to autonomously determine the attentiveness of a diver, and developing a robot controller to allow the AUV to navigate and reorient itself with respect to the diver before initiating interaction. The core element of the framework is a deep convolutional neural network called DATT-Net. It is based on a pyramid structure that can exploit the geometric relations among 10 facial keypoints of a diver to estimate their head orientation, which we use as an indicator of attentiveness. Our on-the-bench experimental evaluations and real-world experiments during both closed- and open-water robot trials confirm the efficacy of the proposed framework.
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2209.00414.pdf' target='_blank'>https://arxiv.org/pdf/2209.00414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hagen Lehmann, Adam Rojik, Kassandra Friebe, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00414">Hey, robot! An investigation of getting robot's attention through touch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Touch is a key part of interaction and communication between humans, but has still been little explored in human-robot interaction. In this work, participants were asked to approach and touch a humanoid robot on the hand (Nao - 26 participants; Pepper - 28 participants) to get its attention. We designed reaction behaviors for the robot that consisted in four different combinations of arm movements with the touched hand moving forward or back and the other hand moving forward or staying in place, with simultaneous leaning back, followed by looking at the participant. We studied which reaction of the robot people found the most appropriate and what was the reason for their choice. For both robots, the preferred reaction of the robot hand being touched was moving back. For the other hand, no movement at all was rated most natural for the Pepper, while it was movement forward for the Nao. A correlation between the anxiety subscale of the participants' personality traits and the passive to active/aggressive nature of the robot reactions was found. Most participants noticed the leaning back and rated it positively. Looking at the participant was commented on positively by some participants in unstructured comments. We also analyzed where and how participants spontaneously touched the robot on the hand. In summary, the touch reaction behaviors designed here are good candidates to be deployed more generally in social robots, possibly including incidental touch in crowded environments. The robot size constitutes one important factor shaping how the robot reaction is perceived.
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2207.10457.pdf' target='_blank'>https://arxiv.org/pdf/2207.10457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonidas Droukas, Zoe Doulgeri, Nikolaos L. Tsakiridis, Dimitra Triantafyllou, Ioannis Kleitsiotis, Ioannis Mariolis, Dimitrios Giakoumis, Dimitrios Tzovaras, Dimitrios Kateris, Dionysis Bochtis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.10457">A Survey of Robotic Harvesting Systems and Enabling Technologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive review of ground agricultural robotic systems and applications with special focus on harvesting that span research and commercial products and results, as well as their enabling technologies. The majority of literature concerns the development of crop detection, field navigation via vision and their related challenges. Health monitoring, yield estimation, water status inspection, seed planting and weed removal are frequently encountered tasks. Regarding robotic harvesting, apples, strawberries, tomatoes and sweet peppers are mainly the crops considered in publications, research projects and commercial products. The reported harvesting agricultural robotic solutions, typically consist of a mobile platform, a single robotic arm/manipulator and various navigation/vision systems. This paper reviews reported development of specific functionalities and hardware, typically required by an operating agricultural robot harvester; they include (a) vision systems, (b) motion planning/navigation methodologies (for the robotic platform and/or arm), (c) Human-Robot-Interaction (HRI) strategies with 3D visualization, (d) system operation planning & grasping strategies and (e) robotic end-effector/gripper design. Clearly, automated agriculture and specifically autonomous harvesting via robotic systems is a research area that remains wide open, offering several challenges where new contributions can be made.
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2205.13242.pdf' target='_blank'>https://arxiv.org/pdf/2205.13242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Gallo, Antonio Barrientos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.13242">GNSS-Denied Semi Direct Visual Navigation for Autonomous UAVs Aided by PI-Inspired Inertial Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article proposes a method to diminish the pose (position plus attitude) drift experienced by an SVO (Semi-Direct Visual Odometry) based visual navigation system installed onboard a UAV (Unmanned Air Vehicle) by supplementing its pose estimation non linear optimizations with priors based on the outputs of a GNSS (Global Navigation Satellite System) Denied inertial navigation system. The method is inspired in a PI (Proportional Integral) control system, in which the attitude, altitude, and rate of climb inertial outputs act as targets to ensure that the visual estimations do not deviate far from their inertial counterparts. The resulting IA-VNS (Inertially Assisted Visual Navigation System) achieves major reductions in the horizontal position drift inherent to the GNSS-Denied navigation of autonomous fixed wing low SWaP (Size, Weight, and Power) UAVs. Additionally, the IA-VNS can be considered as a virtual incremental position (ground velocity) sensor capable of providing observations to the inertial filter. Stochastic high fidelity Monte Carlo simulations of two representative scenarios involving the loss of GNSS signals are employed to evaluate the results and to analyze their sensitivity to the terrain type overflown by the aircraft as well as to the quality of the onboard sensors on which the priors are based. The author releases the C ++ implementation of both the navigation algorithms and the high fidelity simulation as open-source software.
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2204.03538.pdf' target='_blank'>https://arxiv.org/pdf/2204.03538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Puljiz, BjÃ¶rn Hein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.03538">Updating Industrial Robots for Emerging Technologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial arms need to evolve beyond their standard shape to embrace new and emerging technologies. In this paper, we shall first perform an analysis of four popular but different modern industrial robot arms. By seeing the common trends we will try to extrapolate and expand these trends for the future. Here, particular focus will be on interaction based on augmented reality (AR) through head-mounted displays (HMD), but also through smartphones. Long-term human-robot interaction and personalization of said interaction will also be considered. The use of AR in human-robot interaction has proven to enhance communication and information exchange. A basic addition to industrial arm design would be the integration of QR markers on the robot, both for accessing information and adding tracking capabilities to more easily display AR overlays. In a recent example of information access, Mercedes Benz added QR markers on their cars to help rescue workers estimate the best places to cut and evacuate people after car crashes. One has also to deal with safety in an environment that will be more and more about collaboration. The QR markers can therefore be combined with RF-based ranging modules, developed in the EU-project SafeLog, that can be used both for safety as well as for tracking of human positions while in close proximity interactions with the industrial arms. The industrial arms of the future should also be intuitive to program and interact with. This would be achieved through AR and head mounted displays as well as the already mentioned RF-based person tracking. Finally, a more personalized interaction between the robots and humans can be achieved through life-long learning AI and disembodied, personalized agents. We propose a design that not only exists in the physical world, but also partly in the digital world of mixed reality.
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2201.05322.pdf' target='_blank'>https://arxiv.org/pdf/2201.05322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Romi Gideoni, Shanee Honig, Tal Oron-Gilad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.05322">Is it personal? The impact of personally relevant robotic failures (PeRFs) on humans' trust, likeability, and willingness to use the robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In three laboratory experiments, we examine the impact of personally relevant failures (PeRFs) on perceptions of a collaborative robot. PeR is determined by how much a specific issue applies to a particular person, i.e., it affects one's own goals and values. We hypothesized that PeRFs would reduce trust in the robot and the robot's Likeability and Willingness to Use (LWtU) more than failures that are not personal to participants. To achieve PeR in human-robot interaction, we utilized three different manipulation mechanisms: A) damage to property, B) financial loss, and C) first-person versus third-person failure scenarios. In total, 132 participants engaged with a robot in person during a collaborative task of laundry sorting. All three experiments took place in the same experimental environment, carefully designed to simulate a realistic laundry sorting scenario. Results indicate that the impact of PeRFs on perceptions of the robot varied across the studies. In experiments A and B, the encounters with PeRFs reduced trust significantly relative to a no failure session. But not entirely for LWtU. In experiment C, the PeR manipulation had no impact. The work highlights challenges and adjustments needed for studying robotic failures in laboratory settings. We show that PeR manipulations affect how users perceive a failing robot. The results bring about new questions regarding failure types and their perceived severity on users' perception of the robot. Putting PeR aside, we observed differences in the way users perceive interaction failures compared (experiment C) to how they perceive technical ones (A and B).
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2109.06275.pdf' target='_blank'>https://arxiv.org/pdf/2109.06275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristian-Paul Bara, Sky CH-Wang, Joyce Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.06275">MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners' beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2106.07857.pdf' target='_blank'>https://arxiv.org/pdf/2106.07857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Li, Hanjun Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.07857">Bilateral Personalized Dialogue Generation with Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating personalized responses is one of the major challenges in natural human-robot interaction. Current researches in this field mainly focus on generating responses consistent with the robot's pre-assigned persona, while ignoring the user's persona. Such responses may be inappropriate or even offensive, which may lead to the bad user experience. Therefore, we propose a Bilateral Personalized Dialogue Generation (BPDG) method for dyadic conversation, which integrates user and robot personas into dialogue generation via designing a dynamic persona-aware fusion method. To bridge the gap between the learning objective function and evaluation metrics, the Conditional Mutual Information Maximum (CMIM) criterion is adopted with contrastive learning to select the proper response from the generated candidates. Moreover, a bilateral persona accuracy metric is designed to measure the degree of bilateral personalization. Experimental results demonstrate that, compared with several state-of-the-art methods, the final results of the proposed method are more personalized and consistent with bilateral personas in terms of both automatic and manual evaluations.
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2105.13409.pdf' target='_blank'>https://arxiv.org/pdf/2105.13409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanying Zhou, Shijie Li, Jochen Garcke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.13409">Foresight Social-aware Reinforcement Learning for Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When robots handle navigation tasks while avoiding collisions, they perform in crowded and complex environments not as good as in stable and homogeneous environments. This often results in a low success rate and poor efficiency. Therefore, we propose a novel Foresight Social-aware Reinforcement Learning (FSRL) framework for mobile robots to achieve collision-free navigation. Compared to previous learning-based methods, our approach is foresighted. It not only considers the current human-robot interaction to avoid an immediate collision, but also estimates upcoming social interactions to still keep distance in the future. Furthermore, an efficiency constraint is introduced in our approach that significantly reduces navigation time. Comparative experiments are performed to verify the effectiveness and efficiency of our proposed method under more realistic and challenging simulated environments.
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2104.11340.pdf' target='_blank'>https://arxiv.org/pdf/2104.11340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katherine Isbister, Peter Cottrell, Alessia Cecchet, Ella Dagan, Nikki Theofanopoulou, Ferran Altarriba Bertran, Aaron J. Horowitz, Nick Mead, Joel B. Schwartz, Petr Slovak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.11340">Design not Lost in Translation: A Case Study of an Intimate-Space Socially Assistive Robot for Emotion Regulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a Research-through-Design case study of the design and development of an intimate-space tangible device perhaps best understood as a socially assistive robot, aimed at scaffolding children's efforts at emotional regulation. This case study covers the initial research device development, as well as knowledge transfer to a product development company towards translating the research into a workable commercial product that could also serve as a robust research product for field trials. Key contributions to the literature include: 1. sharing of lessons learned from the knowledge transfer process that can be useful to others interested in developing robust products, whether commercial or research, that preserve design values, while allowing for large scale deployment and research; 2. articulation of a design space in HCI/HRI--Human Robot Interaction--of intimate space socially assistive robots, with the current artifact as a central exemplar, contextualized alongside other related HRI artifacts.
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2012.04511.pdf' target='_blank'>https://arxiv.org/pdf/2012.04511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maitreyee Wairagkar, Maria R Lima, Daniel Bazo, Richard Craig, Hugo Weissbart, Appolinaire C Etoundi, Tobias Reichenbach, Prashant Iyenger, Sneh Vaswani, Christopher James, Payam Barnaghi, Chris Melhuish, Ravi Vaidyanathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.04511">Emotive Response to a Hybrid-Face Robot and Translation to Consumer Social Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the conceptual formulation, design, fabrication, control and commercial translation with IoT connection of a hybrid-face social robot and validation of human emotional response to its affective interactions. The hybrid-face robot integrates a 3D printed faceplate and a digital display to simplify conveyance of complex facial movements while providing the impression of three-dimensional depth for natural interaction. We map the space of potential emotions of the robot to specific facial feature parameters and characterise the recognisability of the humanoid hybrid-face robot's archetypal facial expressions. We introduce pupil dilation as an additional degree of freedom for conveyance of emotive states. Human interaction experiments demonstrate the ability to effectively convey emotion from the hybrid-robot face to human observers by mapping their neurophysiological electroencephalography (EEG) response to perceived emotional information and through interviews. Results show main hybrid-face robotic expressions can be discriminated with recognition rates above 80% and invoke human emotive response similar to that of actual human faces as measured by the face-specific N170 event-related potentials in EEG. The hybrid-face robot concept has been modified, implemented, and released in the commercial IoT robotic platform Miko (My Companion), an affective robot with facial and conversational features currently in use for human-robot interaction in children by Emotix Inc. We demonstrate that human EEG responses to Miko emotions are comparative to neurophysiological responses for actual human facial recognition. Finally, interviews show above 90% expression recognition rates in our commercial robot. We conclude that simplified hybrid-face abstraction conveys emotions effectively and enhances human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/1905.01065.pdf' target='_blank'>https://arxiv.org/pdf/1905.01065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florenz Graf, ÃaÄatay OdabaÅÄ±, Theo Jacobs, Birgit Graf, Thomas FÃ¶disch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1905.01065">MobiKa - Low-Cost Mobile Robot for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One way to allow elderly people to stay longer in their homes is to use of service robots to support them with everyday tasks. With this goal, we design, develop and evaluate a low-cost mobile robot to communicate with elderly people. The main idea is to create an affordable communication assistant robot which is optimized for multimodal Human-Robot Interaction (HRI). Our robot can navigate autonomously through dynamic environments using a new algorithm to calculate poses for approaching persons. The robot was tested in a real life scenario in an elderly care home.
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2509.21243.pdf' target='_blank'>https://arxiv.org/pdf/2509.21243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyeon Koo, Taewan Cho, Hyunjoon Kang, Eunseom Pyo, Tae Gyun Oh, Taeryang Kim, Andrew Jaeyong Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21243">RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert. RetoVLA maintains a lightweight structure while leveraging this repurposed spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness through a series of comprehensive experiments. On our custom-built 7-DOF robot arm, the model achieves a 17.1%p absolute improvement in success rates for complex manipulation tasks. Our results confirm that reusing Register Tokens directly enhances spatial reasoning, demonstrating that what was previously discarded as an artifact is in fact a valuable, unexplored resource for robotic intelligence. A video demonstration is available at: https://youtu.be/2CseBR-snZg
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2509.20656.pdf' target='_blank'>https://arxiv.org/pdf/2509.20656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzhe Wang, Jiarui Xie, Pengfei Hao, Zheng Li, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20656">EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable brain-computer interface (BCI) control of robots provides an intuitive and accessible means of human-robot interaction, particularly valuable for individuals with motor impairments. However, existing BCI-Robot systems face major limitations: electroencephalography (EEG) signals are noisy and unstable, target selection is often predefined and inflexible, and most studies remain restricted to simulation without closed-loop validation. These issues hinder real-world deployment in assistive scenarios. To address them, we propose a closed-loop BCI-AR-Robot system that integrates motor imagery (MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic grasping for zero-touch operation. A 14-channel EEG headset enabled individualized MI calibration, a smartphone-based AR interface supported multi-target navigation with direction-congruent feedback to enhance stability, and the robotic arm combined decision outputs with vision-based pose estimation for autonomous grasping. Experiments are conducted to validate the framework: MI training achieved 93.1 percent accuracy with an average information transfer rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2 percent success rate with good efficiency and strong user-reported control. These results show that AR feedback substantially stabilizes EEG-based control and that the proposed framework enables robust zero-touch grasping, advancing assistive robotic applications and future modes of human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2509.19521.pdf' target='_blank'>https://arxiv.org/pdf/2509.19521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Najeeb Ahmed Bhuiyan, M. Nasimul Huq, Sakib H. Chowdhury, Rahul Mangharam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19521">A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gesture-based control for mobile manipulators faces persistent challenges in reliability, efficiency, and intuitiveness. This paper presents a dual-hand gesture interface that integrates TinyML, spectral analysis, and sensor fusion within a ROS framework to address these limitations. The system uses left-hand tilt and finger flexion, captured using accelerometer and flex sensors, for mobile base navigation, while right-hand IMU signals are processed through spectral analysis and classified by a lightweight neural network. This pipeline enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3 manipulator. By supporting simultaneous navigation and manipulation, the framework improves efficiency and coordination compared to sequential methods. Key contributions include a bimanual control architecture, real-time low-power gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based implementation. The proposed approach advances Human-Robot Interaction (HRI) for industrial automation, assistive robotics, and hazardous environments, offering a cost-effective, open-source solution with strong potential for real-world deployment and further optimization.
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2509.17760.pdf' target='_blank'>https://arxiv.org/pdf/2509.17760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Austin Wilson, Sahar Kapasi, Zane Greene, Alexis E. Block
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17760">Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many research groups face challenges when legacy (unsupported) robotic platforms lose manufacturer support and cannot accommodate modern sensing, speech, and interaction capabilities. We present the Enhanced NAO, a revitalized version of Aldebaran's NAO robot that uses upgraded microphones, RGB-D and thermal cameras, and additional compute resources in a fully self-contained package. This system combines cloud and local models for perception and dialogue, while preserving the NAO's expressive body and behaviors. In a pilot validation study, the Enhanced NAO delivered significantly higher conversational quality and stronger user preference compared to the NAO AI Edition, without increasing response latency. Key upgrades, such as beamforming microphones and low-latency audio processing, reduced artifacts like self-hearing and improved multi-party separation. Expanded visual and thermal sensing established a foundation for future interaction capabilities. Beyond the NAO, our framework provides a platform-agnostic strategy for extending the lifespan and research utility of legacy robots, ensuring they remain valuable tools for human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2509.16032.pdf' target='_blank'>https://arxiv.org/pdf/2509.16032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Faber, Andrey Grishko, Julian Waksberg, David Pardo, Tomer Leivy, Yuval Hazan, Emanuel Talmansky, Benny Megidish, Hadas Erel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16032">A Matter of Height: The Impact of a Robotic Object on Human Compliance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots come in various forms and have different characteristics that may shape the interaction with them. In human-human interactions, height is a characteristic that shapes human dynamics, with taller people typically perceived as more persuasive. In this work, we aspired to evaluate if the same impact replicates in a human-robot interaction and specifically with a highly non-humanoid robotic object. The robot was designed with modules that could be easily added or removed, allowing us to change its height without altering other design features. To test the impact of the robot's height, we evaluated participants' compliance with its request to volunteer to perform a tedious task. In the experiment, participants performed a cognitive task on a computer, which was framed as the main experiment. When done, they were informed that the experiment was completed. While waiting to receive their credits, the robotic object, designed as a mobile robotic service table, entered the room, carrying a tablet that invited participants to complete a 300-question questionnaire voluntarily. We compared participants' compliance in two conditions: A Short robot composed of two modules and 95cm in height and a Tall robot consisting of three modules and 132cm in height. Our findings revealed higher compliance with the Short robot's request, demonstrating an opposite pattern to human dynamics. We conclude that while height has a substantial social impact on human-robot interactions, it follows a unique pattern of influence. Our findings suggest that designers cannot simply adopt and implement elements from human social dynamics to robots without testing them first.
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2509.14687.pdf' target='_blank'>https://arxiv.org/pdf/2509.14687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Tai, Zhaoyu Zheng, Haixu Long, Hansheng Wu, Haodong Xiang, Zhengbin Long, Jun Xiong, Rong Shi, Shizhuang Zhang, Gang Qiu, He Wang, Ruifeng Li, Jun Huang, Bin Chang, Shuai Feng, Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14687">RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2509.13948.pdf' target='_blank'>https://arxiv.org/pdf/2509.13948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedict Barrow, Roger K. Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13948">The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trust and the perception of trustworthiness play an important role in decision-making and our behaviour towards others, and this is true not only of human-human interactions but also of human-robot interactions. While significant advances have been made in recent years in the field of social robotics, there is still some way to go before we fully understand the factors that influence human trust in robots. This paper presents the results of a study into the first impressions created by a social robot's facial features, based on the hypothesis that a `babyface' engenders trust. By manipulating the back-projected face of a Furhat robot, the study confirms that eye shape and size have a significant impact on the perception of trustworthiness. The work thus contributes to an understanding of the design choices that need to be made when developing social robots so as to optimise the effectiveness of human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2509.13861.pdf' target='_blank'>https://arxiv.org/pdf/2509.13861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Görkem Kılınç Soylu, Neziha Akalin, Maria Riveiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13861">Using Petri Nets for Context-Adaptive Robot Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction, robots must communicate in a natural and transparent manner to foster trust, which requires adapting their communication to the context. In this paper, we propose using Petri nets (PNs) to model contextual information for adaptive robot explanations. PNs provide a formal, graphical method for representing concurrent actions, causal dependencies, and system states, making them suitable for analyzing dynamic interactions between humans and robots. We demonstrate this approach through a scenario involving a robot that provides explanations based on contextual cues such as user attention and presence. Model analysis confirms key properties, including deadlock-freeness, context-sensitive reachability, boundedness, and liveness, showing the robustness and flexibility of PNs for designing and verifying context-adaptive explanations in human-robot interactions.
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2509.13378.pdf' target='_blank'>https://arxiv.org/pdf/2509.13378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattias Wingren, Sören Andersson, Sara Rosenberg, Malin Andtfolk, Susanne Hägglund, Prashani Jayasingha Arachchige, Linda Nyholm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13378">Using role-play and Hierarchical Task Analysis for designing human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the use of two methods we believe warrant more use than they currently have in the field of human-robot interaction: role-play and Hierarchical Task Analysis. Some of its potential is showcased through our use of them in an ongoing research project which entails developing a robot application meant to assist at a community pharmacy. The two methods have provided us with several advantages. The role-playing provided a controlled and adjustable environment for understanding the customers' needs where pharmacists could act as models for the robot's behavior; and the Hierarchical Task Analysis ensured the behavior displayed was modelled correctly and aided development through facilitating co-design. Future research could focus on developing task analysis methods especially suited for social robot interaction.
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2509.11766.pdf' target='_blank'>https://arxiv.org/pdf/2509.11766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, Zach Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11766">Igniting VLMs toward the Embodied Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While foundation models show remarkable progress in language and vision, existing vision-language models (VLMs) still have limited spatial and embodiment understanding. Transferring VLMs to embodied domains reveals fundamental mismatches between modalities, pretraining distributions, and training objectives, leaving action comprehension and generation as a central bottleneck on the path to AGI. We introduce WALL-OSS, an end-to-end embodied foundation model that leverages large-scale multimodal pretraining to achieve (1) embodiment-aware vision-language understanding, (2) strong language-action association, and (3) robust manipulation capability. Our approach employs a tightly coupled architecture and multi-strategies training curriculum that enables Unified Cross-Level CoT-seamlessly unifying instruction reasoning, subgoal decomposition, and fine-grained action synthesis within a single differentiable framework. Our results show that WALL-OSS attains high success on complex long-horizon manipulations, demonstrates strong instruction-following capabilities, complex understanding and reasoning, and outperforms strong baselines, thereby providing a reliable and scalable path from VLMs to embodied foundation models.
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2509.11663.pdf' target='_blank'>https://arxiv.org/pdf/2509.11663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haisheng Wang, Weiming Zhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11663">ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper formulates the Embodied Questions Answering (EQsA) problem, introduces a corresponding benchmark, and proposes a system to tackle the problem. Classical Embodied Question Answering (EQA) is typically formulated as answering one single question by actively exploring a 3D environment. Real deployments, however, often demand handling multiple questions that may arrive asynchronously and carry different urgencies. We formalize this setting as Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group memory module shared among questions to reduce redundant exploration, and a priority-planning module to dynamically schedule questions. To evaluate this setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs) benchmark containing 40 indoor scenes and five questions per scene (200 in total), featuring asynchronous follow-up questions and urgency labels. We further propose metrics for EQsA performance: Direct Answer Rate (DAR), and Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency and responsiveness of this system. ParaEQsA consistently outperforms strong sequential baselines adapted from recent EQA systems, while reducing exploration and delay. Empirical evaluations investigate the relative contributions of priority, urgency modeling, spatial scope, reward estimation, and dependency reasoning within our framework. Together, these results demonstrate that urgency-aware, parallel scheduling is key to making embodied agents responsive and efficient under realistic, multi-question workloads.
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2509.11622.pdf' target='_blank'>https://arxiv.org/pdf/2509.11622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyun Chen, Qing Xiao, Zitao Zhang, Eli Blevis, Selma Šabanović
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11622">Robots that Evolve with Us: Modular Co-Design for Personalization, Adaptability, and Sustainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many current robot designs prioritize efficiency and one-size-fits-all solutions, oftentimes overlooking personalization, adaptability, and sustainability. To explore alternatives, we conducted two co-design workshops with 23 participants, who engaged with a modular robot co-design framework. Using components we provided as building blocks, participants combined, removed, and invented modules to envision how modular robots could accompany them from childhood through adulthood and into older adulthood. The participants' designs illustrate how modularity (a) enables personalization through open-ended configuration, (b) adaptability across shifting life-stage needs, and (c) sustainability through repair, reuse, and continuity. We therefore derive design principles that establish modularity as a foundation for lifespan-oriented human-robot interaction. This work reframes modular robotics as a flexible and expressive co-design approach, supporting robots that evolve with people, rather than static products optimized for single moments or contexts of use.
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2509.10444.pdf' target='_blank'>https://arxiv.org/pdf/2509.10444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaerim Moon, Joohyung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10444">Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supernumerary Robotic Limbs (SRLs) can enhance human capability within close proximity. However, as a wearable device, the generated moment from its operation acts on the human body as an external torque. When the moments increase, more muscle units are activated for balancing, and it can result in reduced muscular null space. Therefore, this paper suggests a concept of a motion planning layer that reduces the generated moment for enhanced Human-Robot Interaction. It modifies given trajectories with desirable angular acceleration and position deviation limits. Its performance to reduce the moment is demonstrated through the simulation, which uses simplified human and robotic system models.
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2509.07942.pdf' target='_blank'>https://arxiv.org/pdf/2509.07942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James M. Berzuk, Lauren Corcoran, Brannen McKenzie-Lefurgey, Katie Szilagyi, James E. Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07942">Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contemporary robots are increasingly mimicking human social behaviours to facilitate interaction, such as smiling to signal approachability, or hesitating before taking an action to allow people time to react. Such techniques can activate a person's entrenched social instincts, triggering emotional responses as though they are interacting with a fellow human, and can prompt them to treat a robot as if it truly possesses the underlying life-like processes it outwardly presents, raising significant ethical questions. We engage these issues through the lens of informed consent: drawing upon prevailing legal principles and ethics, we examine how social robots can influence user behaviour in novel ways, and whether under those circumstances users can be appropriately informed to consent to these heightened interactions. We explore the complex circumstances of human-robot interaction and highlight how it differs from more familiar interaction contexts, and we apply legal principles relating to informed consent to social robots in order to reconceptualize the current ethical debates surrounding the field. From this investigation, we synthesize design goals for robot developers to achieve more ethical and informed human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2509.06917.pdf' target='_blank'>https://arxiv.org/pdf/2509.06917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Miao, Joe R. Davis, Jonathan K. Pritchard, James Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06917">Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2509.06819.pdf' target='_blank'>https://arxiv.org/pdf/2509.06819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel San José Pro, Oliver Hausdörfer, Ralf Römer, Maximilian Dösch, Martin Schuck, Angela P. Schöllig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06819">CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2509.05263.pdf' target='_blank'>https://arxiv.org/pdf/2509.05263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05263">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2509.01547.pdf' target='_blank'>https://arxiv.org/pdf/2509.01547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01547">FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual SLAM has regained attention due to its ability to provide perceptual capabilities and simulation test data for Embodied AI. However, traditional SLAM methods struggle to meet the demands of high-quality scene reconstruction, and Gaussian SLAM systems, despite their rapid rendering and high-quality mapping capabilities, lack effective pose optimization methods and face challenges in geometric reconstruction. To address these issues, we introduce FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the scene representation to enhance geometric mapping performance. After initial pose estimation, we apply global adjustment to optimize camera poses and sparse point cloud, ensuring robust tracking of our approach. Additionally, we maintain a globally consistent opacity radiance field based on 3D Gaussians and introduce depth distortion and normal consistency terms to refine the scene representation. Furthermore, after constructing tetrahedral grids, we identify level sets to directly extract surfaces from 3D Gaussians. Results across various real-world and large-scale synthetic datasets demonstrate that our method achieves state-of-the-art tracking accuracy and mapping performance.
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2509.00328.pdf' target='_blank'>https://arxiv.org/pdf/2509.00328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bear Häon, Kaylene Stocking, Ian Chuang, Claire Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00328">Mechanistic interpretability for steering vision-language-action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2509.00117.pdf' target='_blank'>https://arxiv.org/pdf/2509.00117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jared Perlo, Alexander Robey, Fazl Barez, Luciano Floridi, Jakob Mökander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00117">Embodied AI: Emerging Risks and Opportunities for Policy Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI systems can exist in, learn from, reason about, and act in the physical world. With recent advances in AI models and hardware, EAI systems are becoming increasingly capable across wider operational domains. While EAI systems can offer many benefits, they also pose significant risks, including physical harm from malicious use, mass surveillance, as well as economic and societal disruption. These risks require urgent attention from policymakers, as existing policies governing industrial robots and autonomous vehicles are insufficient to address the full range of concerns EAI systems present. To help address this issue, this paper makes three contributions. First, we provide a taxonomy of the physical, informational, economic, and social risks EAI systems pose. Second, we analyze policies in the US, EU, and UK to assess how existing frameworks address these risks and to identify critical gaps. We conclude by offering policy recommendations for the safe and beneficial deployment of EAI systems, such as mandatory testing and certification schemes, clarified liability frameworks, and strategies to manage EAI's potentially transformative economic and societal impacts.
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2508.13901.pdf' target='_blank'>https://arxiv.org/pdf/2508.13901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Lu, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13901">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional management systems are ill-equipped to handle. In this survey, we first systematically evaluate five storage architectures (Graph Databases, Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series Databases), focusing on their suitability for addressing EAI's core requirements, including physical grounding, low-latency access, and dynamic scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based Optimization), revealing a fundamental tension between achieving long-term semantic coherence and maintaining real-time responsiveness. Based on this comprehensive analysis, we identify key bottlenecks, spanning from the foundational Physical Grounding Gap to systemic challenges in cross-modal integration, dynamic adaptation, and open-world generalization. Finally, we outline a forward-looking research agenda encompassing physics-aware data models, adaptive storage-retrieval co-optimization, and standardized benchmarking, to guide future research toward principled data management solutions for EAI. Our survey is based on a comprehensive review of more than 180 related studies, providing a rigorous roadmap for designing the robust, high-performance data management frameworks essential for the next generation of autonomous embodied systems.
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2508.13421.pdf' target='_blank'>https://arxiv.org/pdf/2508.13421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabrielle Wehr, Reuben Rideaux, Amaya J. Fox, David R. Lightfoot, Jason Tangen, Jason B. Mattingley, Shane E. Ehrhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13421">Virtuous Machines: Towards Artificial General Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2508.06767.pdf' target='_blank'>https://arxiv.org/pdf/2508.06767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arman Dogru, R. Irem Bor-Yaliniz, Nimal Gamini Senarath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06767">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2508.06547.pdf' target='_blank'>https://arxiv.org/pdf/2508.06547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heran Wu, Zirun Zhou, Jingfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06547">A tutorial note on collecting simulated data for vision-language-action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2508.05855.pdf' target='_blank'>https://arxiv.org/pdf/2508.05855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixia Wang, Jia Hu, Ronghui Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05855">Safety of Embodied Navigation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2507.17727.pdf' target='_blank'>https://arxiv.org/pdf/2507.17727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robel Mamo, Taeyeong Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17727">CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art visual under-canopy navigation methods are designed with deep learning-based perception models to distinguish traversable space from crop rows. While these models have demonstrated successful performance, they require large amounts of training data to ensure reliability in real-world field deployment. However, data collection is costly, demanding significant human resources for in-field sampling and annotation. To address this challenge, various data augmentation techniques are commonly employed during model training, such as color jittering, Gaussian blur, and horizontal flip, to diversify training data and enhance model robustness. In this paper, we hypothesize that utilizing only these augmentation techniques may lead to suboptimal performance, particularly in complex under-canopy environments with frequent occlusions, debris, and non-uniform spacing of crops. Instead, we propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut) which masks random regions out in input images that are spatially distributed around crop rows on the sides to encourage trained models to capture high-level contextual features even when fine-grained information is obstructed. Our extensive experiments with a public cornfield dataset demonstrate that masking-based augmentations are effective for simulating occlusions and significantly improving robustness in semantic keypoint predictions for visual navigation. In particular, we show that biasing the mask distribution toward crop rows in CA-Cut is critical for enhancing both prediction accuracy and generalizability across diverse environments achieving up to a 36.9% reduction in prediction error. In addition, we conduct ablation studies to determine the number of masks, the size of each mask, and the spatial distribution of masks to maximize overall performance.
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2507.16398.pdf' target='_blank'>https://arxiv.org/pdf/2507.16398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lavinia Hriscu, Alberto Sanfeliu, Anais Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16398">AI or Human? Understanding Perceptions of Embodied Robots with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of artificial intelligence has long been associated to the the challenge of effectively measuring intelligence. Even if the Turing Test was introduced as a means of assessing a system intelligence, its relevance and application within the field of human-robot interaction remain largely underexplored. This study investigates the perception of intelligence in embodied robots by performing a Turing Test within a robotic platform. A total of 34 participants were tasked with distinguishing between AI- and human-operated robots while engaging in two interactive tasks: an information retrieval and a package handover. These tasks assessed the robot perception and navigation abilities under both static and dynamic conditions. Results indicate that participants were unable to reliably differentiate between AI- and human-controlled robots beyond chance levels. Furthermore, analysis of participant responses reveals key factors influencing the perception of artificial versus human intelligence in embodied robotic systems. These findings provide insights into the design of future interactive robots and contribute to the ongoing discourse on intelligence assessment in AI-driven systems.
<div id='section'>Paperid: <span id='pid'>1562, <a href='https://arxiv.org/pdf/2507.12473.pdf' target='_blank'>https://arxiv.org/pdf/2507.12473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mia-Katrin Kvalsund, Mikkel Elle LepperÃ¸d
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12473">The Generalist Brain Module: Module Repetition in Neural Networks in Light of the Minicolumn Hypothesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While modern AI continues to advance, the biological brain remains the pinnacle of neural networks in its robustness, adaptability, and efficiency. This review explores an AI architectural path inspired by the brain's structure, particularly the minicolumn hypothesis, which views the neocortex as a distributed system of repeated modules - a structure we connect to collective intelligence (CI). Despite existing work, there is a lack of comprehensive reviews connecting the cortical column to the architectures of repeated neural modules. This review aims to fill that gap by synthesizing historical, theoretical, and methodological perspectives on neural module repetition. We distinguish between architectural repetition - reusing structure - and parameter-shared module repetition, where the same functional unit is repeated across a network. The latter exhibits key CI properties such as robustness, adaptability, and generalization. Evidence suggests that the repeated module tends to converge toward a generalist module: simple, flexible problem solvers capable of handling many roles in the ensemble. This generalist tendency may offer solutions to longstanding challenges in modern AI: improved energy efficiency during training through simplicity and scalability, and robust embodied control via generalization. While empirical results suggest such systems can generalize to out-of-distribution problems, theoretical results are still lacking. Overall, architectures featuring module repetition remain an emerging and unexplored architectural strategy, with significant untapped potential for both efficiency, robustness, and adaptiveness. We believe that a system that adopts the benefits of CI, while adhering to architectural and functional principles of the minicolumns, could challenge the modern AI problems of scalability, energy consumption, and democratization.
<div id='section'>Paperid: <span id='pid'>1563, <a href='https://arxiv.org/pdf/2507.10087.pdf' target='_blank'>https://arxiv.org/pdf/2507.10087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Tayyab Khan, Ammar Waheed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10087">Foundation Model Driven Robotics: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid emergence of foundation models, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), has introduced a transformative paradigm in robotics. These models offer powerful capabilities in semantic understanding, high-level reasoning, and cross-modal generalization, enabling significant advances in perception, planning, control, and human-robot interaction. This critical review provides a structured synthesis of recent developments, categorizing applications across simulation-driven design, open-world execution, sim-to-real transfer, and adaptable robotics. Unlike existing surveys that emphasize isolated capabilities, this work highlights integrated, system-level strategies and evaluates their practical feasibility in real-world environments. Key enabling trends such as procedural scene generation, policy generalization, and multimodal reasoning are discussed alongside core bottlenecks, including limited embodiment, lack of multimodal data, safety risks, and computational constraints. Through this lens, this paper identifies both the architectural strengths and critical limitations of foundation model-based robotics, highlighting open challenges in real-time operation, grounding, resilience, and trust. The review concludes with a roadmap for future research aimed at bridging semantic reasoning and physical intelligence through more robust, interpretable, and embodied models.
<div id='section'>Paperid: <span id='pid'>1564, <a href='https://arxiv.org/pdf/2507.10055.pdf' target='_blank'>https://arxiv.org/pdf/2507.10055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhtadin, I Wayan Agus Darmawan, Muhammad Hilmi Rusydiansyah, I Ketut Eddy Purnama, Chastine Fatichah, Mauridhi Hery Purnomo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10055">Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Direct and natural interaction is essential for intuitive human-robot collaboration, eliminating the need for additional devices such as joysticks, tablets, or wearable sensors. In this paper, we present a lightweight deep learning-based hand gesture recognition system that enables humans to control collaborative robots naturally and efficiently. This model recognizes eight distinct hand gestures with only 1,103 parameters and a compact size of 22 KB, achieving an accuracy of 93.5%. To further optimize the model for real-world deployment on edge devices, we applied quantization and pruning using TensorFlow Lite, reducing the final model size to just 7 KB. The system was successfully implemented and tested on a Universal Robot UR5 collaborative robot within a real-time robotic framework based on ROS2. The results demonstrate that even extremely lightweight models can deliver accurate and responsive hand gesture-based control for collaborative robots, opening new possibilities for natural human-robot interaction in constrained environments.
<div id='section'>Paperid: <span id='pid'>1565, <a href='https://arxiv.org/pdf/2507.08831.pdf' target='_blank'>https://arxiv.org/pdf/2507.08831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Josh Qixuan Sun, Xiaoying Xing, Huaiyuan Weng, Chul Min Yeum, Mark Crowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08831">View Invariant Learning for Vision-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.
<div id='section'>Paperid: <span id='pid'>1566, <a href='https://arxiv.org/pdf/2507.05607.pdf' target='_blank'>https://arxiv.org/pdf/2507.05607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongshan Fan, Shenghai Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05607">Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents Auto-RubikAI, a modular autonomous planning framework that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM), and a large language model (LLM) to solve structured manipulation tasks exemplified by Rubik's Cube restoration. Unlike traditional robot systems based on predefined scripts, or modern approaches relying on pretrained networks and large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step task execution with minimal data requirements and no prior demonstrations. The proposed system employs a KB module to solve group-theoretic restoration steps, overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to construct a semantic 3D scene representation, while the LLM generates structured robotic control code via prompt chaining. This tri-module architecture enables robust performance under spatial uncertainty. We deploy Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic arm, demonstrating effective Sim-to-Real adaptation without retraining. Experiments show a 79% end-to-end task success rate across randomized configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our KB-enhanced method reduces average solution steps while maintaining interpretability and safety. Auto-RubikAI provides a cost-efficient, modular foundation for embodied task planning in smart manufacturing, robotics education, and autonomous execution scenarios. Code, prompts, and hardware modules will be released upon publication.
<div id='section'>Paperid: <span id='pid'>1567, <a href='https://arxiv.org/pdf/2507.02755.pdf' target='_blank'>https://arxiv.org/pdf/2507.02755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caleb Rascon, Luis Gato-Diaz, Eduardo GarcÃ­a-AlarcÃ³n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02755">Multi-agent Auditory Scene Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a publicly available framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.
<div id='section'>Paperid: <span id='pid'>1568, <a href='https://arxiv.org/pdf/2507.02016.pdf' target='_blank'>https://arxiv.org/pdf/2507.02016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wang, Roberto Calandra, Verena KlÃ¶s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02016">Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When robots perform complex and context-dependent tasks in our daily lives, deviations from expectations can confuse users. Explanations of the robot's reasoning process can help users to understand the robot intentions. However, when to provide explanations and what they contain are important to avoid user annoyance. We have investigated user preferences for explanation demand and content for a robot that helps with daily cleaning tasks in a kitchen. Our results show that users want explanations in surprising situations and prefer concise explanations that clearly state the intention behind the confusing action and the contextual factors that were relevant to this decision. Based on these findings, we propose two algorithms to identify surprising actions and to construct effective explanations for Belief-Desire-Intention (BDI) robots. Our algorithms can be easily integrated in the BDI reasoning process and pave the way for better human-robot interaction with context- and user-specific explanations.
<div id='section'>Paperid: <span id='pid'>1569, <a href='https://arxiv.org/pdf/2507.00271.pdf' target='_blank'>https://arxiv.org/pdf/2507.00271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuochao Peng, Jiaxin Xu, Jun Hu, Haian Xue, Laurens A. G. Kolks, Pieter M. A. Desmet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00271">User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept "Mora" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1570, <a href='https://arxiv.org/pdf/2506.10172.pdf' target='_blank'>https://arxiv.org/pdf/2506.10172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Duan, Kaiyu tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10172">A Navigation Framework Utilizing Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.
<div id='section'>Paperid: <span id='pid'>1571, <a href='https://arxiv.org/pdf/2506.06624.pdf' target='_blank'>https://arxiv.org/pdf/2506.06624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba Mollahossein, Farshad Haghgoo Daryakenari, Mohammad Hossein Rohban, Gholamreza Vossoughi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06624">Attention-Based Convolutional Neural Network Model for Human Lower Limb Activity Recognition using sEMG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate classification of lower limb movements using surface electromyography (sEMG) signals plays a crucial role in assistive robotics and rehabilitation systems. In this study, we present a lightweight attention-based deep neural network (DNN) for real-time movement classification using multi-channel sEMG data from the publicly available BASAN dataset. The proposed model consists of only 62,876 parameters and is designed without the need for computationally expensive preprocessing, making it suitable for real-time deployment. We employed a leave-oneout validation strategy to ensure generalizability across subjects, and evaluated the model on three movement classes: walking, standing with knee flexion, and sitting with knee extension. The network achieved 86.74% accuracy on the validation set and 85.38% on the test set, demonstrating strong classification performance under realistic conditions. Comparative analysis with existing models in the literature highlights the efficiency and effectiveness of our approach, especially in scenarios where computational cost and real-time response are critical. The results indicate that the proposed model is a promising candidate for integration into upper-level controllers in human-robot interaction systems.
<div id='section'>Paperid: <span id='pid'>1572, <a href='https://arxiv.org/pdf/2506.05576.pdf' target='_blank'>https://arxiv.org/pdf/2506.05576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerija Holomjova, Jamie Grech, Dewei Yi, Bruno Yun, Andrew Starkey, Pascal MeiÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05576">TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping (TOG) is an essential preliminary step for robotic task execution, which involves predicting grasps on regions of target objects that facilitate intended tasks. Existing literature reveals there is a limited availability of TOG datasets for training and benchmarking despite large demand, which are often synthetic or have artifacts in mask annotations that hinder model performance. Moreover, TOG solutions often require affordance masks, grasps, and object masks for training, however, existing datasets typically provide only a subset of these annotations. To address these limitations, we introduce the Top-down Task-oriented Grasping (TD-TOG) dataset, designed to train and evaluate TOG solutions. TD-TOG comprises 1,449 real-world RGB-D scenes including 30 object categories and 120 subcategories, with hand-annotated object masks, affordances, and planar rectangular grasps. It also features a test set for a novel challenge that assesses a TOG solution's ability to distinguish between object subcategories. To contribute to the demand for TOG solutions that can adapt and manipulate previously unseen objects without re-training, we propose a novel TOG framework, Binary-TOG. Binary-TOG uses zero-shot for object recognition, and one-shot learning for affordance recognition. Zero-shot learning enables Binary-TOG to identify objects in multi-object scenes through textual prompts, eliminating the need for visual references. In multi-object settings, Binary-TOG achieves an average task-oriented grasp accuracy of 68.9%. Lastly, this paper contributes a comparative analysis between one-shot and zero-shot learning for object generalization in TOG to be used in the development of future TOG solutions.
<div id='section'>Paperid: <span id='pid'>1573, <a href='https://arxiv.org/pdf/2505.19803.pdf' target='_blank'>https://arxiv.org/pdf/2505.19803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuze Sun, Lingyu Li, Shixiangyue Meng, Xiaoming Teng, Terry R. Payne, Paul Craig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19803">Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the integration of individual human traits into an empathetically adaptive educational robot tutor system designed to improve student engagement and learning outcomes with corresponding Engagement Vector measurement. While prior research in the field of Human-Robot Interaction (HRI) has examined the integration of the traits, such as emotional intelligence, memory-driven personalization, and non-verbal communication, by themselves, they have thus-far neglected to consider their synchronized integration into a cohesive, operational education framework. To address this gap, we customize a Multi-Modal Large Language Model (LLaMa 3.2 from Meta) deployed with modules for human-like traits (emotion, memory and gestures) into an AI-Agent framework. This constitutes to the robot's intelligent core mimicing the human emotional system, memory architecture and gesture control to allow the robot to behave more empathetically while recognizing and responding appropriately to the student's emotional state. It can also recall the student's past learning record and adapt its style of interaction accordingly. This allows the robot tutor to react to the student in a more sympathetic manner by delivering personalized verbal feedback synchronized with relevant gestures. Our study investigates the extent of this effect through the introduction of Engagement Vector Model which can be a surveyor's pole for judging the quality of HRI experience. Quantitative and qualitative results demonstrate that such an empathetic responsive approach significantly improves student engagement and learning outcomes compared with a baseline humanoid robot without these human-like traits. This indicates that robot tutors with empathetic capabilities can create a more supportive, interactive learning experience that ultimately leads to better outcomes for the student.
<div id='section'>Paperid: <span id='pid'>1574, <a href='https://arxiv.org/pdf/2505.18361.pdf' target='_blank'>https://arxiv.org/pdf/2505.18361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trinity Chung, Yuchen Shen, Nathan C. L. Kong, Aran Nayebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18361">Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.
<div id='section'>Paperid: <span id='pid'>1575, <a href='https://arxiv.org/pdf/2505.16928.pdf' target='_blank'>https://arxiv.org/pdf/2505.16928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bosung Kim, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16928">Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.
<div id='section'>Paperid: <span id='pid'>1576, <a href='https://arxiv.org/pdf/2505.14866.pdf' target='_blank'>https://arxiv.org/pdf/2505.14866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nisarga Nilavadi, Andrey Rudenko, Timm Linder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14866">UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset ``DARKO'' with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/
<div id='section'>Paperid: <span id='pid'>1577, <a href='https://arxiv.org/pdf/2505.12294.pdf' target='_blank'>https://arxiv.org/pdf/2505.12294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weishang Wu, Yifei Shi, Zhizhong Chen, Zhipong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12294">PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented grasping is a crucial yet challenging task in robotic manipulation. Despite the recent progress, few existing methods address task-oriented grasping with dexterous hands. Dexterous hands provide better precision and versatility, enabling robots to perform task-oriented grasping more effectively. In this paper, we argue that part analysis can enhance dexterous grasping by providing detailed information about the object's functionality. We propose PartDexTOG, a method that generates dexterous task-oriented grasps via language-driven part analysis. Taking a 3D object and a manipulation task represented by language as input, the method first generates the category-level and part-level grasp descriptions w.r.t the manipulation task by LLMs. Then, a category-part conditional diffusion model is developed to generate a dexterous grasp for each part, respectively, based on the generated descriptions. To select the most plausible combination of grasp and corresponding part from the generated ones, we propose a measure of geometric consistency between grasp and part. We show that our method greatly benefits from the open-world knowledge reasoning on object parts by LLMs, which naturally facilitates the learning of grasp generation on objects with different geometry and for different manipulation tasks. Our method ranks top on the OakInk-shape dataset over all previous methods, improving the Penetration Volume, the Grasp Displace, and the P-FID over the state-of-the-art by $3.58\%$, $2.87\%$, and $41.43\%$, respectively. Notably, it demonstrates good generality in handling novel categories and tasks.
<div id='section'>Paperid: <span id='pid'>1578, <a href='https://arxiv.org/pdf/2505.11476.pdf' target='_blank'>https://arxiv.org/pdf/2505.11476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runze Zuo, Dong Heon Han, Richard Li, Saima Jamal, Daniel Bruder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11476">UMArm: Untethered, Modular, Wearable, Soft Pneumatic Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic arms are essential to modern industries, however, their adaptability to unstructured environments remains limited. Soft robotic arms, particularly those actuated pneumatically, offer greater adaptability in unstructured environments and enhanced safety for human-robot interaction. However, current pneumatic soft arms are constrained by limited degrees of freedom, precision, payload capacity, and reliance on bulky external pressure regulators. In this work, a novel pneumatically driven rigid-soft hybrid arm, ``UMArm'', is presented. The shortcomings of pneumatically actuated soft arms are addressed by densely integrating high-force-to-weight-ratio, self-regulated McKibben actuators onto a lightweight rigid spine structure. The modified McKibben actuators incorporate valves and controllers directly inside, eliminating the need for individual pressure lines and external regulators, significantly reducing system weight and complexity. Full untethered operation, high payload capacity, precision, and directionally tunable compliance are achieved by the UMArm. Portability is demonstrated through a wearable assistive arm experiment, and versatility is showcased by reconfiguring the system into an inchworm robot. The results of this work show that the high-degree-of-freedom, external-regulator-free pneumatically driven arm systems like the UMArm possess great potential for real-world unstructured environments.
<div id='section'>Paperid: <span id='pid'>1579, <a href='https://arxiv.org/pdf/2505.10705.pdf' target='_blank'>https://arxiv.org/pdf/2505.10705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matej Hoffmann, Shubhan Parag Patni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10705">Embodied AI in Machine Learning -- is it Really Embodied?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the machine learning communities with the goal of leveraging current progress in AI (deep learning, transformers, large language and visual-language models) to empower robots. In this chapter we put this work in the context of "Good Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier 2001). We claim that the AI-powered robots are only weakly embodied and inherit some of the problems of GOFAI. Moreover, we review and critically discuss the possibility of cross-embodiment learning (Padalkar et al. 2024). We identify fundamental roadblocks and propose directions on how to make progress.
<div id='section'>Paperid: <span id='pid'>1580, <a href='https://arxiv.org/pdf/2505.10183.pdf' target='_blank'>https://arxiv.org/pdf/2505.10183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieke Lin, Wanyu Wang, Longxiang Yin, Yinhe Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10183">KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) systems, such as autonomous robots and intelligent vehicles, are increasingly reliant on diverse heterogeneous accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing and energy-efficiency demands. However, the proliferation of vendor-specific proprietary communication libraries creates significant interoperability barriers, hindering seamless collaboration between different accelerator types and leading to suboptimal resource utilization and performance bottlenecks in distributed AI workloads. This paper introduces KAITIAN, a novel distributed communication framework designed to bridge this gap. KAITIAN provides a unified abstraction layer that intelligently integrates vendor-optimized communication libraries for intra-group efficiency with general-purpose communication protocols for inter-group interoperability. Crucially, it incorporates a load-adaptive scheduling mechanism that dynamically balances computational tasks across heterogeneous devices based on their real-time performance characteristics. Implemented as an extension to PyTorch and rigorously evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN demonstrates significant improvements in resource utilization and scalability for distributed training tasks. Experimental results show that KAITIAN can accelerate training time by up to 42% compared to baseline homogeneous systems, while incurring minimal communication overhead (2.8--4.3%) and maintaining model accuracy. KAITIAN paves the way for more flexible and powerful heterogeneous computing in complex embodied AI applications.
<div id='section'>Paperid: <span id='pid'>1581, <a href='https://arxiv.org/pdf/2505.07532.pdf' target='_blank'>https://arxiv.org/pdf/2505.07532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kajetan RachwaÅ, Maciej Majek, BartÅomiej Boczek, Kacper DÄbrowski, PaweÅ Liberadzki, Adam DÄbrowski, Maria Ganzha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07532">RAI: Flexible Agent Framework for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With an increase in the capabilities of generative language models, a growing interest in embodied AI has followed. This contribution introduces RAI - a framework for creating embodied Multi Agent Systems for robotics. The proposed framework implements tools for Agents' integration with robotic stacks, Large Language Models, and simulations. It provides out-of-the-box integration with state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms for the embodiment of Agents. These mechanisms have been tested on a physical robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid prototyping. Furthermore, these mechanisms have been deployed in two simulations: (1) robot arm manipulator and (2) tractor controller. All of these deployments have been evaluated in terms of their control capabilities, effectiveness of embodiment, and perception ability. The proposed framework has been used successfully to build systems with multiple agents. It has demonstrated effectiveness in all the aforementioned tasks. It also enabled identifying and addressing the shortcomings of the generative models used for embodied AI.
<div id='section'>Paperid: <span id='pid'>1582, <a href='https://arxiv.org/pdf/2505.07460.pdf' target='_blank'>https://arxiv.org/pdf/2505.07460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Chen, JiaHao Zhao, HaoHao Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07460">A Survey on Collaborative Mechanisms Between Large and Small Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.
<div id='section'>Paperid: <span id='pid'>1583, <a href='https://arxiv.org/pdf/2505.06628.pdf' target='_blank'>https://arxiv.org/pdf/2505.06628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongquan Zhou, Shuhao Li, Zixian Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06628">ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI research has traditionally emphasized performance metrics such as success rate and cumulative reward, overlooking critical robustness and safety considerations that emerge during real-world deployment. In actual environments, agents continuously encounter unpredicted situations and distribution shifts, causing seemingly reliable policies to experience catastrophic failures, particularly in manipulation tasks. To address this gap, we introduce four novel safety-centric metrics that quantify an agent's resilience to environmental perturbations. Building on these metrics, we present Adaptive Contrastive Optimization for Robust Manipulation (ACORN), a plug-and-play algorithm that enhances policy robustness without sacrificing performance. ACORN leverages contrastive learning to simultaneously align trajectories with expert demonstrations while diverging from potentially unsafe behaviors. Our approach efficiently generates informative negative samples through structured Gaussian noise injection, employing a double perturbation technique that maintains sample diversity while minimizing computational overhead. Comprehensive experiments across diverse manipulation environments validate ACORN's effectiveness, yielding improvements of up to 23% in safety metrics under disturbance compared to baseline methods. These findings underscore ACORN's significant potential for enabling reliable deployment of embodied agents in safety-critical real-world applications.
<div id='section'>Paperid: <span id='pid'>1584, <a href='https://arxiv.org/pdf/2504.21548.pdf' target='_blank'>https://arxiv.org/pdf/2504.21548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria MorÃ£o PatrÃ­cio, Anahita Jamshidnejad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21548">Leveraging Systems and Control Theory for Social Robotics: A Model-Based Behavioral Control Approach to Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots (SRs) should autonomously interact with humans, while exhibiting proper social behaviors associated to their role. By contributing to health-care, education, and companionship, SRs will enhance life quality. However, personalization and sustaining user engagement remain a challenge for SRs, due to their limited understanding of human mental states. Accordingly, we leverage a recently introduced mathematical dynamic model of human perception, cognition, and decision-making for SRs. Identifying the parameters of this model and deploying it in behavioral steering system of SRs allows to effectively personalize the responses of SRs to evolving mental states of their users, enhancing long-term engagement and personalization. Our approach uniquely enables autonomous adaptability of SRs by modeling the dynamics of invisible mental states, significantly contributing to the transparency and awareness of SRs. We validated our model-based control system in experiments with 10 participants who interacted with a Nao robot over three chess puzzle sessions, 45 - 90 minutes each. The identified model achieved a mean squared error (MSE) of 0.067 (i.e., 1.675% of the maximum possible MSE) in tracking beliefs, goals, and emotions of participants. Compared to a model-free controller that did not track mental states of participants, our approach increased engagement by 16% on average. Post-interaction feedback of participants (provided via dedicated questionnaires) further confirmed the perceived engagement and awareness of the model-driven robot. These results highlight the unique potential of model-based approaches and control theory in advancing human-SR interactions.
<div id='section'>Paperid: <span id='pid'>1585, <a href='https://arxiv.org/pdf/2504.20109.pdf' target='_blank'>https://arxiv.org/pdf/2504.20109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajeev Gupta, Suhani Gupta, Ronak Parikh, Divya Gupta, Amir Javaheri, Jairaj Singh Shaktawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20109">Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired AI, and proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms for edge deployment. We review literature on continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss key neuroscience principles of human learning, including Synaptic Pruning, Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for AI systems. Building on these insights, we outline an AI architecture that features complementary fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates to support on-device lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are provided. We address challenges such as catastrophic forgetting, memory efficiency, and system scalability, and present application scenarios for mobile AI assistants and embodied AI systems like humanoid robots. We conclude with key takeaways and future research directions toward truly continual, personalized AGI on the edge. While the architecture is theoretical, it synthesizes diverse findings and offers a roadmap for future implementation.
<div id='section'>Paperid: <span id='pid'>1586, <a href='https://arxiv.org/pdf/2504.12535.pdf' target='_blank'>https://arxiv.org/pdf/2504.12535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andy Dimnaku, Dominic Yurk, Zhiyuan Gao, Arun Padmanabhan, Mandar Aras, Yaser Abu-Mostafa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12535">Decision-based AI Visual Navigation for Cardiac Ultrasounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound imaging of the heart (echocardiography) is widely used to diagnose cardiac diseases. However, obtaining an echocardiogram requires an expert sonographer and a high-quality ultrasound imaging device, which are generally only available in hospitals. Recently, AI-based navigation models and algorithms have been used to aid novice sonographers in acquiring the standardized cardiac views necessary to visualize potential disease pathologies. These navigation systems typically rely on directional guidance to predict the necessary rotation of the ultrasound probe. This paper demonstrates a novel AI navigation system that builds on a decision model for identifying the inferior vena cava (IVC) of the heart. The decision model is trained offline using cardiac ultrasound videos and employs binary classification to determine whether the IVC is present in a given ultrasound video. The underlying model integrates a novel localization algorithm that leverages the learned feature representations to annotate the spatial location of the IVC in real-time. Our model demonstrates strong localization performance on traditional high-quality hospital ultrasound videos, as well as impressive zero-shot performance on lower-quality ultrasound videos from a more affordable Butterfly iQ handheld ultrasound machine. This capability facilitates the expansion of ultrasound diagnostics beyond hospital settings. Currently, the guidance system is undergoing clinical trials and is available on the Butterfly iQ app.
<div id='section'>Paperid: <span id='pid'>1587, <a href='https://arxiv.org/pdf/2504.11419.pdf' target='_blank'>https://arxiv.org/pdf/2504.11419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Jin, Liu Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11419">Embodied World Models Emerge from Navigational Task in Open-Ended Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning in partially observable environments has often been approached through passive predictive models, yet theories of embodied cognition suggest that genuinely useful representations arise only when perception is tightly coupled to action. Here we ask whether a recurrent agent, trained solely by sparse rewards to solve procedurally generated planar mazes, can autonomously internalize metric concepts such as direction, distance and obstacle layout. After training, the agent consistently produces near-optimal paths in unseen mazes, behavior that hints at an underlying spatial model. To probe this possibility, we cast the closed agent-environment loop as a hybrid dynamical system, identify stable limit cycles in its state space, and characterize behavior with a Ridge Representation that embeds whole trajectories into a common metric space. Canonical correlation analysis exposes a robust linear alignment between neural and behavioral manifolds, while targeted perturbations of the most informative neural dimensions sharply degrade navigation performance. Taken together, these dynamical, representational, and causal signatures show that sustained sensorimotor interaction is sufficient for the spontaneous emergence of compact, embodied world models, providing a principled path toward interpretable and transferable navigation policies.
<div id='section'>Paperid: <span id='pid'>1588, <a href='https://arxiv.org/pdf/2504.10294.pdf' target='_blank'>https://arxiv.org/pdf/2504.10294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>J. F. Almeida, J. AndrÃ©, C. P. Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10294">Ankle Exoskeletons in Walking and Load-Carrying Tasks: Insights into Biomechanics and Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Lower limb exoskeletons can enhance quality of life, but widespread adoption is limited by the lack of frameworks to assess their biomechanical and human-robot interaction effects, which are essential for developing adaptive and personalized control strategies. Understanding impacts on kinematics, muscle activity, and HRI dynamics is key to achieve improved usability of wearable robots. Objectives: We propose a systematic methodology evaluate an ankle exoskeleton's effects on human movement during walking and load-carrying (10 kg front pack), focusing on joint kinematics, muscle activity, and HRI torque signals. Materials and Methods: Using Xsens MVN (inertial motion capture), Delsys EMG, and a unilateral exoskeleton, three experiments were conducted: (1) isolated dorsiflexion/plantarflexion; (2) gait analysis (two subjects, passive/active modes); and (3) load-carrying under assistance. Results and Conclusions: The first experiment confirmed that the HRI sensor captured both voluntary and involuntary torques, providing directional torque insights. The second experiment showed that the device slightly restricted ankle range of motion (RoM) but supported normal gait patterns across all assistance modes. The exoskeleton reduced muscle activity, particularly in active mode. HRI torque varied according to gait phases and highlighted reduced synchronization, suggesting a need for improved support. The third experiment revealed that load-carrying increased GM and TA muscle activity, but the device partially mitigated user effort by reducing muscle activity compared to unassisted walking. HRI increased during load-carrying, providing insights into user-device dynamics. These results demonstrate the importance of tailoring exoskeleton evaluation methods to specific devices and users, while offering a framework for future studies on exoskeleton biomechanics and HRI.
<div id='section'>Paperid: <span id='pid'>1589, <a href='https://arxiv.org/pdf/2504.09243.pdf' target='_blank'>https://arxiv.org/pdf/2504.09243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Hagenow, Julie A. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09243">REALM: Real-Time Estimates of Assistance for Learned Models in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are a variety of mechanisms (i.e., input types) for real-time human interaction that can facilitate effective human-robot teaming. For example, previous works have shown how teleoperation, corrective, and discrete (i.e., preference over a small number of choices) input can enable robots to complete complex tasks. However, few previous works have looked at combining different methods, and in particular, opportunities for a robot to estimate and elicit the most effective form of assistance given its understanding of a task. In this paper, we propose a method for estimating the value of different human assistance mechanisms based on the action uncertainty of a robot policy. Our key idea is to construct mathematical expressions for the expected post-interaction differential entropy (i.e., uncertainty) of a stochastic robot policy to compare the expected value of different interactions. As each type of human input imposes a different requirement for human involvement, we demonstrate how differential entropy estimates can be combined with a likelihood penalization approach to effectively balance feedback informational needs with the level of required input. We demonstrate evidence of how our approach interfaces with emergent learning models (e.g., a diffusion model) to produce accurate assistance value estimates through both simulation and a robot user study. Our user study results indicate that the proposed approach can enable task completion with minimal human feedback for uncertain robot behaviors.
<div id='section'>Paperid: <span id='pid'>1590, <a href='https://arxiv.org/pdf/2504.08431.pdf' target='_blank'>https://arxiv.org/pdf/2504.08431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiafan Lu, Dongcheng Hu, Yitian Ye, Anqi Liu, Zixian Zhang, Xin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08431">The Composite Visual-Laser Navigation Method Applied in Indoor Poultry Farming Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor poultry farms require inspection robots to maintain precise environmental control, which is crucial for preventing the rapid spread of disease and large-scale bird mortality. However, the complex conditions within these facilities, characterized by areas of intense illumination and water accumulation, pose significant challenges. Traditional navigation methods that rely on a single sensor often perform poorly in such environments, resulting in issues like laser drift and inaccuracies in visual navigation line extraction. To overcome these limitations, we propose a novel composite navigation method that integrates both laser and vision technologies. This approach dynamically computes a fused yaw angle based on the real-time reliability of each sensor modality, thereby eliminating the need for physical navigation lines. Experimental validation in actual poultry house environments demonstrates that our method not only resolves the inherent drawbacks of single-sensor systems, but also significantly enhances navigation precision and operational efficiency. As such, it presents a promising solution for improving the performance of inspection robots in complex indoor poultry farming settings.
<div id='section'>Paperid: <span id='pid'>1591, <a href='https://arxiv.org/pdf/2504.08395.pdf' target='_blank'>https://arxiv.org/pdf/2504.08395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pepita Barnard, Maria J Galvez Trigo, Dominic Price, Sue Cobb, Gisela Reyes-Cruz, Gustavo Berumen, David Branson, Mojtaba A. Khanesar, Mercedes Torres Torres, Michel Valstar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08395">Human strategies for correcting `human-robot' errors during a laundry sorting task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mental models and expectations underlying human-human interaction (HHI) inform human-robot interaction (HRI) with domestic robots. To ease collaborative home tasks by improving domestic robot speech and behaviours for human-robot communication, we designed a study to understand how people communicated when failure occurs. To identify patterns of natural communication, particularly in response to robotic failures, participants instructed Laundrobot to move laundry into baskets using natural language and gestures. Laundrobot either worked error-free, or in one of two error modes. Participants were not advised Laundrobot would be a human actor, nor given information about error modes. Video analysis from 42 participants found speech patterns, included laughter, verbal expressions, and filler words, such as ``oh'' and ``ok'', also, sequences of body movements, including touching one's own face, increased pointing with a static finger, and expressions of surprise. Common strategies deployed when errors occurred, included correcting and teaching, taking responsibility, and displays of frustration. The strength of reaction to errors diminished with exposure, possibly indicating acceptance or resignation. Some used strategies similar to those used to communicate with other technologies, such as smart assistants. An anthropomorphic robot may not be ideally suited to this kind of task. Laundrobot's appearance, morphology, voice, capabilities, and recovery strategies may have impacted how it was perceived. Some participants indicated Laundrobot's actual skills were not aligned with expectations; this made it difficult to know what to expect and how much Laundrobot understood. Expertise, personality, and cultural differences may affect responses, however these were not assessed.
<div id='section'>Paperid: <span id='pid'>1592, <a href='https://arxiv.org/pdf/2504.06189.pdf' target='_blank'>https://arxiv.org/pdf/2504.06189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco J. RodrÃ­guez Lera, Raquel FernÃ¡ndez HernÃ¡ndez, Sonia Lopez GonzÃ¡lez, Miguel Angel GonzÃ¡lez-Santamarta, Francisco JesÃºs RodrÃ­guez Sedano, Camino Fernandez Llamas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06189">Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI.
<div id='section'>Paperid: <span id='pid'>1593, <a href='https://arxiv.org/pdf/2504.01260.pdf' target='_blank'>https://arxiv.org/pdf/2504.01260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roy El-Helou, Matthew K. X. J Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01260">The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores how human perceptions of a non-anthropomorphic robotic manipulator are shaped by two key dimensions of behaviour: arousal, defined as the robot's movement energy and expressiveness, and attention, defined as the robot's capacity to selectively orient toward and engage with a user. We introduce a novel control architecture that integrates a gaze-like attention engine with an arousal-modulated motion system to generate socially meaningful behaviours. In a user study, we find that robots exhibiting high attention -- actively directing their focus toward users -- are perceived as warmer and more competent, intentional, and lifelike. In contrast, high arousal -- characterized by fast, expansive, and energetic motions -- increases perceptions of discomfort and disturbance. Importantly, a combination of focused attention and moderate arousal yields the highest ratings of trust and sociability, while excessive arousal diminishes social engagement. These findings offer design insights for endowing non-humanoid robots with expressive, intuitive behaviours that support more natural human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1594, <a href='https://arxiv.org/pdf/2503.24110.pdf' target='_blank'>https://arxiv.org/pdf/2503.24110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>FranÃ§ois Olivier, Zied Bouraoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24110">Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advances in embodied AI, agent reasoning systems still struggle to capture the fundamental conceptual structures that humans naturally use to understand and interact with their environment. To address this, we propose a novel framework that bridges embodied cognition theory and agent systems by leveraging a formal characterization of image schemas, which are defined as recurring patterns of sensorimotor experience that structure human cognition. By customizing LLMs to translate natural language descriptions into formal representations based on these sensorimotor patterns, we will be able to create a neurosymbolic system that grounds the agent's understanding in fundamental conceptual structures. We argue that such an approach enhances both efficiency and interpretability while enabling more intuitive human-agent interactions through shared embodied understanding.
<div id='section'>Paperid: <span id='pid'>1595, <a href='https://arxiv.org/pdf/2503.21232.pdf' target='_blank'>https://arxiv.org/pdf/2503.21232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Bheemaiah, Seungyong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21232">Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inability of autonomous vehicles (AVs) to infer the material properties of obstacles limits their decision-making capacity. While AVs rely on sensor systems such as cameras, LiDAR, and radar to detect obstacles, this study suggests combining sensors with a knowledge graph (KG)-based world model to improve AVs' comprehension of physical material qualities. Beyond sensor data, AVs can infer qualities such as malleability, density, and elasticity using a semantic KG that depicts the relationships between obstacles and their attributes. Using the CARLA autonomous driving simulator, we evaluated AV performance with and without KG integration. The findings demonstrate that the KG-based method improves obstacle management, which allows AVs to use material qualities to make better decisions about when to change lanes or apply emergency braking. For example, the KG-integrated AV changed lanes for hard impediments like traffic cones and successfully avoided collisions with flexible items such as plastic bags by passing over them. Compared to the control system, the KG framework demonstrated improved responsiveness to obstacles by resolving conflicting sensor data, causing emergency stops for 13.3% more cases. In addition, our method exhibits a 6.6% higher success rate in lane-changing maneuvers in experimental scenarios, particularly for larger, high-impact obstacles. While we focus particularly on autonomous driving, our work demonstrates the potential of KG-based world models to improve decision-making in embodied AI systems and scale to other domains, including robotics, healthcare, and environmental simulation.
<div id='section'>Paperid: <span id='pid'>1596, <a href='https://arxiv.org/pdf/2503.17730.pdf' target='_blank'>https://arxiv.org/pdf/2503.17730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco J. RodrÃ­guez Lera, Yoana Pita Lorenzo, David SobrÃ­n Hidalgo, Laura FernÃ¡ndez Becerra, Irene GonzÃ¡lez FernÃ¡ndez, Jose Miguel Guerrero HernÃ¡ndez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17730">Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en robÃ³tica y sistemas autÃ³nomos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybersecurity in robotics stands out as a key aspect within Regulation (EU) 2024/1689, also known as the Artificial Intelligence Act, which establishes specific guidelines for intelligent and automated systems. A fundamental distinction in this regulatory framework is the difference between robots with Artificial Intelligence (AI) and those that operate through automation systems without AI, since the former are subject to stricter security requirements due to their learning and autonomy capabilities. This work analyzes cybersecurity tools applicable to advanced robotic systems, with special emphasis on the protection of knowledge bases in cognitive architectures. Furthermore, a list of basic tools is proposed to guarantee the security, integrity, and resilience of these systems, and a practical case is presented, focused on the analysis of robot knowledge management, where ten evaluation criteria are defined to ensure compliance with the regulation and reduce risks in human-robot interaction (HRI) environments.
<div id='section'>Paperid: <span id='pid'>1597, <a href='https://arxiv.org/pdf/2503.16467.pdf' target='_blank'>https://arxiv.org/pdf/2503.16467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anargh Viswanath, Lokesh Veeramacheneni, Hendrik Buschmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16467">Enhancing Explainability with Multimodal Context Representations for Smarter Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence (AI) has significantly advanced in recent years, driving innovation across various fields, especially in robotics. Even though robots can perform complex tasks with increasing autonomy, challenges remain in ensuring explainability and user-centered design for effective interaction. A key issue in Human-Robot Interaction (HRI) is enabling robots to effectively perceive and reason over multimodal inputs, such as audio and vision, to foster trust and seamless collaboration. In this paper, we propose a generalized and explainable multimodal framework for context representation, designed to improve the fusion of speech and vision modalities. We introduce a use case on assessing 'Relevance' between verbal utterances from the user and visual scene perception of the robot. We present our methodology with a Multimodal Joint Representation module and a Temporal Alignment module, which can allow robots to evaluate relevance by temporally aligning multimodal inputs. Finally, we discuss how the proposed framework for context representation can help with various aspects of explainability in HRI.
<div id='section'>Paperid: <span id='pid'>1598, <a href='https://arxiv.org/pdf/2503.15167.pdf' target='_blank'>https://arxiv.org/pdf/2503.15167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fujian Yan, Hui Li, Hongsheng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15167">Volumetric Reconstruction From Partial Views for Task-Oriented Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object affordance and volumetric information are essential in devising effective grasping strategies under task-specific constraints. This paper presents an approach for inferring suitable grasping strategies from limited partial views of an object. To achieve this, a recurrent generative adversarial network (R-GAN) was proposed by incorporating a recurrent generator with long short-term memory (LSTM) units for it to process a variable number of depth scans. To determine object affordances, the AffordPose knowledge dataset is utilized as prior knowledge. Affordance retrieving is defined by the volume similarity measured via Chamfer Distance and action similarities. A Proximal Policy Optimization (PPO) reinforcement learning model is further implemented to refine the retrieved grasp strategies for task-oriented grasping. The retrieved grasp strategies were evaluated on a dual-arm mobile manipulation robot with an overall grasping accuracy of 89% for four tasks: lift, handle grasp, wrap grasp, and press.
<div id='section'>Paperid: <span id='pid'>1599, <a href='https://arxiv.org/pdf/2503.14960.pdf' target='_blank'>https://arxiv.org/pdf/2503.14960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungyeon Cho, Tae-Kyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14960">Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based Human Action Recognition (HAR) is a vital technology in robotics and human-robot interaction. However, most existing methods concentrate primarily on full-body movements and often overlook subtle hand motions that are critical for distinguishing fine-grained actions. Recent work leverages a unified graph representation that combines body, hand, and foot keypoints to capture detailed body dynamics. Yet, these models often blur fine hand details due to the disparity between body and hand action characteristics and the loss of subtle features during the spatial-pooling. In this paper, we propose BHaRNet (Body-Hand action Recognition Network), a novel framework that augments a typical body-expert model with a hand-expert model. Our model jointly trains both streams with an ensemble loss that fosters cooperative specialization, functioning in a manner reminiscent of a Mixture-of-Experts (MoE). Moreover, cross-attention is employed via an expertized branch method and a pooling-attention module to enable feature-level interactions and selectively fuse complementary information. Inspired by MMNet, we also demonstrate the applicability of our approach to multi-modal tasks by leveraging RGB information, where body features guide RGB learning to capture richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive actions -- while maintaining fewer GFLOPs and parameters than the relevant unified methods.
<div id='section'>Paperid: <span id='pid'>1600, <a href='https://arxiv.org/pdf/2503.14931.pdf' target='_blank'>https://arxiv.org/pdf/2503.14931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehud Nahum, Yael Edan, Tal Oron-Gilad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14931">Advancing a taxonomy for proxemics in robot social navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying robots in human environments requires effective social robot navigation. This article focuses on proxemics, proposing a new taxonomy and suggesting future directions through an analysis of state-of-the-art studies and the identification of research gaps. The various factors that affect the dynamic properties of proxemics patterns in human-robot interaction are thoroughly explored. To establish a coherent proxemics framework, we identified and organized the key parameters and attributes that shape proxemics behavior. Building on this framework, we introduce a novel approach to define proxemics in robot navigation, emphasizing the significant attributes that influence its structure and size. This leads to the development of a new taxonomy that serves as a foundation for guiding future research and development. Our findings underscore the complexity of defining personal distance, revealing it as a complex, multi-dimensional challenge. Furthermore, we highlight the flexible and dynamic nature of personal zone boundaries, which should be adaptable to different contexts and circumstances. Additionally, we propose a new layer for implementing proxemics in the navigation of social robots.
<div id='section'>Paperid: <span id='pid'>1601, <a href='https://arxiv.org/pdf/2503.14719.pdf' target='_blank'>https://arxiv.org/pdf/2503.14719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel S. Soriano-GarcÃ­a, Diego A. Mercado-Ravell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14719">ViVa-SAFELAND: a New Freeware for Safe Validation of Vision-based Navigation in Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ViVa-SAFELAND is an open source software library, aimed to test and evaluate vision-based navigation strategies for aerial vehicles, with special interest in autonomous landing, while complying with legal regulations and people's safety. It consists of a collection of high definition aerial videos, focusing on real unstructured urban scenarios, recording moving obstacles of interest, such as cars and people. Then, an Emulated Aerial Vehicle (EAV) with a virtual moving camera is implemented in order to ``navigate" inside the video, according to high-order commands. ViVa-SAFELAND provides a new, safe, simple and fair comparison baseline to evaluate and compare different visual navigation solutions under the same conditions, and to randomize variables along several trials. It also facilitates the development of autonomous landing and navigation strategies, as well as the generation of image datasets for different training tasks. Moreover, it is useful for training either human of autonomous pilots using deep learning. The effectiveness of the framework for validating vision algorithms is demonstrated through two case studies, detection of moving objects and risk assessment segmentation. To our knowledge, this is the first safe validation framework of its kind, to test and compare visual navigation solution for aerial vehicles, which is a crucial aspect for urban deployment in complex real scenarios.
<div id='section'>Paperid: <span id='pid'>1602, <a href='https://arxiv.org/pdf/2503.08174.pdf' target='_blank'>https://arxiv.org/pdf/2503.08174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Veronica Bot, Zheyuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08174">Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated unprecedented capability in reasoning with natural language. Coupled with this development is the emergence of embodied AI in robotics. Despite showing promise for verbal and written reasoning tasks, it remains unknown whether LLMs are capable of navigating complex spatial tasks with physical actions in the real world. To this end, it is of interest to investigate applying LLMs to robotics in zero-shot learning scenarios, and in the absence of fine-tuning - a feat which could significantly improve human-robot interaction, alleviate compute cost, and eliminate low-level programming tasks associated with robot tasks.
  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot in Webots engine for an object search task. We evaluate the effectiveness of three reasoning strategies based on Chain-of-Thought (CoT) sub-task list generation with the Socratic method (SocraCoT) (in order of increasing rigor): (1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was measured in terms of the proportion of tasks successfully completed and execution time (N = 20). Our preliminary results show that when combined with chain-of-thought reasoning, the Socratic method can be used for code generation for robotic tasks that require spatial awareness. In extension of this finding, we propose EVINCE-LoC; a modified EVINCE method that could further enhance performance in highly complex and or dynamic testing scenarios.
<div id='section'>Paperid: <span id='pid'>1603, <a href='https://arxiv.org/pdf/2503.05251.pdf' target='_blank'>https://arxiv.org/pdf/2503.05251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Scarciglia, Antonio Paolillo, Daniele Palossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05251">A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently entered the drone racing scenario, where they are tasked to avoid obstacles and navigate as fast as possible through gates. However, in contrast with their bigger counterparts, i.e., kg-scale drones, nano-drones expose three orders of magnitude less onboard memory and compute power, demanding more efficient and lightweight vision-based pipelines to win the race. This work presents a map-free vision-based (using only a monocular camera) autonomous nano-drone that combines a real-time deep learning gate detection front-end with a classic yet elegant and effective visual servoing control back-end, only relying on onboard resources. Starting from two state-of-the-art tiny deep learning models, we adapt them for our specific task, and after a mixed simulator-real-world training, we integrate and deploy them aboard our nano-drone. Our best-performing pipeline costs of only 24M multiply-accumulate operations per frame, resulting in a closed-loop control performance of 30 Hz, while achieving a gate detection root mean square error of 1.4 pixels, on our ~20k real-world image dataset. In-field experiments highlight the capability of our nano-drone to successfully navigate through 15 gates in 4 min, never crashing and covering a total travel distance of ~100m, with a peak flight speed of 1.9 m/s. Finally, to stress the generalization capability of our system, we also test it in a never-seen-before environment, where it navigates through gates for more than 4 min.
<div id='section'>Paperid: <span id='pid'>1604, <a href='https://arxiv.org/pdf/2503.04296.pdf' target='_blank'>https://arxiv.org/pdf/2503.04296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Cantucci, Marco Marini, Rino Falcone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04296">The Role of Robot Competence, Autonomy, and Personality on Trust Formation in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human trust in social robots is a complex attitude based on cognitive and emotional evaluations, as well as a behavior, like task delegation. While previous research explored the features of robots that influence overall trust attitude, it remains unclear whether these features affect behavioral trust. Additionally, there is limited investigation into which features of robots influence cognitive and emotional attitudes, and how these attitudes impact humans' willingness to delegate new tasks to robots. This study examines the interplay between competence, autonomy, and personality traits of robots and their impact on trust attitudes (cognitive and affective trust) and trust behavior (task delegation), within the context of task-oriented Human-Robot Interaction. Our findings indicate that robot competence is a key determinant of trust, influencing cognitive, affective, and behavioral trust. In contrast, robot personality traits significantly impact only affective trust without affecting cognitive trust or trust behavior. In addition, autonomy was found to moderate the relationship between competence and cognitive trust, as well as between personality and affective trust. Finally, cognitive trust was found to positively influence task delegation, whereas affective trust did not show a significant effect. This paper contributes to the literature on Human-Robot Trust by providing novel evidence that enhances the acceptance and effectiveness of social robots in collaborative scenarios.
<div id='section'>Paperid: <span id='pid'>1605, <a href='https://arxiv.org/pdf/2503.02525.pdf' target='_blank'>https://arxiv.org/pdf/2503.02525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Cooney, Alexey Vinel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02525">Magic in Human-Robot Interaction (HRI)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"Magic" is referred to here and there in the robotics literature, from "magical moments" afforded by a mobile bubble machine, to "spells" intended to entertain and motivate children--but what exactly could this concept mean for designers? Here, we present (1) some theoretical discussion on how magic could inform interaction designs based on reviewing the literature, followed by (2) a practical description of using such ideas to develop a simplified prototype, which received an award in an international robot magic competition. Although this topic can be considered unusual and some negative connotations exist (e.g., unrealistic thinking can be referred to as magical), our results seem to suggest that magic, in the experiential, supernatural, and illusory senses of the term, could be useful to consider in various robot design contexts, also for artifacts like home assistants and autonomous vehicles--thus, inviting further discussion and exploration.
<div id='section'>Paperid: <span id='pid'>1606, <a href='https://arxiv.org/pdf/2503.00676.pdf' target='_blank'>https://arxiv.org/pdf/2503.00676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishikesh Joshi, Junaed Sattar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00676">One-Shot Gesture Recognition for Underwater Diver-To-Robot Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable human-robot communication is essential for underwater human-robot interaction (U-HRI), yet traditional methods such as acoustic signaling and predefined gesture-based models suffer from limitations in adaptability and robustness. In this work, we propose One-Shot Gesture Recognition (OSG), a novel method that enables real-time, pose-based, temporal gesture recognition underwater from a single demonstration, eliminating the need for extensive dataset collection or model retraining. OSG leverages shape-based classification techniques, including Hu moments, Zernike moments, and Fourier descriptors, to robustly recognize gestures in visually-challenging underwater environments. Our system achieves high accuracy on real-world underwater data and operates efficiently on embedded hardware commonly found on autonomous underwater vehicles (AUVs), demonstrating its feasibility for deployment on-board robots. Compared to deep learning approaches, OSG is lightweight, computationally efficient, and highly adaptable, making it ideal for diver-to-robot communication. We evaluate OSG's performance on an augmented gesture dataset and real-world underwater video data, comparing its accuracy against deep learning methods. Our results show OSG's potential to enhance U-HRI by enabling the immediate deployment of user-defined gestures without the constraints of predefined gesture languages.
<div id='section'>Paperid: <span id='pid'>1607, <a href='https://arxiv.org/pdf/2503.00284.pdf' target='_blank'>https://arxiv.org/pdf/2503.00284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maaz Qureshi, Kerstin Dautenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00284">Human-Robot Collaboration: A Non-Verbal Approach with the NAO Humanoid Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots, particularly NAO, are gaining prominence for their potential to revolutionize human-robot collaboration, especially in domestic settings like kitchens. Leveraging the advantages of NAO, this research explores non-verbal communications role in enhancing human-robot interaction during meal preparation tasks. By employing gestures, body movements, and visual cues, NAO provides feedback to users, improving comprehension and safety. Our study investigates user perceptions of NAO feedback and its anthropomorphic attributes. Findings suggest that combining various non-verbal cues enhances communication effectiveness, although achieving full anthropomorphic likeness remains a challenge. Insights from this research inform the design of future robotic systems for improved human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>1608, <a href='https://arxiv.org/pdf/2502.15252.pdf' target='_blank'>https://arxiv.org/pdf/2502.15252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amartaivan Sanjjamts, Hiroshi Morita, Togootogtokh Enkhtogtokh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15252">Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding collective pedestrian movement is crucial for applications in crowd management, autonomous navigation, and human-robot interaction. This paper investigates the use of sequential deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers, for real-time flock detection in multi-pedestrian trajectories. Our proposed approach consists of a two-stage process: first, a pre-trained binary classification model is used for pairwise trajectory classification, and second, the learned representations are applied to identify multi-agent flocks dynamically.
  We validate our method using real-world group movement datasets, demonstrating its robustness across varying sequence lengths and diverse movement patterns. Experimental results indicate that our model consistently detects pedestrian flocks with high accuracy and stability, even in dynamic and noisy environments. Furthermore, we extend our approach to identify other forms of collective motion, such as convoys and swarms, paving the way for more comprehensive multi-agent behavior analysis.
<div id='section'>Paperid: <span id='pid'>1609, <a href='https://arxiv.org/pdf/2502.02967.pdf' target='_blank'>https://arxiv.org/pdf/2502.02967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bastien Muraccioli, Mathieu Celerier, Mehdi Benallegue, Gentiane Venture
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02967">Demonstrating a Control Framework for Physical Human-Robot Interaction Toward Industrial Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0, which focuses on human-centric approaches. However, few studies explore the practical alignment of pHRI to industrial-grade performance. This paper introduces a versatile control framework designed to bridge this gap by incorporating the torque-based control modes: compliance control, null-space compliance, and dual compliance, all in static and dynamic scenarios. Thanks to our second-order Quadratic Programming (QP) formulation, strict kinematic and collision constraints are integrated into the system as safety features, and a weighted hierarchy guarantees singularity-robust task tracking performance. The framework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped with a Bota force/torque sensor. A DualShock 4 game controller is attached to the robot's end-effector to demonstrate the framework's capabilities. This setup enables seamless dynamic switching between the modes, and real-time adjustments of parameters, such as transitioning between position and torque control or selecting a more robust custom-developed low-level torque controller over the default one. Built on the open-source robotic control software mc_rtc, our framework ensures reproducibility for both research and industrial deployment, this framework demonstrates a step toward industrial-grade performance and repeatability, showcasing its potential as a robust pHRI control system for industrial environments.
<div id='section'>Paperid: <span id='pid'>1610, <a href='https://arxiv.org/pdf/2502.02443.pdf' target='_blank'>https://arxiv.org/pdf/2502.02443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Qi Yang, Miaomiao Wang, Mehrdad R. Kermani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02443">A Null Space Compliance Approach for Maintaining Safety and Tracking Performance in Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the focus on developing robot manipulators has shifted towards prioritizing safety in Human-Robot Interaction (HRI). Impedance control is a typical approach for interaction control in collaboration tasks. However, such a control approach has two main limitations: 1) the end-effector (EE)'s limited compliance to adapt to unknown physical interactions, and 2) inability of the robot body to compliantly adapt to unknown physical interactions. In this work, we present an approach to address these drawbacks. We introduce a modified Cartesian impedance control method combined with a Dynamical System (DS)-based motion generator, aimed at enhancing the interaction capability of the EE without compromising main task tracking performance. This approach enables human coworkers to interact with the EE on-the-fly, e.g. tool changeover, after which the robot compliantly resumes its task. Additionally, combining with a new null space impedance control method enables the robot body to exhibit compliant behaviour in response to interactions, avoiding serious injuries from accidental contact while mitigating the impact on main task tracking performance. Finally, we prove the passivity of the system and validate the proposed approach through comprehensive comparative experiments on a 7 Degree-of-Freedom (DOF) KUKA LWR IV+ robot.
<div id='section'>Paperid: <span id='pid'>1611, <a href='https://arxiv.org/pdf/2501.17206.pdf' target='_blank'>https://arxiv.org/pdf/2501.17206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17206">Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores a novel approach to advancing dementia care by integrating socially assistive robotics, reinforcement learning (RL), large language models (LLMs), and clinical domain expertise within a simulated environment. This integration addresses the critical challenge of limited experimental data in socially assistive robotics for dementia care, providing a dynamic simulation environment that realistically models interactions between persons living with dementia (PLWDs) and robotic caregivers. The proposed framework introduces a probabilistic model to represent the cognitive and emotional states of PLWDs, combined with an LLM-based behavior simulation to emulate their responses. We further develop and train an adaptive RL system enabling humanoid robots, such as Pepper, to deliver context-aware and personalized interactions and assistance based on PLWDs' cognitive and emotional states. The framework also generalizes to computer-based agents, highlighting its versatility. Results demonstrate that the RL system, enhanced by LLMs, effectively interprets and responds to the complex needs of PLWDs, providing tailored caregiving strategies. This research contributes to human-computer and human-robot interaction by offering a customizable AI-driven caregiving platform, advancing understanding of dementia-related challenges, and fostering collaborative innovation in assistive technologies. The proposed approach has the potential to enhance the independence and quality of life for PLWDs while alleviating caregiver burden, underscoring the transformative role of interaction-focused AI systems in dementia care.
<div id='section'>Paperid: <span id='pid'>1612, <a href='https://arxiv.org/pdf/2501.00541.pdf' target='_blank'>https://arxiv.org/pdf/2501.00541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnan Rashid, Sa'ed Abed, Osman Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00541">Formalization of Biological Circuit Block Diagrams for formally analyzing Biomedical Control Systems in pHRI Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The control of Biomedical Systems in Physical Human-Robot Interaction (pHRI) plays a pivotal role in achieving the desired behavior by ensuring the intended transfer function and stability of subsystems within the overall system. Traditionally, the control aspects of biomedical systems have been analyzed using manual proofs and computer based analysis tools. However, these approaches provide inaccurate results due to human error in manual proofs and unverified algorithms and round-off errors in computer-based tools. We argue using Interactive reasoning, or frequently called theorem proving, to analyze control systems of biomedical engineering applications, specifically in the context of Physical Human-Robot Interaction (pHRI). Our methodology involves constructing mathematical models of the control components using Higher-order Logic (HOL) and analyzing them through deductive reasoning in the HOL Light theorem prover. We propose to model these control systems in terms of their block diagram representations, which in turn utilize the corresponding differential equations and their transfer function-based representation using the Laplace Transform (LT). These formally represented block diagrams are then analyzed through logical reasoning in the trusted environment of a theorem prover to ensure the correctness of the results. For illustration, we present a real-world case study by analyzing the control system of the ultrafilteration dialysis process.
<div id='section'>Paperid: <span id='pid'>1613, <a href='https://arxiv.org/pdf/2412.20632.pdf' target='_blank'>https://arxiv.org/pdf/2412.20632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordan Sinclair, Christopher Reardon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20632">EVOLVE: Emotion and Visual Output Learning via LLM Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human acceptance of social robots is greatly effected by empathy and perceived understanding. This necessitates accurate and flexible responses to various input data from the user. While systems such as this can become increasingly complex as more states or response types are included, new research in the application of large language models towards human-robot interaction has allowed for more streamlined perception and reaction pipelines. LLM-selected actions and emotional expressions can help reinforce the realism of displayed empathy and allow for improved communication between the robot and user. Beyond portraying empathy in spoken or written responses, this shows the possibilities of using LLMs in actuated, real world scenarios. In this work we extend research in LLM-driven nonverbal behavior for social robots by considering more open-ended emotional response selection leveraging new advances in vision-language models, along with emotionally aligned motion and color pattern selections that strengthen conveyance of meaning and empathy.
<div id='section'>Paperid: <span id='pid'>1614, <a href='https://arxiv.org/pdf/2412.18601.pdf' target='_blank'>https://arxiv.org/pdf/2412.18601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Jia, Jade Zheng, Florence Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18601">Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of GameFi, a fusion of gaming and decentralized finance (DeFi), there exists a critical need to enhance player engagement and economic interaction within gaming ecosystems. Our GameFi ecosystem aims to fundamentally transform this landscape by integrating advanced embodied AI agents into GameFi platforms. These AI agents, developed using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI, are capable of proactive, adaptive, and contextually rich interactions with players. By going beyond traditional scripted responses, these agents become integral participants in the game's narrative and economic systems, directly influencing player strategies and in-game economies. We address the limitations of current GameFi platforms, which often lack immersive AI interactions and mechanisms for community engagement or creator monetization. Through the deep integration of AI agents with blockchain technology, we establish a consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers creators to monetize their contributions and fosters democratic collaboration among players and creators. Furthermore, by embedding DeFi mechanisms into the gaming experience, we enhance economic participation and provide new opportunities for financial interactions within the game. Our approach enhances player immersion and retention and advances the GameFi ecosystem by bridging traditional gaming with Web3 technologies. By integrating sophisticated AI and DeFi elements, we contribute to the development of more engaging, economically robust, and community-centric gaming environments. This project represents a significant advancement in the state-of-the-art in GameFi, offering insights and methodologies that can be applied throughout the gaming industry.
<div id='section'>Paperid: <span id='pid'>1615, <a href='https://arxiv.org/pdf/2412.15462.pdf' target='_blank'>https://arxiv.org/pdf/2412.15462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ammar N. Abbas, Csaba Beleznai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15462">TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through Large/Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TalkWithMachines aims to enhance human-robot interaction by contributing to interpretable industrial robotic systems, especially for safety-critical applications. The presented paper investigates recent advancements in Large Language Models (LLMs) and Vision Language Models (VLMs), in combination with robotic perception and control. This integration allows robots to understand and execute commands given in natural language and to perceive their environment through visual and/or descriptive inputs. Moreover, translating the LLM's internal states and reasoning into text that humans can easily understand ensures that operators gain a clearer insight into the robot's current state and intentions, which is essential for effective and safe operation. Our paper outlines four LLM-assisted simulated robotic control workflows, which explore (i) low-level control, (ii) the generation of language-based feedback that describes the robot's internal states, (iii) the use of visual information as additional input, and (iv) the use of robot structure information for generating task plans and feedback, taking the robot's physical capabilities and limitations into account. The proposed concepts are presented in a set of experiments, along with a brief discussion. Project description, videos, and supplementary materials will be available on the project website: https://talk-machines.github.io.
<div id='section'>Paperid: <span id='pid'>1616, <a href='https://arxiv.org/pdf/2412.14837.pdf' target='_blank'>https://arxiv.org/pdf/2412.14837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Cao, Huangxun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14837">ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging Scenes with Subtly Distinguished Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding is an important task, and there has been a recent surge of research interest in aligning 3D representations of point clouds with text to empower embodied AI. However, due to the lack of comprehensive 3D benchmarks, the capabilities of 3D models in real-world scenes, particularly those that are challenging with subtly distinguished objects, remain insufficiently investigated. To facilitate a more thorough evaluation of 3D models' capabilities, we propose a scheme, ObjVariantEnsemble, to systematically introduce more scenes with specified object classes, colors, shapes, quantities, and spatial relationships to meet model evaluation needs. More importantly, we intentionally construct scenes with similar objects to a certain degree and design an LLM-VLM-cooperated annotator to capture key distinctions as annotations. The resultant benchmark can better challenge 3D models, reveal their shortcomings in understanding, and potentially aid in the further development of 3D models.
<div id='section'>Paperid: <span id='pid'>1617, <a href='https://arxiv.org/pdf/2412.13474.pdf' target='_blank'>https://arxiv.org/pdf/2412.13474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Haninger, Luka Peternel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13474">Planning Human-Robot Co-manipulation with Human Motor Control Objectives and Multi-component Reaching Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For successful goal-directed human-robot interaction, the robot should adapt to the intentions and actions of the collaborating human. This can be supported by musculoskeletal or data-driven human models, where the former are limited to lower-level functioning such as ergonomics, and the latter have limited generalizability or data efficiency. What is missing, is the inclusion of human motor control models that can provide generalizable human behavior estimates and integrate into robot planning methods. We use well-studied models from human motor control based on the speed-accuracy and cost-benefit trade-offs to plan collaborative robot motions. In these models, the human trajectory minimizes an objective function, a formulation we adapt to numerical trajectory optimization. This can then be extended with constraints and new variables to realize collaborative motion planning and goal estimation. We deploy this model, as well as a multi-component movement strategy, in physical collaboration with uncertain goal-reaching and synchronized motion tasks, showing the ability of the approach to produce human-like trajectories over a range of conditions.
<div id='section'>Paperid: <span id='pid'>1618, <a href='https://arxiv.org/pdf/2412.10599.pdf' target='_blank'>https://arxiv.org/pdf/2412.10599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikunj Sanghai, Nik Bear Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10599">Advances in Transformers for Robotic Applications: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of Transformers architecture has brought about significant breakthroughs in Deep Learning (DL), particularly within Natural Language Processing (NLP). Since their inception, Transformers have outperformed many traditional neural network architectures due to their "self-attention" mechanism and their scalability across various applications. In this paper, we cover the use of Transformers in Robotics. We go through recent advances and trends in Transformer architectures and examine their integration into robotic perception, planning, and control for autonomous systems. Furthermore, we review past work and recent research on use of Transformers in Robotics as pre-trained foundation models and integration of Transformers with Deep Reinforcement Learning (DRL) for autonomous systems. We discuss how different Transformer variants are being adapted in robotics for reliable planning and perception, increasing human-robot interaction, long-horizon decision-making, and generalization. Finally, we address limitations and challenges, offering insight and suggestions for future research directions.
<div id='section'>Paperid: <span id='pid'>1619, <a href='https://arxiv.org/pdf/2412.06469.pdf' target='_blank'>https://arxiv.org/pdf/2412.06469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Craig Vear, Johann Benerradi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06469">Jess+: designing embodied AI for interactive music-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we discuss the conceptualisation and design of embodied AI within an inclusive music-making project. The central case study is Jess+ an intelligent digital score system for shared creativity with a mixed ensemble of non-disabled and disabled musicians. The overarching aim is that the digital score enables disabled musicians to thrive in a live music conversation with other musicians regardless of the potential barriers of disability and music-making. After defining what we mean by embodied AI and how this approach supports the aims of the Jess+ project, we outline the main design features of the system. This includes several novel approaches such as its modular design, an AI Factory based on an embodied musicking dataset, and an embedded belief system. Our findings showed that the implemented design decisions and embodied-AI approach led to rich experiences for the musicians which in turn transformed their practice as an inclusive ensemble.
<div id='section'>Paperid: <span id='pid'>1620, <a href='https://arxiv.org/pdf/2412.04820.pdf' target='_blank'>https://arxiv.org/pdf/2412.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Dietzel, Patrick J. Martin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04820">Assessing Similarity Measures for the Evaluation of Human-Robot Motion Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One key area of research in Human-Robot Interaction is solving the human-robot correspondence problem, which asks how a robot can learn to reproduce a human motion demonstration when the human and robot have different dynamics and kinematic structures. Evaluating these correspondence problem solutions often requires the use of qualitative surveys that can be time consuming to design and administer. Additionally, qualitative survey results vary depending on the population of survey participants. In this paper, we propose the use of heterogeneous time-series similarity measures as a quantitative evaluation metric for evaluating motion correspondence to complement these qualitative surveys. To assess the suitability of these measures, we develop a behavioral cloning-based motion correspondence model, and evaluate it with a qualitative survey as well as quantitative measures. By comparing the resulting similarity scores with the human survey results, we identify Gromov Dynamic Time Warping as a promising quantitative measure for evaluating motion correspondence.
<div id='section'>Paperid: <span id='pid'>1621, <a href='https://arxiv.org/pdf/2412.02655.pdf' target='_blank'>https://arxiv.org/pdf/2412.02655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Doma, Aliasghar Arab, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02655">LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous navigation guided by natural language instructions is essential for improving human-robot interaction and enabling complex operations in dynamic environments. While large language models (LLMs) are not inherently designed for planning, they can significantly enhance planning efficiency by providing guidance and informing constraints to ensure safety. This paper introduces a planning framework that integrates LLMs with 2D occupancy grid maps and natural language commands to improve spatial reasoning and task execution in resource-limited settings. By decomposing high-level commands and real-time environmental data, the system generates structured navigation plans for pick-and-place tasks, including obstacle avoidance, goal prioritization, and adaptive behaviors. The framework dynamically recalculates paths to address environmental changes and aligns with implicit social norms for seamless human-robot interaction. Our results demonstrates the potential of LLMs to design context-aware system to enhance navigation efficiency and safety in industrial and dynamic environments.
<div id='section'>Paperid: <span id='pid'>1622, <a href='https://arxiv.org/pdf/2411.18587.pdf' target='_blank'>https://arxiv.org/pdf/2411.18587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suzanne Oliver, Tomoko Kitago, Adam Buchwald, S. Farokh Atashzar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18587">EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot Interaction: Modulating Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User engagement, cognitive participation, and motivation during task execution in physical human-robot interaction are crucial for motor learning. These factors are especially important in contexts like robotic rehabilitation, where neuroplasticity is targeted. However, traditional robotic rehabilitation systems often face challenges in maintaining user engagement, leading to unpredictable therapeutic outcomes. To address this issue, various techniques, such as assist-as-needed controllers, have been developed to prevent user slacking and encourage active participation. In this paper, we introduce a new direction through a novel multi-modal robotic interaction designed to enhance user engagement by synergistically integrating visual, motor, cognitive, and auditory (speech recognition) tasks into a single, comprehensive activity. To assess engagement quantitatively, we compared multiple electroencephalography (EEG) biomarkers between this multi-modal protocol and a traditional motor-only protocol. Fifteen healthy adult participants completed 100 trials of each task type. Our findings revealed that EEG biomarkers, particularly relative alpha power, showed statistically significant improvements in engagement during the multi-modal task compared to the motor-only task. Moreover, while engagement decreased over time in the motor-only task, the multi-modal protocol maintained consistent engagement, suggesting that users could remain engaged for longer therapy sessions. Our observations on neural responses during interaction indicate that the proposed multi-modal approach can effectively enhance user engagement, which is critical for improving outcomes. This is the first time that objective neural response highlights the benefit of a comprehensive robotic intervention combining motor, cognitive, and auditory functions in healthy subjects.
<div id='section'>Paperid: <span id='pid'>1623, <a href='https://arxiv.org/pdf/2411.16723.pdf' target='_blank'>https://arxiv.org/pdf/2411.16723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mitchell Rosser, Marc. G Carmichael
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16723">Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent development of natural language generation models - termed as large language models (LLMs) - a potential use case has opened up to improve the way that humans interact with robot assistants. These LLMs should be able to leverage their large breadth of understanding to interpret natural language commands into effective, task appropriate and safe robot task executions. However, in reality, these models suffer from hallucinations, which may cause safety issues or deviations from the task. In other domains, these issues have been improved through the use of collaborative AI systems where multiple LLM agents can work together to collectively plan, code and self-check outputs. In this research, multiple collaborative AI systems were tested against a single independent AI agent to determine whether the success in other domains would translate into improved human-robot interaction performance. The results show that there is no defined trend between the number of agents and the success of the model. However, it is clear that some collaborative AI agent architectures can exhibit a greatly improved capacity to produce error-free code and to solve abstract problems.
<div id='section'>Paperid: <span id='pid'>1624, <a href='https://arxiv.org/pdf/2411.05122.pdf' target='_blank'>https://arxiv.org/pdf/2411.05122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leanne Oon Hui Yee, Siew Sui Fun, Thit Sar Zin, Zar Nie Aung, Kian Meng Yap, Jiehan Teoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05122">Socially Assistive Robots: A Technological Approach to Emotional Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's high-pressure and isolated society, the demand for emotional support has surged, necessitating innovative solutions. Socially Assistive Robots (SARs) offer a technological approach to providing emotional assistance by leveraging advanced robotics, artificial intelligence, and sensor technologies. This study explores the development of an emotional support robot designed to detect and respond to human emotions, particularly sadness, through facial recognition and gesture analysis. Utilising the Lego Mindstorms Robotic Kit, Raspberry Pi 4, and various Python libraries, the robot is capable of delivering empathetic interactions, including comforting hugs and AI-generated conversations. Experimental findings highlight the robot's effective facial recognition accuracy, user interaction, and hug feedback mechanisms. These results demonstrate the feasibility of using SARs for emotional support, showcasing their potential features and functions. This research underscores the promise of SARs in providing innovative emotional assistance and enhancing human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1625, <a href='https://arxiv.org/pdf/2411.01120.pdf' target='_blank'>https://arxiv.org/pdf/2411.01120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengyu Hou, Hanming Bai, Ye Ding, Han Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01120">Generation of Conservative Dynamical Systems Based on Stiffness Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamical systems (DSs) provide a framework for high flexibility, robustness, and control reliability and are widely used in motion planning and physical human-robot interaction. The properties of the DS directly determine the robot's specific motion patterns and the performance of the closed-loop control system. In this paper, we establish a quantitative relationship between stiffness properties and DS. We propose a stiffness encoding framework to modulate DS properties by embedding specific stiffnesses. In particular, from the perspective of the closed-loop control system's passivity, a conservative DS is learned by encoding a conservative stiffness. The generated DS has a symmetric attraction behavior and a variable stiffness profile. The proposed method is applicable to demonstration trajectories belonging to different manifolds and types (e.g., closed and self-intersecting trajectories), and the closed-loop control system is always guaranteed to be passive in different cases. For controllers tracking the general DS, the passivity of the system needs to be guaranteed by the energy tank. We further propose a generic vector field decomposition strategy based on conservative stiffness, which effectively slows down the decay rate of energy in the energy tank and improves the stability margin of the control system. Finally, a series of simulations in various scenarios and experiments on planar and curved motion tasks demonstrate the validity of our theory and methodology.
<div id='section'>Paperid: <span id='pid'>1626, <a href='https://arxiv.org/pdf/2410.19564.pdf' target='_blank'>https://arxiv.org/pdf/2410.19564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyou Zhou, Oleg Sinavski, Athanasios Polydoros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19564">Robotic Learning in your Backyard: A Neural Simulator from Open Source Components</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of 3D Gaussian Splatting for fast and high-quality novel view synthesize has opened up the possibility to construct photo-realistic simulations from video for robotic reinforcement learning. While the approach has been demonstrated in several research papers, the software tools used to build such a simulator remain unavailable or proprietary. We present SplatGym, an open source neural simulator for training data-driven robotic control policies. The simulator creates a photorealistic virtual environment from a single video. It supports ego camera view generation, collision detection, and virtual object in-painting. We demonstrate training several visual navigation policies via reinforcement learning. SplatGym represents a notable first step towards an open-source general-purpose neural environment for robotic learning. It broadens the range of applications that can effectively utilise reinforcement learning by providing convenient and unrestricted tooling, and by eliminating the need for the manual development of conventional 3D environments.
<div id='section'>Paperid: <span id='pid'>1627, <a href='https://arxiv.org/pdf/2410.14969.pdf' target='_blank'>https://arxiv.org/pdf/2410.14969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marie Roald, Magnus Breder Birkenes, Lars GunnarsÃ¸nn BagÃ¸ien Johnsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14969">Visual Navigation of Digital Libraries: Retrieval and Classification of Images in the National Library of Norway's Digitised Book Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital tools for text analysis have long been essential for the searchability and accessibility of digitised library collections. Recent computer vision advances have introduced similar capabilities for visual materials, with deep learning-based embeddings showing promise for analysing visual heritage. Given that many books feature visuals in addition to text, taking advantage of these breakthroughs is critical to making library collections open and accessible. In this work, we present a proof-of-concept image search application for exploring images in the National Library of Norway's pre-1900 books, comparing Vision Transformer (ViT), Contrastive Language-Image Pre-training (CLIP), and Sigmoid loss for Language-Image Pre-training (SigLIP) embeddings for image retrieval and classification. Our results show that the application performs well for exact image retrieval, with SigLIP embeddings slightly outperforming CLIP and ViT in both retrieval and classification tasks. Additionally, SigLIP-based image classification can aid in cleaning image datasets from a digitisation pipeline.
<div id='section'>Paperid: <span id='pid'>1628, <a href='https://arxiv.org/pdf/2410.14337.pdf' target='_blank'>https://arxiv.org/pdf/2410.14337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chinmaya Mishra, Gabriel Skantze, Peter Hagoort, Rinus Verdonschot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14337">Perception of Emotions in Human and Robot Faces: Is the Eye Region Enough?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increased interest in developing next-gen social robots has raised questions about the factors affecting the perception of robot emotions. This study investigates the impact of robot appearances (humanlike, mechanical) and face regions (full-face, eye-region) on human perception of robot emotions. A between-subjects user study (N = 305) was conducted where participants were asked to identify the emotions being displayed in videos of robot faces, as well as a human baseline. Our findings reveal three important insights for effective social robot face design in Human-Robot Interaction (HRI): Firstly, robots equipped with a back-projected, fully animated face - regardless of whether they are more human-like or more mechanical-looking - demonstrate a capacity for emotional expression comparable to that of humans. Secondly, the recognition accuracy of emotional expressions in both humans and robots declines when only the eye region is visible. Lastly, within the constraint of only the eye region being visible, robots with more human-like features significantly enhance emotion recognition.
<div id='section'>Paperid: <span id='pid'>1629, <a href='https://arxiv.org/pdf/2410.04173.pdf' target='_blank'>https://arxiv.org/pdf/2410.04173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard C. Rodriguez, Jonah Elijah P. Bardos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04173">Fast Object Detection with a Machine Learning Edge Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This machine learning study investigates a lowcost edge device integrated with an embedded system having computer vision and resulting in an improved performance in inferencing time and precision of object detection and classification. A primary aim of this study focused on reducing inferencing time and low-power consumption and to enable an embedded device of a competition-ready autonomous humanoid robot and to support real-time object recognition, scene understanding, visual navigation, motion planning, and autonomous navigation of the robot. This study compares processors for inferencing time performance between a central processing unit (CPU), a graphical processing unit (GPU), and a tensor processing unit (TPU). CPUs, GPUs, and TPUs are all processors that can be used for machine learning tasks. Related to the aim of supporting an autonomous humanoid robot, there was an additional effort to observe whether or not there was a significant difference in using a camera having monocular vision versus stereo vision capability. TPU inference time results for this study reflect a 25% reduction in time over the GPU, and a whopping 87.5% reduction in inference time compared to the CPU. Much information in this paper is contributed to the final selection of Google's Coral brand, Edge TPU device. The Arduino Nano 33 BLE Sense Tiny ML Kit was also considered for comparison but due to initial incompatibilities and in the interest of time to complete this study, a decision was made to review the kit in a future experiment.
<div id='section'>Paperid: <span id='pid'>1630, <a href='https://arxiv.org/pdf/2410.00517.pdf' target='_blank'>https://arxiv.org/pdf/2410.00517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oscar Gil Viyuela, Alberto Sanfeliu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00517">Human-Robot Collaborative Minimum Time Search through Sub-priors in Ant Colony Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-Robot Collaboration (HRC) has evolved into a highly promising issue owing to the latest breakthroughs in Artificial Intelligence (AI) and Human-Robot Interaction (HRI), among other reasons. This emerging growth increases the need to design multi-agent algorithms that can manage also human preferences. This paper presents an extension of the Ant Colony Optimization (ACO) meta-heuristic to solve the Minimum Time Search (MTS) task, in the case where humans and robots perform an object searching task together. The proposed model consists of two main blocks. The first one is a convolutional neural network (CNN) that provides the prior probabilities about where an object may be from a segmented image. The second one is the Sub-prior MTS-ACO algorithm (SP-MTS-ACO), which takes as inputs the prior probabilities and the particular search preferences of the agents in different sub-priors to generate search plans for all agents. The model has been tested in real experiments for the joint search of an object through a Vizanti web-based visualization in a tablet computer. The designed interface allows the communication between a human and our humanoid robot named IVO. The obtained results show an improvement in the search perception of the users without loss of efficiency.
<div id='section'>Paperid: <span id='pid'>1631, <a href='https://arxiv.org/pdf/2409.15305.pdf' target='_blank'>https://arxiv.org/pdf/2409.15305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan M. Deniz, Andre S. Kelboucas, Ricardo Bedin Grando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15305">Real-time Robotics Situation Awareness for Accident Prevention in Industry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores human-robot interaction (HRI) based on a mobile robot and YOLO to increase real-time situation awareness and prevent accidents in the workplace. Using object segmentation, we propose an approach that is capable of analyzing these situations in real-time and providing useful information to avoid critical working situations. In the industry, ensuring the safety of workers is paramount, and solutions based on robots and AI can provide a safer environment. For that, we proposed a methodology evaluated with two different YOLO versions (YOLOv8 and YOLOv5) alongside a LoCoBot robot for supervision and to perform the interaction with a user. We show that our proposed approach is capable of navigating a test scenario and issuing alerts via Text-to-Speech when dangerous situations are faced, such as when hardhats and safety vests are not detected. Based on the results gathered, we can conclude that our system is capable of detecting and informing risk situations such as helmet/no helmet and safety vest/no safety vest situations.
<div id='section'>Paperid: <span id='pid'>1632, <a href='https://arxiv.org/pdf/2409.11906.pdf' target='_blank'>https://arxiv.org/pdf/2409.11906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youssef Mohamed, Severin Lemaignan, Arzu Guneysu, Patric Jensfelt, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11906">Fusion in Context: A Multimodal Approach to Affective State Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recognition of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. However, emotional expressions can be influenced by contextual factors, leading to misinterpretations if context is not considered. Multimodal fusion, combining modalities like facial expressions, speech, and physiological signals, has shown promise in improving affect recognition. This paper proposes a transformer-based multimodal fusion approach that leverages facial thermal data, facial action units, and textual context information for context-aware emotion recognition. We explore modality-specific encoders to learn tailored representations, which are then fused using additive fusion and processed by a shared transformer encoder to capture temporal dependencies and interactions. The proposed method is evaluated on a dataset collected from participants engaged in a tangible tabletop Pacman game designed to induce various affective states. Our results demonstrate the effectiveness of incorporating contextual information and multimodal fusion for affective state recognition.
<div id='section'>Paperid: <span id='pid'>1633, <a href='https://arxiv.org/pdf/2409.10687.pdf' target='_blank'>https://arxiv.org/pdf/2409.10687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10687">Personalized Speech Emotion Recognition in Human-Robot Interaction using Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotions are an essential element in verbal communication, so understanding individuals' affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT/BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs.
<div id='section'>Paperid: <span id='pid'>1634, <a href='https://arxiv.org/pdf/2409.10048.pdf' target='_blank'>https://arxiv.org/pdf/2409.10048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wessel Ledder, Yuzhen Qin, Kiki van der Heijden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10048">Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep reinforcement learning (DRL) approaches in audio signal processing have seen substantial progress in recent years, audio-driven DRL for tasks such as navigation, gaze control and head-orientation control in the context of human-robot interaction have received little attention. Here, we propose an audio-driven DRL framework in which we utilise deep Q-learning to develop an autonomous agent that orients towards a talker in the acoustic environment based on stereo speech recordings. Our results show that the agent learned to perform the task at a near perfect level when trained on speech segments in anechoic environments (that is, without reverberation). The presence of reverberation in naturalistic acoustic environments affected the agent's performance, although the agent still substantially outperformed a baseline, randomly acting agent. Finally, we quantified the degree of generalization of the proposed DRL approach across naturalistic acoustic environments. Our experiments revealed that policies learned by agents trained on medium or high reverb environments generalized to low reverb environments, but policies learned by agents trained on anechoic or low reverb environments did not generalize to medium or high reverb environments. Taken together, this study demonstrates the potential of audio-driven DRL for tasks such as head-orientation control and highlights the need for training strategies that enable robust generalization across environments for real-world audio-driven DRL applications.
<div id='section'>Paperid: <span id='pid'>1635, <a href='https://arxiv.org/pdf/2409.09429.pdf' target='_blank'>https://arxiv.org/pdf/2409.09429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Hostettler, Simon Mayer, Jan Liam Albert, Kay Erik Jenss, Christian Hildebrand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09429">Real-Time Adaptive Industrial Robots: Improving Safety And Comfort In Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial robots become increasingly prevalent, resulting in a growing need for intuitive, comforting human-robot collaboration. We present a user-aware robotic system that adapts to operator behavior in real time while non-intrusively monitoring physiological signals to create a more responsive and empathetic environment. Our prototype dynamically adjusts robot speed and movement patterns while measuring operator pupil dilation and proximity. Our user study compares this adaptive system to a non-adaptive counterpart, and demonstrates that the adaptive system significantly reduces both perceived and physiologically measured cognitive load while enhancing usability. Participants reported increased feelings of comfort, safety, trust, and a stronger sense of collaboration when working with the adaptive robot. This highlights the potential of integrating real-time physiological data into human-robot interaction paradigms. This novel approach creates more intuitive and collaborative industrial environments where robots effectively 'read' and respond to human cognitive states, and we feature all data and code for future use.
<div id='section'>Paperid: <span id='pid'>1636, <a href='https://arxiv.org/pdf/2409.08253.pdf' target='_blank'>https://arxiv.org/pdf/2409.08253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08253">The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The question of how cyber-physical systems should interact with human partners that can take over control or exert oversight is becoming more pressing, as these systems are deployed for an ever larger range of tasks. Drawing on the literatures on handing over control during semi-autonomous driving and human-robot interaction, we propose a design of a take-over request that combines an abstract pre-alert with an informative TOR: Relevant sensor information is highlighted on the controller's display, while a spoken message verbalizes the reason for the TOR. We conduct our study in the context of a semi-autonomous drone control scenario as our testbed. The goal of our online study is to assess in more detail what form a language-based TOR should take. Specifically, we compare a full sentence condition to shorter fragments, and test whether the visual highlighting should be done synchronously or asynchronously with the speech. Participants showed a higher accuracy in choosing the correct solution with our bi-modal TOR and felt that they were better able to recognize the critical situation. Using only fragments in the spoken message rather than full sentences did not lead to improved accuracy or faster reactions. Also, synchronizing the visual highlighting with the spoken message did not result in better accuracy and response times were even increased in this condition.
<div id='section'>Paperid: <span id='pid'>1637, <a href='https://arxiv.org/pdf/2409.05010.pdf' target='_blank'>https://arxiv.org/pdf/2409.05010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyi Tang, Christian Dondrup
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05010">Gesture Generation from Trimodal Context for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural co-speech gestures are essential components to improve the experience of Human-robot interaction (HRI). However, current gesture generation approaches have many limitations of not being natural, not aligning with the speech and content, or the lack of diverse speaker styles. Therefore, this work aims to repoduce the work by Yoon et,al generating natural gestures in simulation based on tri-modal inputs and apply this to a robot. During evaluation, ``motion variance'' and ``Frechet Gesture Distance (FGD)'' is employed to evaluate the performance objectively. Then, human participants were recruited to subjectively evaluate the gestures. Results show that the movements in that paper have been successfully transferred to the robot and the gestures have diverse styles and are correlated with the speech. Moreover, there is a significant likeability and style difference between different gestures.
<div id='section'>Paperid: <span id='pid'>1638, <a href='https://arxiv.org/pdf/2408.15864.pdf' target='_blank'>https://arxiv.org/pdf/2408.15864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timothée Dhaussy, Bassam Jabaian, Fabrice Lefèvre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15864">FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of autonomous systems in the context of human-robot interaction systems necessitates a synergy between the continuous perception of the environment and the potential actions to navigate or interact within it. We present Flowact, a proactive multimodal human-robot interaction architecture, working as an asynchronous endless loop of robot sensors into actuators and organized by two controllers, the Environment State Tracking (EST) and the Action Planner. The EST continuously collects and publishes a representation of the operative environment, ensuring a steady flow of perceptual data. This persistent perceptual flow is pivotal for our advanced Action Planner which orchestrates a collection of modular action subsystems, such as movement and speaking modules, governing their initiation or cessation based on the evolving environmental narrative. The EST employs a fusion of diverse sensory modalities to build a rich, real-time representation of the environment that is distributed to the Action Planner. This planner uses a decision-making framework to dynamically coordinate action modules, allowing them to respond proactively and coherently to changes in the environment. Through a series of real-world experiments, we exhibit the efficacy of the system in maintaining a continuous perception-action loop, substantially enhancing the responsiveness and adaptability of autonomous pro-active agents. The modular architecture of the action subsystems facilitates easy extensibility and adaptability to a broad spectrum of tasks and scenarios.
<div id='section'>Paperid: <span id='pid'>1639, <a href='https://arxiv.org/pdf/2408.14322.pdf' target='_blank'>https://arxiv.org/pdf/2408.14322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristina Getson, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14322">Investigating Persuasive Socially Assistive Robot Behavior Strategies for Sustained Engagement in Long-Term Care</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Socially assistive robots are increasingly being used to support the social, cognitive, and physical well-being of those who provide care (healthcare professionals) and those in need of care (older adults). However, the effectiveness of persuasive socially assistive robot behaviors and their impact on the sustained motivation of older adults is still not well understood. This extended abstract describes our prior human-robot interaction study on investigating the effectiveness of persuasive social robot behaviors with care providers, followed by our current research assessing the impact of these persuasive robot behaviors on the well-being of older adults in long-term care. The findings provide insights into engagement and sustained motivation of older adults when providing assistance.
<div id='section'>Paperid: <span id='pid'>1640, <a href='https://arxiv.org/pdf/2408.13394.pdf' target='_blank'>https://arxiv.org/pdf/2408.13394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Scicluna, Cedric Le Gentil, Sheila Sutjipto, Gavin Paul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13394">Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing adoption of human-robot interaction presents opportunities for technology to positively impact lives, particularly those with visual impairments, through applications such as guide-dog-like assistive robotics. We present a pipeline exploring the perception and "intelligent disobedience" required by such a system. A dataset of two people moving in and out of view has been prepared to compare RGB-based and event-based multi-modal dynamic object detection using LiDAR data for 3D position localisation. Our analysis highlights challenges in accurate 3D localisation using 2D image-LiDAR fusion, indicating the need for further refinement. Compared to the performance of the frame-based detection algorithm utilised (YOLOv4), current cutting-edge event-based detection models appear limited to contextual scenarios, such as for automotive platforms. This is highlighted by weak precision and recall over varying confidence and Intersection over Union (IoU) thresholds when using frame-based detections as a ground truth. Therefore, we have publicly released this dataset to the community, containing RGB, event, point cloud and Inertial Measurement Unit (IMU) data along with ground truth poses for the two people in the scene to fill a gap in the current landscape of publicly available datasets and provide a means to assist in the development of safer and more robust algorithms in the future: https://uts-ri.github.io/revel/.
<div id='section'>Paperid: <span id='pid'>1641, <a href='https://arxiv.org/pdf/2408.08111.pdf' target='_blank'>https://arxiv.org/pdf/2408.08111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. Hasanlu, M. Siavashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08111">Analytical Model of Modular Upper Limb Rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Configurable robots are made up of robotic modules that can be assembled or can configure themselves into multiple robot configurations. In this research plan, a method for upper-body rehabilitation will be discussed in the form of a modular robot with different morphologies. The advantage and superiority of designing an example of a robotic module for upper body rehabilitation is the ability to reset the modular robot system. In this research, a number of modules will be designed and implemented according to the needs of one-hand rehabilitation with different degrees of freedom. The design modules' performance and efficiency will be evaluated by simulating, making samples, and testing them. This article's research includes presenting a modular upper body rehabilitation robot in the wrist, elbow, and shoulder areas, as well as providing a suitable kinematic and dynamic model of the upper body rehabilitation robot to determine human-robot interaction forces and movement. The research also involves analyzing the mathematical model of the upper body rehabilitation robot to identify advanced control strategies that rely on force control and torque control. After reviewing the articles and research of others, we concluded that no one has yet worked on the design of a prototype robotic module for upper body rehabilitation in the specified order. In our pioneering research, we intend to address this important matter.
<div id='section'>Paperid: <span id='pid'>1642, <a href='https://arxiv.org/pdf/2408.05516.pdf' target='_blank'>https://arxiv.org/pdf/2408.05516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Figari Tomenotti, Nicoletta Noceti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05516">Anticipation through Head Pose Estimation: a preliminary study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to anticipate others' goals and intentions is at the basis of human-human social interaction. Such ability, largely based on non-verbal communication, is also a key to having natural and pleasant interactions with artificial agents, like robots. In this work, we discuss a preliminary experiment on the use of head pose as a visual cue to understand and anticipate action goals, particularly reaching and transporting movements. By reasoning on the spatio-temporal connections between the head, hands and objects in the scene, we will show that short-range anticipation is possible, laying the foundations for future applications to human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1643, <a href='https://arxiv.org/pdf/2408.05301.pdf' target='_blank'>https://arxiv.org/pdf/2408.05301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marie Charbonneau, Francisco Javier Andrade Chavez, Katja Mombaur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05301">Dancing with REEM-C: A robot-to-human physical-social communication study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans often work closely together and relay a wealth of information through physical interaction. Robots, on the other hand, are not yet able to work similarly closely with humans and to effectively convey information when engaging in physical-social human-robot interaction (psHRI). This currently limits the potential of human-robot collaboration to solve real-world problems. This paper investigates how to establish clear and intuitive robot-to-human communication, while considering human comfort during psHRI. We approach this question from the perspective of a leader-follower dancing scenario, in which a full-body humanoid robot leads a human by signaling the next steps through a choice of communication modalities including haptic, visual, and audio signals. This is achieved through the development of a split whole-body control framework combining admittance and impedance control on the upper body, with position control on the lower body for balancing and stepping. Robot-led psHRI participant experiments allowed us to verify controller performance, as well as to build an understanding of what types of communication work better from the perspective of human partners, particularly in terms of perceived effectiveness and comfort.
<div id='section'>Paperid: <span id='pid'>1644, <a href='https://arxiv.org/pdf/2407.16956.pdf' target='_blank'>https://arxiv.org/pdf/2407.16956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Savery, Fouad Sukkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16956">Long-Term, Store-Front Robotics: Interactive Music for Robotic Arm, Caxixi and Frame Drums</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an innovative exploration into the integration of interactive robotic musicianship within a commercial retail environment, specifically through a three-week-long in-store installation featuring a UR3 robotic arm, custom-built frame drums, and an adaptive music generation system. Situated in a prominent storefront in one of the world's largest cities, this project aimed to enhance the shopping experience by creating dynamic, engaging musical interactions that respond to the store's ambient soundscape. Key contributions include the novel application of industrial robotics in artistic expression, the deployment of interactive music to enrich retail ambiance, and the demonstration of continuous robotic operation in a public setting over an extended period. Challenges such as system reliability, variation in musical output, safety in interactive contexts, and brand alignment were addressed to ensure the installation's success. The project not only showcased the technical feasibility and artistic potential of robotic musicianship in retail spaces but also offered insights into the practical implications of such integration, including system reliability, the dynamics of human-robot interaction, and the impact on store operations. This exploration opens new avenues for enhancing consumer retail experiences through the intersection of technology, music, and interactive art, suggesting a future where robotic musicianship contributes meaningfully to public and commercial spaces.
<div id='section'>Paperid: <span id='pid'>1645, <a href='https://arxiv.org/pdf/2407.16874.pdf' target='_blank'>https://arxiv.org/pdf/2407.16874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Genova, Eric Cabrera, Vedhus Hoskere
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16874">Vision-Based Adaptive Robotics for Autonomous Surface Crack Repair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface cracks in infrastructure can lead to severe deterioration and expensive maintenance if not efficiently repaired. Manual repair methods are labor-intensive, time-consuming, and imprecise. While advancements in robotic perception and manipulation have progressed autonomous crack repair, three key challenges remain: accurate localization in the robot's coordinate frame, adaptability to varying crack sizes, and realistic validation of repairs. We present an adaptive, autonomous robotic system for surface crack detection and repair using advanced sensing technologies to enhance precision and safety for humans. A laser scanner is used to refine crack coordinates for accurate localization. Furthermore, our adaptive crack filling approach outperforms fixed speed techniques in efficiency and consistency. We validate our method using 3D printed cracks under realistic conditions, demonstrating repeatable testing. This research contributes to the field of human-robot interaction by reducing manual labor, improving safety, and streamlining maintenance operations, ultimately paving the way for more sophisticated and integrated construction robotics.
<div id='section'>Paperid: <span id='pid'>1646, <a href='https://arxiv.org/pdf/2407.14161.pdf' target='_blank'>https://arxiv.org/pdf/2407.14161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pouya P. Niaz, Engin Erzin, Cagatay Basdogan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14161">Efficient and Safe Contact-rich pHRI via Subtask Detection and Motion Estimation using Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an adaptive admittance controller for improving efficiency and safety in physical human-robot interaction (pHRI) tasks in small-batch manufacturing that involve contact with stiff environments, such as drilling, polishing, cutting, etc. We aim to minimize human effort and task completion time while maximizing precision and stability during the contact of the machine tool attached to the robot's end-effector with the workpiece. To this end, a two-layered learning-based human intention recognition mechanism is proposed, utilizing only the kinematic and kinetic data from the robot and two force sensors. A ``subtask detector" recognizes the human intent by estimating which phase of the task is being performed, e.g., \textit{Idle}, \textit{Tool-Attachment}, \textit{Driving}, and \textit{Contact}. Simultaneously, a ``motion estimator" continuously quantifies intent more precisely during the \textit{Driving} to predict when \textit{Contact} will begin. The controller is adapted online according to the subtask while allowing early adaptation before the \textit{Contact} to maximize precision and safety and prevent potential instabilities. Three sets of pHRI experiments were performed with multiple subjects under various conditions. Spring compression experiments were performed in virtual environments to train the data-driven models and validate the proposed adaptive system, and drilling experiments were performed in the physical world to test the proposed methods' efficacy in real-life scenarios. Experimental results show subtask classification accuracy of 84\% and motion estimation R\textsuperscript{2} score of 0.96. Furthermore, 57\% lower human effort was achieved during \textit{Driving} as well as 53\% lower oscillation amplitude at \textit{Contact} as a result of the proposed system.
<div id='section'>Paperid: <span id='pid'>1647, <a href='https://arxiv.org/pdf/2407.03817.pdf' target='_blank'>https://arxiv.org/pdf/2407.03817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Filipa Rodrigues Nogueira, HÃ©lder P. Oliveira, LuÃ­s F. Teixeira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03817">Markerless Multi-view 3D Human Pose Estimation: a survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human pose estimation involves reconstructing the human skeleton by detecting the body joints. Accurate and efficient solutions are required for several real-world applications including animation, human-robot interaction, surveillance, and sports. However, challenges such as occlusions, 2D pose mismatches, random camera perspectives, and limited 3D labelled data have been hampering the models' performance and limiting their deployment in real-world scenarios. The higher availability of cameras has led researchers to explore multi-view solutions to take advantage of the different perspectives to reconstruct the pose.
  Most existing reviews have mainly focused on monocular 3D human pose estimation, so a comprehensive survey on multi-view approaches has been missing since 2012. According to the reviewed articles, the majority of the existing methods are fully-supervised approaches based on geometric constraints, which are often limited by 2D pose mismatches. To mitigate this, researchers have proposed incorporating temporal consistency or depth information. Alternatively, working directly with 3D features has been shown to completely overcome this issue, albeit at the cost of increased computational complexity. Additionally, models with lower levels of supervision have been identified to help address challenges such as annotated data scarcity and generalisation to new setups. Therefore, no method currently addresses all challenges associated with 3D pose reconstruction, and a trade-off between complexity and performance exists. Further research is needed to develop approaches capable of quickly inferring a highly accurate 3D pose with bearable computation cost. Techniques such as active learning, low-supervision methods, temporal consistency, view selection, depth information estimation, and multi-modal approaches are strategies to consider when developing a new method for this task.
<div id='section'>Paperid: <span id='pid'>1648, <a href='https://arxiv.org/pdf/2407.02664.pdf' target='_blank'>https://arxiv.org/pdf/2407.02664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Farajtabar, Marie Charbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02664">The path towards contact-based physical human-robot interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancements in human-robot interaction (HRI), robots are now capable of operating in close proximity and engaging in physical interactions with humans (pHRI). Likewise, contact-based pHRI is becoming increasingly common as robots are equipped with a range of sensors to perceive human motions. Despite the presence of surveys exploring various aspects of HRI and pHRI, there is presently a gap in comprehensive studies that collect, organize and relate developments across all aspects of contact-based pHRI. It has become challenging to gain a comprehensive understanding of the current state of the field, thoroughly analyze the aspects that have been covered, and identify areas needing further attention. Hence, the present survey. While it includes key developments in pHRI, a particular focus is placed on contact-based interaction, which has numerous applications in industrial, rehabilitation and medical robotics. Across the literature, a common denominator is the importance to establish a safe, compliant and human intention-oriented interaction. This endeavour encompasses aspects of perception, planning and control, and how they work together to enhance safety and reliability. Notably, the survey highlights the application of data-driven techniques: backed by a growing body of literature demonstrating their effectiveness, approaches like reinforcement learning and learning from demonstration have become key to improving robot perception and decision-making within complex and uncertain pHRI scenarios. As the field is yet in its early stage, these observations may help guide future developments and steer research towards the responsible integration of physically interactive robots into workplaces, public spaces, and elements of private life.
<div id='section'>Paperid: <span id='pid'>1649, <a href='https://arxiv.org/pdf/2407.01752.pdf' target='_blank'>https://arxiv.org/pdf/2407.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sota Kaneko, Seiji Yamada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01752">Predicting Trust Dynamics with Dynamic SEM in Human-AI Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans' trust in AI constitutes a pivotal element in fostering a synergistic relationship between humans and AI. This is particularly significant in the context of systems that leverage AI technology, such as autonomous driving systems and human-robot interaction. Trust facilitates appropriate utilization of these systems, thereby optimizing their potential benefits. If humans over-trust or under-trust an AI, serious problems such as misuse and accidents occur. To prevent over/under-trust, it is necessary to predict trust dynamics. However, trust is an internal state of humans and hard to directly observe. Therefore, we propose a prediction model for trust dynamics using dynamic structure equation modeling, which extends SEM that can handle time-series data. A path diagram, which shows causalities between variables, is developed in an exploratory way and the resultant path diagram is optimized for effective path structures. Over/under-trust was predicted with 90\% accuracy in a drone simulator task,, and it was predicted with 99\% accuracy in an autonomous driving task. These results show that our proposed method outperformed the conventional method including an auto regression family.
<div id='section'>Paperid: <span id='pid'>1650, <a href='https://arxiv.org/pdf/2406.10848.pdf' target='_blank'>https://arxiv.org/pdf/2406.10848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aharony Naama, Krakovski Maya, Edan Yael
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10848">A transparency-based action model implemented in a robotic physical trainer for improved HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transparency is an important aspect of human-robot interaction (HRI), as it can improve system trust and usability leading to improved communication and performance. However, most transparency models focus only on the amount of information given to users. In this paper, we propose a bidirectional transparency model, termed a transparency-based action (TBA) model, which in addition to providing transparency information (robot-to-human), allows the robot to take actions based on transparency information received from the human (robot-of-human and human-to-robot). We implemented a three-level (High, Medium and Low) TBA model on a robotic system trainer in two pilot studies (with students as participants) to examine its impact on acceptance and HRI. Based on the pilot studies results, the Medium TBA level was not included in the main experiment, which was conducted with older adults (aged 75-85). In that experiment, two TBA levels were compared: Low (basic information including only robot-to-human transparency) and High (including additional information relating to predicted outcomes with robot-of-human and human-to-robot transparency). The results revealed a significant difference between the two TBA levels of the model in terms of perceived usefulness, ease of use, and attitude. The High TBA level resulted in improved user acceptance and was preferred by the users.
<div id='section'>Paperid: <span id='pid'>1651, <a href='https://arxiv.org/pdf/2406.08824.pdf' target='_blank'>https://arxiv.org/pdf/2406.08824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rumaisa Azeem, Andrew Hundt, Masoumeh Mansouri, Martim BrandÃ£o
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08824">LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interactions, doing household and workplace tasks, approximating `common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To address these concerns, we conduct an HRI-based evaluation of discrimination and safety criteria on several highly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness when encountering people across a diverse range of protected identity characteristics (e.g., race, gender, disability status, nationality, religion, and their intersections), producing biased outputs consistent with directly discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled untrustworthy, but not `european' or `able-bodied' people. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions -- such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. Data and code will be made available.
<div id='section'>Paperid: <span id='pid'>1652, <a href='https://arxiv.org/pdf/2406.06904.pdf' target='_blank'>https://arxiv.org/pdf/2406.06904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhi Tan, Elizabeth J. Carter, Aaron Steinfeld
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06904">Person Transfer in the Field: Examining Real World Sequential Human-Robot Interaction Between Two Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With more robots being deployed in the world, users will likely interact with multiple robots sequentially when receiving services. In this paper, we describe an exploratory field study in which unsuspecting participants experienced a ``person transfer'' -- a scenario in which they first interacted with one stationary robot before another mobile robot joined to complete the interaction. In our 7-hour study spanning 4 days, we recorded 18 instances of person transfers with 40+ individuals. We also interviewed 11 participants after the interaction to further understand their experience. We used the recorded video and interview data to extract interesting insights about in-the-field sequential human-robot interaction, such as mobile robot handovers, trust in person transfer, and the importance of the robots' positions. Our findings expose pitfalls and present important factors to consider when designing sequential human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1653, <a href='https://arxiv.org/pdf/2406.00968.pdf' target='_blank'>https://arxiv.org/pdf/2406.00968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinay Gupta, Nihal Gunukula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00968">Evaluating MEDIRL: A Replication and Ablation Study of Maximum Entropy Deep Inverse Reinforcement Learning for Human Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we enhance the Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) framework, targeting its application in human robot interaction (HRI) for modeling pedestrian behavior in crowded environments. Our work is grounded in the pioneering research by Fahad, Chen, and Guo, and aims to elevate MEDIRL's efficacy in real world HRI settings. We replicated the original MEDIRL model and conducted detailed ablation studies, focusing on key model components like learning rates, state dimensions, and network layers. Our findings reveal the effectiveness of a two dimensional state representation over three dimensional approach, significantly improving model accuracy for pedestrian behavior prediction in HRI scenarios. These results not only demonstrate MEDIRL's enhanced performance but also offer valuable insights for future HRI system development, emphasizing the importance of model customization to specific environmental contexts. Our research contributes to advancing the field of socially intelligent navigation systems, promoting more intuitive and safer human robot interactions.
<div id='section'>Paperid: <span id='pid'>1654, <a href='https://arxiv.org/pdf/2405.14116.pdf' target='_blank'>https://arxiv.org/pdf/2405.14116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyuan Zhao, Huijun Li, Tianyuan Miao, Xianyi Zhu, Zhikai Wei, Aiguo Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14116">Learning Multimodal Confidence for Intention Recognition in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of collaborative robotics has provided a new possibility of helping the elderly who has difficulties in daily life, allowing robots to operate according to specific intentions. However, efficient human-robot cooperation requires natural, accurate and reliable intention recognition in shared environments. The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliable result despite current interactive condition. In this work we propose a novel learning-based multimodal fusion framework Batch Multimodal Confidence Learning for Opinion Pool (BMCLOP). Our approach combines Bayesian multimodal fusion method and batch confidence learning algorithm to improve accuracy, uncertainty reduction and success rate given the interactive condition. In particular, the generic and practical multimodal intention recognition framework can be easily extended further. Our desired assistive scenarios consider three modalities gestures, speech and gaze, all of which produce categorical distributions over all the finite intentions. The proposed method is validated with a six-DoF robot through extensive experiments and exhibits high performance compared to baselines.
<div id='section'>Paperid: <span id='pid'>1655, <a href='https://arxiv.org/pdf/2405.09109.pdf' target='_blank'>https://arxiv.org/pdf/2405.09109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stanley Mugisha, Vamsi Krishna Guda, Christine Chevallereau, Damien Chablat, Matteo Zoppi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09109">Motion Prediction with Gaussian Processes for Safe Human-Robot Interaction in Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans use collaborative robots as tools for accomplishing various tasks. The interaction between humans and robots happens in tight shared workspaces. However, these machines must be safe to operate alongside humans to minimize the risk of accidental collisions. Ensuring safety imposes many constraints, such as reduced torque and velocity limits during operation, thus increasing the time to accomplish many tasks. However, for applications such as using collaborative robots as haptic interfaces with intermittent contacts for virtual reality applications, speed limitations result in poor user experiences. This research aims to improve the efficiency of a collaborative robot while improving the safety of the human user. We used Gaussian process models to predict human hand motion and developed strategies for human intention detection based on hand motion and gaze to improve the time for the robot and human security in a virtual environment. We then studied the effect of prediction. Results from comparisons show that the prediction models improved the robot time by 3\% and safety by 17\%. When used alongside gaze, prediction with Gaussian process models resulted in an improvement of the robot time by 2\% and the safety by 13\%.
<div id='section'>Paperid: <span id='pid'>1656, <a href='https://arxiv.org/pdf/2404.13821.pdf' target='_blank'>https://arxiv.org/pdf/2404.13821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stine S. Johansen, Yanto Browning, Anthony Brumpton, Jared Donovan, Markus Rittenbruch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13821">Robotic Blended Sonification: Consequential Robot Sound as Creative Material for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current research in robotic sounds generally focuses on either masking the consequential sound produced by the robot or on sonifying data about the robot to create a synthetic robot sound. We propose to capture, modify, and utilise rather than mask the sounds that robots are already producing. In short, this approach relies on capturing a robot's sounds, processing them according to contextual information (e.g., collaborators' proximity or particular work sequences), and playing back the modified sound. Previous research indicates the usefulness of non-semantic, and even mechanical, sounds as a communication tool for conveying robotic affect and function. Adding to this, this paper presents a novel approach which makes two key contributions: (1) a technique for real-time capture and processing of consequential robot sounds, and (2) an approach to explore these sounds through direct human-robot interaction. Drawing on methodologies from design, human-robot interaction, and creative practice, the resulting 'Robotic Blended Sonification' is a concept which transforms the consequential robot sounds into a creative material that can be explored artistically and within application-based studies.
<div id='section'>Paperid: <span id='pid'>1657, <a href='https://arxiv.org/pdf/2403.16711.pdf' target='_blank'>https://arxiv.org/pdf/2403.16711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Menno van Zutphen, Giannis Delimpaltadakis, Maurice Heemels, Duarte Antunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16711">Predictable Interval MDPs through Entropy Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regularization of control policies using entropy can be instrumental in adjusting predictability of real-world systems. Applications benefiting from such approaches range from, e.g., cybersecurity, which aims at maximal unpredictability, to human-robot interaction, where predictable behavior is highly desirable. In this paper, we consider entropy regularization for interval Markov decision processes (IMDPs). IMDPs are uncertain MDPs, where transition probabilities are only known to belong to intervals. Lately, IMDPs have gained significant popularity in the context of abstracting stochastic systems for control design. In this work, we address robust minimization of the linear combination of entropy and a standard cumulative cost in IMDPs, thereby establishing a trade-off between optimality and predictability. We show that optimal deterministic policies exist, and devise a value-iteration algorithm to compute them. The algorithm solves a number of convex programs at each step. Finally, through an illustrative example we show the benefits of penalizing entropy in IMDPs.
<div id='section'>Paperid: <span id='pid'>1658, <a href='https://arxiv.org/pdf/2403.14293.pdf' target='_blank'>https://arxiv.org/pdf/2403.14293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ponkoj Chandra Shill, Md. Azizul Hakim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14293">Human-Robot Interaction and Perceived Irrationality: A Study of Trust Dynamics and Error Acknowledgment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly integrated into various industries, understanding how humans respond to robotic failures is critical. This study systematically examines trust dynamics and system design by analyzing human reactions to robot failures. We conducted a four-stage survey to explore how trust evolves throughout human-robot interactions. The first stage collected demographic data and initial trust levels. The second stage focused on preliminary expectations and perceptions of robotic capabilities. The third stage examined interaction details, including robot precision and error acknowledgment. Finally, the fourth stage assessed post-interaction perceptions, evaluating trust dynamics, forgiveness, and willingness to recommend robotic technologies. Results indicate that trust in robotic systems significantly increased when robots acknowledged their errors or limitations. Additionally, participants showed greater willingness to suggest robots for future tasks, highlighting the importance of direct engagement in shaping trust dynamics. These findings provide valuable insights for designing more transparent, responsive, and trustworthy robotic systems. By enhancing our understanding of human-robot interaction (HRI), this study contributes to the development of robotic technologies that foster greater public acceptance and adoption.
<div id='section'>Paperid: <span id='pid'>1659, <a href='https://arxiv.org/pdf/2403.14025.pdf' target='_blank'>https://arxiv.org/pdf/2403.14025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason R. Wilson, Emily Jensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14025">HRI Curriculum for a Liberal Arts Education</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we discuss the opportunities and challenges of teaching a human-robot interaction course at an undergraduate liberal arts college. We provide a sample syllabus adapted from a previous version of a course.
<div id='section'>Paperid: <span id='pid'>1660, <a href='https://arxiv.org/pdf/2403.12223.pdf' target='_blank'>https://arxiv.org/pdf/2403.12223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chinmaya Mishra, Anuj Nandanwar, Sashikala Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12223">HRI in Indian Education: Challenges Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent advancements in the field of robotics and the increased focus on having general-purpose robots widely available to the general public, it has become increasingly necessary to pursue research into Human-robot interaction (HRI). While there have been a lot of works discussing frameworks for teaching HRI in educational institutions with a few institutions already offering courses to students, a consensus on the course content still eludes the field. In this work, we highlight a few challenges and opportunities while designing an HRI course from an Indian perspective. These topics warrant further deliberations as they have a direct impact on the design of HRI courses and wider implications for the entire field.
<div id='section'>Paperid: <span id='pid'>1661, <a href='https://arxiv.org/pdf/2403.08061.pdf' target='_blank'>https://arxiv.org/pdf/2403.08061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunwoong Choi, Zaid Abbas Al-Sabbag, Sriram Narasimhan, Chul Min Yeum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08061">Gaze-based Human-Robot Interaction System for Infrastructure Inspections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide. Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable. Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology. This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality. Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time. Additionally, inspectors can monitor the inspection progress online, which enhances the speed of the entire inspection process. Limited controlled experiments demonstrate its effectiveness across various users and defect types. To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections.
<div id='section'>Paperid: <span id='pid'>1662, <a href='https://arxiv.org/pdf/2403.06851.pdf' target='_blank'>https://arxiv.org/pdf/2403.06851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Shushtari, Julia Foellmer, Arash Arami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06851">Human-Exoskeleton Interaction Portrait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot physical interaction contains crucial information for optimizing user experience, enhancing robot performance, and objectively assessing user adaptation. This study introduces a new method to evaluate human-robot co-adaptation in lower limb exoskeletons by analyzing muscle activity and interaction torque as a two-dimensional random variable. We introduce the Interaction Portrait (IP), which visualizes this variable's distribution in polar coordinates. We applied this metric to compare a recent torque controller (HTC) based on kinematic state feedback and a novel feedforward controller (AMTC) with online learning, proposed herein, against a time-based controller (TBC) during treadmill walking at varying speeds. Compared to TBC, both HTC and AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced user-exoskeleton coordination. IP analysis reveals this improvement stems from two distinct co-adaptation strategies, unidentifiable by traditional muscle activity or interaction torque analyses alone. HTC encourages users to yield control to the exoskeleton, decreasing muscular effort but increasing interaction torque, as the exoskeleton compensates for user dynamics. Conversely, AMTC promotes user engagement through increased muscular effort and reduced interaction torques, aligning it more closely with rehabilitation and gait training applications. IP phase evolution provides insight into each user's interaction strategy development, showcasing IP analysis's potential in comparing and designing novel controllers to optimize human-robot interaction in wearable robots.
<div id='section'>Paperid: <span id='pid'>1663, <a href='https://arxiv.org/pdf/2403.05701.pdf' target='_blank'>https://arxiv.org/pdf/2403.05701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lennart Wachowiak, Andrew Coles, Oya Celiktutan, Gerard Canal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05701">Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for LLMs to generate socially acceptable actions that align with people's preferences and values. In this work, we test whether LLMs capture people's intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of LLMs with that of real participants. We find that GPT-4 strongly outperforms other models, generating answers that correlate strongly with users' answers in two studies $\unicode{x2014}$ the first study dealing with selecting the most appropriate communicative act for a robot in various situations ($r_s$ = 0.82), and the second with judging the desirability, intentionality, and surprisingness of behavior ($r_s$ = 0.83). However, for the last study, testing whether people judge the behavior of robots and humans differently, no model achieves strong correlations. Moreover, we show that vision models fail to capture the essence of video stimuli and that LLMs tend to rate different communicative acts and behavior desirability higher than people.
<div id='section'>Paperid: <span id='pid'>1664, <a href='https://arxiv.org/pdf/2403.05588.pdf' target='_blank'>https://arxiv.org/pdf/2403.05588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henny Admoni, Daniel Szafir, Wafa Johal, Anara Sandygulova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05588">Paper index: Designing an introductory HRI course (workshop at HRI 2024)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction is now an established discipline. Dozens of HRI courses exist at universities worldwide, and some institutions even offer degrees in HRI. However, although many students are being taught HRI, there is no agreed-upon curriculum for an introductory HRI course. In this workshop, we aimed to reach community consensus on what should be covered in such a course. Through interactive activities like panels, breakout discussions, and syllabus design, workshop participants explored the many topics and pedagogical approaches for teaching HRI. This collection of articles submitted to the workshop provides examples of HRI courses being offered worldwide.
<div id='section'>Paperid: <span id='pid'>1665, <a href='https://arxiv.org/pdf/2403.05400.pdf' target='_blank'>https://arxiv.org/pdf/2403.05400.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nanami Hashimoto, Emma Hagens, Arkady Zgonnikov, Maria Luce Lupetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05400">Safe Spot: Perceived safety of dominant and submissive appearances of quadruped robots in human-robot interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unprecedented possibilities of quadruped robots have driven much research on the technical aspects of these robots. However, the social perception and acceptability of quadruped robots so far remain poorly understood. This work investigates whether the way we design quadruped robots' behaviors can affect people's perception of safety in interactions with these robots. We designed and tested a dominant and submissive personality for the quadruped robot (Boston Dynamics Spot). These were tested in two different walking scenarios (head-on and crossing interactions) in a 2x2 within-subjects study. We collected both behavioral data and subjective reports on participants' perception of the interaction. The results highlight that participants perceived the submissive robot as safer compared to the dominant one. The behavioral dynamics of interactions did not change depending on the robot's appearance. Participants' previous in-person experience with the robot was associated with lower subjective safety ratings but did not correlate with the interaction dynamics. Our findings have implications for the design of quadruped robots and contribute to the body of knowledge on the social perception of non-humanoid robots. We call for a stronger standing of felt experiences in human-robot interaction research.
<div id='section'>Paperid: <span id='pid'>1666, <a href='https://arxiv.org/pdf/2403.05098.pdf' target='_blank'>https://arxiv.org/pdf/2403.05098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Hundt, Gabrielle Ohlson, Pieter Wolfert, Lux Miranda, Sophia Zhu, Katie Winkle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05098">Love, Joy, and Autism Robots: A Metareview and Provocatype</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous work has observed how Neurodivergence is often harmfully pathologized in Human-Computer Interaction (HCI) and Human-Robot interaction (HRI) research. We conduct a review of autism robot reviews and find the dominant research direction is Autistic people's second to lowest (24 of 25) research priority: interventions and treatments purporting to 'help' neurodivergent individuals to conform to neurotypical social norms, become better behaved, improve social and emotional skills, and otherwise 'fix' us -- rarely prioritizing the internal experiences that might lead to such differences. Furthermore, a growing body of evidence indicates many of the most popular current approaches risk inflicting lasting trauma and damage on Autistic people. We draw on the principles and findings of the latest Autism research, Feminist HRI, and Robotics to imagine a role reversal, analyze the implications, then conclude with actionable guidance on Autistic-led scientific methods and research directions.
<div id='section'>Paperid: <span id='pid'>1667, <a href='https://arxiv.org/pdf/2403.02918.pdf' target='_blank'>https://arxiv.org/pdf/2403.02918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Li, Koen V Hindriks, Florian Kunneman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02918">Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper. We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot. To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE). To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation. Comparing a signal processing approach, with and without post-filtering, and a convolutional recurrent neural network (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation. These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch.
<div id='section'>Paperid: <span id='pid'>1668, <a href='https://arxiv.org/pdf/2402.14563.pdf' target='_blank'>https://arxiv.org/pdf/2402.14563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephan SchlÃ¶gl, Gavin Doherty, Saturnino Luz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14563">Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wizard of OZ (WOZ) is a well-established method for simulating the functionality and user experience of future systems. Using a human wizard to mimic certain operations of a potential system is particularly useful in situations where extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations. The WOZ method has been widely used in connection with speech and language technologies, but advances in sensor technology and pattern recognition as well as new application areas such as human-robot interaction have made it increasingly relevant to the design of a wider range of interactive systems. In such cases achieving acceptable performance at the user interface level often hinges on resource intensive improvements such as domain tuning, which are better done once the overall design is relatively stable. While WOZ is recognised as a valuable prototyping technique, surprisingly little effort has been put into exploring it from a methodological point of view. Starting from a survey of the literature, this paper presents a systematic investigation and analysis of the design space for WOZ for language technology applications, and proposes a generic architecture for tool support that supports the integration of components for speech recognition and synthesis as well as for machine translation. This architecture is instantiated in WebWOZ - a new web-based open-source WOZ prototyping platform. The viability of generic support is explored empirically through a series of evaluations. Researchers from a variety of backgrounds were able to create experiments, independent of their previous experience with WOZ. The approach was further validated through a number of real experiments, which also helped to identify a number of possibilities for additional support, and flagged potential issues relating to consistency in Wizard performance.
<div id='section'>Paperid: <span id='pid'>1669, <a href='https://arxiv.org/pdf/2402.10077.pdf' target='_blank'>https://arxiv.org/pdf/2402.10077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>E. Sherafat, B. Farooq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10077">Towards a large-scale fused and labeled dataset of human pose while interacting with robots in shared urban areas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last decade, Autonomous Delivery Robots (ADRs) have transformed conventional delivery methods, responding to the growing e-commerce demand. However, the readiness of ADRs to navigate safely among pedestrians in shared urban areas remains an open question. We contend that there are crucial research gaps in understanding their interactions with pedestrians in such environments. Human Pose Estimation is a vital stepping stone for various downstream applications, including pose prediction and socially aware robot path-planning. Yet, the absence of an enriched and pose-labeled dataset capturing human-robot interactions in shared urban areas hinders this objective. In this paper, we bridge this gap by repurposing, fusing, and labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and Simultaneous Localization and Mapping (SLAM), respectively. The resulting unique dataset represents thousands of real-world indoor and outdoor human-robot interaction scenarios. Leveraging YOLOv7, we obtained human pose visual and numeric outputs and provided ground truth poses using manual annotation. To overcome the distance bias present in the traditional MPJPE metric, this study introduces a novel human pose estimation error metric called Mean Scaled Joint Error (MSJE) by incorporating bounding box dimensions into it. Findings demonstrate that YOLOv7 effectively estimates human pose in both datasets. However, it exhibits weaker performance in specific scenarios, like indoor, crowded scenes with a focused light source, where both MPJPE and MSJE are recorded as 10.89 and 25.3, respectively. In contrast, YOLOv7 performs better in single-person estimation (NCLT seq 2) and outdoor scenarios (MOT17 seq1), achieving MSJE values of 5.29 and 3.38, respectively.
<div id='section'>Paperid: <span id='pid'>1670, <a href='https://arxiv.org/pdf/2402.08006.pdf' target='_blank'>https://arxiv.org/pdf/2402.08006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Santos, Bernardo Carvalho, Catarina Barata, JosÃ© Santos-Victor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08006">Extending 3D body pose estimation for robotic-assistive therapies of autistic children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic-assistive therapy has demonstrated very encouraging results for children with Autism. Accurate estimation of the child's pose is essential both for human-robot interaction and for therapy assessment purposes. Non-intrusive methods are the sole viable option since these children are sensitive to touch.
  While depth cameras have been used extensively, existing methods face two major limitations: (i) they are usually trained with adult-only data and do not correctly estimate a child's pose, and (ii) they fail in scenarios with a high number of occlusions. Therefore, our goal was to develop a 3D pose estimator for children, by adapting an existing state-of-the-art 3D body modelling method and incorporating a linear regression model to fine-tune one of its inputs, thereby correcting the pose of children's 3D meshes.
  In controlled settings, our method has an error below $0.3m$, which is considered acceptable for this kind of application and lower than current state-of-the-art methods. In real-world settings, the proposed model performs similarly to a Kinect depth camera and manages to successfully estimate the 3D body poses in a much higher number of frames.
<div id='section'>Paperid: <span id='pid'>1671, <a href='https://arxiv.org/pdf/2402.06325.pdf' target='_blank'>https://arxiv.org/pdf/2402.06325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto GarzÃ¡s-Villar, Caspar Boersma, Alexis Derumigny, Arkady Zgonnikov, Laura Marchal-Crespo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06325">The Effect of Haptic Guidance during Robotic-assisted Motor Training is Modulated by Personality Traits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The provision of robotic assistance during motor training has proven to be effective in enhancing motor learning in some healthy trainee groups as well as patients. Personalizing such robotic assistance can help further improve motor (re)learning outcomes and cater better to the trainee's needs and desires. However, the development of personalized haptic assistance is hindered by the lack of understanding of the link between the trainee's personality and the effects of haptic guidance during human-robot interaction. To address this gap, we ran an experiment with 42 healthy participants who trained with a robotic device to control a virtual pendulum to hit incoming targets either with or without haptic guidance. We found that certain personal traits affected how users adapt and interact with the guidance during training. In particular, those participants with an 'Achiever gaming style' performed better and applied lower interaction forces to the robotic device than the average participant as the training progressed. Conversely, participants with the 'Free spirit game style' increased the interaction force in the course of training. We also found an interaction between some personal characteristics and haptic guidance. Specifically, participants with a higher 'Transformation of challenge' trait exhibited poorer performance during training while receiving haptic guidance compared to an average participant receiving haptic guidance. Furthermore, individuals with an external Locus of Control tended to increase their interaction force with the device, deviating from the pattern observed in an average participant under the same guidance. These findings suggest that individual characteristics may play a crucial role in the effectiveness of haptic guidance training strategies.
<div id='section'>Paperid: <span id='pid'>1672, <a href='https://arxiv.org/pdf/2402.04797.pdf' target='_blank'>https://arxiv.org/pdf/2402.04797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Bouzid, Youssef Alj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04797">Offline Deep Model Predictive Control (MPC) for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new visual navigation method based on a single RGB perspective camera. Using the Visual Teach & Repeat (VT&R) methodology, the robot acquires a visual trajectory consisting of multiple subgoal images in the teaching step. In the repeat step, we propose two network architectures, namely ViewNet and VelocityNet. The combination of the two networks allows the robot to follow the visual trajectory. ViewNet is trained to generate a future image based on the current view and the velocity command. The generated future image is combined with the subgoal image for training VelocityNet. We develop an offline Model Predictive Control (MPC) policy within VelocityNet with the dual goals of (1) reducing the difference between current and subgoal images and (2) ensuring smooth trajectories by mitigating velocity discontinuities. Offline training conserves computational resources, making it a more suitable option for scenarios with limited computational capabilities, such as embedded systems. We validate our experiments in a simulation environment, demonstrating that our model can effectively minimize the metric error between real and played trajectories.
<div id='section'>Paperid: <span id='pid'>1673, <a href='https://arxiv.org/pdf/2312.10069.pdf' target='_blank'>https://arxiv.org/pdf/2312.10069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Li, Luca Weihs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10069">Understanding Representations Pretrained with Auxiliary Losses for Embodied Agent Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretrained representations from large-scale vision models have boosted the performance of downstream embodied policy learning. We look to understand whether additional self-supervised pretraining on exploration trajectories can build on these general-purpose visual representations to better support embodied planning in realistic environments. We evaluated four common auxiliary losses in embodied AI, two hindsight-based losses, and a standard imitation learning loss, by pretraining the agent's visual compression module and state belief representations with each objective and using CLIP as a representative visual backbone. The learned representations are then frozen for downstream multi-step evaluation on two goal-directed tasks. Surprisingly, we find that imitation learning on these exploration trajectories out-performs all other auxiliary losses even despite the exploration trajectories being dissimilar from the downstream tasks. This suggests that imitation of exploration may be ''all you need'' for building powerful planning representations. Additionally, we find that popular auxiliary losses can benefit from simple modifications to improve their support for downstream planning ability.
<div id='section'>Paperid: <span id='pid'>1674, <a href='https://arxiv.org/pdf/2312.09468.pdf' target='_blank'>https://arxiv.org/pdf/2312.09468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luka KovaÄ, Igor FarkaÅ¡
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09468">Safe Reinforcement Learning in a Simulated Robotic Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies. In many environments and tasks, safety is of critical importance. The widespread use of simulators offers a number of advantages, including safe exploration which will be inevitable in cases when RL systems need to be trained directly in the physical environment (e.g. in human-robot interaction). The popular Safety Gym library offers three mobile agent types that can learn goal-directed tasks while considering various safety constraints. In this paper, we extend the applicability of safe RL algorithms by creating a customized environment with Panda robotic arm where Safety Gym algorithms can be tested. We performed pilot experiments with the popular PPO algorithm comparing the baseline with the constrained version and show that the constrained version is able to learn the equally good policy while better complying with safety constraints and taking longer training time as expected.
<div id='section'>Paperid: <span id='pid'>1675, <a href='https://arxiv.org/pdf/2312.06566.pdf' target='_blank'>https://arxiv.org/pdf/2312.06566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanyu Huang, Roger K. Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06566">One Size Does not Fit All: Personalised Affordance Design for Social Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalisation is essential to achieve more acceptable and effective results in human-robot interaction. Placing users in the central role, many studies have focused on enhancing the abilities of social robots to perceive and understand users. However, little is known about improving user perceptions and interpretation of a social robot in spoken interactions. The work described in the paper aims to find out what affects the personalisation of affordance of a social robot, namely its appearance, voice and language behaviours. The experimental data presented here is based on an ongoing project. It demonstrates the many and varied ways in which people change their preferences for the affordance of a social robot under different circumstances. It also examines the relationship between such preferences and expectations of characteristics of a social robot, like competence and warmth. It also shows that individuals have different perceptions of the language behaviours of the same robot. These results demonstrate that one-sized personalisation does not fit all. Personalisation should be considered a comprehensive approach, including appropriate affordance design, to suit the user expectations of social roles.
<div id='section'>Paperid: <span id='pid'>1676, <a href='https://arxiv.org/pdf/2312.06475.pdf' target='_blank'>https://arxiv.org/pdf/2312.06475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anestis Dalgkitsis, Christos Verikoukis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06475">NetROS-5G: Enhancing Personalization through 5G Network Slicing and Edge Computing in Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly being used in a variety of applications, from manufacturing and healthcare to education and customer service. However, the mobility, power, and price points of these robots often dictate that they do not have sufficient computing power on board to run modern algorithms for personalization in human-robot interaction at desired rates. This can limit the effectiveness of the interaction and limit the potential applications for these robots. 5G connectivity provides a solution to this problem by offering high data rates, bandwidth, and low latency that can facilitate robotics services. Additionally, the widespread availability of cloud computing has made it easy to access almost unlimited computing power at a low cost. Edge computing, which involves placing compute resources closer to the action, can offer even lower latency than cloud computing. In this paper, we explore the potential of combining 5G, edge, and cloud computing to provide improved personalization in human-robot interaction. We design, develop, and demonstrate a new framework, entitled NetROS-5G, to show how the performance gained by utilizing these technologies can overcome network latency and significantly enhance personalization in robotics. Our results show that the integration of 5G network slicing, edge computing, and cloud computing can collectively offer a cost-efficient and superior level of personalization in a modern human-robot interaction scenario.
<div id='section'>Paperid: <span id='pid'>1677, <a href='https://arxiv.org/pdf/2312.04056.pdf' target='_blank'>https://arxiv.org/pdf/2312.04056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atefeh Fereydooni, Armin Azarakhsh, Ayda Shafiei, Hesam Zandi, Mehdi Delrobaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04056">A Cost-Effective Test Bench for Evaluating Safe Human-Robot Interaction in Mobile Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety concerns have risen as robots become more integrated into our daily lives and continue to interact closely with humans. One of the most crucial safety priorities is preventing collisions between robots and people walking nearby. Despite developing various algorithms to address this issue, evaluating their effectiveness on a cost-effective test bench remains a significant challenge. In this work, we propose a solution by introducing a simple yet functional platform that enables researchers and developers to assess how humans interact with mobile robots. This platform is designed to provide a quick yet accurate evaluation of the performance of safe interaction algorithms and make informed decisions for future development. The platform's features and structure are detailed, along with the initial testing results using two preliminary algorithms. The results obtained from the evaluation were consistent with theoretical calculations, demonstrating its effectiveness in assessing human-robot interaction. Our solution provides a preliminary yet reliable approach to ensure the safety of both robots and humans in their daily interactions.
<div id='section'>Paperid: <span id='pid'>1678, <a href='https://arxiv.org/pdf/2312.02390.pdf' target='_blank'>https://arxiv.org/pdf/2312.02390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nourhan Abdulazeem, Yue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02390">The Impact of Robots' Facial Emotional Expressions on Light Physical Exercises</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To address the global challenge of population aging, our goal is to enhance successful aging through the introduction of robots capable of assisting in daily physical activities and promoting light exercises, which would enhance the cognitive and physical well-being of older adults. Previous studies have shown that facial expressions can increase engagement when interacting with robots. This study aims to investigate how older adults perceive and interact with a robot capable of displaying facial emotions while performing a physical exercise task together. We employed a collaborative robotic arm with a flat panel screen to encourage physical exercise across three different facial emotion conditions. We ran the experiment with older adults aged between 66 and 88. Our findings suggest that individuals perceive robots exhibiting facial expressions as less competent than those without such expressions. Additionally, the presence of facial expressions does not appear to significantly impact participants' levels of engagement, unlike other state-of-the-art studies. This observation is likely linked to our study's emphasis on collaborative physical human-robot interaction (pHRI) applications, as opposed to socially oriented pHRI applications. Additionally, we foresee a requirement for more suitable non-verbal social behavior to effectively enhance participants' engagement levels.
<div id='section'>Paperid: <span id='pid'>1679, <a href='https://arxiv.org/pdf/2311.14175.pdf' target='_blank'>https://arxiv.org/pdf/2311.14175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Herashchenko, Igor FarkaÅ¡
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14175">Appearance-based gaze estimation enhanced with synthetic images using deep neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human eye gaze estimation is an important cognitive ingredient for successful human-robot interaction, enabling the robot to read and predict human behavior. We approach this problem using artificial neural networks and build a modular system estimating gaze from separately cropped eyes, taking advantage of existing well-functioning components for face detection (RetinaFace) and head pose estimation (6DRepNet). Our proposed method does not require any special hardware or infrared filters but uses a standard notebook-builtin RGB camera, as often approached with appearance-based methods. Using the MetaHuman tool, we also generated a large synthetic dataset of more than 57,000 human faces and made it publicly available. The inclusion of this dataset (with eye gaze and head pose information) on top of the standard Columbia Gaze dataset into training the model led to better accuracy with a mean average error below two degrees in eye pitch and yaw directions, which compares favourably to related methods. We also verified the feasibility of our model by its preliminary testing in real-world setting using the builtin 4K camera in NICO semi-humanoid robot's eye.
<div id='section'>Paperid: <span id='pid'>1680, <a href='https://arxiv.org/pdf/2311.06640.pdf' target='_blank'>https://arxiv.org/pdf/2311.06640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelhadi Hireche, Abdelkader Nasreddine Belkacem, Sadia Jamil, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06640">NewsGPT: ChatGPT Integration for Robot-Reporter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of large language models (LLMs) with social robots has emerged as a promising avenue for enhancing human-robot interactions at a time when news reports generated by artificial intelligence (AI) are gaining in credibility. This integration is expected to intensify and become a more productive resource for journalism, media, communication, and education. In this paper a novel system is proposed that integrates AI's generative pretrained transformer (GPT) model with the Pepper robot, with the aim of improving the robot's natural language understanding and response generation capabilities for enhanced social interactions. By leveraging GPT's powerful language processing capabilities, this system offers a comprehensive pipeline that incorporates voice input recording, speech-to-text transcription, context analysis, and text-to-speech synthesis action generation. The Pepper robot is enabled to comprehend user queries, generate informative responses with general knowledge, maintain contextually relevant conversations, and act as a more domain-oriented news reporter. It is also linked with a news resource and powered with a Google search capability. To evaluate the performance of the framework, experiments were conducted involving a set of diverse questions. The robot's responses were assessed on the basis of eight criteria, including relevance, context, and fluency. Despite some identified limitations, this system contributes to the field of journalism and human-robot interaction by showcasing the potential of integrating LLMs with social robots. The proposed framework opens up opportunities for improving the conversational capabilities of robots, enabling interactions that are smoother, more engaging, and more context aware.
<div id='section'>Paperid: <span id='pid'>1681, <a href='https://arxiv.org/pdf/2311.03232.pdf' target='_blank'>https://arxiv.org/pdf/2311.03232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco J. Ruiz-Ruiz, Cristina Urdiales, Manuel FernÃ¡ndez-Carmona, JesÃºs M. GÃ³mez-de-Gabriel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03232">A Reactive performance-based Shared Control Framework for Assistive Robotic Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Physical Human--Robot Interaction (pHRI) grippers, humans and robots may contribute simultaneously to actions, so it is necessary to determine how to combine their commands. Control may be swapped from one to the other within certain limits, or input commands may be combined according to some criteria. The Assist-As-Needed (AAN) paradigm focuses on this second approach, as the controller is expected to provide the minimum required assistance to users. Some AAN systems rely on predicting human intention to adjust actions. However, if prediction is too hard, reactive AAN systems may weigh input commands into an emergent one. This paper proposes a novel AAN reactive control system for a robot gripper where input commands are weighted by their respective local performances. Thus, rather than minimizing tracking errors or differences to expected velocities, humans receive more help depending on their needs. The system has been tested using a gripper attached to a sensitive robot arm, which provides evaluation parameters. Tests consisted of completing an on-air planar path with both arms. After the robot gripped a person's forearm, the path and current position of the robot were displayed on a screen to provide feedback to the human. The proposed control has been compared to results without assistance and to impedance control for benchmarking. A statistical analysis of the results proves that global performance improved and tracking errors decreased for ten volunteers with the proposed controller. Besides, unlike impedance control, the proposed one does not significantly affect exerted forces, command variation, or disagreement, measured as the angular difference between human and output command. Results support that the proposed control scheme fits the AAN paradigm, although future work will require further tests for more complex environments and tasks.
<div id='section'>Paperid: <span id='pid'>1682, <a href='https://arxiv.org/pdf/2310.09626.pdf' target='_blank'>https://arxiv.org/pdf/2310.09626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maike Paetzel-PrÃ¼smann, Alessandra Rossi, Merel Keijsers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09626">Current and Future Challenges in Humanoid Robotics -- An Empirical Investigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of RoboCup is to make research in the area of robotics measurable over time, and grow a community that works together to solve increasingly difficult challenges over the years. The most ambitious of these challenges it to be able to play against the human world champions in soccer in 2050. To better understand what members of the RoboCup community believes to be the state of the art and the main challenges in the next decade and towards the 2050 game, we developed a survey and distributed it to members of different experience level and background within the community. We present data from 39 responses. Results highlighted that locomotion, awareness and decision-making, and robustness of robots are among those considered of high importance for the community, while human-robot interaction and natural language processing and generation are rated of low in importance and difficulty.
<div id='section'>Paperid: <span id='pid'>1683, <a href='https://arxiv.org/pdf/2310.03137.pdf' target='_blank'>https://arxiv.org/pdf/2310.03137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eddie Guo, Christopher Perlette, Mojtaba Sharifi, Lukas Grasse, Matthew Tata, Vivian K. Mushahwar, Mahdi Tavakoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03137">Speech-Based Human-Exoskeleton Interaction for Lower Limb Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a speech-based motion planning strategy (SBMP) developed for lower limb exoskeletons to facilitate safe and compliant human-robot interaction. A speech processing system, finite state machine, and central pattern generator are the building blocks of the proposed strategy for online planning of the exoskeleton's trajectory. According to experimental evaluations, this speech-processing system achieved low levels of word and intent errors. Regarding locomotion, the completion time for users with voice commands was 54% faster than that using a mobile app interface. With the proposed SBMP, users are able to maintain their postural stability with both hands-free. This supports its use as an effective motion planning method for the assistance and rehabilitation of individuals with lower-limb impairments.
<div id='section'>Paperid: <span id='pid'>1684, <a href='https://arxiv.org/pdf/2310.01943.pdf' target='_blank'>https://arxiv.org/pdf/2310.01943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Birkner, Andreas Dolp, Negin Karimi, Nikita Basargin, Alona Kharchenko, Rafael Hostettler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01943">Ravestate: Distributed Composition of a Causal-Specificity-Guided Interaction Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction policy design, a rule-based method is efficient, explainable, expressive and intuitive. In this paper, we present the Signal-Rule-Slot framework, which refines prior work on rule-based symbol system design and introduces a new, Bayesian notion of interaction rule utility called Causal Pathway Self-information. We offer a rigorous theoretical foundation as well as a rich open-source reference implementation Ravestate, with which we conduct user studies in text-, speech-, and vision-based scenarios. The experiments show robust contextual behaviour of our probabilistically informed rule-based system, paving the way for more effective human-machine interaction.
<div id='section'>Paperid: <span id='pid'>1685, <a href='https://arxiv.org/pdf/2309.12562.pdf' target='_blank'>https://arxiv.org/pdf/2309.12562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Syed T. Bukhari, Bashira Akter Anima, David Feil-Seifer, Wajahat M. Qazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12562">Cognitive Approach to Hierarchical Task Selection for Human-Robot Interaction in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an efficient and flexible human-robot collaborative work environment, a robot team member must be able to recognize both explicit requests and implied actions from human users. Identifying "what to do" in such cases requires an agent to have the ability to construct associations between objects, their actions, and the effect of actions on the environment. In this regard, semantic memory is being introduced to understand the explicit cues and their relationships with available objects and required skills to make "tea" and "sandwich". We have extended our previous hierarchical robot control architecture to add the capability to execute the most appropriate task based on both feedback from the user and the environmental context. To validate this system, two types of skills were implemented in the hierarchical task tree: 1) Tea making skills and 2) Sandwich making skills. During the conversation between the robot and the human, the robot was able to determine the hidden context using ontology and began to act accordingly. For instance, if the person says "I am thirsty" or "It is cold outside" the robot will start to perform the tea-making skill. In contrast, if the person says, "I am hungry" or "I need something to eat", the robot will make the sandwich. A humanoid robot Baxter was used for this experiment. We tested three scenarios with objects at different positions on the table for each skill. We observed that in all cases, the robot used only objects that were relevant to the skill.
<div id='section'>Paperid: <span id='pid'>1686, <a href='https://arxiv.org/pdf/2309.08988.pdf' target='_blank'>https://arxiv.org/pdf/2309.08988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Navarro-Cabrera, Niceto R. Luque, Eduardo Ros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08988">Multi-objective tuning for torque PD controllers of cobots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robotics is a new and challenging field in the realm of motion control and human-robot interaction. The safety measures needed for a reliable interaction between the robot and its environment hinder the use of classical control methods, pushing researchers to try new techniques such as machine learning (ML). In this context, reinforcement learning has been adopted as the primary way to create intelligent controllers for collaborative robots, however supervised learning shows great promise in the hope of developing data-driven model based ML controllers in a faster and safer way. In this work we study several aspects of the methodology needed to create a dataset to be used to learn the dynamics of a robot. For this we tune several PD controllers to several trajectories, using a multi-objective genetic algorithm (GA) which takes into account not only their accuracy, but also their safety. We demonstrate the need to tune the controllers individually to each trajectory and empirically explore the best population size for the GA and how the speed of the trajectory affects the tuning and the dynamics of the robot.
<div id='section'>Paperid: <span id='pid'>1687, <a href='https://arxiv.org/pdf/2309.08984.pdf' target='_blank'>https://arxiv.org/pdf/2309.08984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohannad Babli, Jaime A Rincon, Eva Onaindia, Carlos Carrascosa, Vicente Julian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08984">Deliberative Context-Aware Ambient Intelligence System for Assisted Living Homes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring wellbeing and stress is one of the problems covered by ambient intelligence, as stress is a significant cause of human illnesses directly affecting our emotional state. The primary aim was to propose a deliberation architecture for an ambient intelligence healthcare application. The architecture provides a plan for comforting stressed seniors suffering from negative emotions in an assisted living home and executes the plan considering the environment's dynamic nature. Literature was reviewed to identify the convergence between deliberation and ambient intelligence and the latter's latest healthcare trends. A deliberation function was designed to achieve context-aware dynamic human-robot interaction, perception, planning capabilities, reactivity, and context-awareness with regard to the environment. A number of experimental case studies in a simulated assisted living home scenario were conducted to demonstrate the approach's behavior and validity. The proposed methods were validated to show classification accuracy. The validation showed that the deliberation function has effectively achieved its deliberative objectives.
<div id='section'>Paperid: <span id='pid'>1688, <a href='https://arxiv.org/pdf/2308.15097.pdf' target='_blank'>https://arxiv.org/pdf/2308.15097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucien Tisserand, FrÃ©dÃ©ric Armetta, Heike Baldauf-Quilliatre, Antoine Bouquin, Salima Hassas, Mathieu Lefort
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15097">Sequential annotations for naturally-occurring HRI: first insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explain the methodology we developed for improving the interactions accomplished by an embedded conversational agent, drawing from Conversation Analytic sequential and multimodal analysis. The use case is a Pepper robot that is expected to inform and orient users in a library. In order to propose and learn better interactive schema, we are creating a corpus of naturally-occurring interactions that will be made available to the community. To do so, we propose an annotation practice based on some theoretical underpinnings about the use of language and multimodal resources in human-robot interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$ Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing $\rightarrow$ Text input; HCI theory, concepts and models; Field studies.
<div id='section'>Paperid: <span id='pid'>1689, <a href='https://arxiv.org/pdf/2308.04581.pdf' target='_blank'>https://arxiv.org/pdf/2308.04581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Cameron, Emily Collins, Stevienna de Saille, James Law
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04581">The Social Triad model of Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the increasing interest in trust in human-robot interaction (HRI), there is still relatively little exploration of trust as a social construct in HRI. We propose that integration of useful models of human-human trust from psychology, highlight a potentially overlooked aspect of trust in HRI: a robot's apparent trustworthiness may indirectly relate to the user's relationship with, and opinion of, the individual or organisation deploying the robot. Our Social Triad for HRI model (User, Robot, Deployer), identifies areas for consideration in co-creating trustworthy robotics.
<div id='section'>Paperid: <span id='pid'>1690, <a href='https://arxiv.org/pdf/2308.03146.pdf' target='_blank'>https://arxiv.org/pdf/2308.03146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiara Bassetti, Enrico Blanzieri, Stefano Borgo, Sofia Marangon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03146">Towards socially-competent and culturally-adaptive artificial agents Expressive order, interactional disruptions and recovery strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial agents for social interaction pushes to enrich robots with social skills and knowledge about (local) social norms. One possibility is to distinguish the expressive and the functional orders during a human-robot interaction. The overarching aim of this work is to set a framework to make the artificial agent socially-competent beyond dyadic interaction-interaction in varying multi-party social situations-and beyond individual-based user personalization, thereby enlarging the current conception of "culturally-adaptive". The core idea is to provide the artificial agent with the capability to handle different kinds of interactional disruptions, and associated recovery strategies, in microsociology. The result is obtained by classifying functional and social disruptions, and by investigating the requirements a robot's architecture should satisfy to exploit such knowledge. The paper also highlights how this level of competence is achieved by focusing on just three dimensions: (i) social capability, (ii) relational role, and (iii) proximity, leaving aside the further complexity of full-fledged human-human interactions. Without going into technical aspects, End-to-end Data-driven Architectures and Modular Architectures are discussed to evaluate the degree to which they can exploit this new set of social and cultural knowledge. Finally, a list of general requirements for such agents is proposed.
<div id='section'>Paperid: <span id='pid'>1691, <a href='https://arxiv.org/pdf/2308.00219.pdf' target='_blank'>https://arxiv.org/pdf/2308.00219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haru Kondoh, Asako Kanezaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00219">Multi-goal Audio-visual Navigation using Sound Direction Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past few years, there has been a great deal of research on navigation tasks in indoor environments using deep reinforcement learning agents. Most of these tasks use only visual information in the form of first-person images to navigate to a single goal. More recently, tasks that simultaneously use visual and auditory information to navigate to the sound source and even navigation tasks with multiple goals instead of one have been proposed. However, there has been no proposal for a generalized navigation task combining these two types of tasks and using both visual and auditory information in a situation where multiple sound sources are goals. In this paper, we propose a new framework for this generalized task: multi-goal audio-visual navigation. We first define the task in detail, and then we investigate the difficulty of the multi-goal audio-visual navigation task relative to the current navigation tasks by conducting experiments in various situations. The research shows that multi-goal audio-visual navigation has the difficulty of the implicit need to separate the sources of sound. Next, to mitigate the difficulties in this new task, we propose a method named sound direction map (SDM), which dynamically localizes multiple sound sources in a learning-based manner while making use of past memories. Experimental results show that the use of SDM significantly improves the performance of multiple baseline methods, regardless of the number of goals.
<div id='section'>Paperid: <span id='pid'>1692, <a href='https://arxiv.org/pdf/2307.15363.pdf' target='_blank'>https://arxiv.org/pdf/2307.15363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicole Robinson, Brendan Tidd, Dylan Campbell, Dana KuliÄ, Peter Corke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15363">Robotic Vision for Human-Robot Interaction and Collaboration: A Survey and Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic vision for human-robot interaction and collaboration is a critical process for robots to collect and interpret detailed information related to human actions, goals, and preferences, enabling robots to provide more useful services to people. This survey and systematic review presents a comprehensive analysis on robotic vision in human-robot interaction and collaboration over the last 10 years. From a detailed search of 3850 articles, systematic extraction and evaluation was used to identify and explore 310 papers in depth. These papers described robots with some level of autonomy using robotic vision for locomotion, manipulation and/or visual communication to collaborate or interact with people. This paper provides an in-depth analysis of current trends, common domains, methods and procedures, technical processes, data sets and models, experimental testing, sample populations, performance metrics and future challenges. This manuscript found that robotic vision was often used in action and gesture recognition, robot movement in human spaces, object handover and collaborative actions, social communication and learning from demonstration. Few high-impact and novel techniques from the computer vision field had been translated into human-robot interaction and collaboration. Overall, notable advancements have been made on how to develop and deploy robots to assist people.
<div id='section'>Paperid: <span id='pid'>1693, <a href='https://arxiv.org/pdf/2307.12129.pdf' target='_blank'>https://arxiv.org/pdf/2307.12129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Barot, Katja Mombaur, Ewen MacDonald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12129">Estimating speaker direction on a humanoid robot with binaural acoustic signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To achieve human-like behaviour during speech interactions, it is necessary for a humanoid robot to estimate the location of a human talker. Here, we present a method to optimize the parameters used for the direction of arrival (DOA) estimation, while also considering real-time applications for human-robot interaction scenarios. This method is applied to binaural sound source localization framework on a humanoid robotic head. Real data is collected and annotated for this work. Optimizations are performed via a brute force method and a Bayesian model based method, results are validated and discussed, and effects on latency for real-time use are also explored.
<div id='section'>Paperid: <span id='pid'>1694, <a href='https://arxiv.org/pdf/2307.09874.pdf' target='_blank'>https://arxiv.org/pdf/2307.09874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Wenkai, Ji Ruihang, Yue Yiran, Gu Zhonghan, Shu Wanyang, Sam Ge Shuzhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09874">Agricultural Robotic System: The Automation of Detection and Speech Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agriculture industries often face challenges in manual tasks such as planting, harvesting, fertilizing, and detection, which can be time consuming and prone to errors. The "Agricultural Robotic System" project addresses these issues through a modular design that integrates advanced visual, speech recognition, and robotic technologies. This system is comprised of separate but interconnected modules for vision detection and speech recognition, creating a flexible and adaptable solution. The vision detection module uses computer vision techniques, trained on YOLOv5 and deployed on the Jetson Nano in TensorRT format, to accurately detect and identify different items. A robotic arm module then precisely controls the picking up of seedlings or seeds, and arranges them in specific locations. The speech recognition module enhances intelligent human robot interaction, allowing for efficient and intuitive control of the system. This modular approach improves the efficiency and accuracy of agricultural tasks, demonstrating the potential of robotics in the agricultural industry.
<div id='section'>Paperid: <span id='pid'>1695, <a href='https://arxiv.org/pdf/2307.08668.pdf' target='_blank'>https://arxiv.org/pdf/2307.08668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Day, Ioannis Karamouzas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08668">A Study in Zucker: Insights on Interactions Between Humans and Small Service Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advancements in human-robot interaction (HRI), there is still limited knowledge about how humans interact and behave in the presence of small service indoor robots and, subsequently, about the human-centered behavior of such robots. This also raises concerns about the applicability of current trajectory prediction methods to indoor HRI settings as well as the accuracy of existing crowd simulation models in shared environments. To address these issues, we introduce a new HRI dataset focusing on interactions between humans and small differential drive robots running different types of controllers. Our analysis shows that anticipatory and non-anticipatory robot controllers impose similar constraints to humans' safety and efficiency. Additionally, we found that current state-of-the-art models for human trajectory prediction can adequately extend to indoor HRI settings. Finally, we show that humans respond differently to small differential drives than to other humans when collisions are imminent, since interacting with small robots can only cause a finite level of social discomfort as compared to human-human interactions. Our dataset and related code and models are available at: https://motion-lab.github.io/ZuckerDataset.
<div id='section'>Paperid: <span id='pid'>1696, <a href='https://arxiv.org/pdf/2307.05225.pdf' target='_blank'>https://arxiv.org/pdf/2307.05225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Aitsam, Alessandro Di Nuovo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05225">Energy Efficient Personalized Hand-Gesture Recognition with Neuromorphic Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand gestures are a form of non-verbal communication that is used in social interaction and it is therefore required for more natural human-robot interaction. Neuromorphic (brain-inspired) computing offers a low-power solution for Spiking neural networks (SNNs) that can be used for the classification and recognition of gestures. This article introduces the preliminary results of a novel methodology for training spiking convolutional neural networks for hand-gesture recognition so that a humanoid robot with integrated neuromorphic hardware will be able to personalise the interaction with a user according to the shown hand gesture. It also describes other approaches that could improve the overall performance of the model.
<div id='section'>Paperid: <span id='pid'>1697, <a href='https://arxiv.org/pdf/2305.16502.pdf' target='_blank'>https://arxiv.org/pdf/2305.16502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ifueko Igbinedion, Sertac Karaman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16502">Learning When to Ask for Help: Efficient Interactive Navigation via Implicit Uncertainty Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating alongside humans often encounter unfamiliar environments that make autonomous task completion challenging. Though improving models and increasing dataset size can enhance a robot's performance in unseen environments, data collection and model refinement may be impractical in every environment. Approaches that utilize human demonstrations through manual operation can aid in refinement and generalization, but often require significant data collection efforts to generate enough demonstration data to achieve satisfactory task performance. Interactive approaches allow for humans to provide correction to robot action in real time, but intervention policies are often based on explicit factors related to state and task understanding that may be difficult to generalize. Addressing these challenges, we train a lightweight interaction policy that allows robots to decide when to proceed autonomously or request expert assistance at estimated times of uncertainty. An implicit estimate of uncertainty is learned via evaluating the feature extraction capabilities of the robot's visual navigation policy. By incorporating part-time human interaction, robots recover quickly from their mistakes, significantly improving the odds of task completion. Incorporating part-time interaction yields an increase in success of 0.38 with only a 0.3 expert interaction rate within the Habitat simulation environment using a simulated human expert. We further show success transferring this approach to a new domain with a real human expert, improving success from less than 0.1 with an autonomous agent to 0.92 with a 0.23 human interaction rate. This approach provides a practical means for robots to interact and learn from humans in real-world settings.
<div id='section'>Paperid: <span id='pid'>1698, <a href='https://arxiv.org/pdf/2305.13567.pdf' target='_blank'>https://arxiv.org/pdf/2305.13567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bohan Wu, Roberto Martin-Martin, Li Fei-Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13567">M-EMBER: Tackling Long-Horizon Mobile Manipulation via Factorized Domain Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a method to create visuomotor mobile manipulation solutions for long-horizon activities. We propose to leverage the recent advances in simulation to train visual solutions for mobile manipulation. While previous works have shown success applying this procedure to autonomous visual navigation and stationary manipulation, applying it to long-horizon visuomotor mobile manipulation is still an open challenge that demands both perceptual and compositional generalization of multiple skills. In this work, we develop Mobile-EMBER, or M-EMBER, a factorized method that decomposes a long-horizon mobile manipulation activity into a repertoire of primitive visual skills, reinforcement-learns each skill, and composes these skills to a long-horizon mobile manipulation activity. On a mobile manipulation robot, we find that M-EMBER completes a long-horizon mobile manipulation activity, cleaning_kitchen, achieving a 53% success rate. This requires successfully planning and executing five factorized, learned visual skills.
<div id='section'>Paperid: <span id='pid'>1699, <a href='https://arxiv.org/pdf/2305.11772.pdf' target='_blank'>https://arxiv.org/pdf/2305.11772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aran Nayebi, Rishi Rajalingham, Mehrdad Jazayeri, Guangyu Robert Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11772">Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-centric objectives, to models that future predict in the latent space of purely static image-based or dynamic video-based pretrained foundation models. We find strong differentiation across these model classes in their ability to predict neural and behavioral data both within and across diverse environments. In particular, we find that neural responses are currently best predicted by models trained to predict the future state of their environment in the latent space of pretrained foundation models optimized for dynamic scenes in a self-supervised manner. Notably, models that future predict in the latent space of video foundation models that are optimized to support a diverse range of sensorimotor tasks, reasonably match both human behavioral error patterns and neural dynamics across all environmental scenarios that we were able to test. Overall, these findings suggest that the neural mechanisms and behaviors of primate mental simulation are thus far most consistent with being optimized to future predict on dynamic, reusable visual representations that are useful for Embodied AI more generally.
<div id='section'>Paperid: <span id='pid'>1700, <a href='https://arxiv.org/pdf/2305.10941.pdf' target='_blank'>https://arxiv.org/pdf/2305.10941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarah Wingert, Christian Becker-Asano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10941">Evaluating the validity of a German translation of an uncanniness questionnaire</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When researching on the acceptance of robots in Human-Robot-Interaction the Uncanny Valley needs to be considered. Reusable and standardized measures for it are essential. In this paper one such questionnaire got translated into German. The translated indices got evaluated (n=140) for reliability with Cronbach's alpha. Additionally the items were tested with an exploratory and a confirmatory factor analysis for problematic correlations. The results yield a good reliability for the translated indices and showed some items that need to be further checked.
<div id='section'>Paperid: <span id='pid'>1701, <a href='https://arxiv.org/pdf/2305.00282.pdf' target='_blank'>https://arxiv.org/pdf/2305.00282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijun Yuan, Andreas Nuchter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00282">NSLF-OL: Online Learning of Neural Surface Light Fields alongside Real-time Incremental 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive novel view generation is an important technology in the field of graphics and has recently also received attention for operator-based human-robot interaction. However, the involved training is time-consuming, and thus the current test scope is majorly on object capturing. This limits the usage of related models in the robotics community for 3D reconstruction since robots (1) usually only capture a very small range of view directions to surfaces that cause arbitrary predictions on unseen, novel direction, (2) requires real-time algorithms, and (3) work with growing scenes, e.g., in robotic exploration. The paper proposes a novel Neural Surface Light Fields model that copes with the small range of view directions while producing a good result in unseen directions. Exploiting recent encoding techniques, the training of our model is highly efficient.
  In addition, we design Multiple Asynchronous Neural Agents (MANA), a universal framework to learn each small region in parallel for large-scale growing scenes. Our model learns online the Neural Surface Light Fields (NSLF) aside from real-time 3D reconstruction with a sequential data stream as the shared input. In addition to online training, our model also provides real-time rendering after completing the data stream for visualization. We implement experiments using well-known RGBD indoor datasets, showing the high flexibility to embed our model into real-time 3D reconstruction and demonstrating high-fidelity view synthesis for these scenes. The code is available on github.
<div id='section'>Paperid: <span id='pid'>1702, <a href='https://arxiv.org/pdf/2304.12499.pdf' target='_blank'>https://arxiv.org/pdf/2304.12499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arda Yigit, David Breton, Zhou Zhou, Thierry Laliberte, Clement Gosselin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12499">Kinematic Analysis and Design of a Novel (6+3)-DoF Parallel Robot with Fixed Actuators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel kinematically redundant (6+3)-DoF parallel robot is presented in this paper. Three identical 3-DoF RU/2-RUS legs are attached to a configurable platform through spherical joints. With the selected leg mechanism, the motors are mounted at the base, reducing the reflected inertia. The robot is intended to be actuated with direct-drive motors in order to perform intuitive physical human-robot interaction. The design of the leg mechanism maximizes the workspace in which the end-effector of the leg can have a 2g acceleration in all directions. All singularities of the leg mechanism are identified under a simplifying assumption. A CAD model of the (6+3)-DoF robot is presented in order to illustrate the preliminary design of the robot.
<div id='section'>Paperid: <span id='pid'>1703, <a href='https://arxiv.org/pdf/2304.10360.pdf' target='_blank'>https://arxiv.org/pdf/2304.10360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MatouÅ¡ JelÃ­nek, Kerstin Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10360">Trust regulation in Social Robotics: From Violation to Repair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While trust in human-robot interaction is increasingly recognized as necessary for the implementation of social robots, our understanding of regulating trust in human-robot interaction is yet limited. In the current experiment, we evaluated different approaches to trust calibration in human-robot interaction. The within-subject experimental approach utilized five different strategies for trust calibration: proficiency, situation awareness, transparency, trust violation, and trust repair. We implemented these interventions into a within-subject experiment where participants (N=24) teamed up with a social robot and played a collaborative game. The level of trust was measured after each section using the Multi-Dimensional Measure of Trust (MDMT) scale. As expected, the interventions have a significant effect on i) violating and ii) repairing the level of trust throughout the interaction. Consequently, the robot demonstrating situation awareness was perceived as significantly more benevolent than the baseline.
<div id='section'>Paperid: <span id='pid'>1704, <a href='https://arxiv.org/pdf/2304.10246.pdf' target='_blank'>https://arxiv.org/pdf/2304.10246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baris Kayalibay, Atanas Mirchev, Ahmed Agha, Patrick van der Smagt, Justin Bayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10246">Filter-Aware Model-Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partially-observable problems pose a trade-off between reducing costs and gathering information. They can be solved optimally by planning in belief space, but that is often prohibitively expensive. Model-predictive control (MPC) takes the alternative approach of using a state estimator to form a belief over the state, and then plan in state space. This ignores potential future observations during planning and, as a result, cannot actively increase or preserve the certainty of its own state estimate. We find a middle-ground between planning in belief space and completely ignoring its dynamics by only reasoning about its future accuracy. Our approach, filter-aware MPC, penalises the loss of information by what we call "trackability", the expected error of the state estimator. We show that model-based simulation allows condensing trackability into a neural network, which allows fast planning. In experiments involving visual navigation, realistic every-day environments and a two-link robot arm, we show that filter-aware MPC vastly improves regular MPC.
<div id='section'>Paperid: <span id='pid'>1705, <a href='https://arxiv.org/pdf/2304.08379.pdf' target='_blank'>https://arxiv.org/pdf/2304.08379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AntÃ³nio Amorim, Diana GuimarÃ£es, Tiago MendonÃ§a, Pedro Neto, Paulo Costa, AntÃ³nio Paulo Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08379">Robust human position estimation in cooperative robotic cells</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly present in our lives, sharing the workspace and tasks with human co-workers. However, existing interfaces for human-robot interaction / cooperation (HRI/C) have limited levels of intuitiveness to use and safety is a major concern when humans and robots share the same workspace. Many times, this is due to the lack of a reliable estimation of the human pose in space which is the primary input to calculate the human-robot minimum distance (required for safety and collision avoidance) and HRI/C featuring machine learning algorithms classifying human behaviours / gestures. Each sensor type has its own characteristics resulting in problems such as occlusions (vision) and drift (inertial) when used in an isolated fashion. In this paper, it is proposed a combined system that merges the human tracking provided by a 3D vision sensor with the pose estimation provided by a set of inertial measurement units (IMUs) placed in human body limbs. The IMUs compensate the gaps in occluded areas to have tracking continuity. To mitigate the lingering effects of the IMU offset we propose a continuous online calculation of the offset value. Experimental tests were designed to simulate human motion in a human-robot collaborative environment where the robot moves away to avoid unexpected collisions with de human. Results indicate that our approach is able to capture the human\textsc's position, for example the forearm, with a precision in the millimetre range and robustness to occlusions.
<div id='section'>Paperid: <span id='pid'>1706, <a href='https://arxiv.org/pdf/2304.06777.pdf' target='_blank'>https://arxiv.org/pdf/2304.06777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. A. SimÃ£o, O. Gibaru, P. Neto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06777">Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online recognition of gestures is critical for intuitive human-robot interaction (HRI) and further push collaborative robotics into the market, making robots accessible to more people. The problem is that it is difficult to achieve accurate gesture recognition in real unstructured environments, often using distorted and incomplete multisensory data. This paper introduces an HRI framework to classify large vocabularies of interwoven static gestures (SGs) and dynamic gestures (DGs) captured with wearable sensors. DG features are obtained by applying data dimensionality reduction to raw data from sensors (resampling with cubic interpolation and principal component analysis). Experimental tests were conducted using the UC2017 hand gesture dataset with samples from eight different subjects. The classification models show an accuracy of 95.6% for a library of 24 SGs with a random forest and 99.3% for 10 DGs using artificial neural networks. These results compare equally or favorably with different commonly used classifiers. Long short-term memory deep networks achieved similar performance in online frame-by-frame classification using raw incomplete data, performing better in terms of accuracy than static models with specially crafted features, but worse in training and inference time. The recognized gestures are used to teleoperate a robot in a collaborative process that consists in preparing a breakfast meal.
<div id='section'>Paperid: <span id='pid'>1707, <a href='https://arxiv.org/pdf/2304.06319.pdf' target='_blank'>https://arxiv.org/pdf/2304.06319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Cucurull, AnaÃ­s Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06319">Continual Learning of Hand Gestures for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present an efficient method to incrementally learn to classify static hand gestures. This method allows users to teach a robot to recognize new symbols in an incremental manner. Contrary to other works which use special sensors or external devices such as color or data gloves, our proposed approach makes use of a single RGB camera to perform static hand gesture recognition from 2D images. Furthermore, our system is able to incrementally learn up to 38 new symbols using only 5 samples for each old class, achieving a final average accuracy of over 90\%. In addition to that, the incremental training time can be reduced to a 10\% of the time required when using all data available.
<div id='section'>Paperid: <span id='pid'>1708, <a href='https://arxiv.org/pdf/2304.05067.pdf' target='_blank'>https://arxiv.org/pdf/2304.05067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tushar Sandhan, Sukanya Sonowal, Jin Young Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05067">Audio Bank: A High-Level Acoustic Signal Representation for Audio Event Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic audio event recognition plays a pivotal role in making human robot interaction more closer and has a wide applicability in industrial automation, control and surveillance systems. Audio event is composed of intricate phonic patterns which are harmonically entangled. Audio recognition is dominated by low and mid-level features, which have demonstrated their recognition capability but they have high computational cost and low semantic meaning. In this paper, we propose a new computationally efficient framework for audio recognition. Audio Bank, a new high-level representation of audio, is comprised of distinctive audio detectors representing each audio class in frequency-temporal space. Dimensionality of the resulting feature vector is reduced using non-negative matrix factorization preserving its discriminability and rich semantic information. The high audio recognition performance using several classifiers (SVM, neural network, Gaussian process classification and k-nearest neighbors) shows the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1709, <a href='https://arxiv.org/pdf/2304.02639.pdf' target='_blank'>https://arxiv.org/pdf/2304.02639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Klemen Kotar, Aaron Walsman, Roozbeh Mottaghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02639">ENTL: Embodied Navigation Trajectory Learner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Embodied Navigation Trajectory Learner (ENTL), a method for extracting long sequence representations for embodied navigation. Our approach unifies world modeling, localization and imitation learning into a single sequence prediction task. We train our model using vector-quantized predictions of future states conditioned on current states and actions. ENTL's generic architecture enables sharing of the spatio-temporal sequence encoder for multiple challenging embodied tasks. We achieve competitive performance on navigation tasks using significantly less data than strong baselines while performing auxiliary tasks such as localization and future frame prediction (a proxy for world modeling). A key property of our approach is that the model is pre-trained without any explicit reward signal, which makes the resulting model generalizable to multiple tasks and environments.
<div id='section'>Paperid: <span id='pid'>1710, <a href='https://arxiv.org/pdf/2303.17582.pdf' target='_blank'>https://arxiv.org/pdf/2303.17582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Amine, Mostafa Aldilati, Hadi Hasan, Noel Maalouf, Imad H. Elhajj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17582">Human-Robot Interaction using VAHR: Virtual Assistant, Human, and Robots in the Loop</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots have become ubiquitous tools in various industries and households, highlighting the importance of human-robot interaction (HRI). This has increased the need for easy and accessible communication between humans and robots. Recent research has focused on the intersection of virtual assistant technology, such as Amazon's Alexa, with robots and its effect on HRI. This paper presents the Virtual Assistant, Human, and Robots in the loop (VAHR) system, which utilizes bidirectional communication to control multiple robots through Alexa. VAHR's performance was evaluated through a human-subjects experiment, comparing objective and subjective metrics of traditional keyboard and mouse interfaces to VAHR. The results showed that VAHR required 41% less Robot Attention Demand and ensured 91% more Fan-out time compared to the standard method. Additionally, VAHR led to a 62.5% improvement in multi-tasking, highlighting the potential for efficient human-robot interaction in physically- and mentally-demanding scenarios. However, subjective metrics revealed a need for human operators to build confidence and trust with this new method of operation.
<div id='section'>Paperid: <span id='pid'>1711, <a href='https://arxiv.org/pdf/2303.13016.pdf' target='_blank'>https://arxiv.org/pdf/2303.13016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elijah Wyckoff, Ronan Reza, Fernando Moreu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13016">Feedback and Control of Dynamics and Robotics using Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-machine interaction (HMI) and human-robot interaction (HRI) can assist structural monitoring and structural dynamics testing in the laboratory and field. In vibratory experimentation, one mode of generating vibration is to use electrodynamic exciters. Manual control is a common way of setting the input of the exciter by the operator. To measure the structural responses to these generated vibrations sensors are attached to the structure. These sensors can be deployed by repeatable robots with high endurance, which require on-the-fly control. If the interface between operators and the controls was augmented, then operators can visualize the experiments, exciter levels, and define robot input with a better awareness of the area of interest. Robots can provide better aid to humans if intelligent on-the-fly control of the robot is: (1) quantified and presented to the human; (2) conducted in real-time for human feedback informed by data. Information provided by the new interface would be used to change the control input based on their understanding of real-time parameters. This research proposes using Augmented Reality (AR) applications to provide humans with sensor feedback and control of actuators and robots. This method improves cognition by allowing the operator to maintain awareness of structures while adjusting conditions accordingly with the assistance of the new real-time interface. One interface application is developed to plot sensor data in addition to voltage, frequency, and duration controls for vibration generation. Two more applications are developed under similar framework, one to control the position of a mediating robot and one to control the frequency of the robot movement. This paper presents the proposed model for the new control loop and then compares the new approach with a traditional method by measuring time delay in control input and user efficiency.
<div id='section'>Paperid: <span id='pid'>1712, <a href='https://arxiv.org/pdf/2303.10988.pdf' target='_blank'>https://arxiv.org/pdf/2303.10988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khaled Kassem, Alia Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10988">This Was (Not) Intended: How Intent Communication and Biometrics Can Enhance Social Interactions With Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Socially Assistive Robots (SARs) are robots that are designed to replicate the role of a caregiver, coach, or teacher, providing emotional, cognitive, and social cues to support a specific group. SARs are becoming increasingly prevalent, especially in elderly care. Effective communication, both explicit and implicit, is a critical aspect of human-robot interaction involving SARs. Intent communication is necessary for SARs to engage in effective communication with humans. Biometrics can provide crucial information about a person's identity or emotions. By linking these biometric signals to the communication of intent, SARs can gain a profound understanding of their users and tailor their interactions accordingly. The development of reliable and robust biometric sensing and analysis systems is critical to the success of SARs. In this work, we focus on four different aspects to evaluate the communication of intent involving SARs, existing works, and our outlook on future works and applications.
<div id='section'>Paperid: <span id='pid'>1713, <a href='https://arxiv.org/pdf/2303.10195.pdf' target='_blank'>https://arxiv.org/pdf/2303.10195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Furkan Kaynar, Sudarshan Rajagopalan, Shaobo Zhou, Eckehard Steinbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10195">Remote Task-oriented Grasp Area Teaching By Non-Experts through Interactive Segmentation and Few-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A robot operating in unstructured environments must be able to discriminate between different grasping styles depending on the prospective manipulation task. Having a system that allows learning from remote non-expert demonstrations can very feasibly extend the cognitive skills of a robot for task-oriented grasping. We propose a novel two-step framework towards this aim. The first step involves grasp area estimation by segmentation. We receive grasp area demonstrations for a new task via interactive segmentation, and learn from these few demonstrations to estimate the required grasp area on an unseen scene for the given task. The second step is autonomous grasp estimation in the segmented region. To train the segmentation network for few-shot learning, we built a grasp area segmentation (GAS) dataset with 10089 images grouped into 1121 segmentation tasks. We benefit from an efficient meta learning algorithm for training for few-shot adaptation. Experimental evaluation showed that our method successfully detects the correct grasp area on the respective objects in unseen test scenes and effectively allows remote teaching of new grasp strategies by non-experts.
<div id='section'>Paperid: <span id='pid'>1714, <a href='https://arxiv.org/pdf/2303.08109.pdf' target='_blank'>https://arxiv.org/pdf/2303.08109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Yihe, Rana Alkhoury Maroun, Barbara Webb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08109">Vision-based route following by an embodied insect-inspired sparse neural network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded the FlyHash model is more efficient than others, especially in terms of data encoding.
<div id='section'>Paperid: <span id='pid'>1715, <a href='https://arxiv.org/pdf/2303.07451.pdf' target='_blank'>https://arxiv.org/pdf/2303.07451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malay Joshi, Aditi Shukla, Jayesh Srivastava, Manya Rastogi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07451">DRISHTI: Visual Navigation Assistant for Visually Impaired</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's society, where independent living is becoming increasingly important, it can be extremely constricting for those who are blind. Blind and visually impaired (BVI) people face challenges because they need manual support to prompt information about their environment. In this work, we took our first step towards developing an affordable and high-performing eye wearable assistive device, DRISHTI, to provide visual navigation assistance for BVI people. This system comprises a camera module, ESP32 processor, Bluetooth module, smartphone and speakers. Using artificial intelligence, this system is proposed to detect and understand the nature of the users' path and obstacles ahead of the user in that path and then inform BVI users about it via audio output to enable them to acquire directions by themselves on their journey. This first step discussed in this paper involves establishing a proof-of-concept of achieving the right balance of affordability and performance by testing an initial software integration of a currency detection algorithm on a low-cost embedded arrangement. This work will lay the foundation for our upcoming works toward achieving the goal of assisting the maximum of BVI people around the globe in moving independently.
<div id='section'>Paperid: <span id='pid'>1716, <a href='https://arxiv.org/pdf/2303.04841.pdf' target='_blank'>https://arxiv.org/pdf/2303.04841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jimin Rhim, Sonya S. Kwak, Angelica Lim, Jason Millar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04841">The dynamic nature of trust: Trust in Human-Robot Interaction revisited</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The role of robots is expanding from tool to collaborator. Socially assistive robots (SARs) are an example of collaborative robots that assist humans in the real world. As robots enter our social sphere, unforeseen risks occur during human-robot interaction (HRI), as everyday human space is full of uncertainties. Risk introduces an element of trust, so understanding human trust in the robot is imperative to initiate and maintain interactions with robots over time. While many scholars have investigated the issue of human-robot trust, a significant portion of that discussion is rooted in the human-automation interaction literature. As robots are no longer mere instruments, but social agents that co-exist with humans, we need a new lens to investigate the longitudinal dynamic nature of trust in HRI. In this position paper, we contend that focusing on the dynamic nature of trust as a new inquiry will help us better design trustworthy robots.
<div id='section'>Paperid: <span id='pid'>1717, <a href='https://arxiv.org/pdf/2302.05763.pdf' target='_blank'>https://arxiv.org/pdf/2302.05763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Semeraro, Jon Carberry, Angelo Cangelosi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05763">Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed of spatio-temporal graph convolutional networks (STGCN) to recognise the joint activities of the pairs of people. The results showed that it is possible to make use of data collected in this way for pair HRC settings and get similar performances compared to using training data regarding groups of users recorded under the same settings, relieving from the technical difficulties involved in producing these data.
  The related code and collected data are publicly available.
<div id='section'>Paperid: <span id='pid'>1718, <a href='https://arxiv.org/pdf/2302.00495.pdf' target='_blank'>https://arxiv.org/pdf/2302.00495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyuan Zhou, Peter Paik, S. Farokh Atashzar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00495">Upper-limb Geometric MyoPassivity Map for Physical Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intrinsic biomechanical characteristic of the human upper limb plays a central role in absorbing the interactive energy during physical human-robot interaction (pHRI). We have recently shown that based on the concept of ``Excess of Passivity (EoP)," from nonlinear control theory, it is possible to decode such energetic behavior for both upper and lower limbs. The extracted knowledge can be used in the design of controllers for optimizing the transparency and fidelity of force fields in human-robot interaction and in haptic systems. In this paper, for the first time, we investigate the frequency behavior of the passivity map for the upper limb when the muscle co-activation was controlled in real-time through visual electromyographic feedback. Five healthy subjects (age: 27 +/- 5) were included in this study. The energetic behavior was evaluated at two stimulation frequencies at eight interaction directions over two controlled muscle co-activation levels. Electromyography (EMG) was captured using the Delsys Wireless Trigno system. Results showed a correlation between EMG and EoP, which was further altered by increasing the frequency. The proposed energetic behavior is named the Geometric MyoPassivity (GMP) map. The findings indicate that the GMP map has the potential to be used in real-time to quantify the absorbable energy, thus passivity margin of stability for upper limb interaction during pHRI.
<div id='section'>Paperid: <span id='pid'>1719, <a href='https://arxiv.org/pdf/2301.06834.pdf' target='_blank'>https://arxiv.org/pdf/2301.06834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>E. Bartoli, F. Argenziano, V. Suriani, D. Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06834">Knowledge Acquisition and Completion for Long-Term Human-Robot Interactions using Knowledge Graph Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Human-Robot Interaction (HRI) systems, a challenging task is sharing the representation of the operational environment, fusing symbolic knowledge and perceptions, between users and robots. With the existing HRI pipelines, users can teach the robots some concepts to increase their knowledge base. Unfortunately, the data coming from the users are usually not enough dense for building a consistent representation. Furthermore, the existing approaches are not able to incrementally build up their knowledge base, which is very important when robots have to deal with dynamic contexts. To this end, we propose an architecture to gather data from users and environments in long-runs of continual learning. We adopt Knowledge Graph Embedding techniques to generalize the acquired information with the goal of incrementally extending the robot's inner representation of the environment. We evaluate the performance of the overall continual learning architecture by measuring the capabilities of the robot of learning entities and relations coming from unknown contexts through a series of incremental learning sessions.
<div id='section'>Paperid: <span id='pid'>1720, <a href='https://arxiv.org/pdf/2301.02382.pdf' target='_blank'>https://arxiv.org/pdf/2301.02382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjia Liu, Jianfei Guo, Zehui Meng, Jingtao Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02382">ReVoLT: Relational Reasoning and Voronoi Local Graph Planning for Target-driven Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI is an inevitable trend that emphasizes the interaction between intelligent entities and the real world, with broad applications in Robotics, especially target-driven navigation. This task requires the robot to find an object of a certain category efficiently in an unknown domestic environment. Recent works focus on exploiting layout relationships by graph neural networks (GNNs). However, most of them obtain robot actions directly from observations in an end-to-end manner via an incomplete relation graph, which is not interpretable and reliable. We decouple this task and propose ReVoLT, a hierarchical framework: (a) an object detection visual front-end, (b) a high-level reasoner (infers semantic sub-goals), (c) an intermediate-level planner (computes geometrical positions), and (d) a low-level controller (executes actions). ReVoLT operates with a multi-layer semantic-spatial topological graph. The reasoner uses multiform structured relations as priors, which are obtained from combinatorial relation extraction networks composed of unsupervised GraphSAGE, GCN, and GraphRNN-based Region Rollout. The reasoner performs with Upper Confidence Bound for Tree (UCT) to infer semantic sub-goals, accounting for trade-offs between exploitation (depth-first searching) and exploration (regretting). The lightweight intermediate-level planner generates instantaneous spatial sub-goal locations via an online constructed Voronoi local graph. The simulation experiments demonstrate that our framework achieves better performance in the target-driven navigation tasks and generalizes well, which has an 80% improvement compared to the existing state-of-the-art method. The code and result video will be released at https://ventusff.github.io/ReVoLT-website/.
<div id='section'>Paperid: <span id='pid'>1721, <a href='https://arxiv.org/pdf/2211.03267.pdf' target='_blank'>https://arxiv.org/pdf/2211.03267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Inoue, Hiroki Ohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03267">Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Instruction Following (EIF) studies how autonomous mobile manipulation robots should be controlled to accomplish long-horizon tasks described by natural language instructions. While much research on EIF is conducted in simulators, the ultimate goal of the field is to deploy the agents in real life. This is one of the reasons why recent methods have moved away from training models end-to-end and take modular approaches, which do not need the costly expert operation data. However, as it is still in the early days of importing modular ideas to EIF, a search for modules effective in the EIF task is still far from a conclusion. In this paper, we propose to extend the modular design using knowledge obtained from two external sources. First, we show that embedding the physical constraints of the deployed robots into the module design is highly effective. Our design also allows the same modular system to work across robots of different configurations with minimal modifications. Second, we show that the landmark-based object search, previously implemented by a trained model requiring a dedicated set of data, can be replaced by an implementation that prompts pretrained large language models for landmark-object relationships, eliminating the need for collecting dedicated training data. Our proposed Prompter achieves 41.53\% and 45.32\% on the ALFRED benchmark with high-level instructions only and step-by-step instructions, respectively, significantly outperforming the previous state of the art by 5.46\% and 9.91\%.
<div id='section'>Paperid: <span id='pid'>1722, <a href='https://arxiv.org/pdf/2209.01012.pdf' target='_blank'>https://arxiv.org/pdf/2209.01012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuele Vinanzi, Angelo Cangelosi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01012">CASPER: Cognitive Architecture for Social Perception and Engagement in Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our world is being increasingly pervaded by intelligent robots with varying degrees of autonomy. To seamlessly integrate themselves in our society, these machines should possess the ability to navigate the complexities of our daily routines even in the absence of a human's direct input. In other words, we want these robots to understand the intentions of their partners with the purpose of predicting the best way to help them. In this paper, we present CASPER (Cognitive Architecture for Social Perception and Engagement in Robots): a symbolic cognitive architecture that uses qualitative spatial reasoning to anticipate the pursued goal of another agent and to calculate the best collaborative behavior. This is performed through an ensemble of parallel processes that model a low-level action recognition and a high-level goal understanding, both of which are formally verified. We have tested this architecture in a simulated kitchen environment and the results we have collected show that the robot is able to both recognize an ongoing goal and to properly collaborate towards its achievement. This demonstrates a new use of Qualitative Spatial Relations applied to the problem of intention reading in the domain of human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1723, <a href='https://arxiv.org/pdf/2207.01130.pdf' target='_blank'>https://arxiv.org/pdf/2207.01130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulia Perugia, Dominika Lisy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.01130">Robot's Gendering Trouble: A Scoping Review of Gendering Humanoid Robots and its Effects on HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The discussion around the problematic practice of gendering humanoid robots has risen to the foreground in the last few years. To lay the basis for a thorough understanding of how robot's "gender" has been understood within the Human-Robot Interaction (HRI) community - i.e., how it has been manipulated, in which contexts, and which effects it has yield on people's perceptions and interactions with robots - we performed a scoping review of the literature. We identified 553 papers relevant for our review retrieved from 5 different databases. The final sample of reviewed papers included 35 papers written between 2005 and 2021, which involved a total of 3902 participants. In this article, we thoroughly summarize these papers by reporting information about their objectives and assumptions on gender (i.e., definitions and reasons to manipulate gender), their manipulation of robot's "gender" (i.e., gender cues and manipulation checks), their experimental designs (e.g., demographics of participants, employed robots), and their results (i.e., main and interaction effects). The review reveals that robot's "gender" does not affect crucial constructs for the HRI, such as likability and acceptance, but rather bears its strongest effect on stereotyping. We leverage our different epistemological backgrounds in Social Robotics and Gender Studies to provide a comprehensive interdisciplinary perspective on the results of the review and suggest ways to move forward in the field of HRI.
<div id='section'>Paperid: <span id='pid'>1724, <a href='https://arxiv.org/pdf/2207.00052.pdf' target='_blank'>https://arxiv.org/pdf/2207.00052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Wang, Ching-Yun Ko, Pulkit Agrawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.00052">Visual Pre-training for Navigation: What Can We Learn from Noise?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz
<div id='section'>Paperid: <span id='pid'>1725, <a href='https://arxiv.org/pdf/2202.03702.pdf' target='_blank'>https://arxiv.org/pdf/2202.03702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Benitez Sandoval, Ricardo Sosa, Massimiliano Cappuccio, Tomasz Bednarz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.03702">Human-Robot Creative Interactions (HRCI): Exploring Creativity in Artificial Agents Using a Story-Telling Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creativity in social robots requires further attention in the interdisciplinary field of Human-Robot Interaction (HRI). This paper investigates the hypothesised connection between the perceived creative agency and the animacy of social robots. The goal of this work is to assess the relevance of robot movements in the attribution of creativity to robots. The results of this work inform the design of future Human-Robot Creative Interactions (HRCI). The study uses a storytelling game based on visual imagery inspired by the game 'Story Cubes' to explore the perceived creative agency of social robots. This game is used to tell a classic story for children with an alternative ending. A 2x2 experiment was designed to compare two conditions: the robot telling the original version of the story and the robot plot-twisting the end of the story. A Robotis Mini humanoid robot was used for the experiment. As a novel contribution, we propose an adaptation of the Short Scale Creative Self scale (SSCS) to measure perceived creative agency in robots. We also use the Godspeed scale to explore different attributes of social robots in this setting. We did not obtain significant main effects of the robot movements or the story in the participants' scores. However, we identified significant main effects of the robot movements in features of animacy, likeability, and perceived safety. This initial work encourages further studies experimenting with different robot embodiment and movements to evaluate the perceived creative agency in robots and inform the design of future robots that participate in creative interactions.
<div id='section'>Paperid: <span id='pid'>1726, <a href='https://arxiv.org/pdf/2110.10571.pdf' target='_blank'>https://arxiv.org/pdf/2110.10571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nazarova Elena, Sautenkov Oleg, Altamirano Cabrera Miguel, Tirado Jonathan, Serpiva Valerii, Rakhmatulin Viktor, Tsetserukou Dzmitry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.10571">CobotAR: Interaction with Robots using Omnidirectionally Projected Image and DNN-based Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several technological solutions supported the creation of interfaces for Augmented Reality (AR) multi-user collaboration in the last years. However, these technologies require the use of wearable devices. We present CobotAR - a new AR technology to achieve the Human-Robot Interaction (HRI) by gesture recognition based on Deep Neural Network (DNN) - without an extra wearable device for the user. The system allows users to have a more intuitive experience with robotic applications using just their hands. The CobotAR system assumes the AR spatial display created by a mobile projector mounted on a 6 DoF robot. The proposed technology suggests a novel way of interaction with machines to achieve safe, intuitive, and immersive control mediated by a robotic projection system and DNN-based algorithm. We conducted the experiment with several parameters assessment during this research, which allows the users to define the positives and negatives of the new approach. The mental demand of CobotAR system is twice less than Wireless Gamepad and by 16\% less than Teach Pendant.
<div id='section'>Paperid: <span id='pid'>1727, <a href='https://arxiv.org/pdf/2103.01626.pdf' target='_blank'>https://arxiv.org/pdf/2103.01626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan B. Liu, Bastian SchÃ¼rmann, Matthias Althoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.01626">Guarantees for Real Robotic Systems: Unifying Formal Controller Synthesis and Reachset-Conformant Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are used increasingly often in safety-critical scenarios, such as robotic surgery or human-robot interaction. To ensure stringent performance criteria, formal controller synthesis is a promising direction to guarantee that robots behave as desired. However, formally ensured properties only transfer to the real robot when the model is appropriate. We address this problem by combining the identification of a reachset-conformant model with controller synthesis. Since the reachset-conformant model contains all the measured behaviors of the real robot, the safety properties of the model transfer to the real robot. The transferability is demonstrated by experiments on a real robot, for which we synthesize tracking controllers.
<div id='section'>Paperid: <span id='pid'>1728, <a href='https://arxiv.org/pdf/2011.05533.pdf' target='_blank'>https://arxiv.org/pdf/2011.05533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Marge, Carol Espy-Wilson, Nigel Ward
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2011.05533">Spoken Language Interaction with Robots: Research Issues and Recommendations, Report from the NSF Future Directions Workshop</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With robotics rapidly advancing, more effective human-robot interaction is increasingly needed to realize the full potential of robots for society. While spoken language must be part of the solution, our ability to provide spoken language interaction capabilities is still very limited. The National Science Foundation accordingly convened a workshop, bringing together speech, language, and robotics researchers to discuss what needs to be done. The result is this report, in which we identify key scientific and engineering advances needed.
  Our recommendations broadly relate to eight general themes. First, meeting human needs requires addressing new challenges in speech technology and user experience design. Second, this requires better models of the social and interactive aspects of language use. Third, for robustness, robots need higher-bandwidth communication with users and better handling of uncertainty, including simultaneous consideration of multiple hypotheses and goals. Fourth, more powerful adaptation methods are needed, to enable robots to communicate in new environments, for new tasks, and with diverse user populations, without extensive re-engineering or the collection of massive training data. Fifth, since robots are embodied, speech should function together with other communication modalities, such as gaze, gesture, posture, and motion. Sixth, since robots operate in complex environments, speech components need access to rich yet efficient representations of what the robot knows about objects, locations, noise sources, the user, and other humans. Seventh, since robots operate in real time, their speech and language processing components must also. Eighth, in addition to more research, we need more work on infrastructure and resources, including shareable software modules and internal interfaces, inexpensive hardware, baseline systems, and diverse corpora.
<div id='section'>Paperid: <span id='pid'>1729, <a href='https://arxiv.org/pdf/1812.03484.pdf' target='_blank'>https://arxiv.org/pdf/1812.03484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bishal Ghosh, Abhinav Dhall, Ekta Singla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1812.03484">Speech-Gesture Mapping and Engagement Evaluation in Human Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A robot needs contextual awareness, effective speech production and complementing non-verbal gestures for successful communication in society. In this paper, we present our end-to-end system that tries to enhance the effectiveness of non-verbal gestures. For achieving this, we identified prominently used gestures in performances by TED speakers and mapped them to their corresponding speech context and modulated speech based upon the attention of the listener. The proposed method utilized Convolutional Pose Machine [4] to detect the human gesture. Dominant gestures of TED speakers were used for learning the gesture-to-speech mapping. The speeches by them were used for training the model. We also evaluated the engagement of the robot with people by conducting a social survey. The effectiveness of the performance was monitored by the robot and it self-improvised its speech pattern on the basis of the attention level of the audience, which was calculated using visual feedback from the camera. The effectiveness of interaction as well as the decisions made during improvisation was further evaluated based on the head-pose detection and interaction survey.
<div id='section'>Paperid: <span id='pid'>1730, <a href='https://arxiv.org/pdf/2509.18200.pdf' target='_blank'>https://arxiv.org/pdf/2509.18200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Ti Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18200">Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational agents must translate egocentric utterances (e.g., "on my right") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.
<div id='section'>Paperid: <span id='pid'>1731, <a href='https://arxiv.org/pdf/2507.21589.pdf' target='_blank'>https://arxiv.org/pdf/2507.21589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21589">Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied intelligence posits that cognitive capabilities fundamentally emerge from - and are shaped by - an agent's real-time sensorimotor interactions with its environment. Such adaptive behavior inherently requires continuous inference under uncertainty. Bayesian statistics offers a principled probabilistic framework to address this challenge by representing knowledge as probability distributions and updating beliefs in response to new evidence. The core computational processes underlying embodied intelligence - including perception, action selection, learning, and even higher-level cognition - can be effectively understood and modeled as forms of Bayesian inference. Despite the deep conceptual connection between Bayesian statistics and embodied intelligence, Bayesian principles have not been widely or explicitly applied in today's embodied intelligence systems. In this work, we examine both Bayesian and contemporary embodied intelligence approaches through two fundamental lenses: search and learning - the two central themes in modern AI, as highlighted in Rich Sutton's influential essay "The Bitter Lesson". This analysis sheds light on why Bayesian inference has not played a central role in the development of modern embodied intelligence. At the same time, it reveals that current embodied intelligence systems remain largely confined to closed-physical-world environments, and highlights the potential for Bayesian methods to play a key role in extending these systems toward truly open physical-world embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1732, <a href='https://arxiv.org/pdf/2507.20395.pdf' target='_blank'>https://arxiv.org/pdf/2507.20395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hafsteinn Einarsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20395">MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Large Language Models (LLMs) increasingly power autonomous agents in robotics and embodied AI, understanding their spatial reasoning capabilities becomes crucial for ensuring reliable real-world deployment. Despite advances in language understanding, current research lacks evaluation of how LLMs perform spatial navigation without visual cues, a fundamental requirement for agents operating with limited sensory information. This paper addresses this gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our methodology employs a function-calling interface where models navigate mazes of varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate feedback and distance-to-wall information, excluding visual input to test fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across identical mazes in both English and Icelandic to assess cross-linguistic transfer of spatial abilities. Our findings reveal striking disparities: while OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$, other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100% of failures attributed to excessive looping behavior where models revisit a cell at least 10 times. We document a significant performance degradation in Icelandic, with models solving mazes 3-4 sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These results have important implications for global deployment of LLM-powered autonomous systems, showing spatial intelligence remains fundamentally constrained by training data availability and highlighting the need for architectural innovations to achieve reliable navigation across linguistic contexts.
<div id='section'>Paperid: <span id='pid'>1733, <a href='https://arxiv.org/pdf/2507.09217.pdf' target='_blank'>https://arxiv.org/pdf/2507.09217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GÃ¶rkay Aydemir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09217">Online Long-term Point Tracking in the Foundation Model Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point tracking aims to identify the same physical point across video frames and serves as a geometry-aware representation of motion. This representation supports a wide range of applications, from robotics to augmented reality, by enabling accurate modeling of dynamic environments. Most existing long-term tracking approaches operate in an offline setting, where future frames are available to refine predictions and recover from occlusions. However, real-world scenarios often demand online predictions: the model must operate causally, using only current and past frames. This constraint is critical in streaming video and embodied AI, where decisions must be made immediately based on past observations. Under such constraints, viewpoint invariance becomes essential. Visual foundation models, trained on diverse large-scale datasets, offer the potential for robust geometric representations. While they lack temporal reasoning on their own, they can be integrated into tracking pipelines to enrich spatial features. In this thesis, we address the problem of long-term point tracking in an online setting, where frames are processed sequentially without access to future information or sliding windows. We begin by evaluating the suitability of visual foundation models for this task and find that they can serve as useful initializations and be integrated into tracking pipelines. However, to enable long-term tracking in an online setting, a dedicated design is still required. In particular, maintaining coherence over time in this causal regime requires memory to propagate appearance and context across frames. To address this, we introduce Track-On, a transformer-based model that treats each tracked point as a query and processes video frames one at a time. Track-On sets a new state of the art across seven public benchmarks, demonstrating the feasibility of long-term tracking without future access.
<div id='section'>Paperid: <span id='pid'>1734, <a href='https://arxiv.org/pdf/2507.04160.pdf' target='_blank'>https://arxiv.org/pdf/2507.04160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subasish Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04160">HyperSumm-RL: A Dialogue Summarization Framework for Modeling Leadership Perception in Social Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces HyperSumm-RL, a hypertext-aware summarization and interaction analysis framework designed to investigate human perceptions of social robot leadership through long-form dialogue. The system utilizes a structured Natural Language Processing (NLP) workflow that combines transformer-based long dialogue summarization, leadership style modeling, and user response analysis, enabling scalable evaluation of social robots in complex human-robot interaction (HRI) settings. Unlike prior work that focuses on static or task-oriented HRI, HyperSumm-RL captures and hypertextually organizes dynamic conversational exchanges into navigable, semantically rich representations which allows researchers to trace interaction threads, identify influence cues, and analyze leadership framing over time. The contributions of this study are threefold: (1) we present a novel infrastructure for summarizing and linking long, multi-turn dialogues using leadership-style taxonomies; (2) we propose an interactive hypertext model that supports relational navigation across conversational themes, participant responses, and robot behavior modes; and (3) we demonstrate the utility of this system in interpreting participant trust, engagement, and expectation shifts during social robot leadership scenarios. The findings reveal how hypertextual workflows can augment HRI research by enabling transparent, interpretable, and semantically grounded analysis of emergent social dynamics.
<div id='section'>Paperid: <span id='pid'>1735, <a href='https://arxiv.org/pdf/2506.07286.pdf' target='_blank'>https://arxiv.org/pdf/2506.07286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Chakravarty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07286">Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown remarkable flexibility for solving inverse problems without task-specific retraining. However, existing approaches such as Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update per denoising step, limiting restoration fidelity and robustness, especially in embedded or out-of-distribution settings. In this work, we introduce a multistep optimization strategy within each denoising timestep, significantly enhancing image quality, perceptual accuracy, and generalization. Our experiments on super-resolution and Gaussian deblurring demonstrate that increasing the number of gradient updates per step improves LPIPS and PSNR with minimal latency overhead. Notably, we validate this approach on a Jetson Orin Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally trained on face datasets, generalizes effectively to natural and aerial scenes. Our findings highlight MPGD's potential as a lightweight, plug-and-play restoration module for real-time visual perception in embodied AI agents such as drones and mobile robots.
<div id='section'>Paperid: <span id='pid'>1736, <a href='https://arxiv.org/pdf/2505.12312.pdf' target='_blank'>https://arxiv.org/pdf/2505.12312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12312">Visuospatial Cognitive Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
<div id='section'>Paperid: <span id='pid'>1737, <a href='https://arxiv.org/pdf/2505.07668.pdf' target='_blank'>https://arxiv.org/pdf/2505.07668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Torielli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07668">Intuitive Human-Robot Interfaces Leveraging on Autonomy Features for the Control of Highly-redundant Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>[...] With the TelePhysicalOperation interface, the user can teleoperate the different capabilities of a robot (e.g., single/double arm manipulation, wheel/leg locomotion) by applying virtual forces on selected robot body parts. This approach emulates the intuitiveness of physical human-robot interaction, but at the same time it permits to teleoperate the robot from a safe distance, in a way that resembles a "Marionette" interface. The system is further enhanced with wearable haptic feedback functions to align better with the "Marionette" metaphor, and a user study has been conducted to validate its efficacy with and without the haptic channel enabled. Considering the importance of robot independence, the TelePhysicalOperation interface incorporates autonomy modules to face, for example, the teleoperation of dual-arm mobile base robots for bimanual object grasping and transportation tasks.
  With the laser-guided interface, the user can indicate points of interest to the robot through the utilization of a simple but effective laser emitter device. With a neural network-based vision system, the robot tracks the laser projection in real time, allowing the user to indicate not only fixed goals, like objects, but also paths to follow. With the implemented autonomous behavior, a mobile manipulator employs its locomanipulation abilities to follow the indicated goals. The behavior is modeled using Behavior Trees, exploiting their reactivity to promptly respond to changes in goal positions, and their modularity to adapt the motion planning to the task needs. The proposed laser interface has also been employed in an assistive scenario. In this case, users with upper limbs impairments can control an assistive manipulator by directing a head-worn laser emitter to the point of interests, to collaboratively address activities of everyday life. [...]
<div id='section'>Paperid: <span id='pid'>1738, <a href='https://arxiv.org/pdf/2505.04897.pdf' target='_blank'>https://arxiv.org/pdf/2505.04897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taisuke Kobayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04897">CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive imitation learning makes an agent's control policy robust by stepwise supervisions from an expert. The recent algorithms mostly employ expert-agent switching systems to reduce the expert's burden by limitedly selecting the supervision timing. However, the precise selection is difficult and such a switching causes abrupt changes in actions, damaging the dynamic stability. This paper therefore proposes a novel method, so-called CubeDAgger, which improves robustness while reducing dynamic stability violations by making three improvements to a baseline method, EnsembleDAgger. The first improvement adds a regularization to explicitly activate the threshold for deciding the supervision timing. The second transforms the expert-agent switching system to an optimal consensus system of multiple action candidates. Third, autoregressive colored noise to the actions is introduced to make the stochastic exploration consistent over time. These improvements are verified by simulations, showing that the learned policies are sufficiently robust while maintaining dynamic stability during interaction.
<div id='section'>Paperid: <span id='pid'>1739, <a href='https://arxiv.org/pdf/2505.03500.pdf' target='_blank'>https://arxiv.org/pdf/2505.03500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03500">Task Reconstruction and Extrapolation for $Ï_0$ using Text Latent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) often achieve high performance on demonstrated tasks but struggle significantly when required to extrapolate, combining skills learned from different tasks in novel ways. For instance, VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet. In this work, we demonstrate that behaviors from distinct tasks can be effectively recombined by manipulating the VLA's internal representations at inference time. Concretely, we identify the text latent by averaging the text tokens' hidden states across all demonstrated trajectories for a specific base task. For executing an extrapolated task, we can temporally interpolate the text latent of the two base tasks and add it back to the text hidden states, so sub-behaviors from the two tasks will be activated sequentially. We evaluate this approach using the newly created libero-ood benchmark, featuring 20 tasks extrapolated from standard LIBERO suites. The results on libero-ood show that all SOTA VLAs achieve < 15% success rate, while $\pi0$ with text latent interpolation reaches an 83% success rate. Further qualitative analysis reveals a tendency for VLAs to exhibit spatial overfitting, mapping object names to demonstrated locations rather than achieving genuine object and goal understanding. Additionally, we find that decoding the text latent yields human-unreadable prompts that can nevertheless instruct the VLA to achieve a 70% success rate on standard LIBERO suites, enabling private instruction or backdoor attacks.
<div id='section'>Paperid: <span id='pid'>1740, <a href='https://arxiv.org/pdf/2505.00935.pdf' target='_blank'>https://arxiv.org/pdf/2505.00935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Bigazzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00935">Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.
<div id='section'>Paperid: <span id='pid'>1741, <a href='https://arxiv.org/pdf/2503.16469.pdf' target='_blank'>https://arxiv.org/pdf/2503.16469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S M Taslim Uddin Raju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16469">Enhancing Human-Robot Interaction in Healthcare: A Study on Nonverbal Communication Cues and Trust Dynamics with NAO Robot Caregivers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the population of older adults increases, so will the need for both human and robot care providers. While traditional practices involve hiring human caregivers to serve meals and attend to basic needs, older adults often require continuous companionship and health monitoring. However, hiring human caregivers for this job costs a lot of money. However, using a robot like Nao could be cheaper and still helpful. This study explores the integration of humanoid robots, particularly Nao, in health monitoring and caregiving for older adults. Using a mixed-methods approach with a within-subject factorial design, we investigated the effectiveness of nonverbal communication modalities, including touch, gestures, and LED patterns, in enhancing human-robot interactions. Our results indicate that Nao's touch-based health monitoring was well-received by participants, with positive ratings across various dimensions. LED patterns were perceived as more effective and accurate compared to hand and head gestures. Moreover, longer interactions were associated with higher trust levels and perceived empathy, highlighting the importance of prolonged engagement in fostering trust in human-robot interactions. Despite limitations, our study contributes valuable insights into the potential of humanoid robots to improve health monitoring and caregiving for older adults.
<div id='section'>Paperid: <span id='pid'>1742, <a href='https://arxiv.org/pdf/2503.15693.pdf' target='_blank'>https://arxiv.org/pdf/2503.15693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15693">Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning (SL) and reinforcement learning (RL) are both widely used to train general-purpose agents for complex tasks, yet their generalization capabilities and underlying mechanisms are not yet fully understood. In this paper, we provide a direct comparison between SL and RL in terms of zero-shot generalization. Using the Habitat visual navigation task as a testbed, we evaluate Proximal Policy Optimization (PPO) and Behavior Cloning (BC) agents across two levels of generalization: state-goal pair generalization within seen environments and generalization to unseen environments. Our experiments show that PPO consistently outperforms BC across both zero-shot settings and performance metrics-success rate and SPL. Interestingly, even though additional optimal training data enables BC to match PPO's zero-shot performance in SPL, it still falls significantly behind in success rate. We attribute this to a fundamental difference in how models trained by these algorithms generalize: BC-trained models generalize by imitating successful trajectories, whereas TD-based RL-trained models generalize through combinatorial experience stitching-leveraging fragments of past trajectories (mostly failed ones) to construct solutions for new tasks. This allows RL to efficiently find solutions in vast state space and discover novel strategies beyond the scope of human knowledge. Besides providing empirical evidence and understanding, we also propose practical guidelines for improving the generalization capabilities of RL and SL through algorithm design.
<div id='section'>Paperid: <span id='pid'>1743, <a href='https://arxiv.org/pdf/2503.04879.pdf' target='_blank'>https://arxiv.org/pdf/2503.04879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04879">Modeling Dynamic Hand-Object Interactions with Applications to Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans frequently grasp, manipulate, and move objects. Interactive systems assist humans in these tasks, enabling applications in Embodied AI, human-robot interaction, and virtual reality. However, current methods in hand-object synthesis often neglect dynamics and focus on generating static grasps. The first part of this dissertation introduces dynamic grasp synthesis, where a hand grasps and moves an object to a target pose. We approach this task using physical simulation and reinforcement learning. We then extend this to bimanual manipulation and articulated objects, requiring fine-grained coordination between hands. In the second part of this dissertation, we study human-to-robot handovers. We integrate captured human motion into simulation and introduce a student-teacher framework that adapts to human behavior and transfers from sim to real. To overcome data scarcity, we generate synthetic interactions, increasing training diversity by 100x. Our user study finds no difference between policies trained on synthetic vs. real motions.
<div id='section'>Paperid: <span id='pid'>1744, <a href='https://arxiv.org/pdf/2501.18726.pdf' target='_blank'>https://arxiv.org/pdf/2501.18726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18726">Strong and Controllable 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a significant pursuit in generative computer vision with widespread applications in film-making, video games, AR/VR, and human-robot interaction. Current methods mainly utilize either diffusion-based generative models or autoregressive models for text-to-motion generation. However, they face two significant challenges: (1) The generation process is time-consuming, posing a major obstacle for real-time applications such as gaming, robot manipulation, and other online settings. (2) These methods typically learn a relative motion representation guided by text, making it difficult to generate motion sequences with precise joint-level control. These challenges significantly hinder progress and limit the real-world application of human motion generation techniques. To address this gap, we propose a simple yet effective architecture consisting of two key components. Firstly, we aim to improve hardware efficiency and computational complexity in transformer-based diffusion models for human motion generation. By customizing flash linear attention, we can optimize these models specifically for generating human motion efficiently. Furthermore, we will customize the consistency model in the motion latent space to further accelerate motion generation. Secondly, we introduce Motion ControlNet, which enables more precise joint-level control of human motion compared to previous text-to-motion generation methods. These contributions represent a significant advancement for text-to-motion generation, bringing it closer to real-world applications.
<div id='section'>Paperid: <span id='pid'>1745, <a href='https://arxiv.org/pdf/2501.08944.pdf' target='_blank'>https://arxiv.org/pdf/2501.08944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fouad Bousetouane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08944">Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vertical AI Agents are revolutionizing industries by delivering domain-specific intelligence and tailored solutions. However, many sectors, such as manufacturing, healthcare, and logistics, demand AI systems capable of extending their intelligence into the physical world, interacting directly with objects, environments, and dynamic conditions. This need has led to the emergence of Physical AI Agents--systems that integrate cognitive reasoning, powered by specialized LLMs, with precise physical actions to perform real-world tasks.
  This work introduces Physical AI Agents as an evolution of shared principles with Vertical AI Agents, tailored for physical interaction. We propose a modular architecture with three core blocks--perception, cognition, and actuation--offering a scalable framework for diverse industries. Additionally, we present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern, which connects physical intelligence to industry-specific LLMs for real-time decision-making and reporting informed by physical context.
  Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG framework are transforming industries like autonomous vehicles, warehouse robotics, healthcare, and manufacturing, offering businesses a pathway to integrate embodied AI for operational efficiency and innovation.
<div id='section'>Paperid: <span id='pid'>1746, <a href='https://arxiv.org/pdf/2412.12542.pdf' target='_blank'>https://arxiv.org/pdf/2412.12542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katie Seaborn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12542">Bots against Bias: Critical Next Steps for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We humans are biased - and our robotic creations are biased, too. Bias is a natural phenomenon that drives our perceptions and behavior, including when it comes to socially expressive robots that have humanlike features. Recognizing that we embed bias, knowingly or not, within the design of such robots is crucial to studying its implications for people in modern societies. In this chapter, I consider the multifaceted question of bias in the context of humanoid, AI-enabled, and expressive social robots: Where does bias arise, what does it look like, and what can (or should) we do about it. I offer observations on human-robot interaction (HRI) along two parallel tracks: (1) robots designed in bias-conscious ways and (2) robots that may help us tackle bias in the human world. I outline a curated selection of cases for each track drawn from the latest HRI research and positioned against social, legal, and ethical factors. I also propose a set of critical next steps to tackle the challenges and opportunities on bias within HRI research and practice.
<div id='section'>Paperid: <span id='pid'>1747, <a href='https://arxiv.org/pdf/2412.11632.pdf' target='_blank'>https://arxiv.org/pdf/2412.11632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11632">Multi-Scale Incremental Modeling for Enhanced Human Motion Prediction in Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction is crucial for safe human-robot collaboration but remains challenging due to the complexity of modeling intricate and variable human movements. This paper presents Parallel Multi-scale Incremental Prediction (PMS), a novel framework that explicitly models incremental motion across multiple spatio-temporal scales to capture subtle joint evolutions and global trajectory shifts. PMS encodes these multi-scale increments using parallel sequence branches, enabling iterative refinement of predictions. A multi-stage training procedure with a full-timeline loss integrates temporal context. Extensive experiments on four datasets demonstrate substantial improvements in continuity, biomechanical consistency, and long-term forecast stability by modeling inter-frame increments. PMS achieves state-of-the-art performance, increasing prediction accuracy by 16.3%-64.2% over previous methods. The proposed multi-scale incremental approach provides a powerful technique for advancing human motion prediction capabilities critical for seamless human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1748, <a href='https://arxiv.org/pdf/2412.03619.pdf' target='_blank'>https://arxiv.org/pdf/2412.03619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03619">A Teleoperation System with Impedance Control and Disturbance Observer for Robot-Assisted Rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical movement therapy is a crucial method of rehabilitation aimed at reinstating mobility among patients facing motor dysfunction due to neurological conditions or accidents. Such therapy is usually featured as patient-specific, repetitive, and labor-intensive. The conventional method, where therapists collaborate with patients to conduct repetitive physical training, proves strenuous due to these characteristics. The concept of robot-assisted rehabilitation, assisting therapists with robotic systems, has gained substantial popularity. However, building such systems presents challenges, such as diverse task demands, uncertainties in dynamic models, and safety issues. To address these concerns, in this paper, we proposed a bilateral teleoperation system for rehabilitation. The control scheme of the system is designed as an integrated framework of impedance control and disturbance observer where the former can ensure compliant human-robot interaction without the need for force sensors while the latter can compensate for dynamic uncertainties when only a roughly identified dynamic model is available. Furthermore, the scheme allows free switching between tracking tasks and physical human-robot interaction (pHRI). The presented system can execute a wide array of pre-defined trajectories with varying patterns, adaptable to diverse needs. Moreover, the system can capture therapists' demonstrations, replaying them as many times as necessary. The effectiveness of the teleoperation system is experimentally evaluated and demonstrated.
<div id='section'>Paperid: <span id='pid'>1749, <a href='https://arxiv.org/pdf/2409.18982.pdf' target='_blank'>https://arxiv.org/pdf/2409.18982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haresh Karnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18982">Aligning Robot Navigation Behaviors with Human Intentions and Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in the field of machine learning have led to new ways for mobile robots to acquire advanced navigational capabilities. However, these learning-based methods raise the possibility that learned navigation behaviors may not align with the intentions and preferences of people, a problem known as value misalignment. To mitigate this risk, this dissertation aims to answer the question: "How can we use machine learning methods to align the navigational behaviors of autonomous mobile robots with human intentions and preferences?" First, this dissertation addresses this question by introducing a new approach to learning navigation behaviors by imitating human-provided demonstrations of the intended navigation task. This contribution allows mobile robots to acquire autonomous visual navigation capabilities through imitation, using a novel objective function that encourages the agent to align with the human's navigation objectives and penalizes misalignment. Second, this dissertation introduces two algorithms to enhance terrain-aware off-road navigation for mobile robots by learning visual terrain awareness in a self-supervised manner. This contribution enables mobile robots to respect a human operator's preferences for navigating different terrains in urban outdoor environments, while extrapolating these preferences to visually novel terrains by leveraging multi-modal representations. Finally, in the context of robot navigation in human-occupied environments, this dissertation introduces a dataset and an algorithm for robot navigation in a socially compliant manner in both indoor and outdoor environments. In summary, the contributions in this dissertation take significant steps toward addressing the value alignment problem in autonomous navigation, enabling mobile robots to navigate autonomously with objectives that align with human intentions and preferences.
<div id='section'>Paperid: <span id='pid'>1750, <a href='https://arxiv.org/pdf/2409.14692.pdf' target='_blank'>https://arxiv.org/pdf/2409.14692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyang Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14692">Dynamic Realms: 4D Content Analysis, Recovery and Generation with Geometric, Topological and Physical Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>My research focuses on the analysis, recovery, and generation of 4D content, where 4D includes three spatial dimensions (x, y, z) and a temporal dimension t, such as shape and motion. This focus goes beyond static objects to include dynamic changes over time, providing a comprehensive understanding of both spatial and temporal variations. These techniques are critical in applications like AR/VR, embodied AI, and robotics. My research aims to make 4D content generation more efficient, accessible, and higher in quality by incorporating geometric, topological, and physical priors. I also aim to develop effective methods for 4D content recovery and analysis using these priors.
<div id='section'>Paperid: <span id='pid'>1751, <a href='https://arxiv.org/pdf/2406.11759.pdf' target='_blank'>https://arxiv.org/pdf/2406.11759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edoardo Datteri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11759">Folk-ontological stances toward robots and psychological human likeness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has often been argued that people can attribute mental states to robots without making any ontological commitments to the reality of those states. But what does it mean to 'attribute' a mental state to a robot, and what is an 'ontological commitment'? It will be argued that, on a plausible interpretation of these two notions, it is not clear how mental state attribution can occur without any ontological commitment. Taking inspiration from the philosophical debate on scientific realism, a provisional taxonomy of folk-ontological stances towards robots will also be identified, corresponding to different ways of understanding robotic minds. They include realism, non-realism, eliminativism, reductionism, fictionalism and agnosticism. Instrumentalism will also be discussed and presented as a folk-epistemological stance. In the last part of the article it will be argued that people's folk-ontological stances towards robots and humans can influence their perception of the human-likeness of robots. The analysis carried out here can be seen as encouraging a 'folk-ontological turn' in human-robot interaction research, aimed at explicitly determining what beliefs people have about the reality of robot minds.
<div id='section'>Paperid: <span id='pid'>1752, <a href='https://arxiv.org/pdf/2405.03164.pdf' target='_blank'>https://arxiv.org/pdf/2405.03164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ransalu Senanayake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03164">The Role of Predictive Uncertainty and Diversity in Embodied AI and Robot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainty has long been a critical area of study in robotics, particularly when robots are equipped with analytical models. As we move towards the widespread use of deep neural networks in robots, which have demonstrated remarkable performance in research settings, understanding the nuances of uncertainty becomes crucial for their real-world deployment. This guide offers an overview of the importance of uncertainty and provides methods to quantify and evaluate it from an applications perspective.
<div id='section'>Paperid: <span id='pid'>1753, <a href='https://arxiv.org/pdf/2405.00693.pdf' target='_blank'>https://arxiv.org/pdf/2405.00693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesse Atuhurra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00693">Leveraging Large Language Models in Human-Robot Interaction: A Critical Analysis of Potential and Pitfalls</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of large language models (LLM) and, consequently, vision language models (VLM) has ignited new imaginations among robotics researchers. At this point, the range of applications to which LLM and VLM can be applied in human-robot interaction (HRI), particularly socially assistive robots (SARs), is unchartered territory. However, LLM and VLM present unprecedented opportunities and challenges for SAR integration. We aim to illuminate the opportunities and challenges when roboticists deploy LLM and VLM in SARs. First, we conducted a meta-study of more than 250 papers exploring 1) major robots in HRI research and 2) significant applications of SARs, emphasizing education, healthcare, and entertainment while addressing 3) societal norms and issues like trust, bias, and ethics that the robot developers must address. Then, we identified 4) critical components of a robot that LLM or VLM can replace while addressing the 5) benefits of integrating LLM into robot designs and the 6) risks involved. Finally, we outline a pathway for the responsible and effective adoption of LLM or VLM into SARs, and we close our discussion by offering caution regarding this deployment.
<div id='section'>Paperid: <span id='pid'>1754, <a href='https://arxiv.org/pdf/2404.07649.pdf' target='_blank'>https://arxiv.org/pdf/2404.07649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tashmoy Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07649">Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we have present an improved Cycle GAN based model for under water image enhancement. We have utilized the cycle consistent learning technique of the state-of-the-art Cycle GAN model with modification in the loss function in terms of depth-oriented attention which enhance the contrast of the overall image, keeping global content, color, local texture, and style information intact. We trained the Cycle GAN model with the modified loss functions on the benchmarked Enhancing Underwater Visual Perception (EUPV) dataset a large dataset including paired and unpaired sets of underwater images (poor and good quality) taken with seven distinct cameras in a range of visibility situation during research on ocean exploration and human-robot cooperation. In addition, we perform qualitative and quantitative evaluation which supports the given technique applied and provided a better contrast enhancement model of underwater imagery. More significantly, the upgraded images provide better results from conventional models and further for under water navigation, pose estimation, saliency prediction, object detection and tracking. The results validate the appropriateness of the model for autonomous underwater vehicles (AUV) in visual navigation.
<div id='section'>Paperid: <span id='pid'>1755, <a href='https://arxiv.org/pdf/2404.00024.pdf' target='_blank'>https://arxiv.org/pdf/2404.00024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis E. Block
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00024">Hey, Teacher, (Don't) Leave Those Kids Alone: Standardizing HRI Education</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a standardized introduction course becomes more critical as the field of human-robot interaction (HRI) becomes more established. This paper outlines the key components necessary to provide an undergraduate with a sufficient foundational understanding of the interdisciplinary nature of this field and provides proposed course content. It emphasizes the importance of creating a course with theoretical and experimental components to accommodate all different learning preferences. This manuscript also advocates creating or adopting a universal platform to standardize the hands-on component of introductory HRI courses, regardless of university funding or size. Next, it recommends formal training in how to read scientific articles and staying up-to-date with the latest relevant papers. Finally, it provides detailed lecture content and project milestones for a 15-week semester. By creating a standardized course, researchers can ensure consistency and quality are maintained across institutions, which will help students as well as industrial and academic employers understand what foundational knowledge is expected.
<div id='section'>Paperid: <span id='pid'>1756, <a href='https://arxiv.org/pdf/2403.18692.pdf' target='_blank'>https://arxiv.org/pdf/2403.18692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarah Sebo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18692">Teaching Introductory HRI: UChicago Course "Human-Robot Interaction: Research and Practice"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago. Since 2020, I have taught and refined this course each academic year. Human-Robot Interaction: Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI. Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications. Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot. In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course.
<div id='section'>Paperid: <span id='pid'>1757, <a href='https://arxiv.org/pdf/2403.18096.pdf' target='_blank'>https://arxiv.org/pdf/2403.18096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lawrence O'Gorman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18096">Efficient Multi-Band Temporal Video Filter for Reducing Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although mobile robots have on-board sensors to perform navigation, their efficiency in completing paths can be enhanced by planning to avoid human interaction. Infrastructure cameras can capture human activity continuously for the purpose of compiling activity analytics to choose efficient times and routes. We describe a cascade temporal filtering method to efficiently extract short- and long-term activity in two time dimensions, isochronal and chronological, for use in global path planning and local navigation respectively. The temporal filter has application either independently, or, if object recognition is also required, it can be used as a pre-filter to perform activity-gating of the more computationally expensive neural network processing. For a testbed 32-camera network, we show how this hybrid approach can achieve over 8 times improvement in frames per second throughput and 6.5 times reduction of system power use. We also show how the cost map of static objects in the ROS robot software development framework is augmented with dynamic regions determined from the temporal filter.
<div id='section'>Paperid: <span id='pid'>1758, <a href='https://arxiv.org/pdf/2403.15323.pdf' target='_blank'>https://arxiv.org/pdf/2403.15323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15323">Introduction to Human-Robot Interaction: A Multi-Perspective Introductory Course</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper I describe the design of an introductory course in Human-Robot Interaction. This project-driven course is designed to introduce undergraduate and graduate engineering students, especially those enrolled in Computer Science, Mechanical Engineering, and Robotics degree programs, to key theories and methods used in the field of Human-Robot Interaction that they would otherwise be unlikely to see in those degree programs. To achieve this aim, the course takes students all the way from stakeholder analysis to empirical evaluation, covering and integrating key Qualitative, Design, Computational, and Quantitative methods along the way. I detail the goals, audience, and format of the course, and provide a detailed walkthrough of the course syllabus.
<div id='section'>Paperid: <span id='pid'>1759, <a href='https://arxiv.org/pdf/2403.12607.pdf' target='_blank'>https://arxiv.org/pdf/2403.12607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Doernbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12607">Looking for the Human in HRI Teaching: User-Centered Course Design for Tech-Savvy Students</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Top-down, user-centered thinking is not typically a strength of all students, especially tech-savvy computer science-related ones. We propose Human-Robot Interaction (HRI) introductory courses as a highly suitable opportunity to foster these important skills since the HRI discipline includes a focus on humans as users. Our HRI course therefore contains elements like scenario-based design of laboratory projects, discussing and merging ideas and other self-empowerment techniques. Participants describe, implement and present everyday scenarios using Pepper robots and our customized open-source visual programming tool. We observe that students obtain a good grasp of the taught topics and improve their user-centered thinking skills.
<div id='section'>Paperid: <span id='pid'>1760, <a href='https://arxiv.org/pdf/2401.05802.pdf' target='_blank'>https://arxiv.org/pdf/2401.05802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wafa Johal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05802">Transferability of HRI Research: Potential and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With advancement of robotics and artificial intelligence, applications for robotics are flourishing. Human-robot interaction (HRI) is an important area of robotics as it allows robots to work closer to humans (with them or for them). One crucial factor for the success of HRI research is transferability, which refers to the ability of research outputs to be adopted by industry and provide benefits to society. In this paper, we explore the potentials and challenges of transferability in HRI research. Firstly, we examine the current state of HRI research and identify various types of contributions that could lead to successful outcomes. Secondly, we discuss the potential benefits for each type of contribution and identify factors that could facilitate industry adoption of HRI research. However, we also recognize that there are several challenges associated with transferability, such as the diversity of well-defined job/skill-sets required from HRI practitioners, the lack of industry-led research, and the lack of standardization in HRI research methods. We discuss these challenges and propose potential solutions to bridge the gap between industry expectations and academic research in HRI.
<div id='section'>Paperid: <span id='pid'>1761, <a href='https://arxiv.org/pdf/2401.01644.pdf' target='_blank'>https://arxiv.org/pdf/2401.01644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxiao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01644">Motion Control of Interactive Robotic Arms Based on Mixed Reality Development</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality (MR) is constantly evolving to inspire new patterns of robot manipulation for more advanced Human- Robot Interaction under the 4th Industrial Revolution Paradigm. Consider that Mixed Reality aims to connect physical and digital worlds to provide special immersive experiences, it is necessary to establish the information exchange platform and robot control systems within the developed MR scenarios. In this work, we mainly present multiple effective motion control methods applied on different interactive robotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR applications, including GUI control panel, text input control panel, end-effector object dynamic tracking and ROS-Unity digital-twin connection.
<div id='section'>Paperid: <span id='pid'>1762, <a href='https://arxiv.org/pdf/2312.07638.pdf' target='_blank'>https://arxiv.org/pdf/2312.07638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Weber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07638">Teaching Unknown Objects by Leveraging Human Gaze and Augmented Reality in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are becoming increasingly popular in a wide range of environments due to their exceptional work capacity, precision, efficiency, and scalability. This development has been further encouraged by advances in Artificial Intelligence, particularly Machine Learning. By employing sophisticated neural networks, robots are given the ability to detect and interact with objects in their vicinity. However, a significant drawback arises from the underlying dependency on extensive datasets and the availability of substantial amounts of training data for these object detection models. This issue becomes particularly problematic when the specific deployment location of the robot and the surroundings, are not known in advance. The vast and ever-expanding array of objects makes it virtually impossible to comprehensively cover the entire spectrum of existing objects using preexisting datasets alone. The goal of this dissertation was to teach a robot unknown objects in the context of Human-Robot Interaction (HRI) in order to liberate it from its data dependency, unleashing it from predefined scenarios. In this context, the combination of eye tracking and Augmented Reality created a powerful synergy that empowered the human teacher to communicate with the robot and effortlessly point out objects by means of human gaze. This holistic approach led to the development of a multimodal HRI system that enabled the robot to identify and visually segment the Objects of Interest in 3D space. Through the class information provided by the human, the robot was able to learn the objects and redetect them at a later stage. Due to the knowledge gained from this HRI based teaching, the robot's object detection capabilities exhibited comparable performance to state-of-the-art object detectors trained on extensive datasets, without being restricted to predefined classes, showcasing its versatility and adaptability.
<div id='section'>Paperid: <span id='pid'>1763, <a href='https://arxiv.org/pdf/2311.07458.pdf' target='_blank'>https://arxiv.org/pdf/2311.07458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raj Korpan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07458">Trust in Queer Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot interaction (HRI) systems need to build trust with people of diverse identities. This position paper argues that queer (LGBTQIA+) people must be included in the design and evaluation of HRI systems to ensure their trust in and acceptance of robots. Queer people have faced discrimination and harm from artificial intelligence and robotic systems. Despite calls for increased diversity and inclusion, HRI has not systemically addressed queer issues. This paper suggests three approaches to address trust in queer HRI: diversifying human-subject pools, centering queer people in HRI studies, and contextualizing measures of trust.
<div id='section'>Paperid: <span id='pid'>1764, <a href='https://arxiv.org/pdf/2310.19495.pdf' target='_blank'>https://arxiv.org/pdf/2310.19495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. Sunbeam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19495">Deep Learning for Visual Navigation of Underwater Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to briefly survey deep learning methods for visual navigation of underwater robotics. The scope of this paper includes the visual perception of underwater robotics with deep learning methods, the available visual underwater datasets, imitation learning, and reinforcement learning methods for navigation. Additionally, relevant works will be categorized under the imitation learning or deep learning paradigm for underwater robots for clarity of the training methodologies in the current landscape. Literature that uses deep learning algorithms to process non-visual data for underwater navigation will not be considered, except as contrasting examples.
<div id='section'>Paperid: <span id='pid'>1765, <a href='https://arxiv.org/pdf/2310.11386.pdf' target='_blank'>https://arxiv.org/pdf/2310.11386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Imran Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11386">Towards Operationalizing Social Bonding in Human-Robot Dyads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With momentum increasing in the use of social robots as long-term assistive and collaborative partners, humans developing social bonds with these artificial agents appears to be inevitable. In human-human dyads, social bonding plays a powerful role in regulating behaviours, emotions, and even health. If this is to extend to human-robot dyads, the phenomenology of such relationships (including their emergence and stability) must be better understood. In this paper, we discuss potential approaches towards operationalizing the phenomenon of social bonding between human-robot dyads. We will discuss a number of biobehavioural proxies of social bonding, moving away from existing approaches that use subjective, psychological measures, and instead grounding our approach in some of the evolutionary, neurobiological and physiological correlates of social bond formation in natural systems: (a) reductions in physiological stress (the ''social buffering'' phenomenon), (b) narrowing of spatial proximity between dyads, and (c) inter-dyad behavioural synchrony. We provide relevant evolutionary support for each proposed component, with suggestions and considerations for how they can be recorded in (real-time) human-robot interaction scenarios. With this, we aim to inspire more robust operationalisation of ''social bonding'' between human and artificial (robotic) agents.
<div id='section'>Paperid: <span id='pid'>1766, <a href='https://arxiv.org/pdf/2310.02506.pdf' target='_blank'>https://arxiv.org/pdf/2310.02506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranay Mathur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02506">Proactive Human-Robot Interaction using Visuo-Lingual Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures inter and intra-modal dependencies to provide accurate scene descriptions and proactively suggest tasks where applicable. We evaluate our proposed model in simulation and real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1767, <a href='https://arxiv.org/pdf/2308.14232.pdf' target='_blank'>https://arxiv.org/pdf/2308.14232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Reza Ahmadzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14232">Research Report -- Persistent Autonomy and Robot Learning Lab</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots capable of performing manipulation tasks in a broad range of missions in unstructured environments can develop numerous applications to impact and enhance human life. Existing work in robot learning has shown success in applying conventional machine learning algorithms to enable robots for replicating rather simple manipulation tasks in manufacturing, service and healthcare applications, among others. However, learning robust and versatile models for complex manipulation tasks that are inherently multi-faceted and naturally intricate demands algorithmic advancements in robot learning. Our research supports the long-term goal of making robots more accessible and serviceable to the general public by expanding robot applications to real-world scenarios that require systems capable of performing complex tasks. To achieve this goal, we focus on identifying and investigating knowledge gaps in robot learning of complex manipulation tasks by leveraging upon human-robot interaction and robot learning from human instructions. This document presents an overview of the recent research developments in the Persistent Autonomy and Robot Learning (PeARL) lab at the University of Massachusetts Lowell. Here, I briefly discuss different research directions, and present a few proposed approaches in our most recent publications. For each proposed approach, I then mention potential future directions that can advance the field.
<div id='section'>Paperid: <span id='pid'>1768, <a href='https://arxiv.org/pdf/2308.10409.pdf' target='_blank'>https://arxiv.org/pdf/2308.10409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jay Best
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10409">Development of a Novel Impedance-Controlled Quasi-Direct-Drive Robot Hand</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most robotic hands and grippers rely on actuators with large gearboxes and force sensors for controlling gripping force. However, this might not be ideal for tasks which require the robot to interact with an unstructured and/or unknown environment. We propose a novel quasi-direct-drive two-fingered robotic hand with variable impedance control in the joint space and Cartesian space. The hand has a total of four degrees of freedom, a backdrivable gear train, and four brushless direct current (BLDC) motors. Field-Oriented Control (FOC) with current sensing is used to control motor torques. Variable impedance control allows the hand to perform dexterous manipulation tasks while being safe during human-robot interaction. The quasi-direct-drive actuators enable the fingers to handle contact with the environment without the need for complicated tactile or force sensors. A majority 3D printed assembly makes this a low-cost research platform built with affordable off-the-shelf components. The hand demonstrates grasping with force-closure and form-closure, stable grasps in response to disturbances, tasks exploiting contact with the environment, simple in-hand manipulation, and a light touch for handling fragile objects.
<div id='section'>Paperid: <span id='pid'>1769, <a href='https://arxiv.org/pdf/2307.11763.pdf' target='_blank'>https://arxiv.org/pdf/2307.11763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Connor Esterwood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11763">Rethinking Trust Repair in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots become increasingly prevalent in work-oriented collaborations, trust has emerged as a critical factor in their acceptance and effectiveness. However, trust is dynamic and can erode when mistakes are made. Despite emerging research on trust repair in human-robot interaction, significant questions remain about identifying reliable approaches to restoring trust in robots after trust violations occur. To address this problem, my research aims to identify effective strategies for designing robots capable of trust repair in human-robot interaction (HRI) and to explore the underlying mechanisms that make these strategies successful. This paper provides an overview of the fundamental concepts and key components of the trust repair process in HRI, as well as a summary of my current published work in this area. Additionally, I discuss the research questions that will guide my future work and the potential contributions that this research could make to the field.
<div id='section'>Paperid: <span id='pid'>1770, <a href='https://arxiv.org/pdf/2307.09364.pdf' target='_blank'>https://arxiv.org/pdf/2307.09364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roger K. Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09364">Local Minima Drive Communications in Cooperative Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An important open question in human-robot interaction (HRI) is precisely when an agent should decide to communicate, particularly in a cooperative task. Perceptual Control Theory (PCT) tells us that agents are able to cooperate on a joint task simply by sharing the same 'intention', thereby distributing the effort required to complete the task among the agents. This is even true for agents that do not possess the same abilities, so long as the goal is observable, the combined actions are sufficient to complete the task, and there is no local minimum in the search space. If these conditions hold, then a cooperative task can be accomplished without any communication between the contributing agents. However, for tasks that do contain local minima, the global solution can only be reached if at least one of the agents adapts its intention at the appropriate moments, and this can only be achieved by appropriately timed communication. In other words, it is hypothesised that in cooperative tasks, the function of communication is to coordinate actions in a complex search space that contains local minima. These principles have been verified in a computer-based simulation environment in which two independent one-dimensional agents are obliged to cooperate in order to solve a two-dimensional path-finding task.
<div id='section'>Paperid: <span id='pid'>1771, <a href='https://arxiv.org/pdf/2303.09304.pdf' target='_blank'>https://arxiv.org/pdf/2303.09304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Svenja Yvonne SchÃ¶tt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09304">Thermal Feedback for Transparency in Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots can support humans in tedious tasks, as well as provide social support. However, the decision-making and behavior of robots is not always clear to the human interaction partner. In this work, we discuss the opportunity of using thermal feedback as an additional modality to create transparent interactions. We then present scenarios where thermal feedback is incorporated into the interaction e.g. to unobtrusively communicate the behavior of the robot. We highlight the limitations and challenges of temperature-based feedback, which can be explored in future research.
<div id='section'>Paperid: <span id='pid'>1772, <a href='https://arxiv.org/pdf/2303.04356.pdf' target='_blank'>https://arxiv.org/pdf/2303.04356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taisuke Kobayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04356">Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satisfying the equality constraint and checking the lower bound. In Mujoco and Pybullet simulators, the modified SAC statistically achieved the higher robustness for adversarial attacks than before while regularizing the norm of action. A real-robot variable impedance task was demonstrated for showing the applicability of the modified SAC to real-world robot control. In particular, the modified SAC maintained adaptive behaviors for physical human-robot interaction, which had no experience at all during training. https://youtu.be/EH3xVtlVaJw
<div id='section'>Paperid: <span id='pid'>1773, <a href='https://arxiv.org/pdf/2302.04530.pdf' target='_blank'>https://arxiv.org/pdf/2302.04530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Belardinelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04530">Gaze-based intention estimation: principles, methodologies, and applications in HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intention prediction has become a relevant field of research in Human-Machine and Human-Robot Interaction. Indeed, any artificial system (co)-operating with and along humans, designed to assist and coordinate its actions with a human partner, would benefit from first inferring the human's current intention. To spare the user the cognitive burden of explicitly uttering their goals, this inference relies mostly on behavioral cues deemed indicative of the current action. It has been long known that eye movements are highly anticipatory of the single steps unfolding during a task, hence they can serve as a very early and reliable behavioural cue for intention recognition. This review aims to draw a line between insights in the psychological literature on visuomotor control and relevant applications of gaze-based intention recognition in technical domains, with a focus on teleoperated and assistive robotic systems. Starting from the cognitive principles underlying the relationship between intentions, eye movements, and action, the use of eye tracking and gaze-based models for intent recognition in Human-Robot Interaction is considered, with prevalent methodologies and their diverse applications. Finally, special consideration is given to relevant human factors issues and current limitations to be factored in when designing such systems.
<div id='section'>Paperid: <span id='pid'>1774, <a href='https://arxiv.org/pdf/2301.10121.pdf' target='_blank'>https://arxiv.org/pdf/2301.10121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10121">Generalized Object Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Future collaborative robots must be capable of finding objects. As such a fundamental skill, we expect object search to eventually become an off-the-shelf capability for any robot, similar to e.g., object detection, SLAM, and motion planning. However, existing approaches either make unrealistic compromises (e.g., reduce the problem from 3D to 2D), resort to ad-hoc, greedy search strategies, or attempt to learn end-to-end policies in simulation that are yet to generalize across real robots and environments. This thesis argues that through using Partially Observable Markov Decision Processes (POMDPs) to model object search while exploiting structures in the human world (e.g., octrees, correlations) and in human-robot interaction (e.g., spatial language), a practical and effective system for generalized object search can be achieved. In support of this argument, I develop methods and systems for (multi-)object search in 3D environments under uncertainty due to limited field of view, occlusion, noisy, unreliable detectors, spatial correlations between objects, and possibly ambiguous spatial language (e.g., "The red car is behind Chase Bank"). Besides evaluation in simulators such as PyGame, AirSim, and AI2-THOR, I design and implement a robot-independent, environment-agnostic system for generalized object search in 3D and deploy it on the Boston Dynamics Spot robot, the Kinova MOVO robot, and the Universal Robots UR5e robotic arm, to perform object search in different environments. The system enables, for example, a Spot robot to find a toy cat hidden underneath a couch in a kitchen area in under one minute. This thesis also broadly surveys the object search literature, proposing taxonomies in object search problem settings, methods and systems.
<div id='section'>Paperid: <span id='pid'>1775, <a href='https://arxiv.org/pdf/1812.04406.pdf' target='_blank'>https://arxiv.org/pdf/1812.04406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johan F. Hoorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1812.04406">Theory of Robot Communication: II. Befriending a Robot over Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In building on theories of Computer-Mediated Communication (CMC), Human-Robot Interaction, and Media Psychology (i.e. Theory of Affective Bonding), the current paper proposes an explanation of how over time, people experience the mediated or simulated aspects of the interaction with a social robot. In two simultaneously running loops, a more reflective process is balanced with a more affective process. If human interference is detected behind the machine, Robot-Mediated Communication commences, which basically follows CMC assumptions; if human interference remains undetected, Human-Robot Communication comes into play, holding the robot for an autonomous social actor. The more emotionally aroused a robot user is, the more likely they develop an affective relationship with what actually is a machine. The main contribution of this paper is an integration of Computer-Mediated Communication, Human-Robot Communication, and Media Psychology, outlining a full-blown theory of robot communication connected to friendship formation, accounting for communicative features, modes of processing, as well as psychophysiology.
